ğŸŸ¦ ãƒ­ã‚°ã‚’åˆæœŸåŒ–
ğŸŸ© ã‚°ãƒ­ãƒ¼ãƒãƒ«ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆåˆæœŸåŒ– _CONTEXT=Context(is_prefill=False, cu_seqlens_q=None, cu_seqlens_k=None, max_seqlen_q=0, max_seqlen_k=0, slot_mapping=None, context_lens=None, block_tables=None)
ğŸŸ© SamplingParamsåˆæœŸåŒ–ã®å¾Œå‡¦ç† self=SamplingParams(temperature=1.0, max_tokens=64, ignore_eos=False)
ğŸŸ© SamplingParamsåˆæœŸåŒ–ã®å¾Œå‡¦ç† self=SamplingParams(temperature=0.6, max_tokens=1, ignore_eos=False)
ğŸŸ© ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆã‚’é–‹å§‹ len(prompts)=1 sampling_params=SamplingParams(temperature=0.6, max_tokens=1, ignore_eos=False)
ğŸŸ© ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ã«ç™»éŒ²é–‹å§‹ prompt='Hello' sampling_params=SamplingParams(temperature=0.6, max_tokens=1, ignore_eos=False)
ğŸŸ© ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã‚’åˆæœŸåŒ– len(token_ids)=1, sampling_params=SamplingParams(temperature=0.6, max_tokens=1, ignore_eos=False)
ğŸŸ¦ ã‚·ãƒ¼ã‚±ãƒ³ã‚¹IDã‚’å‰²ã‚Šå½“ã¦ self.seq_id=0
ğŸŸ© ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ã«æ–°ã—ã„ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’è¿½åŠ  seq.seq_id=0
ğŸŸ© ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ã«ç™»éŒ²å®Œäº† seq.seq_id=0
ğŸŸ© ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ã®å®Œäº†çŠ¶æ…‹ã‚’ç¢ºèª result=False
ğŸŸ© ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ã«å‡¦ç†ã™ã¹ãã‚¿ã‚¹ã‚¯ãŒæ®‹ã£ã¦ã„ã‚‹ã‹ç¢ºèª res=False
ğŸŸ© æ¨è«–ã‚¨ãƒ³ã‚¸ãƒ³ã®1ã‚µã‚¤ã‚¯ãƒ«ã‚’å®Ÿè¡Œé–‹å§‹
ğŸŸ© ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒªãƒ³ã‚°ã‚’é–‹å§‹
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’å‰²ã‚Šå½“ã¦å¯èƒ½ã‹ç¢ºèª 0 seq.num_blocks=1 result=True
ğŸŸ© ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã«ãƒ–ãƒ­ãƒƒã‚¯ã‚’å‰²ã‚Šå½“ã¦é–‹å§‹ 0
ğŸŸ© è«–ç†ãƒ–ãƒ­ãƒƒã‚¯ã‚’å–å¾— i=0
ğŸŸ¦ è«–ç†ãƒ–ãƒ­ãƒƒã‚¯ã®ãƒˆãƒ¼ã‚¯ãƒ³IDã‚’å–å¾— result=[9707]
ğŸŸ© ç©ºããƒ–ãƒ­ãƒƒã‚¯ã‚’å‰²ã‚Šå½“ã¦é–‹å§‹ block_id=1
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’ãƒªã‚»ãƒƒãƒˆ 1
ğŸŸ© ç©ºããƒ–ãƒ­ãƒƒã‚¯ã‚’å‰²ã‚Šå½“ã¦å®Œäº† block_id=1
ğŸŸ© ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã«ãƒ–ãƒ­ãƒƒã‚¯ã‚’å‰²ã‚Šå½“ã¦å®Œäº† 0 seq.block_table=[1]
ğŸŸ© ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒªãƒ³ã‚°å®Œäº†ï¼ˆPrefillï¼‰ len(scheduled_seqs)=1
ğŸŸ© æ¨è«–ã®1ã‚µã‚¤ã‚¯ãƒ«ã‚’å®Ÿè¡Œé–‹å§‹ len(seqs)=1 is_prefill=True
ğŸŸ© ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’è¨­å®š is_prefill=True cu_seqlens_q=tensor([0, 1], device='cuda:0', dtype=torch.int32) cu_seqlens_k=tensor([0, 1], device='cuda:0', dtype=torch.int32) max_seqlen_q=1 max_seqlen_k=1 slot_mapping=tensor([256], device='cuda:0', dtype=torch.int32) context_lens=None block_tables=None
ğŸŸ© ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æº–å‚™é–‹å§‹ len(seqs)=1
ğŸŸ© ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æº–å‚™å®Œäº† temperatures=tensor([0.6000], device='cuda:0')
ğŸŸ© ãƒ¢ãƒ‡ãƒ«ã®é †ä¼æ¬ã‚’å®Ÿè¡Œé–‹å§‹ input_ids.shape=torch.Size([1]) positions.shape=torch.Size([1]) is_prefill=True
ğŸŸ© Qwen3ForCausalLMã®é †ä¼æ’­é–‹å§‹ input_ids.shape=torch.Size([1]) positions.shape=torch.Size([1])
ğŸŸ© Qwen3Modelã®é †ä¼æ’­é–‹å§‹ input_ids.shape=torch.Size([1]) positions.shape=torch.Size([1])
ğŸŸ© åŸ‹ã‚è¾¼ã¿å±¤ã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1])
ğŸŸ© åŸ‹ã‚è¾¼ã¿å±¤ã®é †ä¼æ’­å®Œäº† y.shape=torch.Size([1, 1024])
ğŸŸ© Qwen3DecoderLayerã®é †ä¼æ’­ positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 1024]) residual is None=True
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 1024]), residual.shape if residual is not None else None=None
ğŸŸ© æ­£è¦åŒ–é–‹å§‹ x.shape=torch.Size([1, 1024])
ğŸŸ© æ­£è¦åŒ–å®Œäº† x.shape=torch.Size([1, 1024])
ğŸŸ© Qwen3Attentionã®é †ä¼æ’­ positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­å®Œäº† result.shape=torch.Size([1, 4096])
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 16, 128]), residual.shape if residual is not None else None=None
ğŸŸ© æ­£è¦åŒ–é–‹å§‹ x.shape=torch.Size([1, 16, 128])
ğŸŸ© æ­£è¦åŒ–å®Œäº† x.shape=torch.Size([1, 16, 128])
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 8, 128]), residual.shape if residual is not None else None=None
ğŸŸ© æ­£è¦åŒ–é–‹å§‹ x.shape=torch.Size([1, 8, 128])
ğŸŸ© æ­£è¦åŒ–å®Œäº† x.shape=torch.Size([1, 8, 128])
ğŸŸ© RoPEã®é †ä¼æ’­é–‹å§‹ positions.shape=torch.Size([1]), query.shape=torch.Size([1, 16, 128]), key.shape=torch.Size([1, 8, 128])
ğŸŸ© RoPEã‚’é©ç”¨ x.shape=torch.Size([1, 16, 128]), cos.shape=torch.Size([1, 1, 64]), sin.shape=torch.Size([1, 1, 64])
ğŸŸ¦ ãƒãƒ£ãƒ³ã‚¯åŒ– x1.shape=torch.Size([1, 16, 64]), x2.shape=torch.Size([1, 16, 64])
ğŸŸ¦ å›è»¢è¡Œåˆ—é©ç”¨ y1.shape=torch.Size([1, 16, 64]), y2.shape=torch.Size([1, 16, 64])
ğŸŸ¦ result.shape=torch.Size([1, 16, 128])
ğŸŸ© RoPEã‚’é©ç”¨ x.shape=torch.Size([1, 8, 128]), cos.shape=torch.Size([1, 1, 64]), sin.shape=torch.Size([1, 1, 64])
ğŸŸ¦ ãƒãƒ£ãƒ³ã‚¯åŒ– x1.shape=torch.Size([1, 8, 64]), x2.shape=torch.Size([1, 8, 64])
ğŸŸ¦ å›è»¢è¡Œåˆ—é©ç”¨ y1.shape=torch.Size([1, 8, 64]), y2.shape=torch.Size([1, 8, 64])
ğŸŸ¦ result.shape=torch.Size([1, 8, 128])
ğŸŸ© RoPEã®é †ä¼æ’­å®Œäº† query.shape=torch.Size([1, 16, 128]), key.shape=torch.Size([1, 8, 128])
ğŸŸ© Attentionå±¤ã®é †ä¼æ’­é–‹å§‹ q.shape=torch.Size([1, 16, 128]), k.shape=torch.Size([1, 8, 128]), v.shape=torch.Size([1, 8, 128])
ğŸŸ© ç¾åœ¨ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆå–å¾— _CONTEXT=Context(is_prefill=True, cu_seqlens_q=tensor([0, 1], device='cuda:0', dtype=torch.int32), cu_seqlens_k=tensor([0, 1], device='cuda:0', dtype=torch.int32), max_seqlen_q=1, max_seqlen_k=1, slot_mapping=tensor([256], device='cuda:0', dtype=torch.int32), context_lens=None, block_tables=None)
ğŸŸ¦ ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæƒ…å ±å–å¾— context.is_prefill=True, context.slot_mapping=tensor([256], device='cuda:0', dtype=torch.int32), context.max_seqlen_q=1, context.cu_seqlens_q=tensor([0, 1], device='cuda:0', dtype=torch.int32), context.max_seqlen_k=1, context.cu_seqlens_k=tensor([0, 1], device='cuda:0', dtype=torch.int32), context.context_lens=None, context.block_tables=None
ğŸŸ© KVã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ã‚­ãƒ¼ã¨ãƒãƒªãƒ¥ãƒ¼ã‚’ä¿å­˜ key.shape=torch.Size([1, 8, 128]), value.shape=torch.Size([1, 8, 128]), k_cache.shape=torch.Size([884, 256, 8, 128]), v_cache.shape=torch.Size([884, 256, 8, 128]), slot_mapping.shape=torch.Size([1])
ğŸŸ¦ Prefillãƒ¢ãƒ¼ãƒ‰ã§FlashAttentionã‚’å®Ÿè¡Œ context.max_seqlen_q=1, context.cu_seqlens_q=tensor([0, 1], device='cuda:0', dtype=torch.int32), context.max_seqlen_k=1, context.cu_seqlens_k=tensor([0, 1], device='cuda:0', dtype=torch.int32)
ğŸŸ© Attentionå±¤ã®é †ä¼æ’­å®Œäº† o.shape=torch.Size([1, 16, 128])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­é–‹å§‹ å…¥åŠ›å½¢çŠ¶: torch.Size([1, 2048])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­å®Œäº† å‡ºåŠ›å½¢çŠ¶: torch.Size([1, 1024])
ğŸŸ© Qwen3Attentionã®é †ä¼æ’­å®Œäº† output.shape=torch.Size([1, 1024])
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 1024]), residual.shape if residual is not None else None=torch.Size([1, 1024])
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨å®Œäº†
ğŸŸ© Qwen3MLPã®é †ä¼æ’­ x.shape=torch.Size([1, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­å®Œäº† result.shape=torch.Size([1, 6144])
ğŸŸ© SwiGLUã‚’é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 6144])
ğŸŸ¦ ãƒãƒ£ãƒ³ã‚¯åŒ– x.shape=torch.Size([1, 3072]), y.shape=torch.Size([1, 3072])
ğŸŸ© SwiGLUã‚’é †ä¼æ’­å®Œäº† result.shape=torch.Size([1, 3072])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­é–‹å§‹ å…¥åŠ›å½¢çŠ¶: torch.Size([1, 3072])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­å®Œäº† å‡ºåŠ›å½¢çŠ¶: torch.Size([1, 1024])
ğŸŸ© Qwen3MLPã®é †ä¼æ’­å®Œäº† x.shape=torch.Size([1, 1024])
ğŸŸ© Qwen3DecoderLayerã®é †ä¼æ’­å®Œäº† hidden_states.shape=torch.Size([1, 1024])
ğŸŸ© Qwen3DecoderLayerã®é †ä¼æ’­ positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 1024]) residual is None=False
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 1024]), residual.shape if residual is not None else None=torch.Size([1, 1024])
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨å®Œäº†
ğŸŸ© Qwen3Attentionã®é †ä¼æ’­ positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­å®Œäº† result.shape=torch.Size([1, 4096])
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 16, 128]), residual.shape if residual is not None else None=None
ğŸŸ© æ­£è¦åŒ–é–‹å§‹ x.shape=torch.Size([1, 16, 128])
ğŸŸ© æ­£è¦åŒ–å®Œäº† x.shape=torch.Size([1, 16, 128])
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 8, 128]), residual.shape if residual is not None else None=None
ğŸŸ© æ­£è¦åŒ–é–‹å§‹ x.shape=torch.Size([1, 8, 128])
ğŸŸ© æ­£è¦åŒ–å®Œäº† x.shape=torch.Size([1, 8, 128])
ğŸŸ© RoPEã®é †ä¼æ’­é–‹å§‹ positions.shape=torch.Size([1]), query.shape=torch.Size([1, 16, 128]), key.shape=torch.Size([1, 8, 128])
ğŸŸ© RoPEã‚’é©ç”¨ x.shape=torch.Size([1, 16, 128]), cos.shape=torch.Size([1, 1, 64]), sin.shape=torch.Size([1, 1, 64])
ğŸŸ¦ ãƒãƒ£ãƒ³ã‚¯åŒ– x1.shape=torch.Size([1, 16, 64]), x2.shape=torch.Size([1, 16, 64])
ğŸŸ¦ å›è»¢è¡Œåˆ—é©ç”¨ y1.shape=torch.Size([1, 16, 64]), y2.shape=torch.Size([1, 16, 64])
ğŸŸ¦ result.shape=torch.Size([1, 16, 128])
ğŸŸ© RoPEã‚’é©ç”¨ x.shape=torch.Size([1, 8, 128]), cos.shape=torch.Size([1, 1, 64]), sin.shape=torch.Size([1, 1, 64])
ğŸŸ¦ ãƒãƒ£ãƒ³ã‚¯åŒ– x1.shape=torch.Size([1, 8, 64]), x2.shape=torch.Size([1, 8, 64])
ğŸŸ¦ å›è»¢è¡Œåˆ—é©ç”¨ y1.shape=torch.Size([1, 8, 64]), y2.shape=torch.Size([1, 8, 64])
ğŸŸ¦ result.shape=torch.Size([1, 8, 128])
ğŸŸ© RoPEã®é †ä¼æ’­å®Œäº† query.shape=torch.Size([1, 16, 128]), key.shape=torch.Size([1, 8, 128])
ğŸŸ© Attentionå±¤ã®é †ä¼æ’­é–‹å§‹ q.shape=torch.Size([1, 16, 128]), k.shape=torch.Size([1, 8, 128]), v.shape=torch.Size([1, 8, 128])
ğŸŸ© ç¾åœ¨ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆå–å¾— _CONTEXT=Context(is_prefill=True, cu_seqlens_q=tensor([0, 1], device='cuda:0', dtype=torch.int32), cu_seqlens_k=tensor([0, 1], device='cuda:0', dtype=torch.int32), max_seqlen_q=1, max_seqlen_k=1, slot_mapping=tensor([256], device='cuda:0', dtype=torch.int32), context_lens=None, block_tables=None)
ğŸŸ¦ ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæƒ…å ±å–å¾— context.is_prefill=True, context.slot_mapping=tensor([256], device='cuda:0', dtype=torch.int32), context.max_seqlen_q=1, context.cu_seqlens_q=tensor([0, 1], device='cuda:0', dtype=torch.int32), context.max_seqlen_k=1, context.cu_seqlens_k=tensor([0, 1], device='cuda:0', dtype=torch.int32), context.context_lens=None, context.block_tables=None
ğŸŸ© KVã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ã‚­ãƒ¼ã¨ãƒãƒªãƒ¥ãƒ¼ã‚’ä¿å­˜ key.shape=torch.Size([1, 8, 128]), value.shape=torch.Size([1, 8, 128]), k_cache.shape=torch.Size([884, 256, 8, 128]), v_cache.shape=torch.Size([884, 256, 8, 128]), slot_mapping.shape=torch.Size([1])
ğŸŸ¦ Prefillãƒ¢ãƒ¼ãƒ‰ã§FlashAttentionã‚’å®Ÿè¡Œ context.max_seqlen_q=1, context.cu_seqlens_q=tensor([0, 1], device='cuda:0', dtype=torch.int32), context.max_seqlen_k=1, context.cu_seqlens_k=tensor([0, 1], device='cuda:0', dtype=torch.int32)
ğŸŸ© Attentionå±¤ã®é †ä¼æ’­å®Œäº† o.shape=torch.Size([1, 16, 128])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­é–‹å§‹ å…¥åŠ›å½¢çŠ¶: torch.Size([1, 2048])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­å®Œäº† å‡ºåŠ›å½¢çŠ¶: torch.Size([1, 1024])
ğŸŸ© Qwen3Attentionã®é †ä¼æ’­å®Œäº† output.shape=torch.Size([1, 1024])
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 1024]), residual.shape if residual is not None else None=torch.Size([1, 1024])
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨å®Œäº†
ğŸŸ© Qwen3MLPã®é †ä¼æ’­ x.shape=torch.Size([1, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­å®Œäº† result.shape=torch.Size([1, 6144])
ğŸŸ© SwiGLUã‚’é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 6144])
ğŸŸ¦ ãƒãƒ£ãƒ³ã‚¯åŒ– x.shape=torch.Size([1, 3072]), y.shape=torch.Size([1, 3072])
ğŸŸ© SwiGLUã‚’é †ä¼æ’­å®Œäº† result.shape=torch.Size([1, 3072])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­é–‹å§‹ å…¥åŠ›å½¢çŠ¶: torch.Size([1, 3072])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­å®Œäº† å‡ºåŠ›å½¢çŠ¶: torch.Size([1, 1024])
ğŸŸ© Qwen3MLPã®é †ä¼æ’­å®Œäº† x.shape=torch.Size([1, 1024])
ğŸŸ© Qwen3DecoderLayerã®é †ä¼æ’­å®Œäº† hidden_states.shape=torch.Size([1, 1024])
ğŸŸ© Qwen3DecoderLayerã®é †ä¼æ’­ positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 1024]) residual is None=False
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 1024]), residual.shape if residual is not None else None=torch.Size([1, 1024])
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨å®Œäº†
ğŸŸ© Qwen3Attentionã®é †ä¼æ’­ positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­å®Œäº† result.shape=torch.Size([1, 4096])
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 16, 128]), residual.shape if residual is not None else None=None
ğŸŸ© æ­£è¦åŒ–é–‹å§‹ x.shape=torch.Size([1, 16, 128])
ğŸŸ© æ­£è¦åŒ–å®Œäº† x.shape=torch.Size([1, 16, 128])
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 8, 128]), residual.shape if residual is not None else None=None
ğŸŸ© æ­£è¦åŒ–é–‹å§‹ x.shape=torch.Size([1, 8, 128])
ğŸŸ© æ­£è¦åŒ–å®Œäº† x.shape=torch.Size([1, 8, 128])
ğŸŸ© RoPEã®é †ä¼æ’­é–‹å§‹ positions.shape=torch.Size([1]), query.shape=torch.Size([1, 16, 128]), key.shape=torch.Size([1, 8, 128])
ğŸŸ© RoPEã‚’é©ç”¨ x.shape=torch.Size([1, 16, 128]), cos.shape=torch.Size([1, 1, 64]), sin.shape=torch.Size([1, 1, 64])
ğŸŸ¦ ãƒãƒ£ãƒ³ã‚¯åŒ– x1.shape=torch.Size([1, 16, 64]), x2.shape=torch.Size([1, 16, 64])
ğŸŸ¦ å›è»¢è¡Œåˆ—é©ç”¨ y1.shape=torch.Size([1, 16, 64]), y2.shape=torch.Size([1, 16, 64])
ğŸŸ¦ result.shape=torch.Size([1, 16, 128])
ğŸŸ© RoPEã‚’é©ç”¨ x.shape=torch.Size([1, 8, 128]), cos.shape=torch.Size([1, 1, 64]), sin.shape=torch.Size([1, 1, 64])
ğŸŸ¦ ãƒãƒ£ãƒ³ã‚¯åŒ– x1.shape=torch.Size([1, 8, 64]), x2.shape=torch.Size([1, 8, 64])
ğŸŸ¦ å›è»¢è¡Œåˆ—é©ç”¨ y1.shape=torch.Size([1, 8, 64]), y2.shape=torch.Size([1, 8, 64])
ğŸŸ¦ result.shape=torch.Size([1, 8, 128])
ğŸŸ© RoPEã®é †ä¼æ’­å®Œäº† query.shape=torch.Size([1, 16, 128]), key.shape=torch.Size([1, 8, 128])
ğŸŸ© Attentionå±¤ã®é †ä¼æ’­é–‹å§‹ q.shape=torch.Size([1, 16, 128]), k.shape=torch.Size([1, 8, 128]), v.shape=torch.Size([1, 8, 128])
ğŸŸ© ç¾åœ¨ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆå–å¾— _CONTEXT=Context(is_prefill=True, cu_seqlens_q=tensor([0, 1], device='cuda:0', dtype=torch.int32), cu_seqlens_k=tensor([0, 1], device='cuda:0', dtype=torch.int32), max_seqlen_q=1, max_seqlen_k=1, slot_mapping=tensor([256], device='cuda:0', dtype=torch.int32), context_lens=None, block_tables=None)
ğŸŸ¦ ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæƒ…å ±å–å¾— context.is_prefill=True, context.slot_mapping=tensor([256], device='cuda:0', dtype=torch.int32), context.max_seqlen_q=1, context.cu_seqlens_q=tensor([0, 1], device='cuda:0', dtype=torch.int32), context.max_seqlen_k=1, context.cu_seqlens_k=tensor([0, 1], device='cuda:0', dtype=torch.int32), context.context_lens=None, context.block_tables=None
ğŸŸ© KVã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ã‚­ãƒ¼ã¨ãƒãƒªãƒ¥ãƒ¼ã‚’ä¿å­˜ key.shape=torch.Size([1, 8, 128]), value.shape=torch.Size([1, 8, 128]), k_cache.shape=torch.Size([884, 256, 8, 128]), v_cache.shape=torch.Size([884, 256, 8, 128]), slot_mapping.shape=torch.Size([1])
ğŸŸ¦ Prefillãƒ¢ãƒ¼ãƒ‰ã§FlashAttentionã‚’å®Ÿè¡Œ context.max_seqlen_q=1, context.cu_seqlens_q=tensor([0, 1], device='cuda:0', dtype=torch.int32), context.max_seqlen_k=1, context.cu_seqlens_k=tensor([0, 1], device='cuda:0', dtype=torch.int32)
ğŸŸ© Attentionå±¤ã®é †ä¼æ’­å®Œäº† o.shape=torch.Size([1, 16, 128])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­é–‹å§‹ å…¥åŠ›å½¢çŠ¶: torch.Size([1, 2048])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­å®Œäº† å‡ºåŠ›å½¢çŠ¶: torch.Size([1, 1024])
ğŸŸ© Qwen3Attentionã®é †ä¼æ’­å®Œäº† output.shape=torch.Size([1, 1024])
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 1024]), residual.shape if residual is not None else None=torch.Size([1, 1024])
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨å®Œäº†
ğŸŸ© Qwen3MLPã®é †ä¼æ’­ x.shape=torch.Size([1, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­å®Œäº† result.shape=torch.Size([1, 6144])
ğŸŸ© SwiGLUã‚’é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 6144])
ğŸŸ¦ ãƒãƒ£ãƒ³ã‚¯åŒ– x.shape=torch.Size([1, 3072]), y.shape=torch.Size([1, 3072])
ğŸŸ© SwiGLUã‚’é †ä¼æ’­å®Œäº† result.shape=torch.Size([1, 3072])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­é–‹å§‹ å…¥åŠ›å½¢çŠ¶: torch.Size([1, 3072])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­å®Œäº† å‡ºåŠ›å½¢çŠ¶: torch.Size([1, 1024])
ğŸŸ© Qwen3MLPã®é †ä¼æ’­å®Œäº† x.shape=torch.Size([1, 1024])
ğŸŸ© Qwen3DecoderLayerã®é †ä¼æ’­å®Œäº† hidden_states.shape=torch.Size([1, 1024])
ğŸŸ© Qwen3DecoderLayerã®é †ä¼æ’­ positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 1024]) residual is None=False
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 1024]), residual.shape if residual is not None else None=torch.Size([1, 1024])
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨å®Œäº†
ğŸŸ© Qwen3Attentionã®é †ä¼æ’­ positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­å®Œäº† result.shape=torch.Size([1, 4096])
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 16, 128]), residual.shape if residual is not None else None=None
ğŸŸ© æ­£è¦åŒ–é–‹å§‹ x.shape=torch.Size([1, 16, 128])
ğŸŸ© æ­£è¦åŒ–å®Œäº† x.shape=torch.Size([1, 16, 128])
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 8, 128]), residual.shape if residual is not None else None=None
ğŸŸ© æ­£è¦åŒ–é–‹å§‹ x.shape=torch.Size([1, 8, 128])
ğŸŸ© æ­£è¦åŒ–å®Œäº† x.shape=torch.Size([1, 8, 128])
ğŸŸ© RoPEã®é †ä¼æ’­é–‹å§‹ positions.shape=torch.Size([1]), query.shape=torch.Size([1, 16, 128]), key.shape=torch.Size([1, 8, 128])
ğŸŸ© RoPEã‚’é©ç”¨ x.shape=torch.Size([1, 16, 128]), cos.shape=torch.Size([1, 1, 64]), sin.shape=torch.Size([1, 1, 64])
ğŸŸ¦ ãƒãƒ£ãƒ³ã‚¯åŒ– x1.shape=torch.Size([1, 16, 64]), x2.shape=torch.Size([1, 16, 64])
ğŸŸ¦ å›è»¢è¡Œåˆ—é©ç”¨ y1.shape=torch.Size([1, 16, 64]), y2.shape=torch.Size([1, 16, 64])
ğŸŸ¦ result.shape=torch.Size([1, 16, 128])
ğŸŸ© RoPEã‚’é©ç”¨ x.shape=torch.Size([1, 8, 128]), cos.shape=torch.Size([1, 1, 64]), sin.shape=torch.Size([1, 1, 64])
ğŸŸ¦ ãƒãƒ£ãƒ³ã‚¯åŒ– x1.shape=torch.Size([1, 8, 64]), x2.shape=torch.Size([1, 8, 64])
ğŸŸ¦ å›è»¢è¡Œåˆ—é©ç”¨ y1.shape=torch.Size([1, 8, 64]), y2.shape=torch.Size([1, 8, 64])
ğŸŸ¦ result.shape=torch.Size([1, 8, 128])
ğŸŸ© RoPEã®é †ä¼æ’­å®Œäº† query.shape=torch.Size([1, 16, 128]), key.shape=torch.Size([1, 8, 128])
ğŸŸ© Attentionå±¤ã®é †ä¼æ’­é–‹å§‹ q.shape=torch.Size([1, 16, 128]), k.shape=torch.Size([1, 8, 128]), v.shape=torch.Size([1, 8, 128])
ğŸŸ© ç¾åœ¨ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆå–å¾— _CONTEXT=Context(is_prefill=True, cu_seqlens_q=tensor([0, 1], device='cuda:0', dtype=torch.int32), cu_seqlens_k=tensor([0, 1], device='cuda:0', dtype=torch.int32), max_seqlen_q=1, max_seqlen_k=1, slot_mapping=tensor([256], device='cuda:0', dtype=torch.int32), context_lens=None, block_tables=None)
ğŸŸ¦ ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæƒ…å ±å–å¾— context.is_prefill=True, context.slot_mapping=tensor([256], device='cuda:0', dtype=torch.int32), context.max_seqlen_q=1, context.cu_seqlens_q=tensor([0, 1], device='cuda:0', dtype=torch.int32), context.max_seqlen_k=1, context.cu_seqlens_k=tensor([0, 1], device='cuda:0', dtype=torch.int32), context.context_lens=None, context.block_tables=None
ğŸŸ© KVã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ã‚­ãƒ¼ã¨ãƒãƒªãƒ¥ãƒ¼ã‚’ä¿å­˜ key.shape=torch.Size([1, 8, 128]), value.shape=torch.Size([1, 8, 128]), k_cache.shape=torch.Size([884, 256, 8, 128]), v_cache.shape=torch.Size([884, 256, 8, 128]), slot_mapping.shape=torch.Size([1])
ğŸŸ¦ Prefillãƒ¢ãƒ¼ãƒ‰ã§FlashAttentionã‚’å®Ÿè¡Œ context.max_seqlen_q=1, context.cu_seqlens_q=tensor([0, 1], device='cuda:0', dtype=torch.int32), context.max_seqlen_k=1, context.cu_seqlens_k=tensor([0, 1], device='cuda:0', dtype=torch.int32)
ğŸŸ© Attentionå±¤ã®é †ä¼æ’­å®Œäº† o.shape=torch.Size([1, 16, 128])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­é–‹å§‹ å…¥åŠ›å½¢çŠ¶: torch.Size([1, 2048])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­å®Œäº† å‡ºåŠ›å½¢çŠ¶: torch.Size([1, 1024])
ğŸŸ© Qwen3Attentionã®é †ä¼æ’­å®Œäº† output.shape=torch.Size([1, 1024])
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 1024]), residual.shape if residual is not None else None=torch.Size([1, 1024])
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨å®Œäº†
ğŸŸ© Qwen3MLPã®é †ä¼æ’­ x.shape=torch.Size([1, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­å®Œäº† result.shape=torch.Size([1, 6144])
ğŸŸ© SwiGLUã‚’é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 6144])
ğŸŸ¦ ãƒãƒ£ãƒ³ã‚¯åŒ– x.shape=torch.Size([1, 3072]), y.shape=torch.Size([1, 3072])
ğŸŸ© SwiGLUã‚’é †ä¼æ’­å®Œäº† result.shape=torch.Size([1, 3072])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­é–‹å§‹ å…¥åŠ›å½¢çŠ¶: torch.Size([1, 3072])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­å®Œäº† å‡ºåŠ›å½¢çŠ¶: torch.Size([1, 1024])
ğŸŸ© Qwen3MLPã®é †ä¼æ’­å®Œäº† x.shape=torch.Size([1, 1024])
ğŸŸ© Qwen3DecoderLayerã®é †ä¼æ’­å®Œäº† hidden_states.shape=torch.Size([1, 1024])
ğŸŸ© Qwen3DecoderLayerã®é †ä¼æ’­ positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 1024]) residual is None=False
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 1024]), residual.shape if residual is not None else None=torch.Size([1, 1024])
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨å®Œäº†
ğŸŸ© Qwen3Attentionã®é †ä¼æ’­ positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­å®Œäº† result.shape=torch.Size([1, 4096])
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 16, 128]), residual.shape if residual is not None else None=None
ğŸŸ© æ­£è¦åŒ–é–‹å§‹ x.shape=torch.Size([1, 16, 128])
ğŸŸ© æ­£è¦åŒ–å®Œäº† x.shape=torch.Size([1, 16, 128])
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 8, 128]), residual.shape if residual is not None else None=None
ğŸŸ© æ­£è¦åŒ–é–‹å§‹ x.shape=torch.Size([1, 8, 128])
ğŸŸ© æ­£è¦åŒ–å®Œäº† x.shape=torch.Size([1, 8, 128])
ğŸŸ© RoPEã®é †ä¼æ’­é–‹å§‹ positions.shape=torch.Size([1]), query.shape=torch.Size([1, 16, 128]), key.shape=torch.Size([1, 8, 128])
ğŸŸ© RoPEã‚’é©ç”¨ x.shape=torch.Size([1, 16, 128]), cos.shape=torch.Size([1, 1, 64]), sin.shape=torch.Size([1, 1, 64])
ğŸŸ¦ ãƒãƒ£ãƒ³ã‚¯åŒ– x1.shape=torch.Size([1, 16, 64]), x2.shape=torch.Size([1, 16, 64])
ğŸŸ¦ å›è»¢è¡Œåˆ—é©ç”¨ y1.shape=torch.Size([1, 16, 64]), y2.shape=torch.Size([1, 16, 64])
ğŸŸ¦ result.shape=torch.Size([1, 16, 128])
ğŸŸ© RoPEã‚’é©ç”¨ x.shape=torch.Size([1, 8, 128]), cos.shape=torch.Size([1, 1, 64]), sin.shape=torch.Size([1, 1, 64])
ğŸŸ¦ ãƒãƒ£ãƒ³ã‚¯åŒ– x1.shape=torch.Size([1, 8, 64]), x2.shape=torch.Size([1, 8, 64])
ğŸŸ¦ å›è»¢è¡Œåˆ—é©ç”¨ y1.shape=torch.Size([1, 8, 64]), y2.shape=torch.Size([1, 8, 64])
ğŸŸ¦ result.shape=torch.Size([1, 8, 128])
ğŸŸ© RoPEã®é †ä¼æ’­å®Œäº† query.shape=torch.Size([1, 16, 128]), key.shape=torch.Size([1, 8, 128])
ğŸŸ© Attentionå±¤ã®é †ä¼æ’­é–‹å§‹ q.shape=torch.Size([1, 16, 128]), k.shape=torch.Size([1, 8, 128]), v.shape=torch.Size([1, 8, 128])
ğŸŸ© ç¾åœ¨ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆå–å¾— _CONTEXT=Context(is_prefill=True, cu_seqlens_q=tensor([0, 1], device='cuda:0', dtype=torch.int32), cu_seqlens_k=tensor([0, 1], device='cuda:0', dtype=torch.int32), max_seqlen_q=1, max_seqlen_k=1, slot_mapping=tensor([256], device='cuda:0', dtype=torch.int32), context_lens=None, block_tables=None)
ğŸŸ¦ ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæƒ…å ±å–å¾— context.is_prefill=True, context.slot_mapping=tensor([256], device='cuda:0', dtype=torch.int32), context.max_seqlen_q=1, context.cu_seqlens_q=tensor([0, 1], device='cuda:0', dtype=torch.int32), context.max_seqlen_k=1, context.cu_seqlens_k=tensor([0, 1], device='cuda:0', dtype=torch.int32), context.context_lens=None, context.block_tables=None
ğŸŸ© KVã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ã‚­ãƒ¼ã¨ãƒãƒªãƒ¥ãƒ¼ã‚’ä¿å­˜ key.shape=torch.Size([1, 8, 128]), value.shape=torch.Size([1, 8, 128]), k_cache.shape=torch.Size([884, 256, 8, 128]), v_cache.shape=torch.Size([884, 256, 8, 128]), slot_mapping.shape=torch.Size([1])
ğŸŸ¦ Prefillãƒ¢ãƒ¼ãƒ‰ã§FlashAttentionã‚’å®Ÿè¡Œ context.max_seqlen_q=1, context.cu_seqlens_q=tensor([0, 1], device='cuda:0', dtype=torch.int32), context.max_seqlen_k=1, context.cu_seqlens_k=tensor([0, 1], device='cuda:0', dtype=torch.int32)
ğŸŸ© Attentionå±¤ã®é †ä¼æ’­å®Œäº† o.shape=torch.Size([1, 16, 128])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­é–‹å§‹ å…¥åŠ›å½¢çŠ¶: torch.Size([1, 2048])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­å®Œäº† å‡ºåŠ›å½¢çŠ¶: torch.Size([1, 1024])
ğŸŸ© Qwen3Attentionã®é †ä¼æ’­å®Œäº† output.shape=torch.Size([1, 1024])
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 1024]), residual.shape if residual is not None else None=torch.Size([1, 1024])
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨å®Œäº†
ğŸŸ© Qwen3MLPã®é †ä¼æ’­ x.shape=torch.Size([1, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­å®Œäº† result.shape=torch.Size([1, 6144])
ğŸŸ© SwiGLUã‚’é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 6144])
ğŸŸ¦ ãƒãƒ£ãƒ³ã‚¯åŒ– x.shape=torch.Size([1, 3072]), y.shape=torch.Size([1, 3072])
ğŸŸ© SwiGLUã‚’é †ä¼æ’­å®Œäº† result.shape=torch.Size([1, 3072])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­é–‹å§‹ å…¥åŠ›å½¢çŠ¶: torch.Size([1, 3072])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­å®Œäº† å‡ºåŠ›å½¢çŠ¶: torch.Size([1, 1024])
ğŸŸ© Qwen3MLPã®é †ä¼æ’­å®Œäº† x.shape=torch.Size([1, 1024])
ğŸŸ© Qwen3DecoderLayerã®é †ä¼æ’­å®Œäº† hidden_states.shape=torch.Size([1, 1024])
ğŸŸ© Qwen3DecoderLayerã®é †ä¼æ’­ positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 1024]) residual is None=False
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 1024]), residual.shape if residual is not None else None=torch.Size([1, 1024])
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨å®Œäº†
ğŸŸ© Qwen3Attentionã®é †ä¼æ’­ positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­å®Œäº† result.shape=torch.Size([1, 4096])
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 16, 128]), residual.shape if residual is not None else None=None
ğŸŸ© æ­£è¦åŒ–é–‹å§‹ x.shape=torch.Size([1, 16, 128])
ğŸŸ© æ­£è¦åŒ–å®Œäº† x.shape=torch.Size([1, 16, 128])
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 8, 128]), residual.shape if residual is not None else None=None
ğŸŸ© æ­£è¦åŒ–é–‹å§‹ x.shape=torch.Size([1, 8, 128])
ğŸŸ© æ­£è¦åŒ–å®Œäº† x.shape=torch.Size([1, 8, 128])
ğŸŸ© RoPEã®é †ä¼æ’­é–‹å§‹ positions.shape=torch.Size([1]), query.shape=torch.Size([1, 16, 128]), key.shape=torch.Size([1, 8, 128])
ğŸŸ© RoPEã‚’é©ç”¨ x.shape=torch.Size([1, 16, 128]), cos.shape=torch.Size([1, 1, 64]), sin.shape=torch.Size([1, 1, 64])
ğŸŸ¦ ãƒãƒ£ãƒ³ã‚¯åŒ– x1.shape=torch.Size([1, 16, 64]), x2.shape=torch.Size([1, 16, 64])
ğŸŸ¦ å›è»¢è¡Œåˆ—é©ç”¨ y1.shape=torch.Size([1, 16, 64]), y2.shape=torch.Size([1, 16, 64])
ğŸŸ¦ result.shape=torch.Size([1, 16, 128])
ğŸŸ© RoPEã‚’é©ç”¨ x.shape=torch.Size([1, 8, 128]), cos.shape=torch.Size([1, 1, 64]), sin.shape=torch.Size([1, 1, 64])
ğŸŸ¦ ãƒãƒ£ãƒ³ã‚¯åŒ– x1.shape=torch.Size([1, 8, 64]), x2.shape=torch.Size([1, 8, 64])
ğŸŸ¦ å›è»¢è¡Œåˆ—é©ç”¨ y1.shape=torch.Size([1, 8, 64]), y2.shape=torch.Size([1, 8, 64])
ğŸŸ¦ result.shape=torch.Size([1, 8, 128])
ğŸŸ© RoPEã®é †ä¼æ’­å®Œäº† query.shape=torch.Size([1, 16, 128]), key.shape=torch.Size([1, 8, 128])
ğŸŸ© Attentionå±¤ã®é †ä¼æ’­é–‹å§‹ q.shape=torch.Size([1, 16, 128]), k.shape=torch.Size([1, 8, 128]), v.shape=torch.Size([1, 8, 128])
ğŸŸ© ç¾åœ¨ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆå–å¾— _CONTEXT=Context(is_prefill=True, cu_seqlens_q=tensor([0, 1], device='cuda:0', dtype=torch.int32), cu_seqlens_k=tensor([0, 1], device='cuda:0', dtype=torch.int32), max_seqlen_q=1, max_seqlen_k=1, slot_mapping=tensor([256], device='cuda:0', dtype=torch.int32), context_lens=None, block_tables=None)
ğŸŸ¦ ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæƒ…å ±å–å¾— context.is_prefill=True, context.slot_mapping=tensor([256], device='cuda:0', dtype=torch.int32), context.max_seqlen_q=1, context.cu_seqlens_q=tensor([0, 1], device='cuda:0', dtype=torch.int32), context.max_seqlen_k=1, context.cu_seqlens_k=tensor([0, 1], device='cuda:0', dtype=torch.int32), context.context_lens=None, context.block_tables=None
ğŸŸ© KVã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ã‚­ãƒ¼ã¨ãƒãƒªãƒ¥ãƒ¼ã‚’ä¿å­˜ key.shape=torch.Size([1, 8, 128]), value.shape=torch.Size([1, 8, 128]), k_cache.shape=torch.Size([884, 256, 8, 128]), v_cache.shape=torch.Size([884, 256, 8, 128]), slot_mapping.shape=torch.Size([1])
ğŸŸ¦ Prefillãƒ¢ãƒ¼ãƒ‰ã§FlashAttentionã‚’å®Ÿè¡Œ context.max_seqlen_q=1, context.cu_seqlens_q=tensor([0, 1], device='cuda:0', dtype=torch.int32), context.max_seqlen_k=1, context.cu_seqlens_k=tensor([0, 1], device='cuda:0', dtype=torch.int32)
ğŸŸ© Attentionå±¤ã®é †ä¼æ’­å®Œäº† o.shape=torch.Size([1, 16, 128])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­é–‹å§‹ å…¥åŠ›å½¢çŠ¶: torch.Size([1, 2048])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­å®Œäº† å‡ºåŠ›å½¢çŠ¶: torch.Size([1, 1024])
ğŸŸ© Qwen3Attentionã®é †ä¼æ’­å®Œäº† output.shape=torch.Size([1, 1024])
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 1024]), residual.shape if residual is not None else None=torch.Size([1, 1024])
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨å®Œäº†
ğŸŸ© Qwen3MLPã®é †ä¼æ’­ x.shape=torch.Size([1, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­å®Œäº† result.shape=torch.Size([1, 6144])
ğŸŸ© SwiGLUã‚’é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 6144])
ğŸŸ¦ ãƒãƒ£ãƒ³ã‚¯åŒ– x.shape=torch.Size([1, 3072]), y.shape=torch.Size([1, 3072])
ğŸŸ© SwiGLUã‚’é †ä¼æ’­å®Œäº† result.shape=torch.Size([1, 3072])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­é–‹å§‹ å…¥åŠ›å½¢çŠ¶: torch.Size([1, 3072])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­å®Œäº† å‡ºåŠ›å½¢çŠ¶: torch.Size([1, 1024])
ğŸŸ© Qwen3MLPã®é †ä¼æ’­å®Œäº† x.shape=torch.Size([1, 1024])
ğŸŸ© Qwen3DecoderLayerã®é †ä¼æ’­å®Œäº† hidden_states.shape=torch.Size([1, 1024])
ğŸŸ© Qwen3DecoderLayerã®é †ä¼æ’­ positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 1024]) residual is None=False
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 1024]), residual.shape if residual is not None else None=torch.Size([1, 1024])
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨å®Œäº†
ğŸŸ© Qwen3Attentionã®é †ä¼æ’­ positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­å®Œäº† result.shape=torch.Size([1, 4096])
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 16, 128]), residual.shape if residual is not None else None=None
ğŸŸ© æ­£è¦åŒ–é–‹å§‹ x.shape=torch.Size([1, 16, 128])
ğŸŸ© æ­£è¦åŒ–å®Œäº† x.shape=torch.Size([1, 16, 128])
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 8, 128]), residual.shape if residual is not None else None=None
ğŸŸ© æ­£è¦åŒ–é–‹å§‹ x.shape=torch.Size([1, 8, 128])
ğŸŸ© æ­£è¦åŒ–å®Œäº† x.shape=torch.Size([1, 8, 128])
ğŸŸ© RoPEã®é †ä¼æ’­é–‹å§‹ positions.shape=torch.Size([1]), query.shape=torch.Size([1, 16, 128]), key.shape=torch.Size([1, 8, 128])
ğŸŸ© RoPEã‚’é©ç”¨ x.shape=torch.Size([1, 16, 128]), cos.shape=torch.Size([1, 1, 64]), sin.shape=torch.Size([1, 1, 64])
ğŸŸ¦ ãƒãƒ£ãƒ³ã‚¯åŒ– x1.shape=torch.Size([1, 16, 64]), x2.shape=torch.Size([1, 16, 64])
ğŸŸ¦ å›è»¢è¡Œåˆ—é©ç”¨ y1.shape=torch.Size([1, 16, 64]), y2.shape=torch.Size([1, 16, 64])
ğŸŸ¦ result.shape=torch.Size([1, 16, 128])
ğŸŸ© RoPEã‚’é©ç”¨ x.shape=torch.Size([1, 8, 128]), cos.shape=torch.Size([1, 1, 64]), sin.shape=torch.Size([1, 1, 64])
ğŸŸ¦ ãƒãƒ£ãƒ³ã‚¯åŒ– x1.shape=torch.Size([1, 8, 64]), x2.shape=torch.Size([1, 8, 64])
ğŸŸ¦ å›è»¢è¡Œåˆ—é©ç”¨ y1.shape=torch.Size([1, 8, 64]), y2.shape=torch.Size([1, 8, 64])
ğŸŸ¦ result.shape=torch.Size([1, 8, 128])
ğŸŸ© RoPEã®é †ä¼æ’­å®Œäº† query.shape=torch.Size([1, 16, 128]), key.shape=torch.Size([1, 8, 128])
ğŸŸ© Attentionå±¤ã®é †ä¼æ’­é–‹å§‹ q.shape=torch.Size([1, 16, 128]), k.shape=torch.Size([1, 8, 128]), v.shape=torch.Size([1, 8, 128])
ğŸŸ© ç¾åœ¨ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆå–å¾— _CONTEXT=Context(is_prefill=True, cu_seqlens_q=tensor([0, 1], device='cuda:0', dtype=torch.int32), cu_seqlens_k=tensor([0, 1], device='cuda:0', dtype=torch.int32), max_seqlen_q=1, max_seqlen_k=1, slot_mapping=tensor([256], device='cuda:0', dtype=torch.int32), context_lens=None, block_tables=None)
ğŸŸ¦ ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæƒ…å ±å–å¾— context.is_prefill=True, context.slot_mapping=tensor([256], device='cuda:0', dtype=torch.int32), context.max_seqlen_q=1, context.cu_seqlens_q=tensor([0, 1], device='cuda:0', dtype=torch.int32), context.max_seqlen_k=1, context.cu_seqlens_k=tensor([0, 1], device='cuda:0', dtype=torch.int32), context.context_lens=None, context.block_tables=None
ğŸŸ© KVã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ã‚­ãƒ¼ã¨ãƒãƒªãƒ¥ãƒ¼ã‚’ä¿å­˜ key.shape=torch.Size([1, 8, 128]), value.shape=torch.Size([1, 8, 128]), k_cache.shape=torch.Size([884, 256, 8, 128]), v_cache.shape=torch.Size([884, 256, 8, 128]), slot_mapping.shape=torch.Size([1])
ğŸŸ¦ Prefillãƒ¢ãƒ¼ãƒ‰ã§FlashAttentionã‚’å®Ÿè¡Œ context.max_seqlen_q=1, context.cu_seqlens_q=tensor([0, 1], device='cuda:0', dtype=torch.int32), context.max_seqlen_k=1, context.cu_seqlens_k=tensor([0, 1], device='cuda:0', dtype=torch.int32)
ğŸŸ© Attentionå±¤ã®é †ä¼æ’­å®Œäº† o.shape=torch.Size([1, 16, 128])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­é–‹å§‹ å…¥åŠ›å½¢çŠ¶: torch.Size([1, 2048])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­å®Œäº† å‡ºåŠ›å½¢çŠ¶: torch.Size([1, 1024])
ğŸŸ© Qwen3Attentionã®é †ä¼æ’­å®Œäº† output.shape=torch.Size([1, 1024])
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 1024]), residual.shape if residual is not None else None=torch.Size([1, 1024])
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨å®Œäº†
ğŸŸ© Qwen3MLPã®é †ä¼æ’­ x.shape=torch.Size([1, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­å®Œäº† result.shape=torch.Size([1, 6144])
ğŸŸ© SwiGLUã‚’é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 6144])
ğŸŸ¦ ãƒãƒ£ãƒ³ã‚¯åŒ– x.shape=torch.Size([1, 3072]), y.shape=torch.Size([1, 3072])
ğŸŸ© SwiGLUã‚’é †ä¼æ’­å®Œäº† result.shape=torch.Size([1, 3072])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­é–‹å§‹ å…¥åŠ›å½¢çŠ¶: torch.Size([1, 3072])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­å®Œäº† å‡ºåŠ›å½¢çŠ¶: torch.Size([1, 1024])
ğŸŸ© Qwen3MLPã®é †ä¼æ’­å®Œäº† x.shape=torch.Size([1, 1024])
ğŸŸ© Qwen3DecoderLayerã®é †ä¼æ’­å®Œäº† hidden_states.shape=torch.Size([1, 1024])
ğŸŸ© Qwen3DecoderLayerã®é †ä¼æ’­ positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 1024]) residual is None=False
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 1024]), residual.shape if residual is not None else None=torch.Size([1, 1024])
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨å®Œäº†
ğŸŸ© Qwen3Attentionã®é †ä¼æ’­ positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­å®Œäº† result.shape=torch.Size([1, 4096])
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 16, 128]), residual.shape if residual is not None else None=None
ğŸŸ© æ­£è¦åŒ–é–‹å§‹ x.shape=torch.Size([1, 16, 128])
ğŸŸ© æ­£è¦åŒ–å®Œäº† x.shape=torch.Size([1, 16, 128])
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 8, 128]), residual.shape if residual is not None else None=None
ğŸŸ© æ­£è¦åŒ–é–‹å§‹ x.shape=torch.Size([1, 8, 128])
ğŸŸ© æ­£è¦åŒ–å®Œäº† x.shape=torch.Size([1, 8, 128])
ğŸŸ© RoPEã®é †ä¼æ’­é–‹å§‹ positions.shape=torch.Size([1]), query.shape=torch.Size([1, 16, 128]), key.shape=torch.Size([1, 8, 128])
ğŸŸ© RoPEã‚’é©ç”¨ x.shape=torch.Size([1, 16, 128]), cos.shape=torch.Size([1, 1, 64]), sin.shape=torch.Size([1, 1, 64])
ğŸŸ¦ ãƒãƒ£ãƒ³ã‚¯åŒ– x1.shape=torch.Size([1, 16, 64]), x2.shape=torch.Size([1, 16, 64])
ğŸŸ¦ å›è»¢è¡Œåˆ—é©ç”¨ y1.shape=torch.Size([1, 16, 64]), y2.shape=torch.Size([1, 16, 64])
ğŸŸ¦ result.shape=torch.Size([1, 16, 128])
ğŸŸ© RoPEã‚’é©ç”¨ x.shape=torch.Size([1, 8, 128]), cos.shape=torch.Size([1, 1, 64]), sin.shape=torch.Size([1, 1, 64])
ğŸŸ¦ ãƒãƒ£ãƒ³ã‚¯åŒ– x1.shape=torch.Size([1, 8, 64]), x2.shape=torch.Size([1, 8, 64])
ğŸŸ¦ å›è»¢è¡Œåˆ—é©ç”¨ y1.shape=torch.Size([1, 8, 64]), y2.shape=torch.Size([1, 8, 64])
ğŸŸ¦ result.shape=torch.Size([1, 8, 128])
ğŸŸ© RoPEã®é †ä¼æ’­å®Œäº† query.shape=torch.Size([1, 16, 128]), key.shape=torch.Size([1, 8, 128])
ğŸŸ© Attentionå±¤ã®é †ä¼æ’­é–‹å§‹ q.shape=torch.Size([1, 16, 128]), k.shape=torch.Size([1, 8, 128]), v.shape=torch.Size([1, 8, 128])
ğŸŸ© ç¾åœ¨ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆå–å¾— _CONTEXT=Context(is_prefill=True, cu_seqlens_q=tensor([0, 1], device='cuda:0', dtype=torch.int32), cu_seqlens_k=tensor([0, 1], device='cuda:0', dtype=torch.int32), max_seqlen_q=1, max_seqlen_k=1, slot_mapping=tensor([256], device='cuda:0', dtype=torch.int32), context_lens=None, block_tables=None)
ğŸŸ¦ ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæƒ…å ±å–å¾— context.is_prefill=True, context.slot_mapping=tensor([256], device='cuda:0', dtype=torch.int32), context.max_seqlen_q=1, context.cu_seqlens_q=tensor([0, 1], device='cuda:0', dtype=torch.int32), context.max_seqlen_k=1, context.cu_seqlens_k=tensor([0, 1], device='cuda:0', dtype=torch.int32), context.context_lens=None, context.block_tables=None
ğŸŸ© KVã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ã‚­ãƒ¼ã¨ãƒãƒªãƒ¥ãƒ¼ã‚’ä¿å­˜ key.shape=torch.Size([1, 8, 128]), value.shape=torch.Size([1, 8, 128]), k_cache.shape=torch.Size([884, 256, 8, 128]), v_cache.shape=torch.Size([884, 256, 8, 128]), slot_mapping.shape=torch.Size([1])
ğŸŸ¦ Prefillãƒ¢ãƒ¼ãƒ‰ã§FlashAttentionã‚’å®Ÿè¡Œ context.max_seqlen_q=1, context.cu_seqlens_q=tensor([0, 1], device='cuda:0', dtype=torch.int32), context.max_seqlen_k=1, context.cu_seqlens_k=tensor([0, 1], device='cuda:0', dtype=torch.int32)
ğŸŸ© Attentionå±¤ã®é †ä¼æ’­å®Œäº† o.shape=torch.Size([1, 16, 128])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­é–‹å§‹ å…¥åŠ›å½¢çŠ¶: torch.Size([1, 2048])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­å®Œäº† å‡ºåŠ›å½¢çŠ¶: torch.Size([1, 1024])
ğŸŸ© Qwen3Attentionã®é †ä¼æ’­å®Œäº† output.shape=torch.Size([1, 1024])
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 1024]), residual.shape if residual is not None else None=torch.Size([1, 1024])
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨å®Œäº†
ğŸŸ© Qwen3MLPã®é †ä¼æ’­ x.shape=torch.Size([1, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­å®Œäº† result.shape=torch.Size([1, 6144])
ğŸŸ© SwiGLUã‚’é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 6144])
ğŸŸ¦ ãƒãƒ£ãƒ³ã‚¯åŒ– x.shape=torch.Size([1, 3072]), y.shape=torch.Size([1, 3072])
ğŸŸ© SwiGLUã‚’é †ä¼æ’­å®Œäº† result.shape=torch.Size([1, 3072])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­é–‹å§‹ å…¥åŠ›å½¢çŠ¶: torch.Size([1, 3072])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­å®Œäº† å‡ºåŠ›å½¢çŠ¶: torch.Size([1, 1024])
ğŸŸ© Qwen3MLPã®é †ä¼æ’­å®Œäº† x.shape=torch.Size([1, 1024])
ğŸŸ© Qwen3DecoderLayerã®é †ä¼æ’­å®Œäº† hidden_states.shape=torch.Size([1, 1024])
ğŸŸ© Qwen3DecoderLayerã®é †ä¼æ’­ positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 1024]) residual is None=False
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 1024]), residual.shape if residual is not None else None=torch.Size([1, 1024])
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨å®Œäº†
ğŸŸ© Qwen3Attentionã®é †ä¼æ’­ positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­å®Œäº† result.shape=torch.Size([1, 4096])
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 16, 128]), residual.shape if residual is not None else None=None
ğŸŸ© æ­£è¦åŒ–é–‹å§‹ x.shape=torch.Size([1, 16, 128])
ğŸŸ© æ­£è¦åŒ–å®Œäº† x.shape=torch.Size([1, 16, 128])
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 8, 128]), residual.shape if residual is not None else None=None
ğŸŸ© æ­£è¦åŒ–é–‹å§‹ x.shape=torch.Size([1, 8, 128])
ğŸŸ© æ­£è¦åŒ–å®Œäº† x.shape=torch.Size([1, 8, 128])
ğŸŸ© RoPEã®é †ä¼æ’­é–‹å§‹ positions.shape=torch.Size([1]), query.shape=torch.Size([1, 16, 128]), key.shape=torch.Size([1, 8, 128])
ğŸŸ© RoPEã‚’é©ç”¨ x.shape=torch.Size([1, 16, 128]), cos.shape=torch.Size([1, 1, 64]), sin.shape=torch.Size([1, 1, 64])
ğŸŸ¦ ãƒãƒ£ãƒ³ã‚¯åŒ– x1.shape=torch.Size([1, 16, 64]), x2.shape=torch.Size([1, 16, 64])
ğŸŸ¦ å›è»¢è¡Œåˆ—é©ç”¨ y1.shape=torch.Size([1, 16, 64]), y2.shape=torch.Size([1, 16, 64])
ğŸŸ¦ result.shape=torch.Size([1, 16, 128])
ğŸŸ© RoPEã‚’é©ç”¨ x.shape=torch.Size([1, 8, 128]), cos.shape=torch.Size([1, 1, 64]), sin.shape=torch.Size([1, 1, 64])
ğŸŸ¦ ãƒãƒ£ãƒ³ã‚¯åŒ– x1.shape=torch.Size([1, 8, 64]), x2.shape=torch.Size([1, 8, 64])
ğŸŸ¦ å›è»¢è¡Œåˆ—é©ç”¨ y1.shape=torch.Size([1, 8, 64]), y2.shape=torch.Size([1, 8, 64])
ğŸŸ¦ result.shape=torch.Size([1, 8, 128])
ğŸŸ© RoPEã®é †ä¼æ’­å®Œäº† query.shape=torch.Size([1, 16, 128]), key.shape=torch.Size([1, 8, 128])
ğŸŸ© Attentionå±¤ã®é †ä¼æ’­é–‹å§‹ q.shape=torch.Size([1, 16, 128]), k.shape=torch.Size([1, 8, 128]), v.shape=torch.Size([1, 8, 128])
ğŸŸ© ç¾åœ¨ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆå–å¾— _CONTEXT=Context(is_prefill=True, cu_seqlens_q=tensor([0, 1], device='cuda:0', dtype=torch.int32), cu_seqlens_k=tensor([0, 1], device='cuda:0', dtype=torch.int32), max_seqlen_q=1, max_seqlen_k=1, slot_mapping=tensor([256], device='cuda:0', dtype=torch.int32), context_lens=None, block_tables=None)
ğŸŸ¦ ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæƒ…å ±å–å¾— context.is_prefill=True, context.slot_mapping=tensor([256], device='cuda:0', dtype=torch.int32), context.max_seqlen_q=1, context.cu_seqlens_q=tensor([0, 1], device='cuda:0', dtype=torch.int32), context.max_seqlen_k=1, context.cu_seqlens_k=tensor([0, 1], device='cuda:0', dtype=torch.int32), context.context_lens=None, context.block_tables=None
ğŸŸ© KVã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ã‚­ãƒ¼ã¨ãƒãƒªãƒ¥ãƒ¼ã‚’ä¿å­˜ key.shape=torch.Size([1, 8, 128]), value.shape=torch.Size([1, 8, 128]), k_cache.shape=torch.Size([884, 256, 8, 128]), v_cache.shape=torch.Size([884, 256, 8, 128]), slot_mapping.shape=torch.Size([1])
ğŸŸ¦ Prefillãƒ¢ãƒ¼ãƒ‰ã§FlashAttentionã‚’å®Ÿè¡Œ context.max_seqlen_q=1, context.cu_seqlens_q=tensor([0, 1], device='cuda:0', dtype=torch.int32), context.max_seqlen_k=1, context.cu_seqlens_k=tensor([0, 1], device='cuda:0', dtype=torch.int32)
ğŸŸ© Attentionå±¤ã®é †ä¼æ’­å®Œäº† o.shape=torch.Size([1, 16, 128])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­é–‹å§‹ å…¥åŠ›å½¢çŠ¶: torch.Size([1, 2048])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­å®Œäº† å‡ºåŠ›å½¢çŠ¶: torch.Size([1, 1024])
ğŸŸ© Qwen3Attentionã®é †ä¼æ’­å®Œäº† output.shape=torch.Size([1, 1024])
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 1024]), residual.shape if residual is not None else None=torch.Size([1, 1024])
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨å®Œäº†
ğŸŸ© Qwen3MLPã®é †ä¼æ’­ x.shape=torch.Size([1, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­å®Œäº† result.shape=torch.Size([1, 6144])
ğŸŸ© SwiGLUã‚’é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 6144])
ğŸŸ¦ ãƒãƒ£ãƒ³ã‚¯åŒ– x.shape=torch.Size([1, 3072]), y.shape=torch.Size([1, 3072])
ğŸŸ© SwiGLUã‚’é †ä¼æ’­å®Œäº† result.shape=torch.Size([1, 3072])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­é–‹å§‹ å…¥åŠ›å½¢çŠ¶: torch.Size([1, 3072])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­å®Œäº† å‡ºåŠ›å½¢çŠ¶: torch.Size([1, 1024])
ğŸŸ© Qwen3MLPã®é †ä¼æ’­å®Œäº† x.shape=torch.Size([1, 1024])
ğŸŸ© Qwen3DecoderLayerã®é †ä¼æ’­å®Œäº† hidden_states.shape=torch.Size([1, 1024])
ğŸŸ© Qwen3DecoderLayerã®é †ä¼æ’­ positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 1024]) residual is None=False
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 1024]), residual.shape if residual is not None else None=torch.Size([1, 1024])
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨å®Œäº†
ğŸŸ© Qwen3Attentionã®é †ä¼æ’­ positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­å®Œäº† result.shape=torch.Size([1, 4096])
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 16, 128]), residual.shape if residual is not None else None=None
ğŸŸ© æ­£è¦åŒ–é–‹å§‹ x.shape=torch.Size([1, 16, 128])
ğŸŸ© æ­£è¦åŒ–å®Œäº† x.shape=torch.Size([1, 16, 128])
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 8, 128]), residual.shape if residual is not None else None=None
ğŸŸ© æ­£è¦åŒ–é–‹å§‹ x.shape=torch.Size([1, 8, 128])
ğŸŸ© æ­£è¦åŒ–å®Œäº† x.shape=torch.Size([1, 8, 128])
ğŸŸ© RoPEã®é †ä¼æ’­é–‹å§‹ positions.shape=torch.Size([1]), query.shape=torch.Size([1, 16, 128]), key.shape=torch.Size([1, 8, 128])
ğŸŸ© RoPEã‚’é©ç”¨ x.shape=torch.Size([1, 16, 128]), cos.shape=torch.Size([1, 1, 64]), sin.shape=torch.Size([1, 1, 64])
ğŸŸ¦ ãƒãƒ£ãƒ³ã‚¯åŒ– x1.shape=torch.Size([1, 16, 64]), x2.shape=torch.Size([1, 16, 64])
ğŸŸ¦ å›è»¢è¡Œåˆ—é©ç”¨ y1.shape=torch.Size([1, 16, 64]), y2.shape=torch.Size([1, 16, 64])
ğŸŸ¦ result.shape=torch.Size([1, 16, 128])
ğŸŸ© RoPEã‚’é©ç”¨ x.shape=torch.Size([1, 8, 128]), cos.shape=torch.Size([1, 1, 64]), sin.shape=torch.Size([1, 1, 64])
ğŸŸ¦ ãƒãƒ£ãƒ³ã‚¯åŒ– x1.shape=torch.Size([1, 8, 64]), x2.shape=torch.Size([1, 8, 64])
ğŸŸ¦ å›è»¢è¡Œåˆ—é©ç”¨ y1.shape=torch.Size([1, 8, 64]), y2.shape=torch.Size([1, 8, 64])
ğŸŸ¦ result.shape=torch.Size([1, 8, 128])
ğŸŸ© RoPEã®é †ä¼æ’­å®Œäº† query.shape=torch.Size([1, 16, 128]), key.shape=torch.Size([1, 8, 128])
ğŸŸ© Attentionå±¤ã®é †ä¼æ’­é–‹å§‹ q.shape=torch.Size([1, 16, 128]), k.shape=torch.Size([1, 8, 128]), v.shape=torch.Size([1, 8, 128])
ğŸŸ© ç¾åœ¨ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆå–å¾— _CONTEXT=Context(is_prefill=True, cu_seqlens_q=tensor([0, 1], device='cuda:0', dtype=torch.int32), cu_seqlens_k=tensor([0, 1], device='cuda:0', dtype=torch.int32), max_seqlen_q=1, max_seqlen_k=1, slot_mapping=tensor([256], device='cuda:0', dtype=torch.int32), context_lens=None, block_tables=None)
ğŸŸ¦ ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæƒ…å ±å–å¾— context.is_prefill=True, context.slot_mapping=tensor([256], device='cuda:0', dtype=torch.int32), context.max_seqlen_q=1, context.cu_seqlens_q=tensor([0, 1], device='cuda:0', dtype=torch.int32), context.max_seqlen_k=1, context.cu_seqlens_k=tensor([0, 1], device='cuda:0', dtype=torch.int32), context.context_lens=None, context.block_tables=None
ğŸŸ© KVã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ã‚­ãƒ¼ã¨ãƒãƒªãƒ¥ãƒ¼ã‚’ä¿å­˜ key.shape=torch.Size([1, 8, 128]), value.shape=torch.Size([1, 8, 128]), k_cache.shape=torch.Size([884, 256, 8, 128]), v_cache.shape=torch.Size([884, 256, 8, 128]), slot_mapping.shape=torch.Size([1])
ğŸŸ¦ Prefillãƒ¢ãƒ¼ãƒ‰ã§FlashAttentionã‚’å®Ÿè¡Œ context.max_seqlen_q=1, context.cu_seqlens_q=tensor([0, 1], device='cuda:0', dtype=torch.int32), context.max_seqlen_k=1, context.cu_seqlens_k=tensor([0, 1], device='cuda:0', dtype=torch.int32)
ğŸŸ© Attentionå±¤ã®é †ä¼æ’­å®Œäº† o.shape=torch.Size([1, 16, 128])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­é–‹å§‹ å…¥åŠ›å½¢çŠ¶: torch.Size([1, 2048])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­å®Œäº† å‡ºåŠ›å½¢çŠ¶: torch.Size([1, 1024])
ğŸŸ© Qwen3Attentionã®é †ä¼æ’­å®Œäº† output.shape=torch.Size([1, 1024])
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 1024]), residual.shape if residual is not None else None=torch.Size([1, 1024])
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨å®Œäº†
ğŸŸ© Qwen3MLPã®é †ä¼æ’­ x.shape=torch.Size([1, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­å®Œäº† result.shape=torch.Size([1, 6144])
ğŸŸ© SwiGLUã‚’é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 6144])
ğŸŸ¦ ãƒãƒ£ãƒ³ã‚¯åŒ– x.shape=torch.Size([1, 3072]), y.shape=torch.Size([1, 3072])
ğŸŸ© SwiGLUã‚’é †ä¼æ’­å®Œäº† result.shape=torch.Size([1, 3072])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­é–‹å§‹ å…¥åŠ›å½¢çŠ¶: torch.Size([1, 3072])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­å®Œäº† å‡ºåŠ›å½¢çŠ¶: torch.Size([1, 1024])
ğŸŸ© Qwen3MLPã®é †ä¼æ’­å®Œäº† x.shape=torch.Size([1, 1024])
ğŸŸ© Qwen3DecoderLayerã®é †ä¼æ’­å®Œäº† hidden_states.shape=torch.Size([1, 1024])
ğŸŸ© Qwen3DecoderLayerã®é †ä¼æ’­ positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 1024]) residual is None=False
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 1024]), residual.shape if residual is not None else None=torch.Size([1, 1024])
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨å®Œäº†
ğŸŸ© Qwen3Attentionã®é †ä¼æ’­ positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­å®Œäº† result.shape=torch.Size([1, 4096])
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 16, 128]), residual.shape if residual is not None else None=None
ğŸŸ© æ­£è¦åŒ–é–‹å§‹ x.shape=torch.Size([1, 16, 128])
ğŸŸ© æ­£è¦åŒ–å®Œäº† x.shape=torch.Size([1, 16, 128])
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 8, 128]), residual.shape if residual is not None else None=None
ğŸŸ© æ­£è¦åŒ–é–‹å§‹ x.shape=torch.Size([1, 8, 128])
ğŸŸ© æ­£è¦åŒ–å®Œäº† x.shape=torch.Size([1, 8, 128])
ğŸŸ© RoPEã®é †ä¼æ’­é–‹å§‹ positions.shape=torch.Size([1]), query.shape=torch.Size([1, 16, 128]), key.shape=torch.Size([1, 8, 128])
ğŸŸ© RoPEã‚’é©ç”¨ x.shape=torch.Size([1, 16, 128]), cos.shape=torch.Size([1, 1, 64]), sin.shape=torch.Size([1, 1, 64])
ğŸŸ¦ ãƒãƒ£ãƒ³ã‚¯åŒ– x1.shape=torch.Size([1, 16, 64]), x2.shape=torch.Size([1, 16, 64])
ğŸŸ¦ å›è»¢è¡Œåˆ—é©ç”¨ y1.shape=torch.Size([1, 16, 64]), y2.shape=torch.Size([1, 16, 64])
ğŸŸ¦ result.shape=torch.Size([1, 16, 128])
ğŸŸ© RoPEã‚’é©ç”¨ x.shape=torch.Size([1, 8, 128]), cos.shape=torch.Size([1, 1, 64]), sin.shape=torch.Size([1, 1, 64])
ğŸŸ¦ ãƒãƒ£ãƒ³ã‚¯åŒ– x1.shape=torch.Size([1, 8, 64]), x2.shape=torch.Size([1, 8, 64])
ğŸŸ¦ å›è»¢è¡Œåˆ—é©ç”¨ y1.shape=torch.Size([1, 8, 64]), y2.shape=torch.Size([1, 8, 64])
ğŸŸ¦ result.shape=torch.Size([1, 8, 128])
ğŸŸ© RoPEã®é †ä¼æ’­å®Œäº† query.shape=torch.Size([1, 16, 128]), key.shape=torch.Size([1, 8, 128])
ğŸŸ© Attentionå±¤ã®é †ä¼æ’­é–‹å§‹ q.shape=torch.Size([1, 16, 128]), k.shape=torch.Size([1, 8, 128]), v.shape=torch.Size([1, 8, 128])
ğŸŸ© ç¾åœ¨ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆå–å¾— _CONTEXT=Context(is_prefill=True, cu_seqlens_q=tensor([0, 1], device='cuda:0', dtype=torch.int32), cu_seqlens_k=tensor([0, 1], device='cuda:0', dtype=torch.int32), max_seqlen_q=1, max_seqlen_k=1, slot_mapping=tensor([256], device='cuda:0', dtype=torch.int32), context_lens=None, block_tables=None)
ğŸŸ¦ ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæƒ…å ±å–å¾— context.is_prefill=True, context.slot_mapping=tensor([256], device='cuda:0', dtype=torch.int32), context.max_seqlen_q=1, context.cu_seqlens_q=tensor([0, 1], device='cuda:0', dtype=torch.int32), context.max_seqlen_k=1, context.cu_seqlens_k=tensor([0, 1], device='cuda:0', dtype=torch.int32), context.context_lens=None, context.block_tables=None
ğŸŸ© KVã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ã‚­ãƒ¼ã¨ãƒãƒªãƒ¥ãƒ¼ã‚’ä¿å­˜ key.shape=torch.Size([1, 8, 128]), value.shape=torch.Size([1, 8, 128]), k_cache.shape=torch.Size([884, 256, 8, 128]), v_cache.shape=torch.Size([884, 256, 8, 128]), slot_mapping.shape=torch.Size([1])
ğŸŸ¦ Prefillãƒ¢ãƒ¼ãƒ‰ã§FlashAttentionã‚’å®Ÿè¡Œ context.max_seqlen_q=1, context.cu_seqlens_q=tensor([0, 1], device='cuda:0', dtype=torch.int32), context.max_seqlen_k=1, context.cu_seqlens_k=tensor([0, 1], device='cuda:0', dtype=torch.int32)
ğŸŸ© Attentionå±¤ã®é †ä¼æ’­å®Œäº† o.shape=torch.Size([1, 16, 128])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­é–‹å§‹ å…¥åŠ›å½¢çŠ¶: torch.Size([1, 2048])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­å®Œäº† å‡ºåŠ›å½¢çŠ¶: torch.Size([1, 1024])
ğŸŸ© Qwen3Attentionã®é †ä¼æ’­å®Œäº† output.shape=torch.Size([1, 1024])
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 1024]), residual.shape if residual is not None else None=torch.Size([1, 1024])
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨å®Œäº†
ğŸŸ© Qwen3MLPã®é †ä¼æ’­ x.shape=torch.Size([1, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­å®Œäº† result.shape=torch.Size([1, 6144])
ğŸŸ© SwiGLUã‚’é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 6144])
ğŸŸ¦ ãƒãƒ£ãƒ³ã‚¯åŒ– x.shape=torch.Size([1, 3072]), y.shape=torch.Size([1, 3072])
ğŸŸ© SwiGLUã‚’é †ä¼æ’­å®Œäº† result.shape=torch.Size([1, 3072])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­é–‹å§‹ å…¥åŠ›å½¢çŠ¶: torch.Size([1, 3072])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­å®Œäº† å‡ºåŠ›å½¢çŠ¶: torch.Size([1, 1024])
ğŸŸ© Qwen3MLPã®é †ä¼æ’­å®Œäº† x.shape=torch.Size([1, 1024])
ğŸŸ© Qwen3DecoderLayerã®é †ä¼æ’­å®Œäº† hidden_states.shape=torch.Size([1, 1024])
ğŸŸ© Qwen3DecoderLayerã®é †ä¼æ’­ positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 1024]) residual is None=False
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 1024]), residual.shape if residual is not None else None=torch.Size([1, 1024])
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨å®Œäº†
ğŸŸ© Qwen3Attentionã®é †ä¼æ’­ positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­å®Œäº† result.shape=torch.Size([1, 4096])
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 16, 128]), residual.shape if residual is not None else None=None
ğŸŸ© æ­£è¦åŒ–é–‹å§‹ x.shape=torch.Size([1, 16, 128])
ğŸŸ© æ­£è¦åŒ–å®Œäº† x.shape=torch.Size([1, 16, 128])
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 8, 128]), residual.shape if residual is not None else None=None
ğŸŸ© æ­£è¦åŒ–é–‹å§‹ x.shape=torch.Size([1, 8, 128])
ğŸŸ© æ­£è¦åŒ–å®Œäº† x.shape=torch.Size([1, 8, 128])
ğŸŸ© RoPEã®é †ä¼æ’­é–‹å§‹ positions.shape=torch.Size([1]), query.shape=torch.Size([1, 16, 128]), key.shape=torch.Size([1, 8, 128])
ğŸŸ© RoPEã‚’é©ç”¨ x.shape=torch.Size([1, 16, 128]), cos.shape=torch.Size([1, 1, 64]), sin.shape=torch.Size([1, 1, 64])
ğŸŸ¦ ãƒãƒ£ãƒ³ã‚¯åŒ– x1.shape=torch.Size([1, 16, 64]), x2.shape=torch.Size([1, 16, 64])
ğŸŸ¦ å›è»¢è¡Œåˆ—é©ç”¨ y1.shape=torch.Size([1, 16, 64]), y2.shape=torch.Size([1, 16, 64])
ğŸŸ¦ result.shape=torch.Size([1, 16, 128])
ğŸŸ© RoPEã‚’é©ç”¨ x.shape=torch.Size([1, 8, 128]), cos.shape=torch.Size([1, 1, 64]), sin.shape=torch.Size([1, 1, 64])
ğŸŸ¦ ãƒãƒ£ãƒ³ã‚¯åŒ– x1.shape=torch.Size([1, 8, 64]), x2.shape=torch.Size([1, 8, 64])
ğŸŸ¦ å›è»¢è¡Œåˆ—é©ç”¨ y1.shape=torch.Size([1, 8, 64]), y2.shape=torch.Size([1, 8, 64])
ğŸŸ¦ result.shape=torch.Size([1, 8, 128])
ğŸŸ© RoPEã®é †ä¼æ’­å®Œäº† query.shape=torch.Size([1, 16, 128]), key.shape=torch.Size([1, 8, 128])
ğŸŸ© Attentionå±¤ã®é †ä¼æ’­é–‹å§‹ q.shape=torch.Size([1, 16, 128]), k.shape=torch.Size([1, 8, 128]), v.shape=torch.Size([1, 8, 128])
ğŸŸ© ç¾åœ¨ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆå–å¾— _CONTEXT=Context(is_prefill=True, cu_seqlens_q=tensor([0, 1], device='cuda:0', dtype=torch.int32), cu_seqlens_k=tensor([0, 1], device='cuda:0', dtype=torch.int32), max_seqlen_q=1, max_seqlen_k=1, slot_mapping=tensor([256], device='cuda:0', dtype=torch.int32), context_lens=None, block_tables=None)
ğŸŸ¦ ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæƒ…å ±å–å¾— context.is_prefill=True, context.slot_mapping=tensor([256], device='cuda:0', dtype=torch.int32), context.max_seqlen_q=1, context.cu_seqlens_q=tensor([0, 1], device='cuda:0', dtype=torch.int32), context.max_seqlen_k=1, context.cu_seqlens_k=tensor([0, 1], device='cuda:0', dtype=torch.int32), context.context_lens=None, context.block_tables=None
ğŸŸ© KVã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ã‚­ãƒ¼ã¨ãƒãƒªãƒ¥ãƒ¼ã‚’ä¿å­˜ key.shape=torch.Size([1, 8, 128]), value.shape=torch.Size([1, 8, 128]), k_cache.shape=torch.Size([884, 256, 8, 128]), v_cache.shape=torch.Size([884, 256, 8, 128]), slot_mapping.shape=torch.Size([1])
ğŸŸ¦ Prefillãƒ¢ãƒ¼ãƒ‰ã§FlashAttentionã‚’å®Ÿè¡Œ context.max_seqlen_q=1, context.cu_seqlens_q=tensor([0, 1], device='cuda:0', dtype=torch.int32), context.max_seqlen_k=1, context.cu_seqlens_k=tensor([0, 1], device='cuda:0', dtype=torch.int32)
ğŸŸ© Attentionå±¤ã®é †ä¼æ’­å®Œäº† o.shape=torch.Size([1, 16, 128])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­é–‹å§‹ å…¥åŠ›å½¢çŠ¶: torch.Size([1, 2048])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­å®Œäº† å‡ºåŠ›å½¢çŠ¶: torch.Size([1, 1024])
ğŸŸ© Qwen3Attentionã®é †ä¼æ’­å®Œäº† output.shape=torch.Size([1, 1024])
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 1024]), residual.shape if residual is not None else None=torch.Size([1, 1024])
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨å®Œäº†
ğŸŸ© Qwen3MLPã®é †ä¼æ’­ x.shape=torch.Size([1, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­å®Œäº† result.shape=torch.Size([1, 6144])
ğŸŸ© SwiGLUã‚’é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 6144])
ğŸŸ¦ ãƒãƒ£ãƒ³ã‚¯åŒ– x.shape=torch.Size([1, 3072]), y.shape=torch.Size([1, 3072])
ğŸŸ© SwiGLUã‚’é †ä¼æ’­å®Œäº† result.shape=torch.Size([1, 3072])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­é–‹å§‹ å…¥åŠ›å½¢çŠ¶: torch.Size([1, 3072])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­å®Œäº† å‡ºåŠ›å½¢çŠ¶: torch.Size([1, 1024])
ğŸŸ© Qwen3MLPã®é †ä¼æ’­å®Œäº† x.shape=torch.Size([1, 1024])
ğŸŸ© Qwen3DecoderLayerã®é †ä¼æ’­å®Œäº† hidden_states.shape=torch.Size([1, 1024])
ğŸŸ© Qwen3DecoderLayerã®é †ä¼æ’­ positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 1024]) residual is None=False
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 1024]), residual.shape if residual is not None else None=torch.Size([1, 1024])
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨å®Œäº†
ğŸŸ© Qwen3Attentionã®é †ä¼æ’­ positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­å®Œäº† result.shape=torch.Size([1, 4096])
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 16, 128]), residual.shape if residual is not None else None=None
ğŸŸ© æ­£è¦åŒ–é–‹å§‹ x.shape=torch.Size([1, 16, 128])
ğŸŸ© æ­£è¦åŒ–å®Œäº† x.shape=torch.Size([1, 16, 128])
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 8, 128]), residual.shape if residual is not None else None=None
ğŸŸ© æ­£è¦åŒ–é–‹å§‹ x.shape=torch.Size([1, 8, 128])
ğŸŸ© æ­£è¦åŒ–å®Œäº† x.shape=torch.Size([1, 8, 128])
ğŸŸ© RoPEã®é †ä¼æ’­é–‹å§‹ positions.shape=torch.Size([1]), query.shape=torch.Size([1, 16, 128]), key.shape=torch.Size([1, 8, 128])
ğŸŸ© RoPEã‚’é©ç”¨ x.shape=torch.Size([1, 16, 128]), cos.shape=torch.Size([1, 1, 64]), sin.shape=torch.Size([1, 1, 64])
ğŸŸ¦ ãƒãƒ£ãƒ³ã‚¯åŒ– x1.shape=torch.Size([1, 16, 64]), x2.shape=torch.Size([1, 16, 64])
ğŸŸ¦ å›è»¢è¡Œåˆ—é©ç”¨ y1.shape=torch.Size([1, 16, 64]), y2.shape=torch.Size([1, 16, 64])
ğŸŸ¦ result.shape=torch.Size([1, 16, 128])
ğŸŸ© RoPEã‚’é©ç”¨ x.shape=torch.Size([1, 8, 128]), cos.shape=torch.Size([1, 1, 64]), sin.shape=torch.Size([1, 1, 64])
ğŸŸ¦ ãƒãƒ£ãƒ³ã‚¯åŒ– x1.shape=torch.Size([1, 8, 64]), x2.shape=torch.Size([1, 8, 64])
ğŸŸ¦ å›è»¢è¡Œåˆ—é©ç”¨ y1.shape=torch.Size([1, 8, 64]), y2.shape=torch.Size([1, 8, 64])
ğŸŸ¦ result.shape=torch.Size([1, 8, 128])
ğŸŸ© RoPEã®é †ä¼æ’­å®Œäº† query.shape=torch.Size([1, 16, 128]), key.shape=torch.Size([1, 8, 128])
ğŸŸ© Attentionå±¤ã®é †ä¼æ’­é–‹å§‹ q.shape=torch.Size([1, 16, 128]), k.shape=torch.Size([1, 8, 128]), v.shape=torch.Size([1, 8, 128])
ğŸŸ© ç¾åœ¨ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆå–å¾— _CONTEXT=Context(is_prefill=True, cu_seqlens_q=tensor([0, 1], device='cuda:0', dtype=torch.int32), cu_seqlens_k=tensor([0, 1], device='cuda:0', dtype=torch.int32), max_seqlen_q=1, max_seqlen_k=1, slot_mapping=tensor([256], device='cuda:0', dtype=torch.int32), context_lens=None, block_tables=None)
ğŸŸ¦ ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæƒ…å ±å–å¾— context.is_prefill=True, context.slot_mapping=tensor([256], device='cuda:0', dtype=torch.int32), context.max_seqlen_q=1, context.cu_seqlens_q=tensor([0, 1], device='cuda:0', dtype=torch.int32), context.max_seqlen_k=1, context.cu_seqlens_k=tensor([0, 1], device='cuda:0', dtype=torch.int32), context.context_lens=None, context.block_tables=None
ğŸŸ© KVã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ã‚­ãƒ¼ã¨ãƒãƒªãƒ¥ãƒ¼ã‚’ä¿å­˜ key.shape=torch.Size([1, 8, 128]), value.shape=torch.Size([1, 8, 128]), k_cache.shape=torch.Size([884, 256, 8, 128]), v_cache.shape=torch.Size([884, 256, 8, 128]), slot_mapping.shape=torch.Size([1])
ğŸŸ¦ Prefillãƒ¢ãƒ¼ãƒ‰ã§FlashAttentionã‚’å®Ÿè¡Œ context.max_seqlen_q=1, context.cu_seqlens_q=tensor([0, 1], device='cuda:0', dtype=torch.int32), context.max_seqlen_k=1, context.cu_seqlens_k=tensor([0, 1], device='cuda:0', dtype=torch.int32)
ğŸŸ© Attentionå±¤ã®é †ä¼æ’­å®Œäº† o.shape=torch.Size([1, 16, 128])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­é–‹å§‹ å…¥åŠ›å½¢çŠ¶: torch.Size([1, 2048])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­å®Œäº† å‡ºåŠ›å½¢çŠ¶: torch.Size([1, 1024])
ğŸŸ© Qwen3Attentionã®é †ä¼æ’­å®Œäº† output.shape=torch.Size([1, 1024])
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 1024]), residual.shape if residual is not None else None=torch.Size([1, 1024])
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨å®Œäº†
ğŸŸ© Qwen3MLPã®é †ä¼æ’­ x.shape=torch.Size([1, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­å®Œäº† result.shape=torch.Size([1, 6144])
ğŸŸ© SwiGLUã‚’é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 6144])
ğŸŸ¦ ãƒãƒ£ãƒ³ã‚¯åŒ– x.shape=torch.Size([1, 3072]), y.shape=torch.Size([1, 3072])
ğŸŸ© SwiGLUã‚’é †ä¼æ’­å®Œäº† result.shape=torch.Size([1, 3072])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­é–‹å§‹ å…¥åŠ›å½¢çŠ¶: torch.Size([1, 3072])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­å®Œäº† å‡ºåŠ›å½¢çŠ¶: torch.Size([1, 1024])
ğŸŸ© Qwen3MLPã®é †ä¼æ’­å®Œäº† x.shape=torch.Size([1, 1024])
ğŸŸ© Qwen3DecoderLayerã®é †ä¼æ’­å®Œäº† hidden_states.shape=torch.Size([1, 1024])
ğŸŸ© Qwen3DecoderLayerã®é †ä¼æ’­ positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 1024]) residual is None=False
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 1024]), residual.shape if residual is not None else None=torch.Size([1, 1024])
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨å®Œäº†
ğŸŸ© Qwen3Attentionã®é †ä¼æ’­ positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­å®Œäº† result.shape=torch.Size([1, 4096])
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 16, 128]), residual.shape if residual is not None else None=None
ğŸŸ© æ­£è¦åŒ–é–‹å§‹ x.shape=torch.Size([1, 16, 128])
ğŸŸ© æ­£è¦åŒ–å®Œäº† x.shape=torch.Size([1, 16, 128])
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 8, 128]), residual.shape if residual is not None else None=None
ğŸŸ© æ­£è¦åŒ–é–‹å§‹ x.shape=torch.Size([1, 8, 128])
ğŸŸ© æ­£è¦åŒ–å®Œäº† x.shape=torch.Size([1, 8, 128])
ğŸŸ© RoPEã®é †ä¼æ’­é–‹å§‹ positions.shape=torch.Size([1]), query.shape=torch.Size([1, 16, 128]), key.shape=torch.Size([1, 8, 128])
ğŸŸ© RoPEã‚’é©ç”¨ x.shape=torch.Size([1, 16, 128]), cos.shape=torch.Size([1, 1, 64]), sin.shape=torch.Size([1, 1, 64])
ğŸŸ¦ ãƒãƒ£ãƒ³ã‚¯åŒ– x1.shape=torch.Size([1, 16, 64]), x2.shape=torch.Size([1, 16, 64])
ğŸŸ¦ å›è»¢è¡Œåˆ—é©ç”¨ y1.shape=torch.Size([1, 16, 64]), y2.shape=torch.Size([1, 16, 64])
ğŸŸ¦ result.shape=torch.Size([1, 16, 128])
ğŸŸ© RoPEã‚’é©ç”¨ x.shape=torch.Size([1, 8, 128]), cos.shape=torch.Size([1, 1, 64]), sin.shape=torch.Size([1, 1, 64])
ğŸŸ¦ ãƒãƒ£ãƒ³ã‚¯åŒ– x1.shape=torch.Size([1, 8, 64]), x2.shape=torch.Size([1, 8, 64])
ğŸŸ¦ å›è»¢è¡Œåˆ—é©ç”¨ y1.shape=torch.Size([1, 8, 64]), y2.shape=torch.Size([1, 8, 64])
ğŸŸ¦ result.shape=torch.Size([1, 8, 128])
ğŸŸ© RoPEã®é †ä¼æ’­å®Œäº† query.shape=torch.Size([1, 16, 128]), key.shape=torch.Size([1, 8, 128])
ğŸŸ© Attentionå±¤ã®é †ä¼æ’­é–‹å§‹ q.shape=torch.Size([1, 16, 128]), k.shape=torch.Size([1, 8, 128]), v.shape=torch.Size([1, 8, 128])
ğŸŸ© ç¾åœ¨ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆå–å¾— _CONTEXT=Context(is_prefill=True, cu_seqlens_q=tensor([0, 1], device='cuda:0', dtype=torch.int32), cu_seqlens_k=tensor([0, 1], device='cuda:0', dtype=torch.int32), max_seqlen_q=1, max_seqlen_k=1, slot_mapping=tensor([256], device='cuda:0', dtype=torch.int32), context_lens=None, block_tables=None)
ğŸŸ¦ ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæƒ…å ±å–å¾— context.is_prefill=True, context.slot_mapping=tensor([256], device='cuda:0', dtype=torch.int32), context.max_seqlen_q=1, context.cu_seqlens_q=tensor([0, 1], device='cuda:0', dtype=torch.int32), context.max_seqlen_k=1, context.cu_seqlens_k=tensor([0, 1], device='cuda:0', dtype=torch.int32), context.context_lens=None, context.block_tables=None
ğŸŸ© KVã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ã‚­ãƒ¼ã¨ãƒãƒªãƒ¥ãƒ¼ã‚’ä¿å­˜ key.shape=torch.Size([1, 8, 128]), value.shape=torch.Size([1, 8, 128]), k_cache.shape=torch.Size([884, 256, 8, 128]), v_cache.shape=torch.Size([884, 256, 8, 128]), slot_mapping.shape=torch.Size([1])
ğŸŸ¦ Prefillãƒ¢ãƒ¼ãƒ‰ã§FlashAttentionã‚’å®Ÿè¡Œ context.max_seqlen_q=1, context.cu_seqlens_q=tensor([0, 1], device='cuda:0', dtype=torch.int32), context.max_seqlen_k=1, context.cu_seqlens_k=tensor([0, 1], device='cuda:0', dtype=torch.int32)
ğŸŸ© Attentionå±¤ã®é †ä¼æ’­å®Œäº† o.shape=torch.Size([1, 16, 128])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­é–‹å§‹ å…¥åŠ›å½¢çŠ¶: torch.Size([1, 2048])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­å®Œäº† å‡ºåŠ›å½¢çŠ¶: torch.Size([1, 1024])
ğŸŸ© Qwen3Attentionã®é †ä¼æ’­å®Œäº† output.shape=torch.Size([1, 1024])
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 1024]), residual.shape if residual is not None else None=torch.Size([1, 1024])
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨å®Œäº†
ğŸŸ© Qwen3MLPã®é †ä¼æ’­ x.shape=torch.Size([1, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­å®Œäº† result.shape=torch.Size([1, 6144])
ğŸŸ© SwiGLUã‚’é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 6144])
ğŸŸ¦ ãƒãƒ£ãƒ³ã‚¯åŒ– x.shape=torch.Size([1, 3072]), y.shape=torch.Size([1, 3072])
ğŸŸ© SwiGLUã‚’é †ä¼æ’­å®Œäº† result.shape=torch.Size([1, 3072])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­é–‹å§‹ å…¥åŠ›å½¢çŠ¶: torch.Size([1, 3072])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­å®Œäº† å‡ºåŠ›å½¢çŠ¶: torch.Size([1, 1024])
ğŸŸ© Qwen3MLPã®é †ä¼æ’­å®Œäº† x.shape=torch.Size([1, 1024])
ğŸŸ© Qwen3DecoderLayerã®é †ä¼æ’­å®Œäº† hidden_states.shape=torch.Size([1, 1024])
ğŸŸ© Qwen3DecoderLayerã®é †ä¼æ’­ positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 1024]) residual is None=False
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 1024]), residual.shape if residual is not None else None=torch.Size([1, 1024])
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨å®Œäº†
ğŸŸ© Qwen3Attentionã®é †ä¼æ’­ positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­å®Œäº† result.shape=torch.Size([1, 4096])
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 16, 128]), residual.shape if residual is not None else None=None
ğŸŸ© æ­£è¦åŒ–é–‹å§‹ x.shape=torch.Size([1, 16, 128])
ğŸŸ© æ­£è¦åŒ–å®Œäº† x.shape=torch.Size([1, 16, 128])
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 8, 128]), residual.shape if residual is not None else None=None
ğŸŸ© æ­£è¦åŒ–é–‹å§‹ x.shape=torch.Size([1, 8, 128])
ğŸŸ© æ­£è¦åŒ–å®Œäº† x.shape=torch.Size([1, 8, 128])
ğŸŸ© RoPEã®é †ä¼æ’­é–‹å§‹ positions.shape=torch.Size([1]), query.shape=torch.Size([1, 16, 128]), key.shape=torch.Size([1, 8, 128])
ğŸŸ© RoPEã‚’é©ç”¨ x.shape=torch.Size([1, 16, 128]), cos.shape=torch.Size([1, 1, 64]), sin.shape=torch.Size([1, 1, 64])
ğŸŸ¦ ãƒãƒ£ãƒ³ã‚¯åŒ– x1.shape=torch.Size([1, 16, 64]), x2.shape=torch.Size([1, 16, 64])
ğŸŸ¦ å›è»¢è¡Œåˆ—é©ç”¨ y1.shape=torch.Size([1, 16, 64]), y2.shape=torch.Size([1, 16, 64])
ğŸŸ¦ result.shape=torch.Size([1, 16, 128])
ğŸŸ© RoPEã‚’é©ç”¨ x.shape=torch.Size([1, 8, 128]), cos.shape=torch.Size([1, 1, 64]), sin.shape=torch.Size([1, 1, 64])
ğŸŸ¦ ãƒãƒ£ãƒ³ã‚¯åŒ– x1.shape=torch.Size([1, 8, 64]), x2.shape=torch.Size([1, 8, 64])
ğŸŸ¦ å›è»¢è¡Œåˆ—é©ç”¨ y1.shape=torch.Size([1, 8, 64]), y2.shape=torch.Size([1, 8, 64])
ğŸŸ¦ result.shape=torch.Size([1, 8, 128])
ğŸŸ© RoPEã®é †ä¼æ’­å®Œäº† query.shape=torch.Size([1, 16, 128]), key.shape=torch.Size([1, 8, 128])
ğŸŸ© Attentionå±¤ã®é †ä¼æ’­é–‹å§‹ q.shape=torch.Size([1, 16, 128]), k.shape=torch.Size([1, 8, 128]), v.shape=torch.Size([1, 8, 128])
ğŸŸ© ç¾åœ¨ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆå–å¾— _CONTEXT=Context(is_prefill=True, cu_seqlens_q=tensor([0, 1], device='cuda:0', dtype=torch.int32), cu_seqlens_k=tensor([0, 1], device='cuda:0', dtype=torch.int32), max_seqlen_q=1, max_seqlen_k=1, slot_mapping=tensor([256], device='cuda:0', dtype=torch.int32), context_lens=None, block_tables=None)
ğŸŸ¦ ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæƒ…å ±å–å¾— context.is_prefill=True, context.slot_mapping=tensor([256], device='cuda:0', dtype=torch.int32), context.max_seqlen_q=1, context.cu_seqlens_q=tensor([0, 1], device='cuda:0', dtype=torch.int32), context.max_seqlen_k=1, context.cu_seqlens_k=tensor([0, 1], device='cuda:0', dtype=torch.int32), context.context_lens=None, context.block_tables=None
ğŸŸ© KVã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ã‚­ãƒ¼ã¨ãƒãƒªãƒ¥ãƒ¼ã‚’ä¿å­˜ key.shape=torch.Size([1, 8, 128]), value.shape=torch.Size([1, 8, 128]), k_cache.shape=torch.Size([884, 256, 8, 128]), v_cache.shape=torch.Size([884, 256, 8, 128]), slot_mapping.shape=torch.Size([1])
ğŸŸ¦ Prefillãƒ¢ãƒ¼ãƒ‰ã§FlashAttentionã‚’å®Ÿè¡Œ context.max_seqlen_q=1, context.cu_seqlens_q=tensor([0, 1], device='cuda:0', dtype=torch.int32), context.max_seqlen_k=1, context.cu_seqlens_k=tensor([0, 1], device='cuda:0', dtype=torch.int32)
ğŸŸ© Attentionå±¤ã®é †ä¼æ’­å®Œäº† o.shape=torch.Size([1, 16, 128])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­é–‹å§‹ å…¥åŠ›å½¢çŠ¶: torch.Size([1, 2048])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­å®Œäº† å‡ºåŠ›å½¢çŠ¶: torch.Size([1, 1024])
ğŸŸ© Qwen3Attentionã®é †ä¼æ’­å®Œäº† output.shape=torch.Size([1, 1024])
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 1024]), residual.shape if residual is not None else None=torch.Size([1, 1024])
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨å®Œäº†
ğŸŸ© Qwen3MLPã®é †ä¼æ’­ x.shape=torch.Size([1, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­å®Œäº† result.shape=torch.Size([1, 6144])
ğŸŸ© SwiGLUã‚’é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 6144])
ğŸŸ¦ ãƒãƒ£ãƒ³ã‚¯åŒ– x.shape=torch.Size([1, 3072]), y.shape=torch.Size([1, 3072])
ğŸŸ© SwiGLUã‚’é †ä¼æ’­å®Œäº† result.shape=torch.Size([1, 3072])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­é–‹å§‹ å…¥åŠ›å½¢çŠ¶: torch.Size([1, 3072])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­å®Œäº† å‡ºåŠ›å½¢çŠ¶: torch.Size([1, 1024])
ğŸŸ© Qwen3MLPã®é †ä¼æ’­å®Œäº† x.shape=torch.Size([1, 1024])
ğŸŸ© Qwen3DecoderLayerã®é †ä¼æ’­å®Œäº† hidden_states.shape=torch.Size([1, 1024])
ğŸŸ© Qwen3DecoderLayerã®é †ä¼æ’­ positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 1024]) residual is None=False
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 1024]), residual.shape if residual is not None else None=torch.Size([1, 1024])
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨å®Œäº†
ğŸŸ© Qwen3Attentionã®é †ä¼æ’­ positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­å®Œäº† result.shape=torch.Size([1, 4096])
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 16, 128]), residual.shape if residual is not None else None=None
ğŸŸ© æ­£è¦åŒ–é–‹å§‹ x.shape=torch.Size([1, 16, 128])
ğŸŸ© æ­£è¦åŒ–å®Œäº† x.shape=torch.Size([1, 16, 128])
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 8, 128]), residual.shape if residual is not None else None=None
ğŸŸ© æ­£è¦åŒ–é–‹å§‹ x.shape=torch.Size([1, 8, 128])
ğŸŸ© æ­£è¦åŒ–å®Œäº† x.shape=torch.Size([1, 8, 128])
ğŸŸ© RoPEã®é †ä¼æ’­é–‹å§‹ positions.shape=torch.Size([1]), query.shape=torch.Size([1, 16, 128]), key.shape=torch.Size([1, 8, 128])
ğŸŸ© RoPEã‚’é©ç”¨ x.shape=torch.Size([1, 16, 128]), cos.shape=torch.Size([1, 1, 64]), sin.shape=torch.Size([1, 1, 64])
ğŸŸ¦ ãƒãƒ£ãƒ³ã‚¯åŒ– x1.shape=torch.Size([1, 16, 64]), x2.shape=torch.Size([1, 16, 64])
ğŸŸ¦ å›è»¢è¡Œåˆ—é©ç”¨ y1.shape=torch.Size([1, 16, 64]), y2.shape=torch.Size([1, 16, 64])
ğŸŸ¦ result.shape=torch.Size([1, 16, 128])
ğŸŸ© RoPEã‚’é©ç”¨ x.shape=torch.Size([1, 8, 128]), cos.shape=torch.Size([1, 1, 64]), sin.shape=torch.Size([1, 1, 64])
ğŸŸ¦ ãƒãƒ£ãƒ³ã‚¯åŒ– x1.shape=torch.Size([1, 8, 64]), x2.shape=torch.Size([1, 8, 64])
ğŸŸ¦ å›è»¢è¡Œåˆ—é©ç”¨ y1.shape=torch.Size([1, 8, 64]), y2.shape=torch.Size([1, 8, 64])
ğŸŸ¦ result.shape=torch.Size([1, 8, 128])
ğŸŸ© RoPEã®é †ä¼æ’­å®Œäº† query.shape=torch.Size([1, 16, 128]), key.shape=torch.Size([1, 8, 128])
ğŸŸ© Attentionå±¤ã®é †ä¼æ’­é–‹å§‹ q.shape=torch.Size([1, 16, 128]), k.shape=torch.Size([1, 8, 128]), v.shape=torch.Size([1, 8, 128])
ğŸŸ© ç¾åœ¨ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆå–å¾— _CONTEXT=Context(is_prefill=True, cu_seqlens_q=tensor([0, 1], device='cuda:0', dtype=torch.int32), cu_seqlens_k=tensor([0, 1], device='cuda:0', dtype=torch.int32), max_seqlen_q=1, max_seqlen_k=1, slot_mapping=tensor([256], device='cuda:0', dtype=torch.int32), context_lens=None, block_tables=None)
ğŸŸ¦ ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæƒ…å ±å–å¾— context.is_prefill=True, context.slot_mapping=tensor([256], device='cuda:0', dtype=torch.int32), context.max_seqlen_q=1, context.cu_seqlens_q=tensor([0, 1], device='cuda:0', dtype=torch.int32), context.max_seqlen_k=1, context.cu_seqlens_k=tensor([0, 1], device='cuda:0', dtype=torch.int32), context.context_lens=None, context.block_tables=None
ğŸŸ© KVã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ã‚­ãƒ¼ã¨ãƒãƒªãƒ¥ãƒ¼ã‚’ä¿å­˜ key.shape=torch.Size([1, 8, 128]), value.shape=torch.Size([1, 8, 128]), k_cache.shape=torch.Size([884, 256, 8, 128]), v_cache.shape=torch.Size([884, 256, 8, 128]), slot_mapping.shape=torch.Size([1])
ğŸŸ¦ Prefillãƒ¢ãƒ¼ãƒ‰ã§FlashAttentionã‚’å®Ÿè¡Œ context.max_seqlen_q=1, context.cu_seqlens_q=tensor([0, 1], device='cuda:0', dtype=torch.int32), context.max_seqlen_k=1, context.cu_seqlens_k=tensor([0, 1], device='cuda:0', dtype=torch.int32)
ğŸŸ© Attentionå±¤ã®é †ä¼æ’­å®Œäº† o.shape=torch.Size([1, 16, 128])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­é–‹å§‹ å…¥åŠ›å½¢çŠ¶: torch.Size([1, 2048])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­å®Œäº† å‡ºåŠ›å½¢çŠ¶: torch.Size([1, 1024])
ğŸŸ© Qwen3Attentionã®é †ä¼æ’­å®Œäº† output.shape=torch.Size([1, 1024])
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 1024]), residual.shape if residual is not None else None=torch.Size([1, 1024])
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨å®Œäº†
ğŸŸ© Qwen3MLPã®é †ä¼æ’­ x.shape=torch.Size([1, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­å®Œäº† result.shape=torch.Size([1, 6144])
ğŸŸ© SwiGLUã‚’é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 6144])
ğŸŸ¦ ãƒãƒ£ãƒ³ã‚¯åŒ– x.shape=torch.Size([1, 3072]), y.shape=torch.Size([1, 3072])
ğŸŸ© SwiGLUã‚’é †ä¼æ’­å®Œäº† result.shape=torch.Size([1, 3072])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­é–‹å§‹ å…¥åŠ›å½¢çŠ¶: torch.Size([1, 3072])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­å®Œäº† å‡ºåŠ›å½¢çŠ¶: torch.Size([1, 1024])
ğŸŸ© Qwen3MLPã®é †ä¼æ’­å®Œäº† x.shape=torch.Size([1, 1024])
ğŸŸ© Qwen3DecoderLayerã®é †ä¼æ’­å®Œäº† hidden_states.shape=torch.Size([1, 1024])
ğŸŸ© Qwen3DecoderLayerã®é †ä¼æ’­ positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 1024]) residual is None=False
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 1024]), residual.shape if residual is not None else None=torch.Size([1, 1024])
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨å®Œäº†
ğŸŸ© Qwen3Attentionã®é †ä¼æ’­ positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­å®Œäº† result.shape=torch.Size([1, 4096])
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 16, 128]), residual.shape if residual is not None else None=None
ğŸŸ© æ­£è¦åŒ–é–‹å§‹ x.shape=torch.Size([1, 16, 128])
ğŸŸ© æ­£è¦åŒ–å®Œäº† x.shape=torch.Size([1, 16, 128])
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 8, 128]), residual.shape if residual is not None else None=None
ğŸŸ© æ­£è¦åŒ–é–‹å§‹ x.shape=torch.Size([1, 8, 128])
ğŸŸ© æ­£è¦åŒ–å®Œäº† x.shape=torch.Size([1, 8, 128])
ğŸŸ© RoPEã®é †ä¼æ’­é–‹å§‹ positions.shape=torch.Size([1]), query.shape=torch.Size([1, 16, 128]), key.shape=torch.Size([1, 8, 128])
ğŸŸ© RoPEã‚’é©ç”¨ x.shape=torch.Size([1, 16, 128]), cos.shape=torch.Size([1, 1, 64]), sin.shape=torch.Size([1, 1, 64])
ğŸŸ¦ ãƒãƒ£ãƒ³ã‚¯åŒ– x1.shape=torch.Size([1, 16, 64]), x2.shape=torch.Size([1, 16, 64])
ğŸŸ¦ å›è»¢è¡Œåˆ—é©ç”¨ y1.shape=torch.Size([1, 16, 64]), y2.shape=torch.Size([1, 16, 64])
ğŸŸ¦ result.shape=torch.Size([1, 16, 128])
ğŸŸ© RoPEã‚’é©ç”¨ x.shape=torch.Size([1, 8, 128]), cos.shape=torch.Size([1, 1, 64]), sin.shape=torch.Size([1, 1, 64])
ğŸŸ¦ ãƒãƒ£ãƒ³ã‚¯åŒ– x1.shape=torch.Size([1, 8, 64]), x2.shape=torch.Size([1, 8, 64])
ğŸŸ¦ å›è»¢è¡Œåˆ—é©ç”¨ y1.shape=torch.Size([1, 8, 64]), y2.shape=torch.Size([1, 8, 64])
ğŸŸ¦ result.shape=torch.Size([1, 8, 128])
ğŸŸ© RoPEã®é †ä¼æ’­å®Œäº† query.shape=torch.Size([1, 16, 128]), key.shape=torch.Size([1, 8, 128])
ğŸŸ© Attentionå±¤ã®é †ä¼æ’­é–‹å§‹ q.shape=torch.Size([1, 16, 128]), k.shape=torch.Size([1, 8, 128]), v.shape=torch.Size([1, 8, 128])
ğŸŸ© ç¾åœ¨ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆå–å¾— _CONTEXT=Context(is_prefill=True, cu_seqlens_q=tensor([0, 1], device='cuda:0', dtype=torch.int32), cu_seqlens_k=tensor([0, 1], device='cuda:0', dtype=torch.int32), max_seqlen_q=1, max_seqlen_k=1, slot_mapping=tensor([256], device='cuda:0', dtype=torch.int32), context_lens=None, block_tables=None)
ğŸŸ¦ ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæƒ…å ±å–å¾— context.is_prefill=True, context.slot_mapping=tensor([256], device='cuda:0', dtype=torch.int32), context.max_seqlen_q=1, context.cu_seqlens_q=tensor([0, 1], device='cuda:0', dtype=torch.int32), context.max_seqlen_k=1, context.cu_seqlens_k=tensor([0, 1], device='cuda:0', dtype=torch.int32), context.context_lens=None, context.block_tables=None
ğŸŸ© KVã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ã‚­ãƒ¼ã¨ãƒãƒªãƒ¥ãƒ¼ã‚’ä¿å­˜ key.shape=torch.Size([1, 8, 128]), value.shape=torch.Size([1, 8, 128]), k_cache.shape=torch.Size([884, 256, 8, 128]), v_cache.shape=torch.Size([884, 256, 8, 128]), slot_mapping.shape=torch.Size([1])
ğŸŸ¦ Prefillãƒ¢ãƒ¼ãƒ‰ã§FlashAttentionã‚’å®Ÿè¡Œ context.max_seqlen_q=1, context.cu_seqlens_q=tensor([0, 1], device='cuda:0', dtype=torch.int32), context.max_seqlen_k=1, context.cu_seqlens_k=tensor([0, 1], device='cuda:0', dtype=torch.int32)
ğŸŸ© Attentionå±¤ã®é †ä¼æ’­å®Œäº† o.shape=torch.Size([1, 16, 128])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­é–‹å§‹ å…¥åŠ›å½¢çŠ¶: torch.Size([1, 2048])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­å®Œäº† å‡ºåŠ›å½¢çŠ¶: torch.Size([1, 1024])
ğŸŸ© Qwen3Attentionã®é †ä¼æ’­å®Œäº† output.shape=torch.Size([1, 1024])
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 1024]), residual.shape if residual is not None else None=torch.Size([1, 1024])
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨å®Œäº†
ğŸŸ© Qwen3MLPã®é †ä¼æ’­ x.shape=torch.Size([1, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­å®Œäº† result.shape=torch.Size([1, 6144])
ğŸŸ© SwiGLUã‚’é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 6144])
ğŸŸ¦ ãƒãƒ£ãƒ³ã‚¯åŒ– x.shape=torch.Size([1, 3072]), y.shape=torch.Size([1, 3072])
ğŸŸ© SwiGLUã‚’é †ä¼æ’­å®Œäº† result.shape=torch.Size([1, 3072])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­é–‹å§‹ å…¥åŠ›å½¢çŠ¶: torch.Size([1, 3072])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­å®Œäº† å‡ºåŠ›å½¢çŠ¶: torch.Size([1, 1024])
ğŸŸ© Qwen3MLPã®é †ä¼æ’­å®Œäº† x.shape=torch.Size([1, 1024])
ğŸŸ© Qwen3DecoderLayerã®é †ä¼æ’­å®Œäº† hidden_states.shape=torch.Size([1, 1024])
ğŸŸ© Qwen3DecoderLayerã®é †ä¼æ’­ positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 1024]) residual is None=False
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 1024]), residual.shape if residual is not None else None=torch.Size([1, 1024])
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨å®Œäº†
ğŸŸ© Qwen3Attentionã®é †ä¼æ’­ positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­å®Œäº† result.shape=torch.Size([1, 4096])
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 16, 128]), residual.shape if residual is not None else None=None
ğŸŸ© æ­£è¦åŒ–é–‹å§‹ x.shape=torch.Size([1, 16, 128])
ğŸŸ© æ­£è¦åŒ–å®Œäº† x.shape=torch.Size([1, 16, 128])
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 8, 128]), residual.shape if residual is not None else None=None
ğŸŸ© æ­£è¦åŒ–é–‹å§‹ x.shape=torch.Size([1, 8, 128])
ğŸŸ© æ­£è¦åŒ–å®Œäº† x.shape=torch.Size([1, 8, 128])
ğŸŸ© RoPEã®é †ä¼æ’­é–‹å§‹ positions.shape=torch.Size([1]), query.shape=torch.Size([1, 16, 128]), key.shape=torch.Size([1, 8, 128])
ğŸŸ© RoPEã‚’é©ç”¨ x.shape=torch.Size([1, 16, 128]), cos.shape=torch.Size([1, 1, 64]), sin.shape=torch.Size([1, 1, 64])
ğŸŸ¦ ãƒãƒ£ãƒ³ã‚¯åŒ– x1.shape=torch.Size([1, 16, 64]), x2.shape=torch.Size([1, 16, 64])
ğŸŸ¦ å›è»¢è¡Œåˆ—é©ç”¨ y1.shape=torch.Size([1, 16, 64]), y2.shape=torch.Size([1, 16, 64])
ğŸŸ¦ result.shape=torch.Size([1, 16, 128])
ğŸŸ© RoPEã‚’é©ç”¨ x.shape=torch.Size([1, 8, 128]), cos.shape=torch.Size([1, 1, 64]), sin.shape=torch.Size([1, 1, 64])
ğŸŸ¦ ãƒãƒ£ãƒ³ã‚¯åŒ– x1.shape=torch.Size([1, 8, 64]), x2.shape=torch.Size([1, 8, 64])
ğŸŸ¦ å›è»¢è¡Œåˆ—é©ç”¨ y1.shape=torch.Size([1, 8, 64]), y2.shape=torch.Size([1, 8, 64])
ğŸŸ¦ result.shape=torch.Size([1, 8, 128])
ğŸŸ© RoPEã®é †ä¼æ’­å®Œäº† query.shape=torch.Size([1, 16, 128]), key.shape=torch.Size([1, 8, 128])
ğŸŸ© Attentionå±¤ã®é †ä¼æ’­é–‹å§‹ q.shape=torch.Size([1, 16, 128]), k.shape=torch.Size([1, 8, 128]), v.shape=torch.Size([1, 8, 128])
ğŸŸ© ç¾åœ¨ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆå–å¾— _CONTEXT=Context(is_prefill=True, cu_seqlens_q=tensor([0, 1], device='cuda:0', dtype=torch.int32), cu_seqlens_k=tensor([0, 1], device='cuda:0', dtype=torch.int32), max_seqlen_q=1, max_seqlen_k=1, slot_mapping=tensor([256], device='cuda:0', dtype=torch.int32), context_lens=None, block_tables=None)
ğŸŸ¦ ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæƒ…å ±å–å¾— context.is_prefill=True, context.slot_mapping=tensor([256], device='cuda:0', dtype=torch.int32), context.max_seqlen_q=1, context.cu_seqlens_q=tensor([0, 1], device='cuda:0', dtype=torch.int32), context.max_seqlen_k=1, context.cu_seqlens_k=tensor([0, 1], device='cuda:0', dtype=torch.int32), context.context_lens=None, context.block_tables=None
ğŸŸ© KVã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ã‚­ãƒ¼ã¨ãƒãƒªãƒ¥ãƒ¼ã‚’ä¿å­˜ key.shape=torch.Size([1, 8, 128]), value.shape=torch.Size([1, 8, 128]), k_cache.shape=torch.Size([884, 256, 8, 128]), v_cache.shape=torch.Size([884, 256, 8, 128]), slot_mapping.shape=torch.Size([1])
ğŸŸ¦ Prefillãƒ¢ãƒ¼ãƒ‰ã§FlashAttentionã‚’å®Ÿè¡Œ context.max_seqlen_q=1, context.cu_seqlens_q=tensor([0, 1], device='cuda:0', dtype=torch.int32), context.max_seqlen_k=1, context.cu_seqlens_k=tensor([0, 1], device='cuda:0', dtype=torch.int32)
ğŸŸ© Attentionå±¤ã®é †ä¼æ’­å®Œäº† o.shape=torch.Size([1, 16, 128])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­é–‹å§‹ å…¥åŠ›å½¢çŠ¶: torch.Size([1, 2048])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­å®Œäº† å‡ºåŠ›å½¢çŠ¶: torch.Size([1, 1024])
ğŸŸ© Qwen3Attentionã®é †ä¼æ’­å®Œäº† output.shape=torch.Size([1, 1024])
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 1024]), residual.shape if residual is not None else None=torch.Size([1, 1024])
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨å®Œäº†
ğŸŸ© Qwen3MLPã®é †ä¼æ’­ x.shape=torch.Size([1, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­å®Œäº† result.shape=torch.Size([1, 6144])
ğŸŸ© SwiGLUã‚’é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 6144])
ğŸŸ¦ ãƒãƒ£ãƒ³ã‚¯åŒ– x.shape=torch.Size([1, 3072]), y.shape=torch.Size([1, 3072])
ğŸŸ© SwiGLUã‚’é †ä¼æ’­å®Œäº† result.shape=torch.Size([1, 3072])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­é–‹å§‹ å…¥åŠ›å½¢çŠ¶: torch.Size([1, 3072])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­å®Œäº† å‡ºåŠ›å½¢çŠ¶: torch.Size([1, 1024])
ğŸŸ© Qwen3MLPã®é †ä¼æ’­å®Œäº† x.shape=torch.Size([1, 1024])
ğŸŸ© Qwen3DecoderLayerã®é †ä¼æ’­å®Œäº† hidden_states.shape=torch.Size([1, 1024])
ğŸŸ© Qwen3DecoderLayerã®é †ä¼æ’­ positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 1024]) residual is None=False
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 1024]), residual.shape if residual is not None else None=torch.Size([1, 1024])
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨å®Œäº†
ğŸŸ© Qwen3Attentionã®é †ä¼æ’­ positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­å®Œäº† result.shape=torch.Size([1, 4096])
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 16, 128]), residual.shape if residual is not None else None=None
ğŸŸ© æ­£è¦åŒ–é–‹å§‹ x.shape=torch.Size([1, 16, 128])
ğŸŸ© æ­£è¦åŒ–å®Œäº† x.shape=torch.Size([1, 16, 128])
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 8, 128]), residual.shape if residual is not None else None=None
ğŸŸ© æ­£è¦åŒ–é–‹å§‹ x.shape=torch.Size([1, 8, 128])
ğŸŸ© æ­£è¦åŒ–å®Œäº† x.shape=torch.Size([1, 8, 128])
ğŸŸ© RoPEã®é †ä¼æ’­é–‹å§‹ positions.shape=torch.Size([1]), query.shape=torch.Size([1, 16, 128]), key.shape=torch.Size([1, 8, 128])
ğŸŸ© RoPEã‚’é©ç”¨ x.shape=torch.Size([1, 16, 128]), cos.shape=torch.Size([1, 1, 64]), sin.shape=torch.Size([1, 1, 64])
ğŸŸ¦ ãƒãƒ£ãƒ³ã‚¯åŒ– x1.shape=torch.Size([1, 16, 64]), x2.shape=torch.Size([1, 16, 64])
ğŸŸ¦ å›è»¢è¡Œåˆ—é©ç”¨ y1.shape=torch.Size([1, 16, 64]), y2.shape=torch.Size([1, 16, 64])
ğŸŸ¦ result.shape=torch.Size([1, 16, 128])
ğŸŸ© RoPEã‚’é©ç”¨ x.shape=torch.Size([1, 8, 128]), cos.shape=torch.Size([1, 1, 64]), sin.shape=torch.Size([1, 1, 64])
ğŸŸ¦ ãƒãƒ£ãƒ³ã‚¯åŒ– x1.shape=torch.Size([1, 8, 64]), x2.shape=torch.Size([1, 8, 64])
ğŸŸ¦ å›è»¢è¡Œåˆ—é©ç”¨ y1.shape=torch.Size([1, 8, 64]), y2.shape=torch.Size([1, 8, 64])
ğŸŸ¦ result.shape=torch.Size([1, 8, 128])
ğŸŸ© RoPEã®é †ä¼æ’­å®Œäº† query.shape=torch.Size([1, 16, 128]), key.shape=torch.Size([1, 8, 128])
ğŸŸ© Attentionå±¤ã®é †ä¼æ’­é–‹å§‹ q.shape=torch.Size([1, 16, 128]), k.shape=torch.Size([1, 8, 128]), v.shape=torch.Size([1, 8, 128])
ğŸŸ© ç¾åœ¨ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆå–å¾— _CONTEXT=Context(is_prefill=True, cu_seqlens_q=tensor([0, 1], device='cuda:0', dtype=torch.int32), cu_seqlens_k=tensor([0, 1], device='cuda:0', dtype=torch.int32), max_seqlen_q=1, max_seqlen_k=1, slot_mapping=tensor([256], device='cuda:0', dtype=torch.int32), context_lens=None, block_tables=None)
ğŸŸ¦ ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæƒ…å ±å–å¾— context.is_prefill=True, context.slot_mapping=tensor([256], device='cuda:0', dtype=torch.int32), context.max_seqlen_q=1, context.cu_seqlens_q=tensor([0, 1], device='cuda:0', dtype=torch.int32), context.max_seqlen_k=1, context.cu_seqlens_k=tensor([0, 1], device='cuda:0', dtype=torch.int32), context.context_lens=None, context.block_tables=None
ğŸŸ© KVã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ã‚­ãƒ¼ã¨ãƒãƒªãƒ¥ãƒ¼ã‚’ä¿å­˜ key.shape=torch.Size([1, 8, 128]), value.shape=torch.Size([1, 8, 128]), k_cache.shape=torch.Size([884, 256, 8, 128]), v_cache.shape=torch.Size([884, 256, 8, 128]), slot_mapping.shape=torch.Size([1])
ğŸŸ¦ Prefillãƒ¢ãƒ¼ãƒ‰ã§FlashAttentionã‚’å®Ÿè¡Œ context.max_seqlen_q=1, context.cu_seqlens_q=tensor([0, 1], device='cuda:0', dtype=torch.int32), context.max_seqlen_k=1, context.cu_seqlens_k=tensor([0, 1], device='cuda:0', dtype=torch.int32)
ğŸŸ© Attentionå±¤ã®é †ä¼æ’­å®Œäº† o.shape=torch.Size([1, 16, 128])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­é–‹å§‹ å…¥åŠ›å½¢çŠ¶: torch.Size([1, 2048])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­å®Œäº† å‡ºåŠ›å½¢çŠ¶: torch.Size([1, 1024])
ğŸŸ© Qwen3Attentionã®é †ä¼æ’­å®Œäº† output.shape=torch.Size([1, 1024])
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 1024]), residual.shape if residual is not None else None=torch.Size([1, 1024])
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨å®Œäº†
ğŸŸ© Qwen3MLPã®é †ä¼æ’­ x.shape=torch.Size([1, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­å®Œäº† result.shape=torch.Size([1, 6144])
ğŸŸ© SwiGLUã‚’é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 6144])
ğŸŸ¦ ãƒãƒ£ãƒ³ã‚¯åŒ– x.shape=torch.Size([1, 3072]), y.shape=torch.Size([1, 3072])
ğŸŸ© SwiGLUã‚’é †ä¼æ’­å®Œäº† result.shape=torch.Size([1, 3072])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­é–‹å§‹ å…¥åŠ›å½¢çŠ¶: torch.Size([1, 3072])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­å®Œäº† å‡ºåŠ›å½¢çŠ¶: torch.Size([1, 1024])
ğŸŸ© Qwen3MLPã®é †ä¼æ’­å®Œäº† x.shape=torch.Size([1, 1024])
ğŸŸ© Qwen3DecoderLayerã®é †ä¼æ’­å®Œäº† hidden_states.shape=torch.Size([1, 1024])
ğŸŸ© Qwen3DecoderLayerã®é †ä¼æ’­ positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 1024]) residual is None=False
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 1024]), residual.shape if residual is not None else None=torch.Size([1, 1024])
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨å®Œäº†
ğŸŸ© Qwen3Attentionã®é †ä¼æ’­ positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­å®Œäº† result.shape=torch.Size([1, 4096])
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 16, 128]), residual.shape if residual is not None else None=None
ğŸŸ© æ­£è¦åŒ–é–‹å§‹ x.shape=torch.Size([1, 16, 128])
ğŸŸ© æ­£è¦åŒ–å®Œäº† x.shape=torch.Size([1, 16, 128])
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 8, 128]), residual.shape if residual is not None else None=None
ğŸŸ© æ­£è¦åŒ–é–‹å§‹ x.shape=torch.Size([1, 8, 128])
ğŸŸ© æ­£è¦åŒ–å®Œäº† x.shape=torch.Size([1, 8, 128])
ğŸŸ© RoPEã®é †ä¼æ’­é–‹å§‹ positions.shape=torch.Size([1]), query.shape=torch.Size([1, 16, 128]), key.shape=torch.Size([1, 8, 128])
ğŸŸ© RoPEã‚’é©ç”¨ x.shape=torch.Size([1, 16, 128]), cos.shape=torch.Size([1, 1, 64]), sin.shape=torch.Size([1, 1, 64])
ğŸŸ¦ ãƒãƒ£ãƒ³ã‚¯åŒ– x1.shape=torch.Size([1, 16, 64]), x2.shape=torch.Size([1, 16, 64])
ğŸŸ¦ å›è»¢è¡Œåˆ—é©ç”¨ y1.shape=torch.Size([1, 16, 64]), y2.shape=torch.Size([1, 16, 64])
ğŸŸ¦ result.shape=torch.Size([1, 16, 128])
ğŸŸ© RoPEã‚’é©ç”¨ x.shape=torch.Size([1, 8, 128]), cos.shape=torch.Size([1, 1, 64]), sin.shape=torch.Size([1, 1, 64])
ğŸŸ¦ ãƒãƒ£ãƒ³ã‚¯åŒ– x1.shape=torch.Size([1, 8, 64]), x2.shape=torch.Size([1, 8, 64])
ğŸŸ¦ å›è»¢è¡Œåˆ—é©ç”¨ y1.shape=torch.Size([1, 8, 64]), y2.shape=torch.Size([1, 8, 64])
ğŸŸ¦ result.shape=torch.Size([1, 8, 128])
ğŸŸ© RoPEã®é †ä¼æ’­å®Œäº† query.shape=torch.Size([1, 16, 128]), key.shape=torch.Size([1, 8, 128])
ğŸŸ© Attentionå±¤ã®é †ä¼æ’­é–‹å§‹ q.shape=torch.Size([1, 16, 128]), k.shape=torch.Size([1, 8, 128]), v.shape=torch.Size([1, 8, 128])
ğŸŸ© ç¾åœ¨ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆå–å¾— _CONTEXT=Context(is_prefill=True, cu_seqlens_q=tensor([0, 1], device='cuda:0', dtype=torch.int32), cu_seqlens_k=tensor([0, 1], device='cuda:0', dtype=torch.int32), max_seqlen_q=1, max_seqlen_k=1, slot_mapping=tensor([256], device='cuda:0', dtype=torch.int32), context_lens=None, block_tables=None)
ğŸŸ¦ ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæƒ…å ±å–å¾— context.is_prefill=True, context.slot_mapping=tensor([256], device='cuda:0', dtype=torch.int32), context.max_seqlen_q=1, context.cu_seqlens_q=tensor([0, 1], device='cuda:0', dtype=torch.int32), context.max_seqlen_k=1, context.cu_seqlens_k=tensor([0, 1], device='cuda:0', dtype=torch.int32), context.context_lens=None, context.block_tables=None
ğŸŸ© KVã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ã‚­ãƒ¼ã¨ãƒãƒªãƒ¥ãƒ¼ã‚’ä¿å­˜ key.shape=torch.Size([1, 8, 128]), value.shape=torch.Size([1, 8, 128]), k_cache.shape=torch.Size([884, 256, 8, 128]), v_cache.shape=torch.Size([884, 256, 8, 128]), slot_mapping.shape=torch.Size([1])
ğŸŸ¦ Prefillãƒ¢ãƒ¼ãƒ‰ã§FlashAttentionã‚’å®Ÿè¡Œ context.max_seqlen_q=1, context.cu_seqlens_q=tensor([0, 1], device='cuda:0', dtype=torch.int32), context.max_seqlen_k=1, context.cu_seqlens_k=tensor([0, 1], device='cuda:0', dtype=torch.int32)
ğŸŸ© Attentionå±¤ã®é †ä¼æ’­å®Œäº† o.shape=torch.Size([1, 16, 128])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­é–‹å§‹ å…¥åŠ›å½¢çŠ¶: torch.Size([1, 2048])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­å®Œäº† å‡ºåŠ›å½¢çŠ¶: torch.Size([1, 1024])
ğŸŸ© Qwen3Attentionã®é †ä¼æ’­å®Œäº† output.shape=torch.Size([1, 1024])
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 1024]), residual.shape if residual is not None else None=torch.Size([1, 1024])
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨å®Œäº†
ğŸŸ© Qwen3MLPã®é †ä¼æ’­ x.shape=torch.Size([1, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­å®Œäº† result.shape=torch.Size([1, 6144])
ğŸŸ© SwiGLUã‚’é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 6144])
ğŸŸ¦ ãƒãƒ£ãƒ³ã‚¯åŒ– x.shape=torch.Size([1, 3072]), y.shape=torch.Size([1, 3072])
ğŸŸ© SwiGLUã‚’é †ä¼æ’­å®Œäº† result.shape=torch.Size([1, 3072])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­é–‹å§‹ å…¥åŠ›å½¢çŠ¶: torch.Size([1, 3072])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­å®Œäº† å‡ºåŠ›å½¢çŠ¶: torch.Size([1, 1024])
ğŸŸ© Qwen3MLPã®é †ä¼æ’­å®Œäº† x.shape=torch.Size([1, 1024])
ğŸŸ© Qwen3DecoderLayerã®é †ä¼æ’­å®Œäº† hidden_states.shape=torch.Size([1, 1024])
ğŸŸ© Qwen3DecoderLayerã®é †ä¼æ’­ positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 1024]) residual is None=False
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 1024]), residual.shape if residual is not None else None=torch.Size([1, 1024])
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨å®Œäº†
ğŸŸ© Qwen3Attentionã®é †ä¼æ’­ positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­å®Œäº† result.shape=torch.Size([1, 4096])
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 16, 128]), residual.shape if residual is not None else None=None
ğŸŸ© æ­£è¦åŒ–é–‹å§‹ x.shape=torch.Size([1, 16, 128])
ğŸŸ© æ­£è¦åŒ–å®Œäº† x.shape=torch.Size([1, 16, 128])
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 8, 128]), residual.shape if residual is not None else None=None
ğŸŸ© æ­£è¦åŒ–é–‹å§‹ x.shape=torch.Size([1, 8, 128])
ğŸŸ© æ­£è¦åŒ–å®Œäº† x.shape=torch.Size([1, 8, 128])
ğŸŸ© RoPEã®é †ä¼æ’­é–‹å§‹ positions.shape=torch.Size([1]), query.shape=torch.Size([1, 16, 128]), key.shape=torch.Size([1, 8, 128])
ğŸŸ© RoPEã‚’é©ç”¨ x.shape=torch.Size([1, 16, 128]), cos.shape=torch.Size([1, 1, 64]), sin.shape=torch.Size([1, 1, 64])
ğŸŸ¦ ãƒãƒ£ãƒ³ã‚¯åŒ– x1.shape=torch.Size([1, 16, 64]), x2.shape=torch.Size([1, 16, 64])
ğŸŸ¦ å›è»¢è¡Œåˆ—é©ç”¨ y1.shape=torch.Size([1, 16, 64]), y2.shape=torch.Size([1, 16, 64])
ğŸŸ¦ result.shape=torch.Size([1, 16, 128])
ğŸŸ© RoPEã‚’é©ç”¨ x.shape=torch.Size([1, 8, 128]), cos.shape=torch.Size([1, 1, 64]), sin.shape=torch.Size([1, 1, 64])
ğŸŸ¦ ãƒãƒ£ãƒ³ã‚¯åŒ– x1.shape=torch.Size([1, 8, 64]), x2.shape=torch.Size([1, 8, 64])
ğŸŸ¦ å›è»¢è¡Œåˆ—é©ç”¨ y1.shape=torch.Size([1, 8, 64]), y2.shape=torch.Size([1, 8, 64])
ğŸŸ¦ result.shape=torch.Size([1, 8, 128])
ğŸŸ© RoPEã®é †ä¼æ’­å®Œäº† query.shape=torch.Size([1, 16, 128]), key.shape=torch.Size([1, 8, 128])
ğŸŸ© Attentionå±¤ã®é †ä¼æ’­é–‹å§‹ q.shape=torch.Size([1, 16, 128]), k.shape=torch.Size([1, 8, 128]), v.shape=torch.Size([1, 8, 128])
ğŸŸ© ç¾åœ¨ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆå–å¾— _CONTEXT=Context(is_prefill=True, cu_seqlens_q=tensor([0, 1], device='cuda:0', dtype=torch.int32), cu_seqlens_k=tensor([0, 1], device='cuda:0', dtype=torch.int32), max_seqlen_q=1, max_seqlen_k=1, slot_mapping=tensor([256], device='cuda:0', dtype=torch.int32), context_lens=None, block_tables=None)
ğŸŸ¦ ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæƒ…å ±å–å¾— context.is_prefill=True, context.slot_mapping=tensor([256], device='cuda:0', dtype=torch.int32), context.max_seqlen_q=1, context.cu_seqlens_q=tensor([0, 1], device='cuda:0', dtype=torch.int32), context.max_seqlen_k=1, context.cu_seqlens_k=tensor([0, 1], device='cuda:0', dtype=torch.int32), context.context_lens=None, context.block_tables=None
ğŸŸ© KVã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ã‚­ãƒ¼ã¨ãƒãƒªãƒ¥ãƒ¼ã‚’ä¿å­˜ key.shape=torch.Size([1, 8, 128]), value.shape=torch.Size([1, 8, 128]), k_cache.shape=torch.Size([884, 256, 8, 128]), v_cache.shape=torch.Size([884, 256, 8, 128]), slot_mapping.shape=torch.Size([1])
ğŸŸ¦ Prefillãƒ¢ãƒ¼ãƒ‰ã§FlashAttentionã‚’å®Ÿè¡Œ context.max_seqlen_q=1, context.cu_seqlens_q=tensor([0, 1], device='cuda:0', dtype=torch.int32), context.max_seqlen_k=1, context.cu_seqlens_k=tensor([0, 1], device='cuda:0', dtype=torch.int32)
ğŸŸ© Attentionå±¤ã®é †ä¼æ’­å®Œäº† o.shape=torch.Size([1, 16, 128])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­é–‹å§‹ å…¥åŠ›å½¢çŠ¶: torch.Size([1, 2048])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­å®Œäº† å‡ºåŠ›å½¢çŠ¶: torch.Size([1, 1024])
ğŸŸ© Qwen3Attentionã®é †ä¼æ’­å®Œäº† output.shape=torch.Size([1, 1024])
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 1024]), residual.shape if residual is not None else None=torch.Size([1, 1024])
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨å®Œäº†
ğŸŸ© Qwen3MLPã®é †ä¼æ’­ x.shape=torch.Size([1, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­å®Œäº† result.shape=torch.Size([1, 6144])
ğŸŸ© SwiGLUã‚’é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 6144])
ğŸŸ¦ ãƒãƒ£ãƒ³ã‚¯åŒ– x.shape=torch.Size([1, 3072]), y.shape=torch.Size([1, 3072])
ğŸŸ© SwiGLUã‚’é †ä¼æ’­å®Œäº† result.shape=torch.Size([1, 3072])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­é–‹å§‹ å…¥åŠ›å½¢çŠ¶: torch.Size([1, 3072])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­å®Œäº† å‡ºåŠ›å½¢çŠ¶: torch.Size([1, 1024])
ğŸŸ© Qwen3MLPã®é †ä¼æ’­å®Œäº† x.shape=torch.Size([1, 1024])
ğŸŸ© Qwen3DecoderLayerã®é †ä¼æ’­å®Œäº† hidden_states.shape=torch.Size([1, 1024])
ğŸŸ© Qwen3DecoderLayerã®é †ä¼æ’­ positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 1024]) residual is None=False
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 1024]), residual.shape if residual is not None else None=torch.Size([1, 1024])
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨å®Œäº†
ğŸŸ© Qwen3Attentionã®é †ä¼æ’­ positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­å®Œäº† result.shape=torch.Size([1, 4096])
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 16, 128]), residual.shape if residual is not None else None=None
ğŸŸ© æ­£è¦åŒ–é–‹å§‹ x.shape=torch.Size([1, 16, 128])
ğŸŸ© æ­£è¦åŒ–å®Œäº† x.shape=torch.Size([1, 16, 128])
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 8, 128]), residual.shape if residual is not None else None=None
ğŸŸ© æ­£è¦åŒ–é–‹å§‹ x.shape=torch.Size([1, 8, 128])
ğŸŸ© æ­£è¦åŒ–å®Œäº† x.shape=torch.Size([1, 8, 128])
ğŸŸ© RoPEã®é †ä¼æ’­é–‹å§‹ positions.shape=torch.Size([1]), query.shape=torch.Size([1, 16, 128]), key.shape=torch.Size([1, 8, 128])
ğŸŸ© RoPEã‚’é©ç”¨ x.shape=torch.Size([1, 16, 128]), cos.shape=torch.Size([1, 1, 64]), sin.shape=torch.Size([1, 1, 64])
ğŸŸ¦ ãƒãƒ£ãƒ³ã‚¯åŒ– x1.shape=torch.Size([1, 16, 64]), x2.shape=torch.Size([1, 16, 64])
ğŸŸ¦ å›è»¢è¡Œåˆ—é©ç”¨ y1.shape=torch.Size([1, 16, 64]), y2.shape=torch.Size([1, 16, 64])
ğŸŸ¦ result.shape=torch.Size([1, 16, 128])
ğŸŸ© RoPEã‚’é©ç”¨ x.shape=torch.Size([1, 8, 128]), cos.shape=torch.Size([1, 1, 64]), sin.shape=torch.Size([1, 1, 64])
ğŸŸ¦ ãƒãƒ£ãƒ³ã‚¯åŒ– x1.shape=torch.Size([1, 8, 64]), x2.shape=torch.Size([1, 8, 64])
ğŸŸ¦ å›è»¢è¡Œåˆ—é©ç”¨ y1.shape=torch.Size([1, 8, 64]), y2.shape=torch.Size([1, 8, 64])
ğŸŸ¦ result.shape=torch.Size([1, 8, 128])
ğŸŸ© RoPEã®é †ä¼æ’­å®Œäº† query.shape=torch.Size([1, 16, 128]), key.shape=torch.Size([1, 8, 128])
ğŸŸ© Attentionå±¤ã®é †ä¼æ’­é–‹å§‹ q.shape=torch.Size([1, 16, 128]), k.shape=torch.Size([1, 8, 128]), v.shape=torch.Size([1, 8, 128])
ğŸŸ© ç¾åœ¨ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆå–å¾— _CONTEXT=Context(is_prefill=True, cu_seqlens_q=tensor([0, 1], device='cuda:0', dtype=torch.int32), cu_seqlens_k=tensor([0, 1], device='cuda:0', dtype=torch.int32), max_seqlen_q=1, max_seqlen_k=1, slot_mapping=tensor([256], device='cuda:0', dtype=torch.int32), context_lens=None, block_tables=None)
ğŸŸ¦ ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæƒ…å ±å–å¾— context.is_prefill=True, context.slot_mapping=tensor([256], device='cuda:0', dtype=torch.int32), context.max_seqlen_q=1, context.cu_seqlens_q=tensor([0, 1], device='cuda:0', dtype=torch.int32), context.max_seqlen_k=1, context.cu_seqlens_k=tensor([0, 1], device='cuda:0', dtype=torch.int32), context.context_lens=None, context.block_tables=None
ğŸŸ© KVã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ã‚­ãƒ¼ã¨ãƒãƒªãƒ¥ãƒ¼ã‚’ä¿å­˜ key.shape=torch.Size([1, 8, 128]), value.shape=torch.Size([1, 8, 128]), k_cache.shape=torch.Size([884, 256, 8, 128]), v_cache.shape=torch.Size([884, 256, 8, 128]), slot_mapping.shape=torch.Size([1])
ğŸŸ¦ Prefillãƒ¢ãƒ¼ãƒ‰ã§FlashAttentionã‚’å®Ÿè¡Œ context.max_seqlen_q=1, context.cu_seqlens_q=tensor([0, 1], device='cuda:0', dtype=torch.int32), context.max_seqlen_k=1, context.cu_seqlens_k=tensor([0, 1], device='cuda:0', dtype=torch.int32)
ğŸŸ© Attentionå±¤ã®é †ä¼æ’­å®Œäº† o.shape=torch.Size([1, 16, 128])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­é–‹å§‹ å…¥åŠ›å½¢çŠ¶: torch.Size([1, 2048])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­å®Œäº† å‡ºåŠ›å½¢çŠ¶: torch.Size([1, 1024])
ğŸŸ© Qwen3Attentionã®é †ä¼æ’­å®Œäº† output.shape=torch.Size([1, 1024])
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 1024]), residual.shape if residual is not None else None=torch.Size([1, 1024])
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨å®Œäº†
ğŸŸ© Qwen3MLPã®é †ä¼æ’­ x.shape=torch.Size([1, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­å®Œäº† result.shape=torch.Size([1, 6144])
ğŸŸ© SwiGLUã‚’é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 6144])
ğŸŸ¦ ãƒãƒ£ãƒ³ã‚¯åŒ– x.shape=torch.Size([1, 3072]), y.shape=torch.Size([1, 3072])
ğŸŸ© SwiGLUã‚’é †ä¼æ’­å®Œäº† result.shape=torch.Size([1, 3072])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­é–‹å§‹ å…¥åŠ›å½¢çŠ¶: torch.Size([1, 3072])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­å®Œäº† å‡ºåŠ›å½¢çŠ¶: torch.Size([1, 1024])
ğŸŸ© Qwen3MLPã®é †ä¼æ’­å®Œäº† x.shape=torch.Size([1, 1024])
ğŸŸ© Qwen3DecoderLayerã®é †ä¼æ’­å®Œäº† hidden_states.shape=torch.Size([1, 1024])
ğŸŸ© Qwen3DecoderLayerã®é †ä¼æ’­ positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 1024]) residual is None=False
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 1024]), residual.shape if residual is not None else None=torch.Size([1, 1024])
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨å®Œäº†
ğŸŸ© Qwen3Attentionã®é †ä¼æ’­ positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­å®Œäº† result.shape=torch.Size([1, 4096])
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 16, 128]), residual.shape if residual is not None else None=None
ğŸŸ© æ­£è¦åŒ–é–‹å§‹ x.shape=torch.Size([1, 16, 128])
ğŸŸ© æ­£è¦åŒ–å®Œäº† x.shape=torch.Size([1, 16, 128])
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 8, 128]), residual.shape if residual is not None else None=None
ğŸŸ© æ­£è¦åŒ–é–‹å§‹ x.shape=torch.Size([1, 8, 128])
ğŸŸ© æ­£è¦åŒ–å®Œäº† x.shape=torch.Size([1, 8, 128])
ğŸŸ© RoPEã®é †ä¼æ’­é–‹å§‹ positions.shape=torch.Size([1]), query.shape=torch.Size([1, 16, 128]), key.shape=torch.Size([1, 8, 128])
ğŸŸ© RoPEã‚’é©ç”¨ x.shape=torch.Size([1, 16, 128]), cos.shape=torch.Size([1, 1, 64]), sin.shape=torch.Size([1, 1, 64])
ğŸŸ¦ ãƒãƒ£ãƒ³ã‚¯åŒ– x1.shape=torch.Size([1, 16, 64]), x2.shape=torch.Size([1, 16, 64])
ğŸŸ¦ å›è»¢è¡Œåˆ—é©ç”¨ y1.shape=torch.Size([1, 16, 64]), y2.shape=torch.Size([1, 16, 64])
ğŸŸ¦ result.shape=torch.Size([1, 16, 128])
ğŸŸ© RoPEã‚’é©ç”¨ x.shape=torch.Size([1, 8, 128]), cos.shape=torch.Size([1, 1, 64]), sin.shape=torch.Size([1, 1, 64])
ğŸŸ¦ ãƒãƒ£ãƒ³ã‚¯åŒ– x1.shape=torch.Size([1, 8, 64]), x2.shape=torch.Size([1, 8, 64])
ğŸŸ¦ å›è»¢è¡Œåˆ—é©ç”¨ y1.shape=torch.Size([1, 8, 64]), y2.shape=torch.Size([1, 8, 64])
ğŸŸ¦ result.shape=torch.Size([1, 8, 128])
ğŸŸ© RoPEã®é †ä¼æ’­å®Œäº† query.shape=torch.Size([1, 16, 128]), key.shape=torch.Size([1, 8, 128])
ğŸŸ© Attentionå±¤ã®é †ä¼æ’­é–‹å§‹ q.shape=torch.Size([1, 16, 128]), k.shape=torch.Size([1, 8, 128]), v.shape=torch.Size([1, 8, 128])
ğŸŸ© ç¾åœ¨ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆå–å¾— _CONTEXT=Context(is_prefill=True, cu_seqlens_q=tensor([0, 1], device='cuda:0', dtype=torch.int32), cu_seqlens_k=tensor([0, 1], device='cuda:0', dtype=torch.int32), max_seqlen_q=1, max_seqlen_k=1, slot_mapping=tensor([256], device='cuda:0', dtype=torch.int32), context_lens=None, block_tables=None)
ğŸŸ¦ ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæƒ…å ±å–å¾— context.is_prefill=True, context.slot_mapping=tensor([256], device='cuda:0', dtype=torch.int32), context.max_seqlen_q=1, context.cu_seqlens_q=tensor([0, 1], device='cuda:0', dtype=torch.int32), context.max_seqlen_k=1, context.cu_seqlens_k=tensor([0, 1], device='cuda:0', dtype=torch.int32), context.context_lens=None, context.block_tables=None
ğŸŸ© KVã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ã‚­ãƒ¼ã¨ãƒãƒªãƒ¥ãƒ¼ã‚’ä¿å­˜ key.shape=torch.Size([1, 8, 128]), value.shape=torch.Size([1, 8, 128]), k_cache.shape=torch.Size([884, 256, 8, 128]), v_cache.shape=torch.Size([884, 256, 8, 128]), slot_mapping.shape=torch.Size([1])
ğŸŸ¦ Prefillãƒ¢ãƒ¼ãƒ‰ã§FlashAttentionã‚’å®Ÿè¡Œ context.max_seqlen_q=1, context.cu_seqlens_q=tensor([0, 1], device='cuda:0', dtype=torch.int32), context.max_seqlen_k=1, context.cu_seqlens_k=tensor([0, 1], device='cuda:0', dtype=torch.int32)
ğŸŸ© Attentionå±¤ã®é †ä¼æ’­å®Œäº† o.shape=torch.Size([1, 16, 128])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­é–‹å§‹ å…¥åŠ›å½¢çŠ¶: torch.Size([1, 2048])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­å®Œäº† å‡ºåŠ›å½¢çŠ¶: torch.Size([1, 1024])
ğŸŸ© Qwen3Attentionã®é †ä¼æ’­å®Œäº† output.shape=torch.Size([1, 1024])
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 1024]), residual.shape if residual is not None else None=torch.Size([1, 1024])
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨å®Œäº†
ğŸŸ© Qwen3MLPã®é †ä¼æ’­ x.shape=torch.Size([1, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­å®Œäº† result.shape=torch.Size([1, 6144])
ğŸŸ© SwiGLUã‚’é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 6144])
ğŸŸ¦ ãƒãƒ£ãƒ³ã‚¯åŒ– x.shape=torch.Size([1, 3072]), y.shape=torch.Size([1, 3072])
ğŸŸ© SwiGLUã‚’é †ä¼æ’­å®Œäº† result.shape=torch.Size([1, 3072])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­é–‹å§‹ å…¥åŠ›å½¢çŠ¶: torch.Size([1, 3072])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­å®Œäº† å‡ºåŠ›å½¢çŠ¶: torch.Size([1, 1024])
ğŸŸ© Qwen3MLPã®é †ä¼æ’­å®Œäº† x.shape=torch.Size([1, 1024])
ğŸŸ© Qwen3DecoderLayerã®é †ä¼æ’­å®Œäº† hidden_states.shape=torch.Size([1, 1024])
ğŸŸ© Qwen3DecoderLayerã®é †ä¼æ’­ positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 1024]) residual is None=False
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 1024]), residual.shape if residual is not None else None=torch.Size([1, 1024])
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨å®Œäº†
ğŸŸ© Qwen3Attentionã®é †ä¼æ’­ positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­å®Œäº† result.shape=torch.Size([1, 4096])
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 16, 128]), residual.shape if residual is not None else None=None
ğŸŸ© æ­£è¦åŒ–é–‹å§‹ x.shape=torch.Size([1, 16, 128])
ğŸŸ© æ­£è¦åŒ–å®Œäº† x.shape=torch.Size([1, 16, 128])
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 8, 128]), residual.shape if residual is not None else None=None
ğŸŸ© æ­£è¦åŒ–é–‹å§‹ x.shape=torch.Size([1, 8, 128])
ğŸŸ© æ­£è¦åŒ–å®Œäº† x.shape=torch.Size([1, 8, 128])
ğŸŸ© RoPEã®é †ä¼æ’­é–‹å§‹ positions.shape=torch.Size([1]), query.shape=torch.Size([1, 16, 128]), key.shape=torch.Size([1, 8, 128])
ğŸŸ© RoPEã‚’é©ç”¨ x.shape=torch.Size([1, 16, 128]), cos.shape=torch.Size([1, 1, 64]), sin.shape=torch.Size([1, 1, 64])
ğŸŸ¦ ãƒãƒ£ãƒ³ã‚¯åŒ– x1.shape=torch.Size([1, 16, 64]), x2.shape=torch.Size([1, 16, 64])
ğŸŸ¦ å›è»¢è¡Œåˆ—é©ç”¨ y1.shape=torch.Size([1, 16, 64]), y2.shape=torch.Size([1, 16, 64])
ğŸŸ¦ result.shape=torch.Size([1, 16, 128])
ğŸŸ© RoPEã‚’é©ç”¨ x.shape=torch.Size([1, 8, 128]), cos.shape=torch.Size([1, 1, 64]), sin.shape=torch.Size([1, 1, 64])
ğŸŸ¦ ãƒãƒ£ãƒ³ã‚¯åŒ– x1.shape=torch.Size([1, 8, 64]), x2.shape=torch.Size([1, 8, 64])
ğŸŸ¦ å›è»¢è¡Œåˆ—é©ç”¨ y1.shape=torch.Size([1, 8, 64]), y2.shape=torch.Size([1, 8, 64])
ğŸŸ¦ result.shape=torch.Size([1, 8, 128])
ğŸŸ© RoPEã®é †ä¼æ’­å®Œäº† query.shape=torch.Size([1, 16, 128]), key.shape=torch.Size([1, 8, 128])
ğŸŸ© Attentionå±¤ã®é †ä¼æ’­é–‹å§‹ q.shape=torch.Size([1, 16, 128]), k.shape=torch.Size([1, 8, 128]), v.shape=torch.Size([1, 8, 128])
ğŸŸ© ç¾åœ¨ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆå–å¾— _CONTEXT=Context(is_prefill=True, cu_seqlens_q=tensor([0, 1], device='cuda:0', dtype=torch.int32), cu_seqlens_k=tensor([0, 1], device='cuda:0', dtype=torch.int32), max_seqlen_q=1, max_seqlen_k=1, slot_mapping=tensor([256], device='cuda:0', dtype=torch.int32), context_lens=None, block_tables=None)
ğŸŸ¦ ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæƒ…å ±å–å¾— context.is_prefill=True, context.slot_mapping=tensor([256], device='cuda:0', dtype=torch.int32), context.max_seqlen_q=1, context.cu_seqlens_q=tensor([0, 1], device='cuda:0', dtype=torch.int32), context.max_seqlen_k=1, context.cu_seqlens_k=tensor([0, 1], device='cuda:0', dtype=torch.int32), context.context_lens=None, context.block_tables=None
ğŸŸ© KVã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ã‚­ãƒ¼ã¨ãƒãƒªãƒ¥ãƒ¼ã‚’ä¿å­˜ key.shape=torch.Size([1, 8, 128]), value.shape=torch.Size([1, 8, 128]), k_cache.shape=torch.Size([884, 256, 8, 128]), v_cache.shape=torch.Size([884, 256, 8, 128]), slot_mapping.shape=torch.Size([1])
ğŸŸ¦ Prefillãƒ¢ãƒ¼ãƒ‰ã§FlashAttentionã‚’å®Ÿè¡Œ context.max_seqlen_q=1, context.cu_seqlens_q=tensor([0, 1], device='cuda:0', dtype=torch.int32), context.max_seqlen_k=1, context.cu_seqlens_k=tensor([0, 1], device='cuda:0', dtype=torch.int32)
ğŸŸ© Attentionå±¤ã®é †ä¼æ’­å®Œäº† o.shape=torch.Size([1, 16, 128])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­é–‹å§‹ å…¥åŠ›å½¢çŠ¶: torch.Size([1, 2048])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­å®Œäº† å‡ºåŠ›å½¢çŠ¶: torch.Size([1, 1024])
ğŸŸ© Qwen3Attentionã®é †ä¼æ’­å®Œäº† output.shape=torch.Size([1, 1024])
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 1024]), residual.shape if residual is not None else None=torch.Size([1, 1024])
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨å®Œäº†
ğŸŸ© Qwen3MLPã®é †ä¼æ’­ x.shape=torch.Size([1, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­å®Œäº† result.shape=torch.Size([1, 6144])
ğŸŸ© SwiGLUã‚’é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 6144])
ğŸŸ¦ ãƒãƒ£ãƒ³ã‚¯åŒ– x.shape=torch.Size([1, 3072]), y.shape=torch.Size([1, 3072])
ğŸŸ© SwiGLUã‚’é †ä¼æ’­å®Œäº† result.shape=torch.Size([1, 3072])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­é–‹å§‹ å…¥åŠ›å½¢çŠ¶: torch.Size([1, 3072])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­å®Œäº† å‡ºåŠ›å½¢çŠ¶: torch.Size([1, 1024])
ğŸŸ© Qwen3MLPã®é †ä¼æ’­å®Œäº† x.shape=torch.Size([1, 1024])
ğŸŸ© Qwen3DecoderLayerã®é †ä¼æ’­å®Œäº† hidden_states.shape=torch.Size([1, 1024])
ğŸŸ© Qwen3DecoderLayerã®é †ä¼æ’­ positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 1024]) residual is None=False
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 1024]), residual.shape if residual is not None else None=torch.Size([1, 1024])
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨å®Œäº†
ğŸŸ© Qwen3Attentionã®é †ä¼æ’­ positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­å®Œäº† result.shape=torch.Size([1, 4096])
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 16, 128]), residual.shape if residual is not None else None=None
ğŸŸ© æ­£è¦åŒ–é–‹å§‹ x.shape=torch.Size([1, 16, 128])
ğŸŸ© æ­£è¦åŒ–å®Œäº† x.shape=torch.Size([1, 16, 128])
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 8, 128]), residual.shape if residual is not None else None=None
ğŸŸ© æ­£è¦åŒ–é–‹å§‹ x.shape=torch.Size([1, 8, 128])
ğŸŸ© æ­£è¦åŒ–å®Œäº† x.shape=torch.Size([1, 8, 128])
ğŸŸ© RoPEã®é †ä¼æ’­é–‹å§‹ positions.shape=torch.Size([1]), query.shape=torch.Size([1, 16, 128]), key.shape=torch.Size([1, 8, 128])
ğŸŸ© RoPEã‚’é©ç”¨ x.shape=torch.Size([1, 16, 128]), cos.shape=torch.Size([1, 1, 64]), sin.shape=torch.Size([1, 1, 64])
ğŸŸ¦ ãƒãƒ£ãƒ³ã‚¯åŒ– x1.shape=torch.Size([1, 16, 64]), x2.shape=torch.Size([1, 16, 64])
ğŸŸ¦ å›è»¢è¡Œåˆ—é©ç”¨ y1.shape=torch.Size([1, 16, 64]), y2.shape=torch.Size([1, 16, 64])
ğŸŸ¦ result.shape=torch.Size([1, 16, 128])
ğŸŸ© RoPEã‚’é©ç”¨ x.shape=torch.Size([1, 8, 128]), cos.shape=torch.Size([1, 1, 64]), sin.shape=torch.Size([1, 1, 64])
ğŸŸ¦ ãƒãƒ£ãƒ³ã‚¯åŒ– x1.shape=torch.Size([1, 8, 64]), x2.shape=torch.Size([1, 8, 64])
ğŸŸ¦ å›è»¢è¡Œåˆ—é©ç”¨ y1.shape=torch.Size([1, 8, 64]), y2.shape=torch.Size([1, 8, 64])
ğŸŸ¦ result.shape=torch.Size([1, 8, 128])
ğŸŸ© RoPEã®é †ä¼æ’­å®Œäº† query.shape=torch.Size([1, 16, 128]), key.shape=torch.Size([1, 8, 128])
ğŸŸ© Attentionå±¤ã®é †ä¼æ’­é–‹å§‹ q.shape=torch.Size([1, 16, 128]), k.shape=torch.Size([1, 8, 128]), v.shape=torch.Size([1, 8, 128])
ğŸŸ© ç¾åœ¨ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆå–å¾— _CONTEXT=Context(is_prefill=True, cu_seqlens_q=tensor([0, 1], device='cuda:0', dtype=torch.int32), cu_seqlens_k=tensor([0, 1], device='cuda:0', dtype=torch.int32), max_seqlen_q=1, max_seqlen_k=1, slot_mapping=tensor([256], device='cuda:0', dtype=torch.int32), context_lens=None, block_tables=None)
ğŸŸ¦ ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæƒ…å ±å–å¾— context.is_prefill=True, context.slot_mapping=tensor([256], device='cuda:0', dtype=torch.int32), context.max_seqlen_q=1, context.cu_seqlens_q=tensor([0, 1], device='cuda:0', dtype=torch.int32), context.max_seqlen_k=1, context.cu_seqlens_k=tensor([0, 1], device='cuda:0', dtype=torch.int32), context.context_lens=None, context.block_tables=None
ğŸŸ© KVã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ã‚­ãƒ¼ã¨ãƒãƒªãƒ¥ãƒ¼ã‚’ä¿å­˜ key.shape=torch.Size([1, 8, 128]), value.shape=torch.Size([1, 8, 128]), k_cache.shape=torch.Size([884, 256, 8, 128]), v_cache.shape=torch.Size([884, 256, 8, 128]), slot_mapping.shape=torch.Size([1])
ğŸŸ¦ Prefillãƒ¢ãƒ¼ãƒ‰ã§FlashAttentionã‚’å®Ÿè¡Œ context.max_seqlen_q=1, context.cu_seqlens_q=tensor([0, 1], device='cuda:0', dtype=torch.int32), context.max_seqlen_k=1, context.cu_seqlens_k=tensor([0, 1], device='cuda:0', dtype=torch.int32)
ğŸŸ© Attentionå±¤ã®é †ä¼æ’­å®Œäº† o.shape=torch.Size([1, 16, 128])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­é–‹å§‹ å…¥åŠ›å½¢çŠ¶: torch.Size([1, 2048])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­å®Œäº† å‡ºåŠ›å½¢çŠ¶: torch.Size([1, 1024])
ğŸŸ© Qwen3Attentionã®é †ä¼æ’­å®Œäº† output.shape=torch.Size([1, 1024])
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 1024]), residual.shape if residual is not None else None=torch.Size([1, 1024])
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨å®Œäº†
ğŸŸ© Qwen3MLPã®é †ä¼æ’­ x.shape=torch.Size([1, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­å®Œäº† result.shape=torch.Size([1, 6144])
ğŸŸ© SwiGLUã‚’é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 6144])
ğŸŸ¦ ãƒãƒ£ãƒ³ã‚¯åŒ– x.shape=torch.Size([1, 3072]), y.shape=torch.Size([1, 3072])
ğŸŸ© SwiGLUã‚’é †ä¼æ’­å®Œäº† result.shape=torch.Size([1, 3072])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­é–‹å§‹ å…¥åŠ›å½¢çŠ¶: torch.Size([1, 3072])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­å®Œäº† å‡ºåŠ›å½¢çŠ¶: torch.Size([1, 1024])
ğŸŸ© Qwen3MLPã®é †ä¼æ’­å®Œäº† x.shape=torch.Size([1, 1024])
ğŸŸ© Qwen3DecoderLayerã®é †ä¼æ’­å®Œäº† hidden_states.shape=torch.Size([1, 1024])
ğŸŸ© Qwen3DecoderLayerã®é †ä¼æ’­ positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 1024]) residual is None=False
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 1024]), residual.shape if residual is not None else None=torch.Size([1, 1024])
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨å®Œäº†
ğŸŸ© Qwen3Attentionã®é †ä¼æ’­ positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­å®Œäº† result.shape=torch.Size([1, 4096])
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 16, 128]), residual.shape if residual is not None else None=None
ğŸŸ© æ­£è¦åŒ–é–‹å§‹ x.shape=torch.Size([1, 16, 128])
ğŸŸ© æ­£è¦åŒ–å®Œäº† x.shape=torch.Size([1, 16, 128])
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 8, 128]), residual.shape if residual is not None else None=None
ğŸŸ© æ­£è¦åŒ–é–‹å§‹ x.shape=torch.Size([1, 8, 128])
ğŸŸ© æ­£è¦åŒ–å®Œäº† x.shape=torch.Size([1, 8, 128])
ğŸŸ© RoPEã®é †ä¼æ’­é–‹å§‹ positions.shape=torch.Size([1]), query.shape=torch.Size([1, 16, 128]), key.shape=torch.Size([1, 8, 128])
ğŸŸ© RoPEã‚’é©ç”¨ x.shape=torch.Size([1, 16, 128]), cos.shape=torch.Size([1, 1, 64]), sin.shape=torch.Size([1, 1, 64])
ğŸŸ¦ ãƒãƒ£ãƒ³ã‚¯åŒ– x1.shape=torch.Size([1, 16, 64]), x2.shape=torch.Size([1, 16, 64])
ğŸŸ¦ å›è»¢è¡Œåˆ—é©ç”¨ y1.shape=torch.Size([1, 16, 64]), y2.shape=torch.Size([1, 16, 64])
ğŸŸ¦ result.shape=torch.Size([1, 16, 128])
ğŸŸ© RoPEã‚’é©ç”¨ x.shape=torch.Size([1, 8, 128]), cos.shape=torch.Size([1, 1, 64]), sin.shape=torch.Size([1, 1, 64])
ğŸŸ¦ ãƒãƒ£ãƒ³ã‚¯åŒ– x1.shape=torch.Size([1, 8, 64]), x2.shape=torch.Size([1, 8, 64])
ğŸŸ¦ å›è»¢è¡Œåˆ—é©ç”¨ y1.shape=torch.Size([1, 8, 64]), y2.shape=torch.Size([1, 8, 64])
ğŸŸ¦ result.shape=torch.Size([1, 8, 128])
ğŸŸ© RoPEã®é †ä¼æ’­å®Œäº† query.shape=torch.Size([1, 16, 128]), key.shape=torch.Size([1, 8, 128])
ğŸŸ© Attentionå±¤ã®é †ä¼æ’­é–‹å§‹ q.shape=torch.Size([1, 16, 128]), k.shape=torch.Size([1, 8, 128]), v.shape=torch.Size([1, 8, 128])
ğŸŸ© ç¾åœ¨ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆå–å¾— _CONTEXT=Context(is_prefill=True, cu_seqlens_q=tensor([0, 1], device='cuda:0', dtype=torch.int32), cu_seqlens_k=tensor([0, 1], device='cuda:0', dtype=torch.int32), max_seqlen_q=1, max_seqlen_k=1, slot_mapping=tensor([256], device='cuda:0', dtype=torch.int32), context_lens=None, block_tables=None)
ğŸŸ¦ ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæƒ…å ±å–å¾— context.is_prefill=True, context.slot_mapping=tensor([256], device='cuda:0', dtype=torch.int32), context.max_seqlen_q=1, context.cu_seqlens_q=tensor([0, 1], device='cuda:0', dtype=torch.int32), context.max_seqlen_k=1, context.cu_seqlens_k=tensor([0, 1], device='cuda:0', dtype=torch.int32), context.context_lens=None, context.block_tables=None
ğŸŸ© KVã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ã‚­ãƒ¼ã¨ãƒãƒªãƒ¥ãƒ¼ã‚’ä¿å­˜ key.shape=torch.Size([1, 8, 128]), value.shape=torch.Size([1, 8, 128]), k_cache.shape=torch.Size([884, 256, 8, 128]), v_cache.shape=torch.Size([884, 256, 8, 128]), slot_mapping.shape=torch.Size([1])
ğŸŸ¦ Prefillãƒ¢ãƒ¼ãƒ‰ã§FlashAttentionã‚’å®Ÿè¡Œ context.max_seqlen_q=1, context.cu_seqlens_q=tensor([0, 1], device='cuda:0', dtype=torch.int32), context.max_seqlen_k=1, context.cu_seqlens_k=tensor([0, 1], device='cuda:0', dtype=torch.int32)
ğŸŸ© Attentionå±¤ã®é †ä¼æ’­å®Œäº† o.shape=torch.Size([1, 16, 128])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­é–‹å§‹ å…¥åŠ›å½¢çŠ¶: torch.Size([1, 2048])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­å®Œäº† å‡ºåŠ›å½¢çŠ¶: torch.Size([1, 1024])
ğŸŸ© Qwen3Attentionã®é †ä¼æ’­å®Œäº† output.shape=torch.Size([1, 1024])
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 1024]), residual.shape if residual is not None else None=torch.Size([1, 1024])
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨å®Œäº†
ğŸŸ© Qwen3MLPã®é †ä¼æ’­ x.shape=torch.Size([1, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­å®Œäº† result.shape=torch.Size([1, 6144])
ğŸŸ© SwiGLUã‚’é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 6144])
ğŸŸ¦ ãƒãƒ£ãƒ³ã‚¯åŒ– x.shape=torch.Size([1, 3072]), y.shape=torch.Size([1, 3072])
ğŸŸ© SwiGLUã‚’é †ä¼æ’­å®Œäº† result.shape=torch.Size([1, 3072])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­é–‹å§‹ å…¥åŠ›å½¢çŠ¶: torch.Size([1, 3072])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­å®Œäº† å‡ºåŠ›å½¢çŠ¶: torch.Size([1, 1024])
ğŸŸ© Qwen3MLPã®é †ä¼æ’­å®Œäº† x.shape=torch.Size([1, 1024])
ğŸŸ© Qwen3DecoderLayerã®é †ä¼æ’­å®Œäº† hidden_states.shape=torch.Size([1, 1024])
ğŸŸ© Qwen3DecoderLayerã®é †ä¼æ’­ positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 1024]) residual is None=False
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 1024]), residual.shape if residual is not None else None=torch.Size([1, 1024])
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨å®Œäº†
ğŸŸ© Qwen3Attentionã®é †ä¼æ’­ positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­å®Œäº† result.shape=torch.Size([1, 4096])
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 16, 128]), residual.shape if residual is not None else None=None
ğŸŸ© æ­£è¦åŒ–é–‹å§‹ x.shape=torch.Size([1, 16, 128])
ğŸŸ© æ­£è¦åŒ–å®Œäº† x.shape=torch.Size([1, 16, 128])
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 8, 128]), residual.shape if residual is not None else None=None
ğŸŸ© æ­£è¦åŒ–é–‹å§‹ x.shape=torch.Size([1, 8, 128])
ğŸŸ© æ­£è¦åŒ–å®Œäº† x.shape=torch.Size([1, 8, 128])
ğŸŸ© RoPEã®é †ä¼æ’­é–‹å§‹ positions.shape=torch.Size([1]), query.shape=torch.Size([1, 16, 128]), key.shape=torch.Size([1, 8, 128])
ğŸŸ© RoPEã‚’é©ç”¨ x.shape=torch.Size([1, 16, 128]), cos.shape=torch.Size([1, 1, 64]), sin.shape=torch.Size([1, 1, 64])
ğŸŸ¦ ãƒãƒ£ãƒ³ã‚¯åŒ– x1.shape=torch.Size([1, 16, 64]), x2.shape=torch.Size([1, 16, 64])
ğŸŸ¦ å›è»¢è¡Œåˆ—é©ç”¨ y1.shape=torch.Size([1, 16, 64]), y2.shape=torch.Size([1, 16, 64])
ğŸŸ¦ result.shape=torch.Size([1, 16, 128])
ğŸŸ© RoPEã‚’é©ç”¨ x.shape=torch.Size([1, 8, 128]), cos.shape=torch.Size([1, 1, 64]), sin.shape=torch.Size([1, 1, 64])
ğŸŸ¦ ãƒãƒ£ãƒ³ã‚¯åŒ– x1.shape=torch.Size([1, 8, 64]), x2.shape=torch.Size([1, 8, 64])
ğŸŸ¦ å›è»¢è¡Œåˆ—é©ç”¨ y1.shape=torch.Size([1, 8, 64]), y2.shape=torch.Size([1, 8, 64])
ğŸŸ¦ result.shape=torch.Size([1, 8, 128])
ğŸŸ© RoPEã®é †ä¼æ’­å®Œäº† query.shape=torch.Size([1, 16, 128]), key.shape=torch.Size([1, 8, 128])
ğŸŸ© Attentionå±¤ã®é †ä¼æ’­é–‹å§‹ q.shape=torch.Size([1, 16, 128]), k.shape=torch.Size([1, 8, 128]), v.shape=torch.Size([1, 8, 128])
ğŸŸ© ç¾åœ¨ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆå–å¾— _CONTEXT=Context(is_prefill=True, cu_seqlens_q=tensor([0, 1], device='cuda:0', dtype=torch.int32), cu_seqlens_k=tensor([0, 1], device='cuda:0', dtype=torch.int32), max_seqlen_q=1, max_seqlen_k=1, slot_mapping=tensor([256], device='cuda:0', dtype=torch.int32), context_lens=None, block_tables=None)
ğŸŸ¦ ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæƒ…å ±å–å¾— context.is_prefill=True, context.slot_mapping=tensor([256], device='cuda:0', dtype=torch.int32), context.max_seqlen_q=1, context.cu_seqlens_q=tensor([0, 1], device='cuda:0', dtype=torch.int32), context.max_seqlen_k=1, context.cu_seqlens_k=tensor([0, 1], device='cuda:0', dtype=torch.int32), context.context_lens=None, context.block_tables=None
ğŸŸ© KVã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ã‚­ãƒ¼ã¨ãƒãƒªãƒ¥ãƒ¼ã‚’ä¿å­˜ key.shape=torch.Size([1, 8, 128]), value.shape=torch.Size([1, 8, 128]), k_cache.shape=torch.Size([884, 256, 8, 128]), v_cache.shape=torch.Size([884, 256, 8, 128]), slot_mapping.shape=torch.Size([1])
ğŸŸ¦ Prefillãƒ¢ãƒ¼ãƒ‰ã§FlashAttentionã‚’å®Ÿè¡Œ context.max_seqlen_q=1, context.cu_seqlens_q=tensor([0, 1], device='cuda:0', dtype=torch.int32), context.max_seqlen_k=1, context.cu_seqlens_k=tensor([0, 1], device='cuda:0', dtype=torch.int32)
ğŸŸ© Attentionå±¤ã®é †ä¼æ’­å®Œäº† o.shape=torch.Size([1, 16, 128])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­é–‹å§‹ å…¥åŠ›å½¢çŠ¶: torch.Size([1, 2048])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­å®Œäº† å‡ºåŠ›å½¢çŠ¶: torch.Size([1, 1024])
ğŸŸ© Qwen3Attentionã®é †ä¼æ’­å®Œäº† output.shape=torch.Size([1, 1024])
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 1024]), residual.shape if residual is not None else None=torch.Size([1, 1024])
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨å®Œäº†
ğŸŸ© Qwen3MLPã®é †ä¼æ’­ x.shape=torch.Size([1, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­å®Œäº† result.shape=torch.Size([1, 6144])
ğŸŸ© SwiGLUã‚’é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 6144])
ğŸŸ¦ ãƒãƒ£ãƒ³ã‚¯åŒ– x.shape=torch.Size([1, 3072]), y.shape=torch.Size([1, 3072])
ğŸŸ© SwiGLUã‚’é †ä¼æ’­å®Œäº† result.shape=torch.Size([1, 3072])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­é–‹å§‹ å…¥åŠ›å½¢çŠ¶: torch.Size([1, 3072])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­å®Œäº† å‡ºåŠ›å½¢çŠ¶: torch.Size([1, 1024])
ğŸŸ© Qwen3MLPã®é †ä¼æ’­å®Œäº† x.shape=torch.Size([1, 1024])
ğŸŸ© Qwen3DecoderLayerã®é †ä¼æ’­å®Œäº† hidden_states.shape=torch.Size([1, 1024])
ğŸŸ© Qwen3DecoderLayerã®é †ä¼æ’­ positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 1024]) residual is None=False
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 1024]), residual.shape if residual is not None else None=torch.Size([1, 1024])
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨å®Œäº†
ğŸŸ© Qwen3Attentionã®é †ä¼æ’­ positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­å®Œäº† result.shape=torch.Size([1, 4096])
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 16, 128]), residual.shape if residual is not None else None=None
ğŸŸ© æ­£è¦åŒ–é–‹å§‹ x.shape=torch.Size([1, 16, 128])
ğŸŸ© æ­£è¦åŒ–å®Œäº† x.shape=torch.Size([1, 16, 128])
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 8, 128]), residual.shape if residual is not None else None=None
ğŸŸ© æ­£è¦åŒ–é–‹å§‹ x.shape=torch.Size([1, 8, 128])
ğŸŸ© æ­£è¦åŒ–å®Œäº† x.shape=torch.Size([1, 8, 128])
ğŸŸ© RoPEã®é †ä¼æ’­é–‹å§‹ positions.shape=torch.Size([1]), query.shape=torch.Size([1, 16, 128]), key.shape=torch.Size([1, 8, 128])
ğŸŸ© RoPEã‚’é©ç”¨ x.shape=torch.Size([1, 16, 128]), cos.shape=torch.Size([1, 1, 64]), sin.shape=torch.Size([1, 1, 64])
ğŸŸ¦ ãƒãƒ£ãƒ³ã‚¯åŒ– x1.shape=torch.Size([1, 16, 64]), x2.shape=torch.Size([1, 16, 64])
ğŸŸ¦ å›è»¢è¡Œåˆ—é©ç”¨ y1.shape=torch.Size([1, 16, 64]), y2.shape=torch.Size([1, 16, 64])
ğŸŸ¦ result.shape=torch.Size([1, 16, 128])
ğŸŸ© RoPEã‚’é©ç”¨ x.shape=torch.Size([1, 8, 128]), cos.shape=torch.Size([1, 1, 64]), sin.shape=torch.Size([1, 1, 64])
ğŸŸ¦ ãƒãƒ£ãƒ³ã‚¯åŒ– x1.shape=torch.Size([1, 8, 64]), x2.shape=torch.Size([1, 8, 64])
ğŸŸ¦ å›è»¢è¡Œåˆ—é©ç”¨ y1.shape=torch.Size([1, 8, 64]), y2.shape=torch.Size([1, 8, 64])
ğŸŸ¦ result.shape=torch.Size([1, 8, 128])
ğŸŸ© RoPEã®é †ä¼æ’­å®Œäº† query.shape=torch.Size([1, 16, 128]), key.shape=torch.Size([1, 8, 128])
ğŸŸ© Attentionå±¤ã®é †ä¼æ’­é–‹å§‹ q.shape=torch.Size([1, 16, 128]), k.shape=torch.Size([1, 8, 128]), v.shape=torch.Size([1, 8, 128])
ğŸŸ© ç¾åœ¨ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆå–å¾— _CONTEXT=Context(is_prefill=True, cu_seqlens_q=tensor([0, 1], device='cuda:0', dtype=torch.int32), cu_seqlens_k=tensor([0, 1], device='cuda:0', dtype=torch.int32), max_seqlen_q=1, max_seqlen_k=1, slot_mapping=tensor([256], device='cuda:0', dtype=torch.int32), context_lens=None, block_tables=None)
ğŸŸ¦ ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæƒ…å ±å–å¾— context.is_prefill=True, context.slot_mapping=tensor([256], device='cuda:0', dtype=torch.int32), context.max_seqlen_q=1, context.cu_seqlens_q=tensor([0, 1], device='cuda:0', dtype=torch.int32), context.max_seqlen_k=1, context.cu_seqlens_k=tensor([0, 1], device='cuda:0', dtype=torch.int32), context.context_lens=None, context.block_tables=None
ğŸŸ© KVã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ã‚­ãƒ¼ã¨ãƒãƒªãƒ¥ãƒ¼ã‚’ä¿å­˜ key.shape=torch.Size([1, 8, 128]), value.shape=torch.Size([1, 8, 128]), k_cache.shape=torch.Size([884, 256, 8, 128]), v_cache.shape=torch.Size([884, 256, 8, 128]), slot_mapping.shape=torch.Size([1])
ğŸŸ¦ Prefillãƒ¢ãƒ¼ãƒ‰ã§FlashAttentionã‚’å®Ÿè¡Œ context.max_seqlen_q=1, context.cu_seqlens_q=tensor([0, 1], device='cuda:0', dtype=torch.int32), context.max_seqlen_k=1, context.cu_seqlens_k=tensor([0, 1], device='cuda:0', dtype=torch.int32)
ğŸŸ© Attentionå±¤ã®é †ä¼æ’­å®Œäº† o.shape=torch.Size([1, 16, 128])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­é–‹å§‹ å…¥åŠ›å½¢çŠ¶: torch.Size([1, 2048])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­å®Œäº† å‡ºåŠ›å½¢çŠ¶: torch.Size([1, 1024])
ğŸŸ© Qwen3Attentionã®é †ä¼æ’­å®Œäº† output.shape=torch.Size([1, 1024])
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 1024]), residual.shape if residual is not None else None=torch.Size([1, 1024])
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨å®Œäº†
ğŸŸ© Qwen3MLPã®é †ä¼æ’­ x.shape=torch.Size([1, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­å®Œäº† result.shape=torch.Size([1, 6144])
ğŸŸ© SwiGLUã‚’é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 6144])
ğŸŸ¦ ãƒãƒ£ãƒ³ã‚¯åŒ– x.shape=torch.Size([1, 3072]), y.shape=torch.Size([1, 3072])
ğŸŸ© SwiGLUã‚’é †ä¼æ’­å®Œäº† result.shape=torch.Size([1, 3072])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­é–‹å§‹ å…¥åŠ›å½¢çŠ¶: torch.Size([1, 3072])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­å®Œäº† å‡ºåŠ›å½¢çŠ¶: torch.Size([1, 1024])
ğŸŸ© Qwen3MLPã®é †ä¼æ’­å®Œäº† x.shape=torch.Size([1, 1024])
ğŸŸ© Qwen3DecoderLayerã®é †ä¼æ’­å®Œäº† hidden_states.shape=torch.Size([1, 1024])
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 1024]), residual.shape if residual is not None else None=torch.Size([1, 1024])
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨å®Œäº†
ğŸŸ© Qwen3Modelã®é †ä¼æ’­å®Œäº† hidden_states.shape=torch.Size([1, 1024])
ğŸŸ© Qwen3ForCausalLMã®é †ä¼æ’­å®Œäº† output.shape=torch.Size([1, 1024])
ğŸŸ© Qwen3ForCausalLMã®ãƒ­ã‚¸ãƒƒãƒˆè¨ˆç®—é–‹å§‹ hidden_states.shape=torch.Size([1, 1024])
ğŸŸ© å‡ºåŠ›ã®è¨€èªãƒ¢ãƒ‡ãƒ«ãƒ˜ãƒƒãƒ‰ã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 1024])
ğŸŸ© ç¾åœ¨ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆå–å¾— _CONTEXT=Context(is_prefill=True, cu_seqlens_q=tensor([0, 1], device='cuda:0', dtype=torch.int32), cu_seqlens_k=tensor([0, 1], device='cuda:0', dtype=torch.int32), max_seqlen_q=1, max_seqlen_k=1, slot_mapping=tensor([256], device='cuda:0', dtype=torch.int32), context_lens=None, block_tables=None)
ğŸŸ¦ æœ€å¾Œã®ãƒˆãƒ¼ã‚¯ãƒ³ã‚’æŠ½å‡ºï¼ˆPrefillï¼‰ x.shape=torch.Size([1, 1024])
ğŸŸ© å‡ºåŠ›ã®è¨€èªãƒ¢ãƒ‡ãƒ«ãƒ˜ãƒƒãƒ‰ã®é †ä¼æ’­å®Œäº† logits.shape if logits is not None else None=torch.Size([1, 151936])
ğŸŸ© Qwen3ForCausalLMã®ãƒ­ã‚¸ãƒƒãƒˆè¨ˆç®—å®Œäº† logits.shape=torch.Size([1, 151936])
ğŸŸ© ãƒ¢ãƒ‡ãƒ«ã®é †ä¼æ¬ï¼ˆprefillï¼‰ã‚’å®Ÿè¡Œå®Œäº† result.shape=torch.Size([1, 151936])
ğŸŸ© ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°é–‹å§‹ logits.shape=torch.Size([1, 151936]), temperatures.shape=torch.Size([1])
ğŸŸ© ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°å®Œäº† sample_tokens.shape=torch.Size([1])
ğŸŸ© ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒªã‚»ãƒƒãƒˆ
ğŸŸ© æ¨è«–ã®1ã‚µã‚¤ã‚¯ãƒ«ã‚’å®Ÿè¡Œå®Œäº† (len(token_ids) if token_ids else None)=1
ğŸŸ© ç”Ÿæˆãƒˆãƒ¼ã‚¯ãƒ³ã®å¾Œå‡¦ç†ã‚’é–‹å§‹ len(seqs)=1 len(token_ids)=1
ğŸŸ© ãƒˆãƒ¼ã‚¯ãƒ³ã‚’è¿½åŠ  token_id=51803
ğŸŸ© ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã®ãƒ–ãƒ­ãƒƒã‚¯ã‚’è§£æ”¾é–‹å§‹ 0
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’è§£æ”¾é–‹å§‹ block_id=1
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’è§£æ”¾å®Œäº† block_id=1
ğŸŸ© ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã®ãƒ–ãƒ­ãƒƒã‚¯ã‚’è§£æ”¾å®Œäº† 0
ğŸŸ© ç”Ÿæˆãƒˆãƒ¼ã‚¯ãƒ³ã®å¾Œå‡¦ç†å®Œäº† len(seqs)=1
ğŸŸ© æ¨è«–ã‚¨ãƒ³ã‚¸ãƒ³ã®1ã‚µã‚¤ã‚¯ãƒ«ã‚’å®Ÿè¡Œå®Œäº† outputs=[(0, [51803])] num_tokens=2
ğŸŸ© ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ã®å®Œäº†çŠ¶æ…‹ã‚’ç¢ºèª result=True
ğŸŸ© ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ã«å‡¦ç†ã™ã¹ãã‚¿ã‚¹ã‚¯ãŒæ®‹ã£ã¦ã„ã‚‹ã‹ç¢ºèª res=True
ğŸŸ© ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆã‚’å®Œäº† len(outputs)=1
