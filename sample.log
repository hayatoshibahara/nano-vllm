SamplingParams初期化の後処理 self=SamplingParams(temperature=1.0, max_tokens=64, ignore_eos=False)
グローバルコンテキスト初期化 _CONTEXT=Context(is_prefill=False, cu_seqlens_q=None, cu_seqlens_k=None, max_seqlen_q=0, max_seqlen_k=0, slot_mapping=None, context_lens=None, block_tables=None)
エンジンを初期化開始 model='/root/huggingface/Qwen3-0.6B'
Config初期化の後処理 self=Config(model='/root/huggingface/Qwen3-0.6B', max_num_batched_tokens=16384, max_num_seqs=512, max_model_len=4096, gpu_memory_utilization=0.9, tensor_parallel_size=1, enforce_eager=True, hf_config=None, eos=-1, kvcache_block_size=256, num_kvcache_blocks=-1)
Initializing ModelRunner on rank 0
Qwen3ForCausalLMを初期化開始 config.vocab_size=151936 config.hidden_size=1024 config.tie_word_embeddings=True
Qwen3Modelを初期化開始 config.vocab_size=151936 config.hidden_size=1024 config.num_hidden_layers=28
入力の埋め込み層を初期化開始 num_embeddings=151936, embedding_dim=1024
分散環境情報取得 self.tp_rank=0, self.tp_size=1
担当する語彙の範囲計算 self.vocab_start_idx=0, self.vocab_end_idx=151936
担当する語彙の重み行列初期化 self.weight.shape=torch.Size([151936, 1024])
入力の埋め込み層を初期化完了
Qwen3DecoderLayerを初期化開始 config.hidden_size=1024 config.num_attention_heads=16 config.num_key_value_heads=8 config.intermediate_size=3072 config.rms_norm_eps=1e-06 config.hidden_act='silu'
Qwen3Attentionを初期化開始 hidden_size: 1024, num_heads: 16, num_kv_heads: 8, max_position: 40960, head_dim: 128, rms_norm_eps: 1e-06, qkv_bias: False, rope_theta: 1000000, rope_scaling: None
QKVParallelLinearを初期化開始 hidden_size=1024, head_size=128, total_num_heads=16, total_num_kv_heads=8, bias=False
出力次元を並列化する線形層を初期化開始 input_size=1024, output_size=4096, bias=False
線形層の基底クラスを初期化開始 input_size=1024, output_size=4096, bias=False, tp_dim=0
線形層の基底クラスを初期化完了
出力次元を並列化する線形層を初期化完了 self.tp_size=1
QKVParallelLinearを初期化完了 self.tp_size=1, self.num_heads=16, self.num_kv_heads=8
RowParallelLinearを初期化開始 input_size=2048, output_size=1024, bias=False
線形層の基底クラスを初期化開始 input_size=2048, output_size=1024, bias=False, tp_dim=1
線形層の基底クラスを初期化完了
RowParallelLinearを初期化完了 self.tp_size=1
RoPEを初期化 head_size=128, rotary_dim=128, max_position_embeddings=40960, base=1000000
周波数計算 freqs.shape=torch.Size([40960, 64])
コサイン・サインキャッシュ計算 cache.shape=torch.Size([40960, 1, 128])
RoPEの初期化完了
Attention層を初期化 num_heads=16, head_dim=128, scale=0.08838834764831845, num_kv_heads=8
Attention層を初期化完了
RMSNormを初期化 hidden_size=128, eps=1e-06
RMSNormの初期化完了
RMSNormを初期化 hidden_size=128, eps=1e-06
RMSNormの初期化完了
Qwen3Attentionの初期化完了
Qwen3MLPを初期化開始 hidden_size=1024 intermediate_size=3072 hidden_act='silu'
MergedColumnParallelLinearを初期化開始 input_size=1024, output_sizes=[3072, 3072], bias=False
出力次元を並列化する線形層を初期化開始 input_size=1024, output_size=6144, bias=False
線形層の基底クラスを初期化開始 input_size=1024, output_size=6144, bias=False, tp_dim=0
線形層の基底クラスを初期化完了
出力次元を並列化する線形層を初期化完了 self.tp_size=1
MergedColumnParallelLinearを初期化完了
RowParallelLinearを初期化開始 input_size=3072, output_size=1024, bias=False
線形層の基底クラスを初期化開始 input_size=3072, output_size=1024, bias=False, tp_dim=1
線形層の基底クラスを初期化完了
RowParallelLinearを初期化完了 self.tp_size=1
SwiGLUを初期化
Qwen3MLPの初期化完了
RMSNormを初期化 hidden_size=1024, eps=1e-06
RMSNormの初期化完了
RMSNormを初期化 hidden_size=1024, eps=1e-06
RMSNormの初期化完了
Qwen3DecoderLayerの初期化完了
Qwen3DecoderLayerを初期化開始 config.hidden_size=1024 config.num_attention_heads=16 config.num_key_value_heads=8 config.intermediate_size=3072 config.rms_norm_eps=1e-06 config.hidden_act='silu'
Qwen3Attentionを初期化開始 hidden_size: 1024, num_heads: 16, num_kv_heads: 8, max_position: 40960, head_dim: 128, rms_norm_eps: 1e-06, qkv_bias: False, rope_theta: 1000000, rope_scaling: None
QKVParallelLinearを初期化開始 hidden_size=1024, head_size=128, total_num_heads=16, total_num_kv_heads=8, bias=False
出力次元を並列化する線形層を初期化開始 input_size=1024, output_size=4096, bias=False
線形層の基底クラスを初期化開始 input_size=1024, output_size=4096, bias=False, tp_dim=0
線形層の基底クラスを初期化完了
出力次元を並列化する線形層を初期化完了 self.tp_size=1
QKVParallelLinearを初期化完了 self.tp_size=1, self.num_heads=16, self.num_kv_heads=8
RowParallelLinearを初期化開始 input_size=2048, output_size=1024, bias=False
線形層の基底クラスを初期化開始 input_size=2048, output_size=1024, bias=False, tp_dim=1
線形層の基底クラスを初期化完了
RowParallelLinearを初期化完了 self.tp_size=1
Attention層を初期化 num_heads=16, head_dim=128, scale=0.08838834764831845, num_kv_heads=8
Attention層を初期化完了
RMSNormを初期化 hidden_size=128, eps=1e-06
RMSNormの初期化完了
RMSNormを初期化 hidden_size=128, eps=1e-06
RMSNormの初期化完了
Qwen3Attentionの初期化完了
Qwen3MLPを初期化開始 hidden_size=1024 intermediate_size=3072 hidden_act='silu'
MergedColumnParallelLinearを初期化開始 input_size=1024, output_sizes=[3072, 3072], bias=False
出力次元を並列化する線形層を初期化開始 input_size=1024, output_size=6144, bias=False
線形層の基底クラスを初期化開始 input_size=1024, output_size=6144, bias=False, tp_dim=0
線形層の基底クラスを初期化完了
出力次元を並列化する線形層を初期化完了 self.tp_size=1
MergedColumnParallelLinearを初期化完了
RowParallelLinearを初期化開始 input_size=3072, output_size=1024, bias=False
線形層の基底クラスを初期化開始 input_size=3072, output_size=1024, bias=False, tp_dim=1
線形層の基底クラスを初期化完了
RowParallelLinearを初期化完了 self.tp_size=1
SwiGLUを初期化
Qwen3MLPの初期化完了
RMSNormを初期化 hidden_size=1024, eps=1e-06
RMSNormの初期化完了
RMSNormを初期化 hidden_size=1024, eps=1e-06
RMSNormの初期化完了
Qwen3DecoderLayerの初期化完了
Qwen3DecoderLayerを初期化開始 config.hidden_size=1024 config.num_attention_heads=16 config.num_key_value_heads=8 config.intermediate_size=3072 config.rms_norm_eps=1e-06 config.hidden_act='silu'
Qwen3Attentionを初期化開始 hidden_size: 1024, num_heads: 16, num_kv_heads: 8, max_position: 40960, head_dim: 128, rms_norm_eps: 1e-06, qkv_bias: False, rope_theta: 1000000, rope_scaling: None
QKVParallelLinearを初期化開始 hidden_size=1024, head_size=128, total_num_heads=16, total_num_kv_heads=8, bias=False
出力次元を並列化する線形層を初期化開始 input_size=1024, output_size=4096, bias=False
線形層の基底クラスを初期化開始 input_size=1024, output_size=4096, bias=False, tp_dim=0
線形層の基底クラスを初期化完了
出力次元を並列化する線形層を初期化完了 self.tp_size=1
QKVParallelLinearを初期化完了 self.tp_size=1, self.num_heads=16, self.num_kv_heads=8
RowParallelLinearを初期化開始 input_size=2048, output_size=1024, bias=False
線形層の基底クラスを初期化開始 input_size=2048, output_size=1024, bias=False, tp_dim=1
線形層の基底クラスを初期化完了
RowParallelLinearを初期化完了 self.tp_size=1
Attention層を初期化 num_heads=16, head_dim=128, scale=0.08838834764831845, num_kv_heads=8
Attention層を初期化完了
RMSNormを初期化 hidden_size=128, eps=1e-06
RMSNormの初期化完了
RMSNormを初期化 hidden_size=128, eps=1e-06
RMSNormの初期化完了
Qwen3Attentionの初期化完了
Qwen3MLPを初期化開始 hidden_size=1024 intermediate_size=3072 hidden_act='silu'
MergedColumnParallelLinearを初期化開始 input_size=1024, output_sizes=[3072, 3072], bias=False
出力次元を並列化する線形層を初期化開始 input_size=1024, output_size=6144, bias=False
線形層の基底クラスを初期化開始 input_size=1024, output_size=6144, bias=False, tp_dim=0
線形層の基底クラスを初期化完了
出力次元を並列化する線形層を初期化完了 self.tp_size=1
MergedColumnParallelLinearを初期化完了
RowParallelLinearを初期化開始 input_size=3072, output_size=1024, bias=False
線形層の基底クラスを初期化開始 input_size=3072, output_size=1024, bias=False, tp_dim=1
線形層の基底クラスを初期化完了
RowParallelLinearを初期化完了 self.tp_size=1
SwiGLUを初期化
Qwen3MLPの初期化完了
RMSNormを初期化 hidden_size=1024, eps=1e-06
RMSNormの初期化完了
RMSNormを初期化 hidden_size=1024, eps=1e-06
RMSNormの初期化完了
Qwen3DecoderLayerの初期化完了
Qwen3DecoderLayerを初期化開始 config.hidden_size=1024 config.num_attention_heads=16 config.num_key_value_heads=8 config.intermediate_size=3072 config.rms_norm_eps=1e-06 config.hidden_act='silu'
Qwen3Attentionを初期化開始 hidden_size: 1024, num_heads: 16, num_kv_heads: 8, max_position: 40960, head_dim: 128, rms_norm_eps: 1e-06, qkv_bias: False, rope_theta: 1000000, rope_scaling: None
QKVParallelLinearを初期化開始 hidden_size=1024, head_size=128, total_num_heads=16, total_num_kv_heads=8, bias=False
出力次元を並列化する線形層を初期化開始 input_size=1024, output_size=4096, bias=False
線形層の基底クラスを初期化開始 input_size=1024, output_size=4096, bias=False, tp_dim=0
線形層の基底クラスを初期化完了
出力次元を並列化する線形層を初期化完了 self.tp_size=1
QKVParallelLinearを初期化完了 self.tp_size=1, self.num_heads=16, self.num_kv_heads=8
RowParallelLinearを初期化開始 input_size=2048, output_size=1024, bias=False
線形層の基底クラスを初期化開始 input_size=2048, output_size=1024, bias=False, tp_dim=1
線形層の基底クラスを初期化完了
RowParallelLinearを初期化完了 self.tp_size=1
Attention層を初期化 num_heads=16, head_dim=128, scale=0.08838834764831845, num_kv_heads=8
Attention層を初期化完了
RMSNormを初期化 hidden_size=128, eps=1e-06
RMSNormの初期化完了
RMSNormを初期化 hidden_size=128, eps=1e-06
RMSNormの初期化完了
Qwen3Attentionの初期化完了
Qwen3MLPを初期化開始 hidden_size=1024 intermediate_size=3072 hidden_act='silu'
MergedColumnParallelLinearを初期化開始 input_size=1024, output_sizes=[3072, 3072], bias=False
出力次元を並列化する線形層を初期化開始 input_size=1024, output_size=6144, bias=False
線形層の基底クラスを初期化開始 input_size=1024, output_size=6144, bias=False, tp_dim=0
線形層の基底クラスを初期化完了
出力次元を並列化する線形層を初期化完了 self.tp_size=1
MergedColumnParallelLinearを初期化完了
RowParallelLinearを初期化開始 input_size=3072, output_size=1024, bias=False
線形層の基底クラスを初期化開始 input_size=3072, output_size=1024, bias=False, tp_dim=1
線形層の基底クラスを初期化完了
RowParallelLinearを初期化完了 self.tp_size=1
SwiGLUを初期化
Qwen3MLPの初期化完了
RMSNormを初期化 hidden_size=1024, eps=1e-06
RMSNormの初期化完了
RMSNormを初期化 hidden_size=1024, eps=1e-06
RMSNormの初期化完了
Qwen3DecoderLayerの初期化完了
Qwen3DecoderLayerを初期化開始 config.hidden_size=1024 config.num_attention_heads=16 config.num_key_value_heads=8 config.intermediate_size=3072 config.rms_norm_eps=1e-06 config.hidden_act='silu'
Qwen3Attentionを初期化開始 hidden_size: 1024, num_heads: 16, num_kv_heads: 8, max_position: 40960, head_dim: 128, rms_norm_eps: 1e-06, qkv_bias: False, rope_theta: 1000000, rope_scaling: None
QKVParallelLinearを初期化開始 hidden_size=1024, head_size=128, total_num_heads=16, total_num_kv_heads=8, bias=False
出力次元を並列化する線形層を初期化開始 input_size=1024, output_size=4096, bias=False
線形層の基底クラスを初期化開始 input_size=1024, output_size=4096, bias=False, tp_dim=0
線形層の基底クラスを初期化完了
出力次元を並列化する線形層を初期化完了 self.tp_size=1
QKVParallelLinearを初期化完了 self.tp_size=1, self.num_heads=16, self.num_kv_heads=8
RowParallelLinearを初期化開始 input_size=2048, output_size=1024, bias=False
線形層の基底クラスを初期化開始 input_size=2048, output_size=1024, bias=False, tp_dim=1
線形層の基底クラスを初期化完了
RowParallelLinearを初期化完了 self.tp_size=1
Attention層を初期化 num_heads=16, head_dim=128, scale=0.08838834764831845, num_kv_heads=8
Attention層を初期化完了
RMSNormを初期化 hidden_size=128, eps=1e-06
RMSNormの初期化完了
RMSNormを初期化 hidden_size=128, eps=1e-06
RMSNormの初期化完了
Qwen3Attentionの初期化完了
Qwen3MLPを初期化開始 hidden_size=1024 intermediate_size=3072 hidden_act='silu'
MergedColumnParallelLinearを初期化開始 input_size=1024, output_sizes=[3072, 3072], bias=False
出力次元を並列化する線形層を初期化開始 input_size=1024, output_size=6144, bias=False
線形層の基底クラスを初期化開始 input_size=1024, output_size=6144, bias=False, tp_dim=0
線形層の基底クラスを初期化完了
出力次元を並列化する線形層を初期化完了 self.tp_size=1
MergedColumnParallelLinearを初期化完了
RowParallelLinearを初期化開始 input_size=3072, output_size=1024, bias=False
線形層の基底クラスを初期化開始 input_size=3072, output_size=1024, bias=False, tp_dim=1
線形層の基底クラスを初期化完了
RowParallelLinearを初期化完了 self.tp_size=1
SwiGLUを初期化
Qwen3MLPの初期化完了
RMSNormを初期化 hidden_size=1024, eps=1e-06
RMSNormの初期化完了
RMSNormを初期化 hidden_size=1024, eps=1e-06
RMSNormの初期化完了
Qwen3DecoderLayerの初期化完了
Qwen3DecoderLayerを初期化開始 config.hidden_size=1024 config.num_attention_heads=16 config.num_key_value_heads=8 config.intermediate_size=3072 config.rms_norm_eps=1e-06 config.hidden_act='silu'
Qwen3Attentionを初期化開始 hidden_size: 1024, num_heads: 16, num_kv_heads: 8, max_position: 40960, head_dim: 128, rms_norm_eps: 1e-06, qkv_bias: False, rope_theta: 1000000, rope_scaling: None
QKVParallelLinearを初期化開始 hidden_size=1024, head_size=128, total_num_heads=16, total_num_kv_heads=8, bias=False
出力次元を並列化する線形層を初期化開始 input_size=1024, output_size=4096, bias=False
線形層の基底クラスを初期化開始 input_size=1024, output_size=4096, bias=False, tp_dim=0
線形層の基底クラスを初期化完了
出力次元を並列化する線形層を初期化完了 self.tp_size=1
QKVParallelLinearを初期化完了 self.tp_size=1, self.num_heads=16, self.num_kv_heads=8
RowParallelLinearを初期化開始 input_size=2048, output_size=1024, bias=False
線形層の基底クラスを初期化開始 input_size=2048, output_size=1024, bias=False, tp_dim=1
線形層の基底クラスを初期化完了
RowParallelLinearを初期化完了 self.tp_size=1
Attention層を初期化 num_heads=16, head_dim=128, scale=0.08838834764831845, num_kv_heads=8
Attention層を初期化完了
RMSNormを初期化 hidden_size=128, eps=1e-06
RMSNormの初期化完了
RMSNormを初期化 hidden_size=128, eps=1e-06
RMSNormの初期化完了
Qwen3Attentionの初期化完了
Qwen3MLPを初期化開始 hidden_size=1024 intermediate_size=3072 hidden_act='silu'
MergedColumnParallelLinearを初期化開始 input_size=1024, output_sizes=[3072, 3072], bias=False
出力次元を並列化する線形層を初期化開始 input_size=1024, output_size=6144, bias=False
線形層の基底クラスを初期化開始 input_size=1024, output_size=6144, bias=False, tp_dim=0
線形層の基底クラスを初期化完了
出力次元を並列化する線形層を初期化完了 self.tp_size=1
MergedColumnParallelLinearを初期化完了
RowParallelLinearを初期化開始 input_size=3072, output_size=1024, bias=False
線形層の基底クラスを初期化開始 input_size=3072, output_size=1024, bias=False, tp_dim=1
線形層の基底クラスを初期化完了
RowParallelLinearを初期化完了 self.tp_size=1
SwiGLUを初期化
Qwen3MLPの初期化完了
RMSNormを初期化 hidden_size=1024, eps=1e-06
RMSNormの初期化完了
RMSNormを初期化 hidden_size=1024, eps=1e-06
RMSNormの初期化完了
Qwen3DecoderLayerの初期化完了
Qwen3DecoderLayerを初期化開始 config.hidden_size=1024 config.num_attention_heads=16 config.num_key_value_heads=8 config.intermediate_size=3072 config.rms_norm_eps=1e-06 config.hidden_act='silu'
Qwen3Attentionを初期化開始 hidden_size: 1024, num_heads: 16, num_kv_heads: 8, max_position: 40960, head_dim: 128, rms_norm_eps: 1e-06, qkv_bias: False, rope_theta: 1000000, rope_scaling: None
QKVParallelLinearを初期化開始 hidden_size=1024, head_size=128, total_num_heads=16, total_num_kv_heads=8, bias=False
出力次元を並列化する線形層を初期化開始 input_size=1024, output_size=4096, bias=False
線形層の基底クラスを初期化開始 input_size=1024, output_size=4096, bias=False, tp_dim=0
線形層の基底クラスを初期化完了
出力次元を並列化する線形層を初期化完了 self.tp_size=1
QKVParallelLinearを初期化完了 self.tp_size=1, self.num_heads=16, self.num_kv_heads=8
RowParallelLinearを初期化開始 input_size=2048, output_size=1024, bias=False
線形層の基底クラスを初期化開始 input_size=2048, output_size=1024, bias=False, tp_dim=1
線形層の基底クラスを初期化完了
RowParallelLinearを初期化完了 self.tp_size=1
Attention層を初期化 num_heads=16, head_dim=128, scale=0.08838834764831845, num_kv_heads=8
Attention層を初期化完了
RMSNormを初期化 hidden_size=128, eps=1e-06
RMSNormの初期化完了
RMSNormを初期化 hidden_size=128, eps=1e-06
RMSNormの初期化完了
Qwen3Attentionの初期化完了
Qwen3MLPを初期化開始 hidden_size=1024 intermediate_size=3072 hidden_act='silu'
MergedColumnParallelLinearを初期化開始 input_size=1024, output_sizes=[3072, 3072], bias=False
出力次元を並列化する線形層を初期化開始 input_size=1024, output_size=6144, bias=False
線形層の基底クラスを初期化開始 input_size=1024, output_size=6144, bias=False, tp_dim=0
線形層の基底クラスを初期化完了
出力次元を並列化する線形層を初期化完了 self.tp_size=1
MergedColumnParallelLinearを初期化完了
RowParallelLinearを初期化開始 input_size=3072, output_size=1024, bias=False
線形層の基底クラスを初期化開始 input_size=3072, output_size=1024, bias=False, tp_dim=1
線形層の基底クラスを初期化完了
RowParallelLinearを初期化完了 self.tp_size=1
SwiGLUを初期化
Qwen3MLPの初期化完了
RMSNormを初期化 hidden_size=1024, eps=1e-06
RMSNormの初期化完了
RMSNormを初期化 hidden_size=1024, eps=1e-06
RMSNormの初期化完了
Qwen3DecoderLayerの初期化完了
Qwen3DecoderLayerを初期化開始 config.hidden_size=1024 config.num_attention_heads=16 config.num_key_value_heads=8 config.intermediate_size=3072 config.rms_norm_eps=1e-06 config.hidden_act='silu'
Qwen3Attentionを初期化開始 hidden_size: 1024, num_heads: 16, num_kv_heads: 8, max_position: 40960, head_dim: 128, rms_norm_eps: 1e-06, qkv_bias: False, rope_theta: 1000000, rope_scaling: None
QKVParallelLinearを初期化開始 hidden_size=1024, head_size=128, total_num_heads=16, total_num_kv_heads=8, bias=False
出力次元を並列化する線形層を初期化開始 input_size=1024, output_size=4096, bias=False
線形層の基底クラスを初期化開始 input_size=1024, output_size=4096, bias=False, tp_dim=0
線形層の基底クラスを初期化完了
出力次元を並列化する線形層を初期化完了 self.tp_size=1
QKVParallelLinearを初期化完了 self.tp_size=1, self.num_heads=16, self.num_kv_heads=8
RowParallelLinearを初期化開始 input_size=2048, output_size=1024, bias=False
線形層の基底クラスを初期化開始 input_size=2048, output_size=1024, bias=False, tp_dim=1
線形層の基底クラスを初期化完了
RowParallelLinearを初期化完了 self.tp_size=1
Attention層を初期化 num_heads=16, head_dim=128, scale=0.08838834764831845, num_kv_heads=8
Attention層を初期化完了
RMSNormを初期化 hidden_size=128, eps=1e-06
RMSNormの初期化完了
RMSNormを初期化 hidden_size=128, eps=1e-06
RMSNormの初期化完了
Qwen3Attentionの初期化完了
Qwen3MLPを初期化開始 hidden_size=1024 intermediate_size=3072 hidden_act='silu'
MergedColumnParallelLinearを初期化開始 input_size=1024, output_sizes=[3072, 3072], bias=False
出力次元を並列化する線形層を初期化開始 input_size=1024, output_size=6144, bias=False
線形層の基底クラスを初期化開始 input_size=1024, output_size=6144, bias=False, tp_dim=0
線形層の基底クラスを初期化完了
出力次元を並列化する線形層を初期化完了 self.tp_size=1
MergedColumnParallelLinearを初期化完了
RowParallelLinearを初期化開始 input_size=3072, output_size=1024, bias=False
線形層の基底クラスを初期化開始 input_size=3072, output_size=1024, bias=False, tp_dim=1
線形層の基底クラスを初期化完了
RowParallelLinearを初期化完了 self.tp_size=1
SwiGLUを初期化
Qwen3MLPの初期化完了
RMSNormを初期化 hidden_size=1024, eps=1e-06
RMSNormの初期化完了
RMSNormを初期化 hidden_size=1024, eps=1e-06
RMSNormの初期化完了
Qwen3DecoderLayerの初期化完了
Qwen3DecoderLayerを初期化開始 config.hidden_size=1024 config.num_attention_heads=16 config.num_key_value_heads=8 config.intermediate_size=3072 config.rms_norm_eps=1e-06 config.hidden_act='silu'
Qwen3Attentionを初期化開始 hidden_size: 1024, num_heads: 16, num_kv_heads: 8, max_position: 40960, head_dim: 128, rms_norm_eps: 1e-06, qkv_bias: False, rope_theta: 1000000, rope_scaling: None
QKVParallelLinearを初期化開始 hidden_size=1024, head_size=128, total_num_heads=16, total_num_kv_heads=8, bias=False
出力次元を並列化する線形層を初期化開始 input_size=1024, output_size=4096, bias=False
線形層の基底クラスを初期化開始 input_size=1024, output_size=4096, bias=False, tp_dim=0
線形層の基底クラスを初期化完了
出力次元を並列化する線形層を初期化完了 self.tp_size=1
QKVParallelLinearを初期化完了 self.tp_size=1, self.num_heads=16, self.num_kv_heads=8
RowParallelLinearを初期化開始 input_size=2048, output_size=1024, bias=False
線形層の基底クラスを初期化開始 input_size=2048, output_size=1024, bias=False, tp_dim=1
線形層の基底クラスを初期化完了
RowParallelLinearを初期化完了 self.tp_size=1
Attention層を初期化 num_heads=16, head_dim=128, scale=0.08838834764831845, num_kv_heads=8
Attention層を初期化完了
RMSNormを初期化 hidden_size=128, eps=1e-06
RMSNormの初期化完了
RMSNormを初期化 hidden_size=128, eps=1e-06
RMSNormの初期化完了
Qwen3Attentionの初期化完了
Qwen3MLPを初期化開始 hidden_size=1024 intermediate_size=3072 hidden_act='silu'
MergedColumnParallelLinearを初期化開始 input_size=1024, output_sizes=[3072, 3072], bias=False
出力次元を並列化する線形層を初期化開始 input_size=1024, output_size=6144, bias=False
線形層の基底クラスを初期化開始 input_size=1024, output_size=6144, bias=False, tp_dim=0
線形層の基底クラスを初期化完了
出力次元を並列化する線形層を初期化完了 self.tp_size=1
MergedColumnParallelLinearを初期化完了
RowParallelLinearを初期化開始 input_size=3072, output_size=1024, bias=False
線形層の基底クラスを初期化開始 input_size=3072, output_size=1024, bias=False, tp_dim=1
線形層の基底クラスを初期化完了
RowParallelLinearを初期化完了 self.tp_size=1
SwiGLUを初期化
Qwen3MLPの初期化完了
RMSNormを初期化 hidden_size=1024, eps=1e-06
RMSNormの初期化完了
RMSNormを初期化 hidden_size=1024, eps=1e-06
RMSNormの初期化完了
Qwen3DecoderLayerの初期化完了
Qwen3DecoderLayerを初期化開始 config.hidden_size=1024 config.num_attention_heads=16 config.num_key_value_heads=8 config.intermediate_size=3072 config.rms_norm_eps=1e-06 config.hidden_act='silu'
Qwen3Attentionを初期化開始 hidden_size: 1024, num_heads: 16, num_kv_heads: 8, max_position: 40960, head_dim: 128, rms_norm_eps: 1e-06, qkv_bias: False, rope_theta: 1000000, rope_scaling: None
QKVParallelLinearを初期化開始 hidden_size=1024, head_size=128, total_num_heads=16, total_num_kv_heads=8, bias=False
出力次元を並列化する線形層を初期化開始 input_size=1024, output_size=4096, bias=False
線形層の基底クラスを初期化開始 input_size=1024, output_size=4096, bias=False, tp_dim=0
線形層の基底クラスを初期化完了
出力次元を並列化する線形層を初期化完了 self.tp_size=1
QKVParallelLinearを初期化完了 self.tp_size=1, self.num_heads=16, self.num_kv_heads=8
RowParallelLinearを初期化開始 input_size=2048, output_size=1024, bias=False
線形層の基底クラスを初期化開始 input_size=2048, output_size=1024, bias=False, tp_dim=1
線形層の基底クラスを初期化完了
RowParallelLinearを初期化完了 self.tp_size=1
Attention層を初期化 num_heads=16, head_dim=128, scale=0.08838834764831845, num_kv_heads=8
Attention層を初期化完了
RMSNormを初期化 hidden_size=128, eps=1e-06
RMSNormの初期化完了
RMSNormを初期化 hidden_size=128, eps=1e-06
RMSNormの初期化完了
Qwen3Attentionの初期化完了
Qwen3MLPを初期化開始 hidden_size=1024 intermediate_size=3072 hidden_act='silu'
MergedColumnParallelLinearを初期化開始 input_size=1024, output_sizes=[3072, 3072], bias=False
出力次元を並列化する線形層を初期化開始 input_size=1024, output_size=6144, bias=False
線形層の基底クラスを初期化開始 input_size=1024, output_size=6144, bias=False, tp_dim=0
線形層の基底クラスを初期化完了
出力次元を並列化する線形層を初期化完了 self.tp_size=1
MergedColumnParallelLinearを初期化完了
RowParallelLinearを初期化開始 input_size=3072, output_size=1024, bias=False
線形層の基底クラスを初期化開始 input_size=3072, output_size=1024, bias=False, tp_dim=1
線形層の基底クラスを初期化完了
RowParallelLinearを初期化完了 self.tp_size=1
SwiGLUを初期化
Qwen3MLPの初期化完了
RMSNormを初期化 hidden_size=1024, eps=1e-06
RMSNormの初期化完了
RMSNormを初期化 hidden_size=1024, eps=1e-06
RMSNormの初期化完了
Qwen3DecoderLayerの初期化完了
Qwen3DecoderLayerを初期化開始 config.hidden_size=1024 config.num_attention_heads=16 config.num_key_value_heads=8 config.intermediate_size=3072 config.rms_norm_eps=1e-06 config.hidden_act='silu'
Qwen3Attentionを初期化開始 hidden_size: 1024, num_heads: 16, num_kv_heads: 8, max_position: 40960, head_dim: 128, rms_norm_eps: 1e-06, qkv_bias: False, rope_theta: 1000000, rope_scaling: None
QKVParallelLinearを初期化開始 hidden_size=1024, head_size=128, total_num_heads=16, total_num_kv_heads=8, bias=False
出力次元を並列化する線形層を初期化開始 input_size=1024, output_size=4096, bias=False
線形層の基底クラスを初期化開始 input_size=1024, output_size=4096, bias=False, tp_dim=0
線形層の基底クラスを初期化完了
出力次元を並列化する線形層を初期化完了 self.tp_size=1
QKVParallelLinearを初期化完了 self.tp_size=1, self.num_heads=16, self.num_kv_heads=8
RowParallelLinearを初期化開始 input_size=2048, output_size=1024, bias=False
線形層の基底クラスを初期化開始 input_size=2048, output_size=1024, bias=False, tp_dim=1
線形層の基底クラスを初期化完了
RowParallelLinearを初期化完了 self.tp_size=1
Attention層を初期化 num_heads=16, head_dim=128, scale=0.08838834764831845, num_kv_heads=8
Attention層を初期化完了
RMSNormを初期化 hidden_size=128, eps=1e-06
RMSNormの初期化完了
RMSNormを初期化 hidden_size=128, eps=1e-06
RMSNormの初期化完了
Qwen3Attentionの初期化完了
Qwen3MLPを初期化開始 hidden_size=1024 intermediate_size=3072 hidden_act='silu'
MergedColumnParallelLinearを初期化開始 input_size=1024, output_sizes=[3072, 3072], bias=False
出力次元を並列化する線形層を初期化開始 input_size=1024, output_size=6144, bias=False
線形層の基底クラスを初期化開始 input_size=1024, output_size=6144, bias=False, tp_dim=0
線形層の基底クラスを初期化完了
出力次元を並列化する線形層を初期化完了 self.tp_size=1
MergedColumnParallelLinearを初期化完了
RowParallelLinearを初期化開始 input_size=3072, output_size=1024, bias=False
線形層の基底クラスを初期化開始 input_size=3072, output_size=1024, bias=False, tp_dim=1
線形層の基底クラスを初期化完了
RowParallelLinearを初期化完了 self.tp_size=1
SwiGLUを初期化
Qwen3MLPの初期化完了
RMSNormを初期化 hidden_size=1024, eps=1e-06
RMSNormの初期化完了
RMSNormを初期化 hidden_size=1024, eps=1e-06
RMSNormの初期化完了
Qwen3DecoderLayerの初期化完了
Qwen3DecoderLayerを初期化開始 config.hidden_size=1024 config.num_attention_heads=16 config.num_key_value_heads=8 config.intermediate_size=3072 config.rms_norm_eps=1e-06 config.hidden_act='silu'
Qwen3Attentionを初期化開始 hidden_size: 1024, num_heads: 16, num_kv_heads: 8, max_position: 40960, head_dim: 128, rms_norm_eps: 1e-06, qkv_bias: False, rope_theta: 1000000, rope_scaling: None
QKVParallelLinearを初期化開始 hidden_size=1024, head_size=128, total_num_heads=16, total_num_kv_heads=8, bias=False
出力次元を並列化する線形層を初期化開始 input_size=1024, output_size=4096, bias=False
線形層の基底クラスを初期化開始 input_size=1024, output_size=4096, bias=False, tp_dim=0
線形層の基底クラスを初期化完了
出力次元を並列化する線形層を初期化完了 self.tp_size=1
QKVParallelLinearを初期化完了 self.tp_size=1, self.num_heads=16, self.num_kv_heads=8
RowParallelLinearを初期化開始 input_size=2048, output_size=1024, bias=False
線形層の基底クラスを初期化開始 input_size=2048, output_size=1024, bias=False, tp_dim=1
線形層の基底クラスを初期化完了
RowParallelLinearを初期化完了 self.tp_size=1
Attention層を初期化 num_heads=16, head_dim=128, scale=0.08838834764831845, num_kv_heads=8
Attention層を初期化完了
RMSNormを初期化 hidden_size=128, eps=1e-06
RMSNormの初期化完了
RMSNormを初期化 hidden_size=128, eps=1e-06
RMSNormの初期化完了
Qwen3Attentionの初期化完了
Qwen3MLPを初期化開始 hidden_size=1024 intermediate_size=3072 hidden_act='silu'
MergedColumnParallelLinearを初期化開始 input_size=1024, output_sizes=[3072, 3072], bias=False
出力次元を並列化する線形層を初期化開始 input_size=1024, output_size=6144, bias=False
線形層の基底クラスを初期化開始 input_size=1024, output_size=6144, bias=False, tp_dim=0
線形層の基底クラスを初期化完了
出力次元を並列化する線形層を初期化完了 self.tp_size=1
MergedColumnParallelLinearを初期化完了
RowParallelLinearを初期化開始 input_size=3072, output_size=1024, bias=False
線形層の基底クラスを初期化開始 input_size=3072, output_size=1024, bias=False, tp_dim=1
線形層の基底クラスを初期化完了
RowParallelLinearを初期化完了 self.tp_size=1
SwiGLUを初期化
Qwen3MLPの初期化完了
RMSNormを初期化 hidden_size=1024, eps=1e-06
RMSNormの初期化完了
RMSNormを初期化 hidden_size=1024, eps=1e-06
RMSNormの初期化完了
Qwen3DecoderLayerの初期化完了
Qwen3DecoderLayerを初期化開始 config.hidden_size=1024 config.num_attention_heads=16 config.num_key_value_heads=8 config.intermediate_size=3072 config.rms_norm_eps=1e-06 config.hidden_act='silu'
Qwen3Attentionを初期化開始 hidden_size: 1024, num_heads: 16, num_kv_heads: 8, max_position: 40960, head_dim: 128, rms_norm_eps: 1e-06, qkv_bias: False, rope_theta: 1000000, rope_scaling: None
QKVParallelLinearを初期化開始 hidden_size=1024, head_size=128, total_num_heads=16, total_num_kv_heads=8, bias=False
出力次元を並列化する線形層を初期化開始 input_size=1024, output_size=4096, bias=False
線形層の基底クラスを初期化開始 input_size=1024, output_size=4096, bias=False, tp_dim=0
線形層の基底クラスを初期化完了
出力次元を並列化する線形層を初期化完了 self.tp_size=1
QKVParallelLinearを初期化完了 self.tp_size=1, self.num_heads=16, self.num_kv_heads=8
RowParallelLinearを初期化開始 input_size=2048, output_size=1024, bias=False
線形層の基底クラスを初期化開始 input_size=2048, output_size=1024, bias=False, tp_dim=1
線形層の基底クラスを初期化完了
RowParallelLinearを初期化完了 self.tp_size=1
Attention層を初期化 num_heads=16, head_dim=128, scale=0.08838834764831845, num_kv_heads=8
Attention層を初期化完了
RMSNormを初期化 hidden_size=128, eps=1e-06
RMSNormの初期化完了
RMSNormを初期化 hidden_size=128, eps=1e-06
RMSNormの初期化完了
Qwen3Attentionの初期化完了
Qwen3MLPを初期化開始 hidden_size=1024 intermediate_size=3072 hidden_act='silu'
MergedColumnParallelLinearを初期化開始 input_size=1024, output_sizes=[3072, 3072], bias=False
出力次元を並列化する線形層を初期化開始 input_size=1024, output_size=6144, bias=False
線形層の基底クラスを初期化開始 input_size=1024, output_size=6144, bias=False, tp_dim=0
線形層の基底クラスを初期化完了
出力次元を並列化する線形層を初期化完了 self.tp_size=1
MergedColumnParallelLinearを初期化完了
RowParallelLinearを初期化開始 input_size=3072, output_size=1024, bias=False
線形層の基底クラスを初期化開始 input_size=3072, output_size=1024, bias=False, tp_dim=1
線形層の基底クラスを初期化完了
RowParallelLinearを初期化完了 self.tp_size=1
SwiGLUを初期化
Qwen3MLPの初期化完了
RMSNormを初期化 hidden_size=1024, eps=1e-06
RMSNormの初期化完了
RMSNormを初期化 hidden_size=1024, eps=1e-06
RMSNormの初期化完了
Qwen3DecoderLayerの初期化完了
Qwen3DecoderLayerを初期化開始 config.hidden_size=1024 config.num_attention_heads=16 config.num_key_value_heads=8 config.intermediate_size=3072 config.rms_norm_eps=1e-06 config.hidden_act='silu'
Qwen3Attentionを初期化開始 hidden_size: 1024, num_heads: 16, num_kv_heads: 8, max_position: 40960, head_dim: 128, rms_norm_eps: 1e-06, qkv_bias: False, rope_theta: 1000000, rope_scaling: None
QKVParallelLinearを初期化開始 hidden_size=1024, head_size=128, total_num_heads=16, total_num_kv_heads=8, bias=False
出力次元を並列化する線形層を初期化開始 input_size=1024, output_size=4096, bias=False
線形層の基底クラスを初期化開始 input_size=1024, output_size=4096, bias=False, tp_dim=0
線形層の基底クラスを初期化完了
出力次元を並列化する線形層を初期化完了 self.tp_size=1
QKVParallelLinearを初期化完了 self.tp_size=1, self.num_heads=16, self.num_kv_heads=8
RowParallelLinearを初期化開始 input_size=2048, output_size=1024, bias=False
線形層の基底クラスを初期化開始 input_size=2048, output_size=1024, bias=False, tp_dim=1
線形層の基底クラスを初期化完了
RowParallelLinearを初期化完了 self.tp_size=1
Attention層を初期化 num_heads=16, head_dim=128, scale=0.08838834764831845, num_kv_heads=8
Attention層を初期化完了
RMSNormを初期化 hidden_size=128, eps=1e-06
RMSNormの初期化完了
RMSNormを初期化 hidden_size=128, eps=1e-06
RMSNormの初期化完了
Qwen3Attentionの初期化完了
Qwen3MLPを初期化開始 hidden_size=1024 intermediate_size=3072 hidden_act='silu'
MergedColumnParallelLinearを初期化開始 input_size=1024, output_sizes=[3072, 3072], bias=False
出力次元を並列化する線形層を初期化開始 input_size=1024, output_size=6144, bias=False
線形層の基底クラスを初期化開始 input_size=1024, output_size=6144, bias=False, tp_dim=0
線形層の基底クラスを初期化完了
出力次元を並列化する線形層を初期化完了 self.tp_size=1
MergedColumnParallelLinearを初期化完了
RowParallelLinearを初期化開始 input_size=3072, output_size=1024, bias=False
線形層の基底クラスを初期化開始 input_size=3072, output_size=1024, bias=False, tp_dim=1
線形層の基底クラスを初期化完了
RowParallelLinearを初期化完了 self.tp_size=1
SwiGLUを初期化
Qwen3MLPの初期化完了
RMSNormを初期化 hidden_size=1024, eps=1e-06
RMSNormの初期化完了
RMSNormを初期化 hidden_size=1024, eps=1e-06
RMSNormの初期化完了
Qwen3DecoderLayerの初期化完了
Qwen3DecoderLayerを初期化開始 config.hidden_size=1024 config.num_attention_heads=16 config.num_key_value_heads=8 config.intermediate_size=3072 config.rms_norm_eps=1e-06 config.hidden_act='silu'
Qwen3Attentionを初期化開始 hidden_size: 1024, num_heads: 16, num_kv_heads: 8, max_position: 40960, head_dim: 128, rms_norm_eps: 1e-06, qkv_bias: False, rope_theta: 1000000, rope_scaling: None
QKVParallelLinearを初期化開始 hidden_size=1024, head_size=128, total_num_heads=16, total_num_kv_heads=8, bias=False
出力次元を並列化する線形層を初期化開始 input_size=1024, output_size=4096, bias=False
線形層の基底クラスを初期化開始 input_size=1024, output_size=4096, bias=False, tp_dim=0
線形層の基底クラスを初期化完了
出力次元を並列化する線形層を初期化完了 self.tp_size=1
QKVParallelLinearを初期化完了 self.tp_size=1, self.num_heads=16, self.num_kv_heads=8
RowParallelLinearを初期化開始 input_size=2048, output_size=1024, bias=False
線形層の基底クラスを初期化開始 input_size=2048, output_size=1024, bias=False, tp_dim=1
線形層の基底クラスを初期化完了
RowParallelLinearを初期化完了 self.tp_size=1
Attention層を初期化 num_heads=16, head_dim=128, scale=0.08838834764831845, num_kv_heads=8
Attention層を初期化完了
RMSNormを初期化 hidden_size=128, eps=1e-06
RMSNormの初期化完了
RMSNormを初期化 hidden_size=128, eps=1e-06
RMSNormの初期化完了
Qwen3Attentionの初期化完了
Qwen3MLPを初期化開始 hidden_size=1024 intermediate_size=3072 hidden_act='silu'
MergedColumnParallelLinearを初期化開始 input_size=1024, output_sizes=[3072, 3072], bias=False
出力次元を並列化する線形層を初期化開始 input_size=1024, output_size=6144, bias=False
線形層の基底クラスを初期化開始 input_size=1024, output_size=6144, bias=False, tp_dim=0
線形層の基底クラスを初期化完了
出力次元を並列化する線形層を初期化完了 self.tp_size=1
MergedColumnParallelLinearを初期化完了
RowParallelLinearを初期化開始 input_size=3072, output_size=1024, bias=False
線形層の基底クラスを初期化開始 input_size=3072, output_size=1024, bias=False, tp_dim=1
線形層の基底クラスを初期化完了
RowParallelLinearを初期化完了 self.tp_size=1
SwiGLUを初期化
Qwen3MLPの初期化完了
RMSNormを初期化 hidden_size=1024, eps=1e-06
RMSNormの初期化完了
RMSNormを初期化 hidden_size=1024, eps=1e-06
RMSNormの初期化完了
Qwen3DecoderLayerの初期化完了
Qwen3DecoderLayerを初期化開始 config.hidden_size=1024 config.num_attention_heads=16 config.num_key_value_heads=8 config.intermediate_size=3072 config.rms_norm_eps=1e-06 config.hidden_act='silu'
Qwen3Attentionを初期化開始 hidden_size: 1024, num_heads: 16, num_kv_heads: 8, max_position: 40960, head_dim: 128, rms_norm_eps: 1e-06, qkv_bias: False, rope_theta: 1000000, rope_scaling: None
QKVParallelLinearを初期化開始 hidden_size=1024, head_size=128, total_num_heads=16, total_num_kv_heads=8, bias=False
出力次元を並列化する線形層を初期化開始 input_size=1024, output_size=4096, bias=False
線形層の基底クラスを初期化開始 input_size=1024, output_size=4096, bias=False, tp_dim=0
線形層の基底クラスを初期化完了
出力次元を並列化する線形層を初期化完了 self.tp_size=1
QKVParallelLinearを初期化完了 self.tp_size=1, self.num_heads=16, self.num_kv_heads=8
RowParallelLinearを初期化開始 input_size=2048, output_size=1024, bias=False
線形層の基底クラスを初期化開始 input_size=2048, output_size=1024, bias=False, tp_dim=1
線形層の基底クラスを初期化完了
RowParallelLinearを初期化完了 self.tp_size=1
Attention層を初期化 num_heads=16, head_dim=128, scale=0.08838834764831845, num_kv_heads=8
Attention層を初期化完了
RMSNormを初期化 hidden_size=128, eps=1e-06
RMSNormの初期化完了
RMSNormを初期化 hidden_size=128, eps=1e-06
RMSNormの初期化完了
Qwen3Attentionの初期化完了
Qwen3MLPを初期化開始 hidden_size=1024 intermediate_size=3072 hidden_act='silu'
MergedColumnParallelLinearを初期化開始 input_size=1024, output_sizes=[3072, 3072], bias=False
出力次元を並列化する線形層を初期化開始 input_size=1024, output_size=6144, bias=False
線形層の基底クラスを初期化開始 input_size=1024, output_size=6144, bias=False, tp_dim=0
線形層の基底クラスを初期化完了
出力次元を並列化する線形層を初期化完了 self.tp_size=1
MergedColumnParallelLinearを初期化完了
RowParallelLinearを初期化開始 input_size=3072, output_size=1024, bias=False
線形層の基底クラスを初期化開始 input_size=3072, output_size=1024, bias=False, tp_dim=1
線形層の基底クラスを初期化完了
RowParallelLinearを初期化完了 self.tp_size=1
SwiGLUを初期化
Qwen3MLPの初期化完了
RMSNormを初期化 hidden_size=1024, eps=1e-06
RMSNormの初期化完了
RMSNormを初期化 hidden_size=1024, eps=1e-06
RMSNormの初期化完了
Qwen3DecoderLayerの初期化完了
Qwen3DecoderLayerを初期化開始 config.hidden_size=1024 config.num_attention_heads=16 config.num_key_value_heads=8 config.intermediate_size=3072 config.rms_norm_eps=1e-06 config.hidden_act='silu'
Qwen3Attentionを初期化開始 hidden_size: 1024, num_heads: 16, num_kv_heads: 8, max_position: 40960, head_dim: 128, rms_norm_eps: 1e-06, qkv_bias: False, rope_theta: 1000000, rope_scaling: None
QKVParallelLinearを初期化開始 hidden_size=1024, head_size=128, total_num_heads=16, total_num_kv_heads=8, bias=False
出力次元を並列化する線形層を初期化開始 input_size=1024, output_size=4096, bias=False
線形層の基底クラスを初期化開始 input_size=1024, output_size=4096, bias=False, tp_dim=0
線形層の基底クラスを初期化完了
出力次元を並列化する線形層を初期化完了 self.tp_size=1
QKVParallelLinearを初期化完了 self.tp_size=1, self.num_heads=16, self.num_kv_heads=8
RowParallelLinearを初期化開始 input_size=2048, output_size=1024, bias=False
線形層の基底クラスを初期化開始 input_size=2048, output_size=1024, bias=False, tp_dim=1
線形層の基底クラスを初期化完了
RowParallelLinearを初期化完了 self.tp_size=1
Attention層を初期化 num_heads=16, head_dim=128, scale=0.08838834764831845, num_kv_heads=8
Attention層を初期化完了
RMSNormを初期化 hidden_size=128, eps=1e-06
RMSNormの初期化完了
RMSNormを初期化 hidden_size=128, eps=1e-06
RMSNormの初期化完了
Qwen3Attentionの初期化完了
Qwen3MLPを初期化開始 hidden_size=1024 intermediate_size=3072 hidden_act='silu'
MergedColumnParallelLinearを初期化開始 input_size=1024, output_sizes=[3072, 3072], bias=False
出力次元を並列化する線形層を初期化開始 input_size=1024, output_size=6144, bias=False
線形層の基底クラスを初期化開始 input_size=1024, output_size=6144, bias=False, tp_dim=0
線形層の基底クラスを初期化完了
出力次元を並列化する線形層を初期化完了 self.tp_size=1
MergedColumnParallelLinearを初期化完了
RowParallelLinearを初期化開始 input_size=3072, output_size=1024, bias=False
線形層の基底クラスを初期化開始 input_size=3072, output_size=1024, bias=False, tp_dim=1
線形層の基底クラスを初期化完了
RowParallelLinearを初期化完了 self.tp_size=1
SwiGLUを初期化
Qwen3MLPの初期化完了
RMSNormを初期化 hidden_size=1024, eps=1e-06
RMSNormの初期化完了
RMSNormを初期化 hidden_size=1024, eps=1e-06
RMSNormの初期化完了
Qwen3DecoderLayerの初期化完了
Qwen3DecoderLayerを初期化開始 config.hidden_size=1024 config.num_attention_heads=16 config.num_key_value_heads=8 config.intermediate_size=3072 config.rms_norm_eps=1e-06 config.hidden_act='silu'
Qwen3Attentionを初期化開始 hidden_size: 1024, num_heads: 16, num_kv_heads: 8, max_position: 40960, head_dim: 128, rms_norm_eps: 1e-06, qkv_bias: False, rope_theta: 1000000, rope_scaling: None
QKVParallelLinearを初期化開始 hidden_size=1024, head_size=128, total_num_heads=16, total_num_kv_heads=8, bias=False
出力次元を並列化する線形層を初期化開始 input_size=1024, output_size=4096, bias=False
線形層の基底クラスを初期化開始 input_size=1024, output_size=4096, bias=False, tp_dim=0
線形層の基底クラスを初期化完了
出力次元を並列化する線形層を初期化完了 self.tp_size=1
QKVParallelLinearを初期化完了 self.tp_size=1, self.num_heads=16, self.num_kv_heads=8
RowParallelLinearを初期化開始 input_size=2048, output_size=1024, bias=False
線形層の基底クラスを初期化開始 input_size=2048, output_size=1024, bias=False, tp_dim=1
線形層の基底クラスを初期化完了
RowParallelLinearを初期化完了 self.tp_size=1
Attention層を初期化 num_heads=16, head_dim=128, scale=0.08838834764831845, num_kv_heads=8
Attention層を初期化完了
RMSNormを初期化 hidden_size=128, eps=1e-06
RMSNormの初期化完了
RMSNormを初期化 hidden_size=128, eps=1e-06
RMSNormの初期化完了
Qwen3Attentionの初期化完了
Qwen3MLPを初期化開始 hidden_size=1024 intermediate_size=3072 hidden_act='silu'
MergedColumnParallelLinearを初期化開始 input_size=1024, output_sizes=[3072, 3072], bias=False
出力次元を並列化する線形層を初期化開始 input_size=1024, output_size=6144, bias=False
線形層の基底クラスを初期化開始 input_size=1024, output_size=6144, bias=False, tp_dim=0
線形層の基底クラスを初期化完了
出力次元を並列化する線形層を初期化完了 self.tp_size=1
MergedColumnParallelLinearを初期化完了
RowParallelLinearを初期化開始 input_size=3072, output_size=1024, bias=False
線形層の基底クラスを初期化開始 input_size=3072, output_size=1024, bias=False, tp_dim=1
線形層の基底クラスを初期化完了
RowParallelLinearを初期化完了 self.tp_size=1
SwiGLUを初期化
Qwen3MLPの初期化完了
RMSNormを初期化 hidden_size=1024, eps=1e-06
RMSNormの初期化完了
RMSNormを初期化 hidden_size=1024, eps=1e-06
RMSNormの初期化完了
Qwen3DecoderLayerの初期化完了
Qwen3DecoderLayerを初期化開始 config.hidden_size=1024 config.num_attention_heads=16 config.num_key_value_heads=8 config.intermediate_size=3072 config.rms_norm_eps=1e-06 config.hidden_act='silu'
Qwen3Attentionを初期化開始 hidden_size: 1024, num_heads: 16, num_kv_heads: 8, max_position: 40960, head_dim: 128, rms_norm_eps: 1e-06, qkv_bias: False, rope_theta: 1000000, rope_scaling: None
QKVParallelLinearを初期化開始 hidden_size=1024, head_size=128, total_num_heads=16, total_num_kv_heads=8, bias=False
出力次元を並列化する線形層を初期化開始 input_size=1024, output_size=4096, bias=False
線形層の基底クラスを初期化開始 input_size=1024, output_size=4096, bias=False, tp_dim=0
線形層の基底クラスを初期化完了
出力次元を並列化する線形層を初期化完了 self.tp_size=1
QKVParallelLinearを初期化完了 self.tp_size=1, self.num_heads=16, self.num_kv_heads=8
RowParallelLinearを初期化開始 input_size=2048, output_size=1024, bias=False
線形層の基底クラスを初期化開始 input_size=2048, output_size=1024, bias=False, tp_dim=1
線形層の基底クラスを初期化完了
RowParallelLinearを初期化完了 self.tp_size=1
Attention層を初期化 num_heads=16, head_dim=128, scale=0.08838834764831845, num_kv_heads=8
Attention層を初期化完了
RMSNormを初期化 hidden_size=128, eps=1e-06
RMSNormの初期化完了
RMSNormを初期化 hidden_size=128, eps=1e-06
RMSNormの初期化完了
Qwen3Attentionの初期化完了
Qwen3MLPを初期化開始 hidden_size=1024 intermediate_size=3072 hidden_act='silu'
MergedColumnParallelLinearを初期化開始 input_size=1024, output_sizes=[3072, 3072], bias=False
出力次元を並列化する線形層を初期化開始 input_size=1024, output_size=6144, bias=False
線形層の基底クラスを初期化開始 input_size=1024, output_size=6144, bias=False, tp_dim=0
線形層の基底クラスを初期化完了
出力次元を並列化する線形層を初期化完了 self.tp_size=1
MergedColumnParallelLinearを初期化完了
RowParallelLinearを初期化開始 input_size=3072, output_size=1024, bias=False
線形層の基底クラスを初期化開始 input_size=3072, output_size=1024, bias=False, tp_dim=1
線形層の基底クラスを初期化完了
RowParallelLinearを初期化完了 self.tp_size=1
SwiGLUを初期化
Qwen3MLPの初期化完了
RMSNormを初期化 hidden_size=1024, eps=1e-06
RMSNormの初期化完了
RMSNormを初期化 hidden_size=1024, eps=1e-06
RMSNormの初期化完了
Qwen3DecoderLayerの初期化完了
Qwen3DecoderLayerを初期化開始 config.hidden_size=1024 config.num_attention_heads=16 config.num_key_value_heads=8 config.intermediate_size=3072 config.rms_norm_eps=1e-06 config.hidden_act='silu'
Qwen3Attentionを初期化開始 hidden_size: 1024, num_heads: 16, num_kv_heads: 8, max_position: 40960, head_dim: 128, rms_norm_eps: 1e-06, qkv_bias: False, rope_theta: 1000000, rope_scaling: None
QKVParallelLinearを初期化開始 hidden_size=1024, head_size=128, total_num_heads=16, total_num_kv_heads=8, bias=False
出力次元を並列化する線形層を初期化開始 input_size=1024, output_size=4096, bias=False
線形層の基底クラスを初期化開始 input_size=1024, output_size=4096, bias=False, tp_dim=0
線形層の基底クラスを初期化完了
出力次元を並列化する線形層を初期化完了 self.tp_size=1
QKVParallelLinearを初期化完了 self.tp_size=1, self.num_heads=16, self.num_kv_heads=8
RowParallelLinearを初期化開始 input_size=2048, output_size=1024, bias=False
線形層の基底クラスを初期化開始 input_size=2048, output_size=1024, bias=False, tp_dim=1
線形層の基底クラスを初期化完了
RowParallelLinearを初期化完了 self.tp_size=1
Attention層を初期化 num_heads=16, head_dim=128, scale=0.08838834764831845, num_kv_heads=8
Attention層を初期化完了
RMSNormを初期化 hidden_size=128, eps=1e-06
RMSNormの初期化完了
RMSNormを初期化 hidden_size=128, eps=1e-06
RMSNormの初期化完了
Qwen3Attentionの初期化完了
Qwen3MLPを初期化開始 hidden_size=1024 intermediate_size=3072 hidden_act='silu'
MergedColumnParallelLinearを初期化開始 input_size=1024, output_sizes=[3072, 3072], bias=False
出力次元を並列化する線形層を初期化開始 input_size=1024, output_size=6144, bias=False
線形層の基底クラスを初期化開始 input_size=1024, output_size=6144, bias=False, tp_dim=0
線形層の基底クラスを初期化完了
出力次元を並列化する線形層を初期化完了 self.tp_size=1
MergedColumnParallelLinearを初期化完了
RowParallelLinearを初期化開始 input_size=3072, output_size=1024, bias=False
線形層の基底クラスを初期化開始 input_size=3072, output_size=1024, bias=False, tp_dim=1
線形層の基底クラスを初期化完了
RowParallelLinearを初期化完了 self.tp_size=1
SwiGLUを初期化
Qwen3MLPの初期化完了
RMSNormを初期化 hidden_size=1024, eps=1e-06
RMSNormの初期化完了
RMSNormを初期化 hidden_size=1024, eps=1e-06
RMSNormの初期化完了
Qwen3DecoderLayerの初期化完了
Qwen3DecoderLayerを初期化開始 config.hidden_size=1024 config.num_attention_heads=16 config.num_key_value_heads=8 config.intermediate_size=3072 config.rms_norm_eps=1e-06 config.hidden_act='silu'
Qwen3Attentionを初期化開始 hidden_size: 1024, num_heads: 16, num_kv_heads: 8, max_position: 40960, head_dim: 128, rms_norm_eps: 1e-06, qkv_bias: False, rope_theta: 1000000, rope_scaling: None
QKVParallelLinearを初期化開始 hidden_size=1024, head_size=128, total_num_heads=16, total_num_kv_heads=8, bias=False
出力次元を並列化する線形層を初期化開始 input_size=1024, output_size=4096, bias=False
線形層の基底クラスを初期化開始 input_size=1024, output_size=4096, bias=False, tp_dim=0
線形層の基底クラスを初期化完了
出力次元を並列化する線形層を初期化完了 self.tp_size=1
QKVParallelLinearを初期化完了 self.tp_size=1, self.num_heads=16, self.num_kv_heads=8
RowParallelLinearを初期化開始 input_size=2048, output_size=1024, bias=False
線形層の基底クラスを初期化開始 input_size=2048, output_size=1024, bias=False, tp_dim=1
線形層の基底クラスを初期化完了
RowParallelLinearを初期化完了 self.tp_size=1
Attention層を初期化 num_heads=16, head_dim=128, scale=0.08838834764831845, num_kv_heads=8
Attention層を初期化完了
RMSNormを初期化 hidden_size=128, eps=1e-06
RMSNormの初期化完了
RMSNormを初期化 hidden_size=128, eps=1e-06
RMSNormの初期化完了
Qwen3Attentionの初期化完了
Qwen3MLPを初期化開始 hidden_size=1024 intermediate_size=3072 hidden_act='silu'
MergedColumnParallelLinearを初期化開始 input_size=1024, output_sizes=[3072, 3072], bias=False
出力次元を並列化する線形層を初期化開始 input_size=1024, output_size=6144, bias=False
線形層の基底クラスを初期化開始 input_size=1024, output_size=6144, bias=False, tp_dim=0
線形層の基底クラスを初期化完了
出力次元を並列化する線形層を初期化完了 self.tp_size=1
MergedColumnParallelLinearを初期化完了
RowParallelLinearを初期化開始 input_size=3072, output_size=1024, bias=False
線形層の基底クラスを初期化開始 input_size=3072, output_size=1024, bias=False, tp_dim=1
線形層の基底クラスを初期化完了
RowParallelLinearを初期化完了 self.tp_size=1
SwiGLUを初期化
Qwen3MLPの初期化完了
RMSNormを初期化 hidden_size=1024, eps=1e-06
RMSNormの初期化完了
RMSNormを初期化 hidden_size=1024, eps=1e-06
RMSNormの初期化完了
Qwen3DecoderLayerの初期化完了
Qwen3DecoderLayerを初期化開始 config.hidden_size=1024 config.num_attention_heads=16 config.num_key_value_heads=8 config.intermediate_size=3072 config.rms_norm_eps=1e-06 config.hidden_act='silu'
Qwen3Attentionを初期化開始 hidden_size: 1024, num_heads: 16, num_kv_heads: 8, max_position: 40960, head_dim: 128, rms_norm_eps: 1e-06, qkv_bias: False, rope_theta: 1000000, rope_scaling: None
QKVParallelLinearを初期化開始 hidden_size=1024, head_size=128, total_num_heads=16, total_num_kv_heads=8, bias=False
出力次元を並列化する線形層を初期化開始 input_size=1024, output_size=4096, bias=False
線形層の基底クラスを初期化開始 input_size=1024, output_size=4096, bias=False, tp_dim=0
線形層の基底クラスを初期化完了
出力次元を並列化する線形層を初期化完了 self.tp_size=1
QKVParallelLinearを初期化完了 self.tp_size=1, self.num_heads=16, self.num_kv_heads=8
RowParallelLinearを初期化開始 input_size=2048, output_size=1024, bias=False
線形層の基底クラスを初期化開始 input_size=2048, output_size=1024, bias=False, tp_dim=1
線形層の基底クラスを初期化完了
RowParallelLinearを初期化完了 self.tp_size=1
Attention層を初期化 num_heads=16, head_dim=128, scale=0.08838834764831845, num_kv_heads=8
Attention層を初期化完了
RMSNormを初期化 hidden_size=128, eps=1e-06
RMSNormの初期化完了
RMSNormを初期化 hidden_size=128, eps=1e-06
RMSNormの初期化完了
Qwen3Attentionの初期化完了
Qwen3MLPを初期化開始 hidden_size=1024 intermediate_size=3072 hidden_act='silu'
MergedColumnParallelLinearを初期化開始 input_size=1024, output_sizes=[3072, 3072], bias=False
出力次元を並列化する線形層を初期化開始 input_size=1024, output_size=6144, bias=False
線形層の基底クラスを初期化開始 input_size=1024, output_size=6144, bias=False, tp_dim=0
線形層の基底クラスを初期化完了
出力次元を並列化する線形層を初期化完了 self.tp_size=1
MergedColumnParallelLinearを初期化完了
RowParallelLinearを初期化開始 input_size=3072, output_size=1024, bias=False
線形層の基底クラスを初期化開始 input_size=3072, output_size=1024, bias=False, tp_dim=1
線形層の基底クラスを初期化完了
RowParallelLinearを初期化完了 self.tp_size=1
SwiGLUを初期化
Qwen3MLPの初期化完了
RMSNormを初期化 hidden_size=1024, eps=1e-06
RMSNormの初期化完了
RMSNormを初期化 hidden_size=1024, eps=1e-06
RMSNormの初期化完了
Qwen3DecoderLayerの初期化完了
Qwen3DecoderLayerを初期化開始 config.hidden_size=1024 config.num_attention_heads=16 config.num_key_value_heads=8 config.intermediate_size=3072 config.rms_norm_eps=1e-06 config.hidden_act='silu'
Qwen3Attentionを初期化開始 hidden_size: 1024, num_heads: 16, num_kv_heads: 8, max_position: 40960, head_dim: 128, rms_norm_eps: 1e-06, qkv_bias: False, rope_theta: 1000000, rope_scaling: None
QKVParallelLinearを初期化開始 hidden_size=1024, head_size=128, total_num_heads=16, total_num_kv_heads=8, bias=False
出力次元を並列化する線形層を初期化開始 input_size=1024, output_size=4096, bias=False
線形層の基底クラスを初期化開始 input_size=1024, output_size=4096, bias=False, tp_dim=0
線形層の基底クラスを初期化完了
出力次元を並列化する線形層を初期化完了 self.tp_size=1
QKVParallelLinearを初期化完了 self.tp_size=1, self.num_heads=16, self.num_kv_heads=8
RowParallelLinearを初期化開始 input_size=2048, output_size=1024, bias=False
線形層の基底クラスを初期化開始 input_size=2048, output_size=1024, bias=False, tp_dim=1
線形層の基底クラスを初期化完了
RowParallelLinearを初期化完了 self.tp_size=1
Attention層を初期化 num_heads=16, head_dim=128, scale=0.08838834764831845, num_kv_heads=8
Attention層を初期化完了
RMSNormを初期化 hidden_size=128, eps=1e-06
RMSNormの初期化完了
RMSNormを初期化 hidden_size=128, eps=1e-06
RMSNormの初期化完了
Qwen3Attentionの初期化完了
Qwen3MLPを初期化開始 hidden_size=1024 intermediate_size=3072 hidden_act='silu'
MergedColumnParallelLinearを初期化開始 input_size=1024, output_sizes=[3072, 3072], bias=False
出力次元を並列化する線形層を初期化開始 input_size=1024, output_size=6144, bias=False
線形層の基底クラスを初期化開始 input_size=1024, output_size=6144, bias=False, tp_dim=0
線形層の基底クラスを初期化完了
出力次元を並列化する線形層を初期化完了 self.tp_size=1
MergedColumnParallelLinearを初期化完了
RowParallelLinearを初期化開始 input_size=3072, output_size=1024, bias=False
線形層の基底クラスを初期化開始 input_size=3072, output_size=1024, bias=False, tp_dim=1
線形層の基底クラスを初期化完了
RowParallelLinearを初期化完了 self.tp_size=1
SwiGLUを初期化
Qwen3MLPの初期化完了
RMSNormを初期化 hidden_size=1024, eps=1e-06
RMSNormの初期化完了
RMSNormを初期化 hidden_size=1024, eps=1e-06
RMSNormの初期化完了
Qwen3DecoderLayerの初期化完了
Qwen3DecoderLayerを初期化開始 config.hidden_size=1024 config.num_attention_heads=16 config.num_key_value_heads=8 config.intermediate_size=3072 config.rms_norm_eps=1e-06 config.hidden_act='silu'
Qwen3Attentionを初期化開始 hidden_size: 1024, num_heads: 16, num_kv_heads: 8, max_position: 40960, head_dim: 128, rms_norm_eps: 1e-06, qkv_bias: False, rope_theta: 1000000, rope_scaling: None
QKVParallelLinearを初期化開始 hidden_size=1024, head_size=128, total_num_heads=16, total_num_kv_heads=8, bias=False
出力次元を並列化する線形層を初期化開始 input_size=1024, output_size=4096, bias=False
線形層の基底クラスを初期化開始 input_size=1024, output_size=4096, bias=False, tp_dim=0
線形層の基底クラスを初期化完了
出力次元を並列化する線形層を初期化完了 self.tp_size=1
QKVParallelLinearを初期化完了 self.tp_size=1, self.num_heads=16, self.num_kv_heads=8
RowParallelLinearを初期化開始 input_size=2048, output_size=1024, bias=False
線形層の基底クラスを初期化開始 input_size=2048, output_size=1024, bias=False, tp_dim=1
線形層の基底クラスを初期化完了
RowParallelLinearを初期化完了 self.tp_size=1
Attention層を初期化 num_heads=16, head_dim=128, scale=0.08838834764831845, num_kv_heads=8
Attention層を初期化完了
RMSNormを初期化 hidden_size=128, eps=1e-06
RMSNormの初期化完了
RMSNormを初期化 hidden_size=128, eps=1e-06
RMSNormの初期化完了
Qwen3Attentionの初期化完了
Qwen3MLPを初期化開始 hidden_size=1024 intermediate_size=3072 hidden_act='silu'
MergedColumnParallelLinearを初期化開始 input_size=1024, output_sizes=[3072, 3072], bias=False
出力次元を並列化する線形層を初期化開始 input_size=1024, output_size=6144, bias=False
線形層の基底クラスを初期化開始 input_size=1024, output_size=6144, bias=False, tp_dim=0
線形層の基底クラスを初期化完了
出力次元を並列化する線形層を初期化完了 self.tp_size=1
MergedColumnParallelLinearを初期化完了
RowParallelLinearを初期化開始 input_size=3072, output_size=1024, bias=False
線形層の基底クラスを初期化開始 input_size=3072, output_size=1024, bias=False, tp_dim=1
線形層の基底クラスを初期化完了
RowParallelLinearを初期化完了 self.tp_size=1
SwiGLUを初期化
Qwen3MLPの初期化完了
RMSNormを初期化 hidden_size=1024, eps=1e-06
RMSNormの初期化完了
RMSNormを初期化 hidden_size=1024, eps=1e-06
RMSNormの初期化完了
Qwen3DecoderLayerの初期化完了
Qwen3DecoderLayerを初期化開始 config.hidden_size=1024 config.num_attention_heads=16 config.num_key_value_heads=8 config.intermediate_size=3072 config.rms_norm_eps=1e-06 config.hidden_act='silu'
Qwen3Attentionを初期化開始 hidden_size: 1024, num_heads: 16, num_kv_heads: 8, max_position: 40960, head_dim: 128, rms_norm_eps: 1e-06, qkv_bias: False, rope_theta: 1000000, rope_scaling: None
QKVParallelLinearを初期化開始 hidden_size=1024, head_size=128, total_num_heads=16, total_num_kv_heads=8, bias=False
出力次元を並列化する線形層を初期化開始 input_size=1024, output_size=4096, bias=False
線形層の基底クラスを初期化開始 input_size=1024, output_size=4096, bias=False, tp_dim=0
線形層の基底クラスを初期化完了
出力次元を並列化する線形層を初期化完了 self.tp_size=1
QKVParallelLinearを初期化完了 self.tp_size=1, self.num_heads=16, self.num_kv_heads=8
RowParallelLinearを初期化開始 input_size=2048, output_size=1024, bias=False
線形層の基底クラスを初期化開始 input_size=2048, output_size=1024, bias=False, tp_dim=1
線形層の基底クラスを初期化完了
RowParallelLinearを初期化完了 self.tp_size=1
Attention層を初期化 num_heads=16, head_dim=128, scale=0.08838834764831845, num_kv_heads=8
Attention層を初期化完了
RMSNormを初期化 hidden_size=128, eps=1e-06
RMSNormの初期化完了
RMSNormを初期化 hidden_size=128, eps=1e-06
RMSNormの初期化完了
Qwen3Attentionの初期化完了
Qwen3MLPを初期化開始 hidden_size=1024 intermediate_size=3072 hidden_act='silu'
MergedColumnParallelLinearを初期化開始 input_size=1024, output_sizes=[3072, 3072], bias=False
出力次元を並列化する線形層を初期化開始 input_size=1024, output_size=6144, bias=False
線形層の基底クラスを初期化開始 input_size=1024, output_size=6144, bias=False, tp_dim=0
線形層の基底クラスを初期化完了
出力次元を並列化する線形層を初期化完了 self.tp_size=1
MergedColumnParallelLinearを初期化完了
RowParallelLinearを初期化開始 input_size=3072, output_size=1024, bias=False
線形層の基底クラスを初期化開始 input_size=3072, output_size=1024, bias=False, tp_dim=1
線形層の基底クラスを初期化完了
RowParallelLinearを初期化完了 self.tp_size=1
SwiGLUを初期化
Qwen3MLPの初期化完了
RMSNormを初期化 hidden_size=1024, eps=1e-06
RMSNormの初期化完了
RMSNormを初期化 hidden_size=1024, eps=1e-06
RMSNormの初期化完了
Qwen3DecoderLayerの初期化完了
Qwen3DecoderLayerを初期化開始 config.hidden_size=1024 config.num_attention_heads=16 config.num_key_value_heads=8 config.intermediate_size=3072 config.rms_norm_eps=1e-06 config.hidden_act='silu'
Qwen3Attentionを初期化開始 hidden_size: 1024, num_heads: 16, num_kv_heads: 8, max_position: 40960, head_dim: 128, rms_norm_eps: 1e-06, qkv_bias: False, rope_theta: 1000000, rope_scaling: None
QKVParallelLinearを初期化開始 hidden_size=1024, head_size=128, total_num_heads=16, total_num_kv_heads=8, bias=False
出力次元を並列化する線形層を初期化開始 input_size=1024, output_size=4096, bias=False
線形層の基底クラスを初期化開始 input_size=1024, output_size=4096, bias=False, tp_dim=0
線形層の基底クラスを初期化完了
出力次元を並列化する線形層を初期化完了 self.tp_size=1
QKVParallelLinearを初期化完了 self.tp_size=1, self.num_heads=16, self.num_kv_heads=8
RowParallelLinearを初期化開始 input_size=2048, output_size=1024, bias=False
線形層の基底クラスを初期化開始 input_size=2048, output_size=1024, bias=False, tp_dim=1
線形層の基底クラスを初期化完了
RowParallelLinearを初期化完了 self.tp_size=1
Attention層を初期化 num_heads=16, head_dim=128, scale=0.08838834764831845, num_kv_heads=8
Attention層を初期化完了
RMSNormを初期化 hidden_size=128, eps=1e-06
RMSNormの初期化完了
RMSNormを初期化 hidden_size=128, eps=1e-06
RMSNormの初期化完了
Qwen3Attentionの初期化完了
Qwen3MLPを初期化開始 hidden_size=1024 intermediate_size=3072 hidden_act='silu'
MergedColumnParallelLinearを初期化開始 input_size=1024, output_sizes=[3072, 3072], bias=False
出力次元を並列化する線形層を初期化開始 input_size=1024, output_size=6144, bias=False
線形層の基底クラスを初期化開始 input_size=1024, output_size=6144, bias=False, tp_dim=0
線形層の基底クラスを初期化完了
出力次元を並列化する線形層を初期化完了 self.tp_size=1
MergedColumnParallelLinearを初期化完了
RowParallelLinearを初期化開始 input_size=3072, output_size=1024, bias=False
線形層の基底クラスを初期化開始 input_size=3072, output_size=1024, bias=False, tp_dim=1
線形層の基底クラスを初期化完了
RowParallelLinearを初期化完了 self.tp_size=1
SwiGLUを初期化
Qwen3MLPの初期化完了
RMSNormを初期化 hidden_size=1024, eps=1e-06
RMSNormの初期化完了
RMSNormを初期化 hidden_size=1024, eps=1e-06
RMSNormの初期化完了
Qwen3DecoderLayerの初期化完了
Qwen3DecoderLayerを初期化開始 config.hidden_size=1024 config.num_attention_heads=16 config.num_key_value_heads=8 config.intermediate_size=3072 config.rms_norm_eps=1e-06 config.hidden_act='silu'
Qwen3Attentionを初期化開始 hidden_size: 1024, num_heads: 16, num_kv_heads: 8, max_position: 40960, head_dim: 128, rms_norm_eps: 1e-06, qkv_bias: False, rope_theta: 1000000, rope_scaling: None
QKVParallelLinearを初期化開始 hidden_size=1024, head_size=128, total_num_heads=16, total_num_kv_heads=8, bias=False
出力次元を並列化する線形層を初期化開始 input_size=1024, output_size=4096, bias=False
線形層の基底クラスを初期化開始 input_size=1024, output_size=4096, bias=False, tp_dim=0
線形層の基底クラスを初期化完了
出力次元を並列化する線形層を初期化完了 self.tp_size=1
QKVParallelLinearを初期化完了 self.tp_size=1, self.num_heads=16, self.num_kv_heads=8
RowParallelLinearを初期化開始 input_size=2048, output_size=1024, bias=False
線形層の基底クラスを初期化開始 input_size=2048, output_size=1024, bias=False, tp_dim=1
線形層の基底クラスを初期化完了
RowParallelLinearを初期化完了 self.tp_size=1
Attention層を初期化 num_heads=16, head_dim=128, scale=0.08838834764831845, num_kv_heads=8
Attention層を初期化完了
RMSNormを初期化 hidden_size=128, eps=1e-06
RMSNormの初期化完了
RMSNormを初期化 hidden_size=128, eps=1e-06
RMSNormの初期化完了
Qwen3Attentionの初期化完了
Qwen3MLPを初期化開始 hidden_size=1024 intermediate_size=3072 hidden_act='silu'
MergedColumnParallelLinearを初期化開始 input_size=1024, output_sizes=[3072, 3072], bias=False
出力次元を並列化する線形層を初期化開始 input_size=1024, output_size=6144, bias=False
線形層の基底クラスを初期化開始 input_size=1024, output_size=6144, bias=False, tp_dim=0
線形層の基底クラスを初期化完了
出力次元を並列化する線形層を初期化完了 self.tp_size=1
MergedColumnParallelLinearを初期化完了
RowParallelLinearを初期化開始 input_size=3072, output_size=1024, bias=False
線形層の基底クラスを初期化開始 input_size=3072, output_size=1024, bias=False, tp_dim=1
線形層の基底クラスを初期化完了
RowParallelLinearを初期化完了 self.tp_size=1
SwiGLUを初期化
Qwen3MLPの初期化完了
RMSNormを初期化 hidden_size=1024, eps=1e-06
RMSNormの初期化完了
RMSNormを初期化 hidden_size=1024, eps=1e-06
RMSNormの初期化完了
Qwen3DecoderLayerの初期化完了
Qwen3DecoderLayerを初期化開始 config.hidden_size=1024 config.num_attention_heads=16 config.num_key_value_heads=8 config.intermediate_size=3072 config.rms_norm_eps=1e-06 config.hidden_act='silu'
Qwen3Attentionを初期化開始 hidden_size: 1024, num_heads: 16, num_kv_heads: 8, max_position: 40960, head_dim: 128, rms_norm_eps: 1e-06, qkv_bias: False, rope_theta: 1000000, rope_scaling: None
QKVParallelLinearを初期化開始 hidden_size=1024, head_size=128, total_num_heads=16, total_num_kv_heads=8, bias=False
出力次元を並列化する線形層を初期化開始 input_size=1024, output_size=4096, bias=False
線形層の基底クラスを初期化開始 input_size=1024, output_size=4096, bias=False, tp_dim=0
線形層の基底クラスを初期化完了
出力次元を並列化する線形層を初期化完了 self.tp_size=1
QKVParallelLinearを初期化完了 self.tp_size=1, self.num_heads=16, self.num_kv_heads=8
RowParallelLinearを初期化開始 input_size=2048, output_size=1024, bias=False
線形層の基底クラスを初期化開始 input_size=2048, output_size=1024, bias=False, tp_dim=1
線形層の基底クラスを初期化完了
RowParallelLinearを初期化完了 self.tp_size=1
Attention層を初期化 num_heads=16, head_dim=128, scale=0.08838834764831845, num_kv_heads=8
Attention層を初期化完了
RMSNormを初期化 hidden_size=128, eps=1e-06
RMSNormの初期化完了
RMSNormを初期化 hidden_size=128, eps=1e-06
RMSNormの初期化完了
Qwen3Attentionの初期化完了
Qwen3MLPを初期化開始 hidden_size=1024 intermediate_size=3072 hidden_act='silu'
MergedColumnParallelLinearを初期化開始 input_size=1024, output_sizes=[3072, 3072], bias=False
出力次元を並列化する線形層を初期化開始 input_size=1024, output_size=6144, bias=False
線形層の基底クラスを初期化開始 input_size=1024, output_size=6144, bias=False, tp_dim=0
線形層の基底クラスを初期化完了
出力次元を並列化する線形層を初期化完了 self.tp_size=1
MergedColumnParallelLinearを初期化完了
RowParallelLinearを初期化開始 input_size=3072, output_size=1024, bias=False
線形層の基底クラスを初期化開始 input_size=3072, output_size=1024, bias=False, tp_dim=1
線形層の基底クラスを初期化完了
RowParallelLinearを初期化完了 self.tp_size=1
SwiGLUを初期化
Qwen3MLPの初期化完了
RMSNormを初期化 hidden_size=1024, eps=1e-06
RMSNormの初期化完了
RMSNormを初期化 hidden_size=1024, eps=1e-06
RMSNormの初期化完了
Qwen3DecoderLayerの初期化完了
RMSNormを初期化 hidden_size=1024, eps=1e-06
RMSNormの初期化完了
Qwen3Modelの初期化完了
出力の言語モデルヘッドを初期化開始 num_embeddings=151936, embedding_dim=1024, bias=False
入力の埋め込み層を初期化開始 num_embeddings=151936, embedding_dim=1024
分散環境情報取得 self.tp_rank=0, self.tp_size=1
担当する語彙の範囲計算 self.vocab_start_idx=0, self.vocab_end_idx=151936
担当する語彙の重み行列初期化 self.weight.shape=torch.Size([151936, 1024])
入力の埋め込み層を初期化完了
出力の言語モデルヘッドを初期化完了
Qwen3ForCausalLMの初期化完了
モデルの重みを読み込み開始 path='/root/huggingface/Qwen3-0.6B'
重みファイルを処理中 file='/root/huggingface/Qwen3-0.6B/model.safetensors'
重みを読み込み中 weight_name='lm_head.weight'
通常の重み読み込み weight_name='lm_head.weight'
埋め込み層の重みをロード開始 param.shape=torch.Size([151936, 1024]), loaded_weight.shape=torch.Size([151936, 1024])
埋め込み層の重みをロード完了 loaded_weight.shape=torch.Size([151936, 1024])
重みを読み込み中 weight_name='model.embed_tokens.weight'
通常の重み読み込み weight_name='model.embed_tokens.weight'
埋め込み層の重みをロード開始 param.shape=torch.Size([151936, 1024]), loaded_weight.shape=torch.Size([151936, 1024])
埋め込み層の重みをロード完了 loaded_weight.shape=torch.Size([151936, 1024])
重みを読み込み中 weight_name='model.layers.0.input_layernorm.weight'
通常の重み読み込み weight_name='model.layers.0.input_layernorm.weight'
重みを読み込み param.shape=torch.Size([1024]) loaded_weight.shape=torch.Size([1024])
重みを読み込み中 weight_name='model.layers.0.mlp.down_proj.weight'
通常の重み読み込み weight_name='model.layers.0.mlp.down_proj.weight'
RowParallelLinearの重みを読み込み開始 param.shape=torch.Size([1024, 3072]), loaded_weight.shape=torch.Size([1024, 3072])
RowParallelLinearの重みを読み込み完了
重みを読み込み中 weight_name='model.layers.0.mlp.gate_proj.weight'
マッピングによる重みを読み込み weight_name='model.layers.0.mlp.gate_proj.weight' k='gate_proj'
MergedColumnParallelLinearの重みを読み込み開始 param.shape=torch.Size([6144, 1024]), loaded_weight.shape=torch.Size([3072, 1024]), loaded_shard_id=0
MergedColumnParallelLinearの重みを読み込み完了
重みを読み込み中 weight_name='model.layers.0.mlp.up_proj.weight'
マッピングによる重みを読み込み weight_name='model.layers.0.mlp.up_proj.weight' k='up_proj'
MergedColumnParallelLinearの重みを読み込み開始 param.shape=torch.Size([6144, 1024]), loaded_weight.shape=torch.Size([3072, 1024]), loaded_shard_id=1
MergedColumnParallelLinearの重みを読み込み完了
重みを読み込み中 weight_name='model.layers.0.post_attention_layernorm.weight'
通常の重み読み込み weight_name='model.layers.0.post_attention_layernorm.weight'
重みを読み込み param.shape=torch.Size([1024]) loaded_weight.shape=torch.Size([1024])
重みを読み込み中 weight_name='model.layers.0.self_attn.k_norm.weight'
通常の重み読み込み weight_name='model.layers.0.self_attn.k_norm.weight'
重みを読み込み param.shape=torch.Size([128]) loaded_weight.shape=torch.Size([128])
重みを読み込み中 weight_name='model.layers.0.self_attn.k_proj.weight'
マッピングによる重みを読み込み weight_name='model.layers.0.self_attn.k_proj.weight' k='k_proj'
QKVParallelLinearの重みを読み込み開始 param.shape=torch.Size([4096, 1024]), loaded_weight.shape=torch.Size([1024, 1024]), loaded_shard_id='k'
QKVParallelLinearの重みを読み込み完了
重みを読み込み中 weight_name='model.layers.0.self_attn.o_proj.weight'
通常の重み読み込み weight_name='model.layers.0.self_attn.o_proj.weight'
RowParallelLinearの重みを読み込み開始 param.shape=torch.Size([1024, 2048]), loaded_weight.shape=torch.Size([1024, 2048])
RowParallelLinearの重みを読み込み完了
重みを読み込み中 weight_name='model.layers.0.self_attn.q_norm.weight'
通常の重み読み込み weight_name='model.layers.0.self_attn.q_norm.weight'
重みを読み込み param.shape=torch.Size([128]) loaded_weight.shape=torch.Size([128])
重みを読み込み中 weight_name='model.layers.0.self_attn.q_proj.weight'
マッピングによる重みを読み込み weight_name='model.layers.0.self_attn.q_proj.weight' k='q_proj'
QKVParallelLinearの重みを読み込み開始 param.shape=torch.Size([4096, 1024]), loaded_weight.shape=torch.Size([2048, 1024]), loaded_shard_id='q'
QKVParallelLinearの重みを読み込み完了
重みを読み込み中 weight_name='model.layers.0.self_attn.v_proj.weight'
マッピングによる重みを読み込み weight_name='model.layers.0.self_attn.v_proj.weight' k='v_proj'
QKVParallelLinearの重みを読み込み開始 param.shape=torch.Size([4096, 1024]), loaded_weight.shape=torch.Size([1024, 1024]), loaded_shard_id='v'
QKVParallelLinearの重みを読み込み完了
重みを読み込み中 weight_name='model.layers.1.input_layernorm.weight'
通常の重み読み込み weight_name='model.layers.1.input_layernorm.weight'
重みを読み込み param.shape=torch.Size([1024]) loaded_weight.shape=torch.Size([1024])
重みを読み込み中 weight_name='model.layers.1.mlp.down_proj.weight'
通常の重み読み込み weight_name='model.layers.1.mlp.down_proj.weight'
RowParallelLinearの重みを読み込み開始 param.shape=torch.Size([1024, 3072]), loaded_weight.shape=torch.Size([1024, 3072])
RowParallelLinearの重みを読み込み完了
重みを読み込み中 weight_name='model.layers.1.mlp.gate_proj.weight'
マッピングによる重みを読み込み weight_name='model.layers.1.mlp.gate_proj.weight' k='gate_proj'
MergedColumnParallelLinearの重みを読み込み開始 param.shape=torch.Size([6144, 1024]), loaded_weight.shape=torch.Size([3072, 1024]), loaded_shard_id=0
MergedColumnParallelLinearの重みを読み込み完了
重みを読み込み中 weight_name='model.layers.1.mlp.up_proj.weight'
マッピングによる重みを読み込み weight_name='model.layers.1.mlp.up_proj.weight' k='up_proj'
MergedColumnParallelLinearの重みを読み込み開始 param.shape=torch.Size([6144, 1024]), loaded_weight.shape=torch.Size([3072, 1024]), loaded_shard_id=1
MergedColumnParallelLinearの重みを読み込み完了
重みを読み込み中 weight_name='model.layers.1.post_attention_layernorm.weight'
通常の重み読み込み weight_name='model.layers.1.post_attention_layernorm.weight'
重みを読み込み param.shape=torch.Size([1024]) loaded_weight.shape=torch.Size([1024])
重みを読み込み中 weight_name='model.layers.1.self_attn.k_norm.weight'
通常の重み読み込み weight_name='model.layers.1.self_attn.k_norm.weight'
重みを読み込み param.shape=torch.Size([128]) loaded_weight.shape=torch.Size([128])
重みを読み込み中 weight_name='model.layers.1.self_attn.k_proj.weight'
マッピングによる重みを読み込み weight_name='model.layers.1.self_attn.k_proj.weight' k='k_proj'
QKVParallelLinearの重みを読み込み開始 param.shape=torch.Size([4096, 1024]), loaded_weight.shape=torch.Size([1024, 1024]), loaded_shard_id='k'
QKVParallelLinearの重みを読み込み完了
重みを読み込み中 weight_name='model.layers.1.self_attn.o_proj.weight'
通常の重み読み込み weight_name='model.layers.1.self_attn.o_proj.weight'
RowParallelLinearの重みを読み込み開始 param.shape=torch.Size([1024, 2048]), loaded_weight.shape=torch.Size([1024, 2048])
RowParallelLinearの重みを読み込み完了
重みを読み込み中 weight_name='model.layers.1.self_attn.q_norm.weight'
通常の重み読み込み weight_name='model.layers.1.self_attn.q_norm.weight'
重みを読み込み param.shape=torch.Size([128]) loaded_weight.shape=torch.Size([128])
重みを読み込み中 weight_name='model.layers.1.self_attn.q_proj.weight'
マッピングによる重みを読み込み weight_name='model.layers.1.self_attn.q_proj.weight' k='q_proj'
QKVParallelLinearの重みを読み込み開始 param.shape=torch.Size([4096, 1024]), loaded_weight.shape=torch.Size([2048, 1024]), loaded_shard_id='q'
QKVParallelLinearの重みを読み込み完了
重みを読み込み中 weight_name='model.layers.1.self_attn.v_proj.weight'
マッピングによる重みを読み込み weight_name='model.layers.1.self_attn.v_proj.weight' k='v_proj'
QKVParallelLinearの重みを読み込み開始 param.shape=torch.Size([4096, 1024]), loaded_weight.shape=torch.Size([1024, 1024]), loaded_shard_id='v'
QKVParallelLinearの重みを読み込み完了
重みを読み込み中 weight_name='model.layers.10.input_layernorm.weight'
通常の重み読み込み weight_name='model.layers.10.input_layernorm.weight'
重みを読み込み param.shape=torch.Size([1024]) loaded_weight.shape=torch.Size([1024])
重みを読み込み中 weight_name='model.layers.10.mlp.down_proj.weight'
通常の重み読み込み weight_name='model.layers.10.mlp.down_proj.weight'
RowParallelLinearの重みを読み込み開始 param.shape=torch.Size([1024, 3072]), loaded_weight.shape=torch.Size([1024, 3072])
RowParallelLinearの重みを読み込み完了
重みを読み込み中 weight_name='model.layers.10.mlp.gate_proj.weight'
マッピングによる重みを読み込み weight_name='model.layers.10.mlp.gate_proj.weight' k='gate_proj'
MergedColumnParallelLinearの重みを読み込み開始 param.shape=torch.Size([6144, 1024]), loaded_weight.shape=torch.Size([3072, 1024]), loaded_shard_id=0
MergedColumnParallelLinearの重みを読み込み完了
重みを読み込み中 weight_name='model.layers.10.mlp.up_proj.weight'
マッピングによる重みを読み込み weight_name='model.layers.10.mlp.up_proj.weight' k='up_proj'
MergedColumnParallelLinearの重みを読み込み開始 param.shape=torch.Size([6144, 1024]), loaded_weight.shape=torch.Size([3072, 1024]), loaded_shard_id=1
MergedColumnParallelLinearの重みを読み込み完了
重みを読み込み中 weight_name='model.layers.10.post_attention_layernorm.weight'
通常の重み読み込み weight_name='model.layers.10.post_attention_layernorm.weight'
重みを読み込み param.shape=torch.Size([1024]) loaded_weight.shape=torch.Size([1024])
重みを読み込み中 weight_name='model.layers.10.self_attn.k_norm.weight'
通常の重み読み込み weight_name='model.layers.10.self_attn.k_norm.weight'
重みを読み込み param.shape=torch.Size([128]) loaded_weight.shape=torch.Size([128])
重みを読み込み中 weight_name='model.layers.10.self_attn.k_proj.weight'
マッピングによる重みを読み込み weight_name='model.layers.10.self_attn.k_proj.weight' k='k_proj'
QKVParallelLinearの重みを読み込み開始 param.shape=torch.Size([4096, 1024]), loaded_weight.shape=torch.Size([1024, 1024]), loaded_shard_id='k'
QKVParallelLinearの重みを読み込み完了
重みを読み込み中 weight_name='model.layers.10.self_attn.o_proj.weight'
通常の重み読み込み weight_name='model.layers.10.self_attn.o_proj.weight'
RowParallelLinearの重みを読み込み開始 param.shape=torch.Size([1024, 2048]), loaded_weight.shape=torch.Size([1024, 2048])
RowParallelLinearの重みを読み込み完了
重みを読み込み中 weight_name='model.layers.10.self_attn.q_norm.weight'
通常の重み読み込み weight_name='model.layers.10.self_attn.q_norm.weight'
重みを読み込み param.shape=torch.Size([128]) loaded_weight.shape=torch.Size([128])
重みを読み込み中 weight_name='model.layers.10.self_attn.q_proj.weight'
マッピングによる重みを読み込み weight_name='model.layers.10.self_attn.q_proj.weight' k='q_proj'
QKVParallelLinearの重みを読み込み開始 param.shape=torch.Size([4096, 1024]), loaded_weight.shape=torch.Size([2048, 1024]), loaded_shard_id='q'
QKVParallelLinearの重みを読み込み完了
重みを読み込み中 weight_name='model.layers.10.self_attn.v_proj.weight'
マッピングによる重みを読み込み weight_name='model.layers.10.self_attn.v_proj.weight' k='v_proj'
QKVParallelLinearの重みを読み込み開始 param.shape=torch.Size([4096, 1024]), loaded_weight.shape=torch.Size([1024, 1024]), loaded_shard_id='v'
QKVParallelLinearの重みを読み込み完了
重みを読み込み中 weight_name='model.layers.11.input_layernorm.weight'
通常の重み読み込み weight_name='model.layers.11.input_layernorm.weight'
重みを読み込み param.shape=torch.Size([1024]) loaded_weight.shape=torch.Size([1024])
重みを読み込み中 weight_name='model.layers.11.mlp.down_proj.weight'
通常の重み読み込み weight_name='model.layers.11.mlp.down_proj.weight'
RowParallelLinearの重みを読み込み開始 param.shape=torch.Size([1024, 3072]), loaded_weight.shape=torch.Size([1024, 3072])
RowParallelLinearの重みを読み込み完了
重みを読み込み中 weight_name='model.layers.11.mlp.gate_proj.weight'
マッピングによる重みを読み込み weight_name='model.layers.11.mlp.gate_proj.weight' k='gate_proj'
MergedColumnParallelLinearの重みを読み込み開始 param.shape=torch.Size([6144, 1024]), loaded_weight.shape=torch.Size([3072, 1024]), loaded_shard_id=0
MergedColumnParallelLinearの重みを読み込み完了
重みを読み込み中 weight_name='model.layers.11.mlp.up_proj.weight'
マッピングによる重みを読み込み weight_name='model.layers.11.mlp.up_proj.weight' k='up_proj'
MergedColumnParallelLinearの重みを読み込み開始 param.shape=torch.Size([6144, 1024]), loaded_weight.shape=torch.Size([3072, 1024]), loaded_shard_id=1
MergedColumnParallelLinearの重みを読み込み完了
重みを読み込み中 weight_name='model.layers.11.post_attention_layernorm.weight'
通常の重み読み込み weight_name='model.layers.11.post_attention_layernorm.weight'
重みを読み込み param.shape=torch.Size([1024]) loaded_weight.shape=torch.Size([1024])
重みを読み込み中 weight_name='model.layers.11.self_attn.k_norm.weight'
通常の重み読み込み weight_name='model.layers.11.self_attn.k_norm.weight'
重みを読み込み param.shape=torch.Size([128]) loaded_weight.shape=torch.Size([128])
重みを読み込み中 weight_name='model.layers.11.self_attn.k_proj.weight'
マッピングによる重みを読み込み weight_name='model.layers.11.self_attn.k_proj.weight' k='k_proj'
QKVParallelLinearの重みを読み込み開始 param.shape=torch.Size([4096, 1024]), loaded_weight.shape=torch.Size([1024, 1024]), loaded_shard_id='k'
QKVParallelLinearの重みを読み込み完了
重みを読み込み中 weight_name='model.layers.11.self_attn.o_proj.weight'
通常の重み読み込み weight_name='model.layers.11.self_attn.o_proj.weight'
RowParallelLinearの重みを読み込み開始 param.shape=torch.Size([1024, 2048]), loaded_weight.shape=torch.Size([1024, 2048])
RowParallelLinearの重みを読み込み完了
重みを読み込み中 weight_name='model.layers.11.self_attn.q_norm.weight'
通常の重み読み込み weight_name='model.layers.11.self_attn.q_norm.weight'
重みを読み込み param.shape=torch.Size([128]) loaded_weight.shape=torch.Size([128])
重みを読み込み中 weight_name='model.layers.11.self_attn.q_proj.weight'
マッピングによる重みを読み込み weight_name='model.layers.11.self_attn.q_proj.weight' k='q_proj'
QKVParallelLinearの重みを読み込み開始 param.shape=torch.Size([4096, 1024]), loaded_weight.shape=torch.Size([2048, 1024]), loaded_shard_id='q'
QKVParallelLinearの重みを読み込み完了
重みを読み込み中 weight_name='model.layers.11.self_attn.v_proj.weight'
マッピングによる重みを読み込み weight_name='model.layers.11.self_attn.v_proj.weight' k='v_proj'
QKVParallelLinearの重みを読み込み開始 param.shape=torch.Size([4096, 1024]), loaded_weight.shape=torch.Size([1024, 1024]), loaded_shard_id='v'
QKVParallelLinearの重みを読み込み完了
重みを読み込み中 weight_name='model.layers.12.input_layernorm.weight'
通常の重み読み込み weight_name='model.layers.12.input_layernorm.weight'
重みを読み込み param.shape=torch.Size([1024]) loaded_weight.shape=torch.Size([1024])
重みを読み込み中 weight_name='model.layers.12.mlp.down_proj.weight'
通常の重み読み込み weight_name='model.layers.12.mlp.down_proj.weight'
RowParallelLinearの重みを読み込み開始 param.shape=torch.Size([1024, 3072]), loaded_weight.shape=torch.Size([1024, 3072])
RowParallelLinearの重みを読み込み完了
重みを読み込み中 weight_name='model.layers.12.mlp.gate_proj.weight'
マッピングによる重みを読み込み weight_name='model.layers.12.mlp.gate_proj.weight' k='gate_proj'
MergedColumnParallelLinearの重みを読み込み開始 param.shape=torch.Size([6144, 1024]), loaded_weight.shape=torch.Size([3072, 1024]), loaded_shard_id=0
MergedColumnParallelLinearの重みを読み込み完了
重みを読み込み中 weight_name='model.layers.12.mlp.up_proj.weight'
マッピングによる重みを読み込み weight_name='model.layers.12.mlp.up_proj.weight' k='up_proj'
MergedColumnParallelLinearの重みを読み込み開始 param.shape=torch.Size([6144, 1024]), loaded_weight.shape=torch.Size([3072, 1024]), loaded_shard_id=1
MergedColumnParallelLinearの重みを読み込み完了
重みを読み込み中 weight_name='model.layers.12.post_attention_layernorm.weight'
通常の重み読み込み weight_name='model.layers.12.post_attention_layernorm.weight'
重みを読み込み param.shape=torch.Size([1024]) loaded_weight.shape=torch.Size([1024])
重みを読み込み中 weight_name='model.layers.12.self_attn.k_norm.weight'
通常の重み読み込み weight_name='model.layers.12.self_attn.k_norm.weight'
重みを読み込み param.shape=torch.Size([128]) loaded_weight.shape=torch.Size([128])
重みを読み込み中 weight_name='model.layers.12.self_attn.k_proj.weight'
マッピングによる重みを読み込み weight_name='model.layers.12.self_attn.k_proj.weight' k='k_proj'
QKVParallelLinearの重みを読み込み開始 param.shape=torch.Size([4096, 1024]), loaded_weight.shape=torch.Size([1024, 1024]), loaded_shard_id='k'
QKVParallelLinearの重みを読み込み完了
重みを読み込み中 weight_name='model.layers.12.self_attn.o_proj.weight'
通常の重み読み込み weight_name='model.layers.12.self_attn.o_proj.weight'
RowParallelLinearの重みを読み込み開始 param.shape=torch.Size([1024, 2048]), loaded_weight.shape=torch.Size([1024, 2048])
RowParallelLinearの重みを読み込み完了
重みを読み込み中 weight_name='model.layers.12.self_attn.q_norm.weight'
通常の重み読み込み weight_name='model.layers.12.self_attn.q_norm.weight'
重みを読み込み param.shape=torch.Size([128]) loaded_weight.shape=torch.Size([128])
重みを読み込み中 weight_name='model.layers.12.self_attn.q_proj.weight'
マッピングによる重みを読み込み weight_name='model.layers.12.self_attn.q_proj.weight' k='q_proj'
QKVParallelLinearの重みを読み込み開始 param.shape=torch.Size([4096, 1024]), loaded_weight.shape=torch.Size([2048, 1024]), loaded_shard_id='q'
QKVParallelLinearの重みを読み込み完了
重みを読み込み中 weight_name='model.layers.12.self_attn.v_proj.weight'
マッピングによる重みを読み込み weight_name='model.layers.12.self_attn.v_proj.weight' k='v_proj'
QKVParallelLinearの重みを読み込み開始 param.shape=torch.Size([4096, 1024]), loaded_weight.shape=torch.Size([1024, 1024]), loaded_shard_id='v'
QKVParallelLinearの重みを読み込み完了
重みを読み込み中 weight_name='model.layers.13.input_layernorm.weight'
通常の重み読み込み weight_name='model.layers.13.input_layernorm.weight'
重みを読み込み param.shape=torch.Size([1024]) loaded_weight.shape=torch.Size([1024])
重みを読み込み中 weight_name='model.layers.13.mlp.down_proj.weight'
通常の重み読み込み weight_name='model.layers.13.mlp.down_proj.weight'
RowParallelLinearの重みを読み込み開始 param.shape=torch.Size([1024, 3072]), loaded_weight.shape=torch.Size([1024, 3072])
RowParallelLinearの重みを読み込み完了
重みを読み込み中 weight_name='model.layers.13.mlp.gate_proj.weight'
マッピングによる重みを読み込み weight_name='model.layers.13.mlp.gate_proj.weight' k='gate_proj'
MergedColumnParallelLinearの重みを読み込み開始 param.shape=torch.Size([6144, 1024]), loaded_weight.shape=torch.Size([3072, 1024]), loaded_shard_id=0
MergedColumnParallelLinearの重みを読み込み完了
重みを読み込み中 weight_name='model.layers.13.mlp.up_proj.weight'
マッピングによる重みを読み込み weight_name='model.layers.13.mlp.up_proj.weight' k='up_proj'
MergedColumnParallelLinearの重みを読み込み開始 param.shape=torch.Size([6144, 1024]), loaded_weight.shape=torch.Size([3072, 1024]), loaded_shard_id=1
MergedColumnParallelLinearの重みを読み込み完了
重みを読み込み中 weight_name='model.layers.13.post_attention_layernorm.weight'
通常の重み読み込み weight_name='model.layers.13.post_attention_layernorm.weight'
重みを読み込み param.shape=torch.Size([1024]) loaded_weight.shape=torch.Size([1024])
重みを読み込み中 weight_name='model.layers.13.self_attn.k_norm.weight'
通常の重み読み込み weight_name='model.layers.13.self_attn.k_norm.weight'
重みを読み込み param.shape=torch.Size([128]) loaded_weight.shape=torch.Size([128])
重みを読み込み中 weight_name='model.layers.13.self_attn.k_proj.weight'
マッピングによる重みを読み込み weight_name='model.layers.13.self_attn.k_proj.weight' k='k_proj'
QKVParallelLinearの重みを読み込み開始 param.shape=torch.Size([4096, 1024]), loaded_weight.shape=torch.Size([1024, 1024]), loaded_shard_id='k'
QKVParallelLinearの重みを読み込み完了
重みを読み込み中 weight_name='model.layers.13.self_attn.o_proj.weight'
通常の重み読み込み weight_name='model.layers.13.self_attn.o_proj.weight'
RowParallelLinearの重みを読み込み開始 param.shape=torch.Size([1024, 2048]), loaded_weight.shape=torch.Size([1024, 2048])
RowParallelLinearの重みを読み込み完了
重みを読み込み中 weight_name='model.layers.13.self_attn.q_norm.weight'
通常の重み読み込み weight_name='model.layers.13.self_attn.q_norm.weight'
重みを読み込み param.shape=torch.Size([128]) loaded_weight.shape=torch.Size([128])
重みを読み込み中 weight_name='model.layers.13.self_attn.q_proj.weight'
マッピングによる重みを読み込み weight_name='model.layers.13.self_attn.q_proj.weight' k='q_proj'
QKVParallelLinearの重みを読み込み開始 param.shape=torch.Size([4096, 1024]), loaded_weight.shape=torch.Size([2048, 1024]), loaded_shard_id='q'
QKVParallelLinearの重みを読み込み完了
重みを読み込み中 weight_name='model.layers.13.self_attn.v_proj.weight'
マッピングによる重みを読み込み weight_name='model.layers.13.self_attn.v_proj.weight' k='v_proj'
QKVParallelLinearの重みを読み込み開始 param.shape=torch.Size([4096, 1024]), loaded_weight.shape=torch.Size([1024, 1024]), loaded_shard_id='v'
QKVParallelLinearの重みを読み込み完了
重みを読み込み中 weight_name='model.layers.14.input_layernorm.weight'
通常の重み読み込み weight_name='model.layers.14.input_layernorm.weight'
重みを読み込み param.shape=torch.Size([1024]) loaded_weight.shape=torch.Size([1024])
重みを読み込み中 weight_name='model.layers.14.mlp.down_proj.weight'
通常の重み読み込み weight_name='model.layers.14.mlp.down_proj.weight'
RowParallelLinearの重みを読み込み開始 param.shape=torch.Size([1024, 3072]), loaded_weight.shape=torch.Size([1024, 3072])
RowParallelLinearの重みを読み込み完了
重みを読み込み中 weight_name='model.layers.14.mlp.gate_proj.weight'
マッピングによる重みを読み込み weight_name='model.layers.14.mlp.gate_proj.weight' k='gate_proj'
MergedColumnParallelLinearの重みを読み込み開始 param.shape=torch.Size([6144, 1024]), loaded_weight.shape=torch.Size([3072, 1024]), loaded_shard_id=0
MergedColumnParallelLinearの重みを読み込み完了
重みを読み込み中 weight_name='model.layers.14.mlp.up_proj.weight'
マッピングによる重みを読み込み weight_name='model.layers.14.mlp.up_proj.weight' k='up_proj'
MergedColumnParallelLinearの重みを読み込み開始 param.shape=torch.Size([6144, 1024]), loaded_weight.shape=torch.Size([3072, 1024]), loaded_shard_id=1
MergedColumnParallelLinearの重みを読み込み完了
重みを読み込み中 weight_name='model.layers.14.post_attention_layernorm.weight'
通常の重み読み込み weight_name='model.layers.14.post_attention_layernorm.weight'
重みを読み込み param.shape=torch.Size([1024]) loaded_weight.shape=torch.Size([1024])
重みを読み込み中 weight_name='model.layers.14.self_attn.k_norm.weight'
通常の重み読み込み weight_name='model.layers.14.self_attn.k_norm.weight'
重みを読み込み param.shape=torch.Size([128]) loaded_weight.shape=torch.Size([128])
重みを読み込み中 weight_name='model.layers.14.self_attn.k_proj.weight'
マッピングによる重みを読み込み weight_name='model.layers.14.self_attn.k_proj.weight' k='k_proj'
QKVParallelLinearの重みを読み込み開始 param.shape=torch.Size([4096, 1024]), loaded_weight.shape=torch.Size([1024, 1024]), loaded_shard_id='k'
QKVParallelLinearの重みを読み込み完了
重みを読み込み中 weight_name='model.layers.14.self_attn.o_proj.weight'
通常の重み読み込み weight_name='model.layers.14.self_attn.o_proj.weight'
RowParallelLinearの重みを読み込み開始 param.shape=torch.Size([1024, 2048]), loaded_weight.shape=torch.Size([1024, 2048])
RowParallelLinearの重みを読み込み完了
重みを読み込み中 weight_name='model.layers.14.self_attn.q_norm.weight'
通常の重み読み込み weight_name='model.layers.14.self_attn.q_norm.weight'
重みを読み込み param.shape=torch.Size([128]) loaded_weight.shape=torch.Size([128])
重みを読み込み中 weight_name='model.layers.14.self_attn.q_proj.weight'
マッピングによる重みを読み込み weight_name='model.layers.14.self_attn.q_proj.weight' k='q_proj'
QKVParallelLinearの重みを読み込み開始 param.shape=torch.Size([4096, 1024]), loaded_weight.shape=torch.Size([2048, 1024]), loaded_shard_id='q'
QKVParallelLinearの重みを読み込み完了
重みを読み込み中 weight_name='model.layers.14.self_attn.v_proj.weight'
マッピングによる重みを読み込み weight_name='model.layers.14.self_attn.v_proj.weight' k='v_proj'
QKVParallelLinearの重みを読み込み開始 param.shape=torch.Size([4096, 1024]), loaded_weight.shape=torch.Size([1024, 1024]), loaded_shard_id='v'
QKVParallelLinearの重みを読み込み完了
重みを読み込み中 weight_name='model.layers.15.input_layernorm.weight'
通常の重み読み込み weight_name='model.layers.15.input_layernorm.weight'
重みを読み込み param.shape=torch.Size([1024]) loaded_weight.shape=torch.Size([1024])
重みを読み込み中 weight_name='model.layers.15.mlp.down_proj.weight'
通常の重み読み込み weight_name='model.layers.15.mlp.down_proj.weight'
RowParallelLinearの重みを読み込み開始 param.shape=torch.Size([1024, 3072]), loaded_weight.shape=torch.Size([1024, 3072])
RowParallelLinearの重みを読み込み完了
重みを読み込み中 weight_name='model.layers.15.mlp.gate_proj.weight'
マッピングによる重みを読み込み weight_name='model.layers.15.mlp.gate_proj.weight' k='gate_proj'
MergedColumnParallelLinearの重みを読み込み開始 param.shape=torch.Size([6144, 1024]), loaded_weight.shape=torch.Size([3072, 1024]), loaded_shard_id=0
MergedColumnParallelLinearの重みを読み込み完了
重みを読み込み中 weight_name='model.layers.15.mlp.up_proj.weight'
マッピングによる重みを読み込み weight_name='model.layers.15.mlp.up_proj.weight' k='up_proj'
MergedColumnParallelLinearの重みを読み込み開始 param.shape=torch.Size([6144, 1024]), loaded_weight.shape=torch.Size([3072, 1024]), loaded_shard_id=1
MergedColumnParallelLinearの重みを読み込み完了
重みを読み込み中 weight_name='model.layers.15.post_attention_layernorm.weight'
通常の重み読み込み weight_name='model.layers.15.post_attention_layernorm.weight'
重みを読み込み param.shape=torch.Size([1024]) loaded_weight.shape=torch.Size([1024])
重みを読み込み中 weight_name='model.layers.15.self_attn.k_norm.weight'
通常の重み読み込み weight_name='model.layers.15.self_attn.k_norm.weight'
重みを読み込み param.shape=torch.Size([128]) loaded_weight.shape=torch.Size([128])
重みを読み込み中 weight_name='model.layers.15.self_attn.k_proj.weight'
マッピングによる重みを読み込み weight_name='model.layers.15.self_attn.k_proj.weight' k='k_proj'
QKVParallelLinearの重みを読み込み開始 param.shape=torch.Size([4096, 1024]), loaded_weight.shape=torch.Size([1024, 1024]), loaded_shard_id='k'
QKVParallelLinearの重みを読み込み完了
重みを読み込み中 weight_name='model.layers.15.self_attn.o_proj.weight'
通常の重み読み込み weight_name='model.layers.15.self_attn.o_proj.weight'
RowParallelLinearの重みを読み込み開始 param.shape=torch.Size([1024, 2048]), loaded_weight.shape=torch.Size([1024, 2048])
RowParallelLinearの重みを読み込み完了
重みを読み込み中 weight_name='model.layers.15.self_attn.q_norm.weight'
通常の重み読み込み weight_name='model.layers.15.self_attn.q_norm.weight'
重みを読み込み param.shape=torch.Size([128]) loaded_weight.shape=torch.Size([128])
重みを読み込み中 weight_name='model.layers.15.self_attn.q_proj.weight'
マッピングによる重みを読み込み weight_name='model.layers.15.self_attn.q_proj.weight' k='q_proj'
QKVParallelLinearの重みを読み込み開始 param.shape=torch.Size([4096, 1024]), loaded_weight.shape=torch.Size([2048, 1024]), loaded_shard_id='q'
QKVParallelLinearの重みを読み込み完了
重みを読み込み中 weight_name='model.layers.15.self_attn.v_proj.weight'
マッピングによる重みを読み込み weight_name='model.layers.15.self_attn.v_proj.weight' k='v_proj'
QKVParallelLinearの重みを読み込み開始 param.shape=torch.Size([4096, 1024]), loaded_weight.shape=torch.Size([1024, 1024]), loaded_shard_id='v'
QKVParallelLinearの重みを読み込み完了
重みを読み込み中 weight_name='model.layers.16.input_layernorm.weight'
通常の重み読み込み weight_name='model.layers.16.input_layernorm.weight'
重みを読み込み param.shape=torch.Size([1024]) loaded_weight.shape=torch.Size([1024])
重みを読み込み中 weight_name='model.layers.16.mlp.down_proj.weight'
通常の重み読み込み weight_name='model.layers.16.mlp.down_proj.weight'
RowParallelLinearの重みを読み込み開始 param.shape=torch.Size([1024, 3072]), loaded_weight.shape=torch.Size([1024, 3072])
RowParallelLinearの重みを読み込み完了
重みを読み込み中 weight_name='model.layers.16.mlp.gate_proj.weight'
マッピングによる重みを読み込み weight_name='model.layers.16.mlp.gate_proj.weight' k='gate_proj'
MergedColumnParallelLinearの重みを読み込み開始 param.shape=torch.Size([6144, 1024]), loaded_weight.shape=torch.Size([3072, 1024]), loaded_shard_id=0
MergedColumnParallelLinearの重みを読み込み完了
重みを読み込み中 weight_name='model.layers.16.mlp.up_proj.weight'
マッピングによる重みを読み込み weight_name='model.layers.16.mlp.up_proj.weight' k='up_proj'
MergedColumnParallelLinearの重みを読み込み開始 param.shape=torch.Size([6144, 1024]), loaded_weight.shape=torch.Size([3072, 1024]), loaded_shard_id=1
MergedColumnParallelLinearの重みを読み込み完了
重みを読み込み中 weight_name='model.layers.16.post_attention_layernorm.weight'
通常の重み読み込み weight_name='model.layers.16.post_attention_layernorm.weight'
重みを読み込み param.shape=torch.Size([1024]) loaded_weight.shape=torch.Size([1024])
重みを読み込み中 weight_name='model.layers.16.self_attn.k_norm.weight'
通常の重み読み込み weight_name='model.layers.16.self_attn.k_norm.weight'
重みを読み込み param.shape=torch.Size([128]) loaded_weight.shape=torch.Size([128])
重みを読み込み中 weight_name='model.layers.16.self_attn.k_proj.weight'
マッピングによる重みを読み込み weight_name='model.layers.16.self_attn.k_proj.weight' k='k_proj'
QKVParallelLinearの重みを読み込み開始 param.shape=torch.Size([4096, 1024]), loaded_weight.shape=torch.Size([1024, 1024]), loaded_shard_id='k'
QKVParallelLinearの重みを読み込み完了
重みを読み込み中 weight_name='model.layers.16.self_attn.o_proj.weight'
通常の重み読み込み weight_name='model.layers.16.self_attn.o_proj.weight'
RowParallelLinearの重みを読み込み開始 param.shape=torch.Size([1024, 2048]), loaded_weight.shape=torch.Size([1024, 2048])
RowParallelLinearの重みを読み込み完了
重みを読み込み中 weight_name='model.layers.16.self_attn.q_norm.weight'
通常の重み読み込み weight_name='model.layers.16.self_attn.q_norm.weight'
重みを読み込み param.shape=torch.Size([128]) loaded_weight.shape=torch.Size([128])
重みを読み込み中 weight_name='model.layers.16.self_attn.q_proj.weight'
マッピングによる重みを読み込み weight_name='model.layers.16.self_attn.q_proj.weight' k='q_proj'
QKVParallelLinearの重みを読み込み開始 param.shape=torch.Size([4096, 1024]), loaded_weight.shape=torch.Size([2048, 1024]), loaded_shard_id='q'
QKVParallelLinearの重みを読み込み完了
重みを読み込み中 weight_name='model.layers.16.self_attn.v_proj.weight'
マッピングによる重みを読み込み weight_name='model.layers.16.self_attn.v_proj.weight' k='v_proj'
QKVParallelLinearの重みを読み込み開始 param.shape=torch.Size([4096, 1024]), loaded_weight.shape=torch.Size([1024, 1024]), loaded_shard_id='v'
QKVParallelLinearの重みを読み込み完了
重みを読み込み中 weight_name='model.layers.17.input_layernorm.weight'
通常の重み読み込み weight_name='model.layers.17.input_layernorm.weight'
重みを読み込み param.shape=torch.Size([1024]) loaded_weight.shape=torch.Size([1024])
重みを読み込み中 weight_name='model.layers.17.mlp.down_proj.weight'
通常の重み読み込み weight_name='model.layers.17.mlp.down_proj.weight'
RowParallelLinearの重みを読み込み開始 param.shape=torch.Size([1024, 3072]), loaded_weight.shape=torch.Size([1024, 3072])
RowParallelLinearの重みを読み込み完了
重みを読み込み中 weight_name='model.layers.17.mlp.gate_proj.weight'
マッピングによる重みを読み込み weight_name='model.layers.17.mlp.gate_proj.weight' k='gate_proj'
MergedColumnParallelLinearの重みを読み込み開始 param.shape=torch.Size([6144, 1024]), loaded_weight.shape=torch.Size([3072, 1024]), loaded_shard_id=0
MergedColumnParallelLinearの重みを読み込み完了
重みを読み込み中 weight_name='model.layers.17.mlp.up_proj.weight'
マッピングによる重みを読み込み weight_name='model.layers.17.mlp.up_proj.weight' k='up_proj'
MergedColumnParallelLinearの重みを読み込み開始 param.shape=torch.Size([6144, 1024]), loaded_weight.shape=torch.Size([3072, 1024]), loaded_shard_id=1
MergedColumnParallelLinearの重みを読み込み完了
重みを読み込み中 weight_name='model.layers.17.post_attention_layernorm.weight'
通常の重み読み込み weight_name='model.layers.17.post_attention_layernorm.weight'
重みを読み込み param.shape=torch.Size([1024]) loaded_weight.shape=torch.Size([1024])
重みを読み込み中 weight_name='model.layers.17.self_attn.k_norm.weight'
通常の重み読み込み weight_name='model.layers.17.self_attn.k_norm.weight'
重みを読み込み param.shape=torch.Size([128]) loaded_weight.shape=torch.Size([128])
重みを読み込み中 weight_name='model.layers.17.self_attn.k_proj.weight'
マッピングによる重みを読み込み weight_name='model.layers.17.self_attn.k_proj.weight' k='k_proj'
QKVParallelLinearの重みを読み込み開始 param.shape=torch.Size([4096, 1024]), loaded_weight.shape=torch.Size([1024, 1024]), loaded_shard_id='k'
QKVParallelLinearの重みを読み込み完了
重みを読み込み中 weight_name='model.layers.17.self_attn.o_proj.weight'
通常の重み読み込み weight_name='model.layers.17.self_attn.o_proj.weight'
RowParallelLinearの重みを読み込み開始 param.shape=torch.Size([1024, 2048]), loaded_weight.shape=torch.Size([1024, 2048])
RowParallelLinearの重みを読み込み完了
重みを読み込み中 weight_name='model.layers.17.self_attn.q_norm.weight'
通常の重み読み込み weight_name='model.layers.17.self_attn.q_norm.weight'
重みを読み込み param.shape=torch.Size([128]) loaded_weight.shape=torch.Size([128])
重みを読み込み中 weight_name='model.layers.17.self_attn.q_proj.weight'
マッピングによる重みを読み込み weight_name='model.layers.17.self_attn.q_proj.weight' k='q_proj'
QKVParallelLinearの重みを読み込み開始 param.shape=torch.Size([4096, 1024]), loaded_weight.shape=torch.Size([2048, 1024]), loaded_shard_id='q'
QKVParallelLinearの重みを読み込み完了
重みを読み込み中 weight_name='model.layers.17.self_attn.v_proj.weight'
マッピングによる重みを読み込み weight_name='model.layers.17.self_attn.v_proj.weight' k='v_proj'
QKVParallelLinearの重みを読み込み開始 param.shape=torch.Size([4096, 1024]), loaded_weight.shape=torch.Size([1024, 1024]), loaded_shard_id='v'
QKVParallelLinearの重みを読み込み完了
重みを読み込み中 weight_name='model.layers.18.input_layernorm.weight'
通常の重み読み込み weight_name='model.layers.18.input_layernorm.weight'
重みを読み込み param.shape=torch.Size([1024]) loaded_weight.shape=torch.Size([1024])
重みを読み込み中 weight_name='model.layers.18.mlp.down_proj.weight'
通常の重み読み込み weight_name='model.layers.18.mlp.down_proj.weight'
RowParallelLinearの重みを読み込み開始 param.shape=torch.Size([1024, 3072]), loaded_weight.shape=torch.Size([1024, 3072])
RowParallelLinearの重みを読み込み完了
重みを読み込み中 weight_name='model.layers.18.mlp.gate_proj.weight'
マッピングによる重みを読み込み weight_name='model.layers.18.mlp.gate_proj.weight' k='gate_proj'
MergedColumnParallelLinearの重みを読み込み開始 param.shape=torch.Size([6144, 1024]), loaded_weight.shape=torch.Size([3072, 1024]), loaded_shard_id=0
MergedColumnParallelLinearの重みを読み込み完了
重みを読み込み中 weight_name='model.layers.18.mlp.up_proj.weight'
マッピングによる重みを読み込み weight_name='model.layers.18.mlp.up_proj.weight' k='up_proj'
MergedColumnParallelLinearの重みを読み込み開始 param.shape=torch.Size([6144, 1024]), loaded_weight.shape=torch.Size([3072, 1024]), loaded_shard_id=1
MergedColumnParallelLinearの重みを読み込み完了
重みを読み込み中 weight_name='model.layers.18.post_attention_layernorm.weight'
通常の重み読み込み weight_name='model.layers.18.post_attention_layernorm.weight'
重みを読み込み param.shape=torch.Size([1024]) loaded_weight.shape=torch.Size([1024])
重みを読み込み中 weight_name='model.layers.18.self_attn.k_norm.weight'
通常の重み読み込み weight_name='model.layers.18.self_attn.k_norm.weight'
重みを読み込み param.shape=torch.Size([128]) loaded_weight.shape=torch.Size([128])
重みを読み込み中 weight_name='model.layers.18.self_attn.k_proj.weight'
マッピングによる重みを読み込み weight_name='model.layers.18.self_attn.k_proj.weight' k='k_proj'
QKVParallelLinearの重みを読み込み開始 param.shape=torch.Size([4096, 1024]), loaded_weight.shape=torch.Size([1024, 1024]), loaded_shard_id='k'
QKVParallelLinearの重みを読み込み完了
重みを読み込み中 weight_name='model.layers.18.self_attn.o_proj.weight'
通常の重み読み込み weight_name='model.layers.18.self_attn.o_proj.weight'
RowParallelLinearの重みを読み込み開始 param.shape=torch.Size([1024, 2048]), loaded_weight.shape=torch.Size([1024, 2048])
RowParallelLinearの重みを読み込み完了
重みを読み込み中 weight_name='model.layers.18.self_attn.q_norm.weight'
通常の重み読み込み weight_name='model.layers.18.self_attn.q_norm.weight'
重みを読み込み param.shape=torch.Size([128]) loaded_weight.shape=torch.Size([128])
重みを読み込み中 weight_name='model.layers.18.self_attn.q_proj.weight'
マッピングによる重みを読み込み weight_name='model.layers.18.self_attn.q_proj.weight' k='q_proj'
QKVParallelLinearの重みを読み込み開始 param.shape=torch.Size([4096, 1024]), loaded_weight.shape=torch.Size([2048, 1024]), loaded_shard_id='q'
QKVParallelLinearの重みを読み込み完了
重みを読み込み中 weight_name='model.layers.18.self_attn.v_proj.weight'
マッピングによる重みを読み込み weight_name='model.layers.18.self_attn.v_proj.weight' k='v_proj'
QKVParallelLinearの重みを読み込み開始 param.shape=torch.Size([4096, 1024]), loaded_weight.shape=torch.Size([1024, 1024]), loaded_shard_id='v'
QKVParallelLinearの重みを読み込み完了
重みを読み込み中 weight_name='model.layers.19.input_layernorm.weight'
通常の重み読み込み weight_name='model.layers.19.input_layernorm.weight'
重みを読み込み param.shape=torch.Size([1024]) loaded_weight.shape=torch.Size([1024])
重みを読み込み中 weight_name='model.layers.19.mlp.down_proj.weight'
通常の重み読み込み weight_name='model.layers.19.mlp.down_proj.weight'
RowParallelLinearの重みを読み込み開始 param.shape=torch.Size([1024, 3072]), loaded_weight.shape=torch.Size([1024, 3072])
RowParallelLinearの重みを読み込み完了
重みを読み込み中 weight_name='model.layers.19.mlp.gate_proj.weight'
マッピングによる重みを読み込み weight_name='model.layers.19.mlp.gate_proj.weight' k='gate_proj'
MergedColumnParallelLinearの重みを読み込み開始 param.shape=torch.Size([6144, 1024]), loaded_weight.shape=torch.Size([3072, 1024]), loaded_shard_id=0
MergedColumnParallelLinearの重みを読み込み完了
重みを読み込み中 weight_name='model.layers.19.mlp.up_proj.weight'
マッピングによる重みを読み込み weight_name='model.layers.19.mlp.up_proj.weight' k='up_proj'
MergedColumnParallelLinearの重みを読み込み開始 param.shape=torch.Size([6144, 1024]), loaded_weight.shape=torch.Size([3072, 1024]), loaded_shard_id=1
MergedColumnParallelLinearの重みを読み込み完了
重みを読み込み中 weight_name='model.layers.19.post_attention_layernorm.weight'
通常の重み読み込み weight_name='model.layers.19.post_attention_layernorm.weight'
重みを読み込み param.shape=torch.Size([1024]) loaded_weight.shape=torch.Size([1024])
重みを読み込み中 weight_name='model.layers.19.self_attn.k_norm.weight'
通常の重み読み込み weight_name='model.layers.19.self_attn.k_norm.weight'
重みを読み込み param.shape=torch.Size([128]) loaded_weight.shape=torch.Size([128])
重みを読み込み中 weight_name='model.layers.19.self_attn.k_proj.weight'
マッピングによる重みを読み込み weight_name='model.layers.19.self_attn.k_proj.weight' k='k_proj'
QKVParallelLinearの重みを読み込み開始 param.shape=torch.Size([4096, 1024]), loaded_weight.shape=torch.Size([1024, 1024]), loaded_shard_id='k'
QKVParallelLinearの重みを読み込み完了
重みを読み込み中 weight_name='model.layers.19.self_attn.o_proj.weight'
通常の重み読み込み weight_name='model.layers.19.self_attn.o_proj.weight'
RowParallelLinearの重みを読み込み開始 param.shape=torch.Size([1024, 2048]), loaded_weight.shape=torch.Size([1024, 2048])
RowParallelLinearの重みを読み込み完了
重みを読み込み中 weight_name='model.layers.19.self_attn.q_norm.weight'
通常の重み読み込み weight_name='model.layers.19.self_attn.q_norm.weight'
重みを読み込み param.shape=torch.Size([128]) loaded_weight.shape=torch.Size([128])
重みを読み込み中 weight_name='model.layers.19.self_attn.q_proj.weight'
マッピングによる重みを読み込み weight_name='model.layers.19.self_attn.q_proj.weight' k='q_proj'
QKVParallelLinearの重みを読み込み開始 param.shape=torch.Size([4096, 1024]), loaded_weight.shape=torch.Size([2048, 1024]), loaded_shard_id='q'
QKVParallelLinearの重みを読み込み完了
重みを読み込み中 weight_name='model.layers.19.self_attn.v_proj.weight'
マッピングによる重みを読み込み weight_name='model.layers.19.self_attn.v_proj.weight' k='v_proj'
QKVParallelLinearの重みを読み込み開始 param.shape=torch.Size([4096, 1024]), loaded_weight.shape=torch.Size([1024, 1024]), loaded_shard_id='v'
QKVParallelLinearの重みを読み込み完了
重みを読み込み中 weight_name='model.layers.2.input_layernorm.weight'
通常の重み読み込み weight_name='model.layers.2.input_layernorm.weight'
重みを読み込み param.shape=torch.Size([1024]) loaded_weight.shape=torch.Size([1024])
重みを読み込み中 weight_name='model.layers.2.mlp.down_proj.weight'
通常の重み読み込み weight_name='model.layers.2.mlp.down_proj.weight'
RowParallelLinearの重みを読み込み開始 param.shape=torch.Size([1024, 3072]), loaded_weight.shape=torch.Size([1024, 3072])
RowParallelLinearの重みを読み込み完了
重みを読み込み中 weight_name='model.layers.2.mlp.gate_proj.weight'
マッピングによる重みを読み込み weight_name='model.layers.2.mlp.gate_proj.weight' k='gate_proj'
MergedColumnParallelLinearの重みを読み込み開始 param.shape=torch.Size([6144, 1024]), loaded_weight.shape=torch.Size([3072, 1024]), loaded_shard_id=0
MergedColumnParallelLinearの重みを読み込み完了
重みを読み込み中 weight_name='model.layers.2.mlp.up_proj.weight'
マッピングによる重みを読み込み weight_name='model.layers.2.mlp.up_proj.weight' k='up_proj'
MergedColumnParallelLinearの重みを読み込み開始 param.shape=torch.Size([6144, 1024]), loaded_weight.shape=torch.Size([3072, 1024]), loaded_shard_id=1
MergedColumnParallelLinearの重みを読み込み完了
重みを読み込み中 weight_name='model.layers.2.post_attention_layernorm.weight'
通常の重み読み込み weight_name='model.layers.2.post_attention_layernorm.weight'
重みを読み込み param.shape=torch.Size([1024]) loaded_weight.shape=torch.Size([1024])
重みを読み込み中 weight_name='model.layers.2.self_attn.k_norm.weight'
通常の重み読み込み weight_name='model.layers.2.self_attn.k_norm.weight'
重みを読み込み param.shape=torch.Size([128]) loaded_weight.shape=torch.Size([128])
重みを読み込み中 weight_name='model.layers.2.self_attn.k_proj.weight'
マッピングによる重みを読み込み weight_name='model.layers.2.self_attn.k_proj.weight' k='k_proj'
QKVParallelLinearの重みを読み込み開始 param.shape=torch.Size([4096, 1024]), loaded_weight.shape=torch.Size([1024, 1024]), loaded_shard_id='k'
QKVParallelLinearの重みを読み込み完了
重みを読み込み中 weight_name='model.layers.2.self_attn.o_proj.weight'
通常の重み読み込み weight_name='model.layers.2.self_attn.o_proj.weight'
RowParallelLinearの重みを読み込み開始 param.shape=torch.Size([1024, 2048]), loaded_weight.shape=torch.Size([1024, 2048])
RowParallelLinearの重みを読み込み完了
重みを読み込み中 weight_name='model.layers.2.self_attn.q_norm.weight'
通常の重み読み込み weight_name='model.layers.2.self_attn.q_norm.weight'
重みを読み込み param.shape=torch.Size([128]) loaded_weight.shape=torch.Size([128])
重みを読み込み中 weight_name='model.layers.2.self_attn.q_proj.weight'
マッピングによる重みを読み込み weight_name='model.layers.2.self_attn.q_proj.weight' k='q_proj'
QKVParallelLinearの重みを読み込み開始 param.shape=torch.Size([4096, 1024]), loaded_weight.shape=torch.Size([2048, 1024]), loaded_shard_id='q'
QKVParallelLinearの重みを読み込み完了
重みを読み込み中 weight_name='model.layers.2.self_attn.v_proj.weight'
マッピングによる重みを読み込み weight_name='model.layers.2.self_attn.v_proj.weight' k='v_proj'
QKVParallelLinearの重みを読み込み開始 param.shape=torch.Size([4096, 1024]), loaded_weight.shape=torch.Size([1024, 1024]), loaded_shard_id='v'
QKVParallelLinearの重みを読み込み完了
重みを読み込み中 weight_name='model.layers.20.input_layernorm.weight'
通常の重み読み込み weight_name='model.layers.20.input_layernorm.weight'
重みを読み込み param.shape=torch.Size([1024]) loaded_weight.shape=torch.Size([1024])
重みを読み込み中 weight_name='model.layers.20.mlp.down_proj.weight'
通常の重み読み込み weight_name='model.layers.20.mlp.down_proj.weight'
RowParallelLinearの重みを読み込み開始 param.shape=torch.Size([1024, 3072]), loaded_weight.shape=torch.Size([1024, 3072])
RowParallelLinearの重みを読み込み完了
重みを読み込み中 weight_name='model.layers.20.mlp.gate_proj.weight'
マッピングによる重みを読み込み weight_name='model.layers.20.mlp.gate_proj.weight' k='gate_proj'
MergedColumnParallelLinearの重みを読み込み開始 param.shape=torch.Size([6144, 1024]), loaded_weight.shape=torch.Size([3072, 1024]), loaded_shard_id=0
MergedColumnParallelLinearの重みを読み込み完了
重みを読み込み中 weight_name='model.layers.20.mlp.up_proj.weight'
マッピングによる重みを読み込み weight_name='model.layers.20.mlp.up_proj.weight' k='up_proj'
MergedColumnParallelLinearの重みを読み込み開始 param.shape=torch.Size([6144, 1024]), loaded_weight.shape=torch.Size([3072, 1024]), loaded_shard_id=1
MergedColumnParallelLinearの重みを読み込み完了
重みを読み込み中 weight_name='model.layers.20.post_attention_layernorm.weight'
通常の重み読み込み weight_name='model.layers.20.post_attention_layernorm.weight'
重みを読み込み param.shape=torch.Size([1024]) loaded_weight.shape=torch.Size([1024])
重みを読み込み中 weight_name='model.layers.20.self_attn.k_norm.weight'
通常の重み読み込み weight_name='model.layers.20.self_attn.k_norm.weight'
重みを読み込み param.shape=torch.Size([128]) loaded_weight.shape=torch.Size([128])
重みを読み込み中 weight_name='model.layers.20.self_attn.k_proj.weight'
マッピングによる重みを読み込み weight_name='model.layers.20.self_attn.k_proj.weight' k='k_proj'
QKVParallelLinearの重みを読み込み開始 param.shape=torch.Size([4096, 1024]), loaded_weight.shape=torch.Size([1024, 1024]), loaded_shard_id='k'
QKVParallelLinearの重みを読み込み完了
重みを読み込み中 weight_name='model.layers.20.self_attn.o_proj.weight'
通常の重み読み込み weight_name='model.layers.20.self_attn.o_proj.weight'
RowParallelLinearの重みを読み込み開始 param.shape=torch.Size([1024, 2048]), loaded_weight.shape=torch.Size([1024, 2048])
RowParallelLinearの重みを読み込み完了
重みを読み込み中 weight_name='model.layers.20.self_attn.q_norm.weight'
通常の重み読み込み weight_name='model.layers.20.self_attn.q_norm.weight'
重みを読み込み param.shape=torch.Size([128]) loaded_weight.shape=torch.Size([128])
重みを読み込み中 weight_name='model.layers.20.self_attn.q_proj.weight'
マッピングによる重みを読み込み weight_name='model.layers.20.self_attn.q_proj.weight' k='q_proj'
QKVParallelLinearの重みを読み込み開始 param.shape=torch.Size([4096, 1024]), loaded_weight.shape=torch.Size([2048, 1024]), loaded_shard_id='q'
QKVParallelLinearの重みを読み込み完了
重みを読み込み中 weight_name='model.layers.20.self_attn.v_proj.weight'
マッピングによる重みを読み込み weight_name='model.layers.20.self_attn.v_proj.weight' k='v_proj'
QKVParallelLinearの重みを読み込み開始 param.shape=torch.Size([4096, 1024]), loaded_weight.shape=torch.Size([1024, 1024]), loaded_shard_id='v'
QKVParallelLinearの重みを読み込み完了
重みを読み込み中 weight_name='model.layers.21.input_layernorm.weight'
通常の重み読み込み weight_name='model.layers.21.input_layernorm.weight'
重みを読み込み param.shape=torch.Size([1024]) loaded_weight.shape=torch.Size([1024])
重みを読み込み中 weight_name='model.layers.21.mlp.down_proj.weight'
通常の重み読み込み weight_name='model.layers.21.mlp.down_proj.weight'
RowParallelLinearの重みを読み込み開始 param.shape=torch.Size([1024, 3072]), loaded_weight.shape=torch.Size([1024, 3072])
RowParallelLinearの重みを読み込み完了
重みを読み込み中 weight_name='model.layers.21.mlp.gate_proj.weight'
マッピングによる重みを読み込み weight_name='model.layers.21.mlp.gate_proj.weight' k='gate_proj'
MergedColumnParallelLinearの重みを読み込み開始 param.shape=torch.Size([6144, 1024]), loaded_weight.shape=torch.Size([3072, 1024]), loaded_shard_id=0
MergedColumnParallelLinearの重みを読み込み完了
重みを読み込み中 weight_name='model.layers.21.mlp.up_proj.weight'
マッピングによる重みを読み込み weight_name='model.layers.21.mlp.up_proj.weight' k='up_proj'
MergedColumnParallelLinearの重みを読み込み開始 param.shape=torch.Size([6144, 1024]), loaded_weight.shape=torch.Size([3072, 1024]), loaded_shard_id=1
MergedColumnParallelLinearの重みを読み込み完了
重みを読み込み中 weight_name='model.layers.21.post_attention_layernorm.weight'
通常の重み読み込み weight_name='model.layers.21.post_attention_layernorm.weight'
重みを読み込み param.shape=torch.Size([1024]) loaded_weight.shape=torch.Size([1024])
重みを読み込み中 weight_name='model.layers.21.self_attn.k_norm.weight'
通常の重み読み込み weight_name='model.layers.21.self_attn.k_norm.weight'
重みを読み込み param.shape=torch.Size([128]) loaded_weight.shape=torch.Size([128])
重みを読み込み中 weight_name='model.layers.21.self_attn.k_proj.weight'
マッピングによる重みを読み込み weight_name='model.layers.21.self_attn.k_proj.weight' k='k_proj'
QKVParallelLinearの重みを読み込み開始 param.shape=torch.Size([4096, 1024]), loaded_weight.shape=torch.Size([1024, 1024]), loaded_shard_id='k'
QKVParallelLinearの重みを読み込み完了
重みを読み込み中 weight_name='model.layers.21.self_attn.o_proj.weight'
通常の重み読み込み weight_name='model.layers.21.self_attn.o_proj.weight'
RowParallelLinearの重みを読み込み開始 param.shape=torch.Size([1024, 2048]), loaded_weight.shape=torch.Size([1024, 2048])
RowParallelLinearの重みを読み込み完了
重みを読み込み中 weight_name='model.layers.21.self_attn.q_norm.weight'
通常の重み読み込み weight_name='model.layers.21.self_attn.q_norm.weight'
重みを読み込み param.shape=torch.Size([128]) loaded_weight.shape=torch.Size([128])
重みを読み込み中 weight_name='model.layers.21.self_attn.q_proj.weight'
マッピングによる重みを読み込み weight_name='model.layers.21.self_attn.q_proj.weight' k='q_proj'
QKVParallelLinearの重みを読み込み開始 param.shape=torch.Size([4096, 1024]), loaded_weight.shape=torch.Size([2048, 1024]), loaded_shard_id='q'
QKVParallelLinearの重みを読み込み完了
重みを読み込み中 weight_name='model.layers.21.self_attn.v_proj.weight'
マッピングによる重みを読み込み weight_name='model.layers.21.self_attn.v_proj.weight' k='v_proj'
QKVParallelLinearの重みを読み込み開始 param.shape=torch.Size([4096, 1024]), loaded_weight.shape=torch.Size([1024, 1024]), loaded_shard_id='v'
QKVParallelLinearの重みを読み込み完了
重みを読み込み中 weight_name='model.layers.22.input_layernorm.weight'
通常の重み読み込み weight_name='model.layers.22.input_layernorm.weight'
重みを読み込み param.shape=torch.Size([1024]) loaded_weight.shape=torch.Size([1024])
重みを読み込み中 weight_name='model.layers.22.mlp.down_proj.weight'
通常の重み読み込み weight_name='model.layers.22.mlp.down_proj.weight'
RowParallelLinearの重みを読み込み開始 param.shape=torch.Size([1024, 3072]), loaded_weight.shape=torch.Size([1024, 3072])
RowParallelLinearの重みを読み込み完了
重みを読み込み中 weight_name='model.layers.22.mlp.gate_proj.weight'
マッピングによる重みを読み込み weight_name='model.layers.22.mlp.gate_proj.weight' k='gate_proj'
MergedColumnParallelLinearの重みを読み込み開始 param.shape=torch.Size([6144, 1024]), loaded_weight.shape=torch.Size([3072, 1024]), loaded_shard_id=0
MergedColumnParallelLinearの重みを読み込み完了
重みを読み込み中 weight_name='model.layers.22.mlp.up_proj.weight'
マッピングによる重みを読み込み weight_name='model.layers.22.mlp.up_proj.weight' k='up_proj'
MergedColumnParallelLinearの重みを読み込み開始 param.shape=torch.Size([6144, 1024]), loaded_weight.shape=torch.Size([3072, 1024]), loaded_shard_id=1
MergedColumnParallelLinearの重みを読み込み完了
重みを読み込み中 weight_name='model.layers.22.post_attention_layernorm.weight'
通常の重み読み込み weight_name='model.layers.22.post_attention_layernorm.weight'
重みを読み込み param.shape=torch.Size([1024]) loaded_weight.shape=torch.Size([1024])
重みを読み込み中 weight_name='model.layers.22.self_attn.k_norm.weight'
通常の重み読み込み weight_name='model.layers.22.self_attn.k_norm.weight'
重みを読み込み param.shape=torch.Size([128]) loaded_weight.shape=torch.Size([128])
重みを読み込み中 weight_name='model.layers.22.self_attn.k_proj.weight'
マッピングによる重みを読み込み weight_name='model.layers.22.self_attn.k_proj.weight' k='k_proj'
QKVParallelLinearの重みを読み込み開始 param.shape=torch.Size([4096, 1024]), loaded_weight.shape=torch.Size([1024, 1024]), loaded_shard_id='k'
QKVParallelLinearの重みを読み込み完了
重みを読み込み中 weight_name='model.layers.22.self_attn.o_proj.weight'
通常の重み読み込み weight_name='model.layers.22.self_attn.o_proj.weight'
RowParallelLinearの重みを読み込み開始 param.shape=torch.Size([1024, 2048]), loaded_weight.shape=torch.Size([1024, 2048])
RowParallelLinearの重みを読み込み完了
重みを読み込み中 weight_name='model.layers.22.self_attn.q_norm.weight'
通常の重み読み込み weight_name='model.layers.22.self_attn.q_norm.weight'
重みを読み込み param.shape=torch.Size([128]) loaded_weight.shape=torch.Size([128])
重みを読み込み中 weight_name='model.layers.22.self_attn.q_proj.weight'
マッピングによる重みを読み込み weight_name='model.layers.22.self_attn.q_proj.weight' k='q_proj'
QKVParallelLinearの重みを読み込み開始 param.shape=torch.Size([4096, 1024]), loaded_weight.shape=torch.Size([2048, 1024]), loaded_shard_id='q'
QKVParallelLinearの重みを読み込み完了
重みを読み込み中 weight_name='model.layers.22.self_attn.v_proj.weight'
マッピングによる重みを読み込み weight_name='model.layers.22.self_attn.v_proj.weight' k='v_proj'
QKVParallelLinearの重みを読み込み開始 param.shape=torch.Size([4096, 1024]), loaded_weight.shape=torch.Size([1024, 1024]), loaded_shard_id='v'
QKVParallelLinearの重みを読み込み完了
重みを読み込み中 weight_name='model.layers.23.input_layernorm.weight'
通常の重み読み込み weight_name='model.layers.23.input_layernorm.weight'
重みを読み込み param.shape=torch.Size([1024]) loaded_weight.shape=torch.Size([1024])
重みを読み込み中 weight_name='model.layers.23.mlp.down_proj.weight'
通常の重み読み込み weight_name='model.layers.23.mlp.down_proj.weight'
RowParallelLinearの重みを読み込み開始 param.shape=torch.Size([1024, 3072]), loaded_weight.shape=torch.Size([1024, 3072])
RowParallelLinearの重みを読み込み完了
重みを読み込み中 weight_name='model.layers.23.mlp.gate_proj.weight'
マッピングによる重みを読み込み weight_name='model.layers.23.mlp.gate_proj.weight' k='gate_proj'
MergedColumnParallelLinearの重みを読み込み開始 param.shape=torch.Size([6144, 1024]), loaded_weight.shape=torch.Size([3072, 1024]), loaded_shard_id=0
MergedColumnParallelLinearの重みを読み込み完了
重みを読み込み中 weight_name='model.layers.23.mlp.up_proj.weight'
マッピングによる重みを読み込み weight_name='model.layers.23.mlp.up_proj.weight' k='up_proj'
MergedColumnParallelLinearの重みを読み込み開始 param.shape=torch.Size([6144, 1024]), loaded_weight.shape=torch.Size([3072, 1024]), loaded_shard_id=1
MergedColumnParallelLinearの重みを読み込み完了
重みを読み込み中 weight_name='model.layers.23.post_attention_layernorm.weight'
通常の重み読み込み weight_name='model.layers.23.post_attention_layernorm.weight'
重みを読み込み param.shape=torch.Size([1024]) loaded_weight.shape=torch.Size([1024])
重みを読み込み中 weight_name='model.layers.23.self_attn.k_norm.weight'
通常の重み読み込み weight_name='model.layers.23.self_attn.k_norm.weight'
重みを読み込み param.shape=torch.Size([128]) loaded_weight.shape=torch.Size([128])
重みを読み込み中 weight_name='model.layers.23.self_attn.k_proj.weight'
マッピングによる重みを読み込み weight_name='model.layers.23.self_attn.k_proj.weight' k='k_proj'
QKVParallelLinearの重みを読み込み開始 param.shape=torch.Size([4096, 1024]), loaded_weight.shape=torch.Size([1024, 1024]), loaded_shard_id='k'
QKVParallelLinearの重みを読み込み完了
重みを読み込み中 weight_name='model.layers.23.self_attn.o_proj.weight'
通常の重み読み込み weight_name='model.layers.23.self_attn.o_proj.weight'
RowParallelLinearの重みを読み込み開始 param.shape=torch.Size([1024, 2048]), loaded_weight.shape=torch.Size([1024, 2048])
RowParallelLinearの重みを読み込み完了
重みを読み込み中 weight_name='model.layers.23.self_attn.q_norm.weight'
通常の重み読み込み weight_name='model.layers.23.self_attn.q_norm.weight'
重みを読み込み param.shape=torch.Size([128]) loaded_weight.shape=torch.Size([128])
重みを読み込み中 weight_name='model.layers.23.self_attn.q_proj.weight'
マッピングによる重みを読み込み weight_name='model.layers.23.self_attn.q_proj.weight' k='q_proj'
QKVParallelLinearの重みを読み込み開始 param.shape=torch.Size([4096, 1024]), loaded_weight.shape=torch.Size([2048, 1024]), loaded_shard_id='q'
QKVParallelLinearの重みを読み込み完了
重みを読み込み中 weight_name='model.layers.23.self_attn.v_proj.weight'
マッピングによる重みを読み込み weight_name='model.layers.23.self_attn.v_proj.weight' k='v_proj'
QKVParallelLinearの重みを読み込み開始 param.shape=torch.Size([4096, 1024]), loaded_weight.shape=torch.Size([1024, 1024]), loaded_shard_id='v'
QKVParallelLinearの重みを読み込み完了
重みを読み込み中 weight_name='model.layers.24.input_layernorm.weight'
通常の重み読み込み weight_name='model.layers.24.input_layernorm.weight'
重みを読み込み param.shape=torch.Size([1024]) loaded_weight.shape=torch.Size([1024])
重みを読み込み中 weight_name='model.layers.24.mlp.down_proj.weight'
通常の重み読み込み weight_name='model.layers.24.mlp.down_proj.weight'
RowParallelLinearの重みを読み込み開始 param.shape=torch.Size([1024, 3072]), loaded_weight.shape=torch.Size([1024, 3072])
RowParallelLinearの重みを読み込み完了
重みを読み込み中 weight_name='model.layers.24.mlp.gate_proj.weight'
マッピングによる重みを読み込み weight_name='model.layers.24.mlp.gate_proj.weight' k='gate_proj'
MergedColumnParallelLinearの重みを読み込み開始 param.shape=torch.Size([6144, 1024]), loaded_weight.shape=torch.Size([3072, 1024]), loaded_shard_id=0
MergedColumnParallelLinearの重みを読み込み完了
重みを読み込み中 weight_name='model.layers.24.mlp.up_proj.weight'
マッピングによる重みを読み込み weight_name='model.layers.24.mlp.up_proj.weight' k='up_proj'
MergedColumnParallelLinearの重みを読み込み開始 param.shape=torch.Size([6144, 1024]), loaded_weight.shape=torch.Size([3072, 1024]), loaded_shard_id=1
MergedColumnParallelLinearの重みを読み込み完了
重みを読み込み中 weight_name='model.layers.24.post_attention_layernorm.weight'
通常の重み読み込み weight_name='model.layers.24.post_attention_layernorm.weight'
重みを読み込み param.shape=torch.Size([1024]) loaded_weight.shape=torch.Size([1024])
重みを読み込み中 weight_name='model.layers.24.self_attn.k_norm.weight'
通常の重み読み込み weight_name='model.layers.24.self_attn.k_norm.weight'
重みを読み込み param.shape=torch.Size([128]) loaded_weight.shape=torch.Size([128])
重みを読み込み中 weight_name='model.layers.24.self_attn.k_proj.weight'
マッピングによる重みを読み込み weight_name='model.layers.24.self_attn.k_proj.weight' k='k_proj'
QKVParallelLinearの重みを読み込み開始 param.shape=torch.Size([4096, 1024]), loaded_weight.shape=torch.Size([1024, 1024]), loaded_shard_id='k'
QKVParallelLinearの重みを読み込み完了
重みを読み込み中 weight_name='model.layers.24.self_attn.o_proj.weight'
通常の重み読み込み weight_name='model.layers.24.self_attn.o_proj.weight'
RowParallelLinearの重みを読み込み開始 param.shape=torch.Size([1024, 2048]), loaded_weight.shape=torch.Size([1024, 2048])
RowParallelLinearの重みを読み込み完了
重みを読み込み中 weight_name='model.layers.24.self_attn.q_norm.weight'
通常の重み読み込み weight_name='model.layers.24.self_attn.q_norm.weight'
重みを読み込み param.shape=torch.Size([128]) loaded_weight.shape=torch.Size([128])
重みを読み込み中 weight_name='model.layers.24.self_attn.q_proj.weight'
マッピングによる重みを読み込み weight_name='model.layers.24.self_attn.q_proj.weight' k='q_proj'
QKVParallelLinearの重みを読み込み開始 param.shape=torch.Size([4096, 1024]), loaded_weight.shape=torch.Size([2048, 1024]), loaded_shard_id='q'
QKVParallelLinearの重みを読み込み完了
重みを読み込み中 weight_name='model.layers.24.self_attn.v_proj.weight'
マッピングによる重みを読み込み weight_name='model.layers.24.self_attn.v_proj.weight' k='v_proj'
QKVParallelLinearの重みを読み込み開始 param.shape=torch.Size([4096, 1024]), loaded_weight.shape=torch.Size([1024, 1024]), loaded_shard_id='v'
QKVParallelLinearの重みを読み込み完了
重みを読み込み中 weight_name='model.layers.25.input_layernorm.weight'
通常の重み読み込み weight_name='model.layers.25.input_layernorm.weight'
重みを読み込み param.shape=torch.Size([1024]) loaded_weight.shape=torch.Size([1024])
重みを読み込み中 weight_name='model.layers.25.mlp.down_proj.weight'
通常の重み読み込み weight_name='model.layers.25.mlp.down_proj.weight'
RowParallelLinearの重みを読み込み開始 param.shape=torch.Size([1024, 3072]), loaded_weight.shape=torch.Size([1024, 3072])
RowParallelLinearの重みを読み込み完了
重みを読み込み中 weight_name='model.layers.25.mlp.gate_proj.weight'
マッピングによる重みを読み込み weight_name='model.layers.25.mlp.gate_proj.weight' k='gate_proj'
MergedColumnParallelLinearの重みを読み込み開始 param.shape=torch.Size([6144, 1024]), loaded_weight.shape=torch.Size([3072, 1024]), loaded_shard_id=0
MergedColumnParallelLinearの重みを読み込み完了
重みを読み込み中 weight_name='model.layers.25.mlp.up_proj.weight'
マッピングによる重みを読み込み weight_name='model.layers.25.mlp.up_proj.weight' k='up_proj'
MergedColumnParallelLinearの重みを読み込み開始 param.shape=torch.Size([6144, 1024]), loaded_weight.shape=torch.Size([3072, 1024]), loaded_shard_id=1
MergedColumnParallelLinearの重みを読み込み完了
重みを読み込み中 weight_name='model.layers.25.post_attention_layernorm.weight'
通常の重み読み込み weight_name='model.layers.25.post_attention_layernorm.weight'
重みを読み込み param.shape=torch.Size([1024]) loaded_weight.shape=torch.Size([1024])
重みを読み込み中 weight_name='model.layers.25.self_attn.k_norm.weight'
通常の重み読み込み weight_name='model.layers.25.self_attn.k_norm.weight'
重みを読み込み param.shape=torch.Size([128]) loaded_weight.shape=torch.Size([128])
重みを読み込み中 weight_name='model.layers.25.self_attn.k_proj.weight'
マッピングによる重みを読み込み weight_name='model.layers.25.self_attn.k_proj.weight' k='k_proj'
QKVParallelLinearの重みを読み込み開始 param.shape=torch.Size([4096, 1024]), loaded_weight.shape=torch.Size([1024, 1024]), loaded_shard_id='k'
QKVParallelLinearの重みを読み込み完了
重みを読み込み中 weight_name='model.layers.25.self_attn.o_proj.weight'
通常の重み読み込み weight_name='model.layers.25.self_attn.o_proj.weight'
RowParallelLinearの重みを読み込み開始 param.shape=torch.Size([1024, 2048]), loaded_weight.shape=torch.Size([1024, 2048])
RowParallelLinearの重みを読み込み完了
重みを読み込み中 weight_name='model.layers.25.self_attn.q_norm.weight'
通常の重み読み込み weight_name='model.layers.25.self_attn.q_norm.weight'
重みを読み込み param.shape=torch.Size([128]) loaded_weight.shape=torch.Size([128])
重みを読み込み中 weight_name='model.layers.25.self_attn.q_proj.weight'
マッピングによる重みを読み込み weight_name='model.layers.25.self_attn.q_proj.weight' k='q_proj'
QKVParallelLinearの重みを読み込み開始 param.shape=torch.Size([4096, 1024]), loaded_weight.shape=torch.Size([2048, 1024]), loaded_shard_id='q'
QKVParallelLinearの重みを読み込み完了
重みを読み込み中 weight_name='model.layers.25.self_attn.v_proj.weight'
マッピングによる重みを読み込み weight_name='model.layers.25.self_attn.v_proj.weight' k='v_proj'
QKVParallelLinearの重みを読み込み開始 param.shape=torch.Size([4096, 1024]), loaded_weight.shape=torch.Size([1024, 1024]), loaded_shard_id='v'
QKVParallelLinearの重みを読み込み完了
重みを読み込み中 weight_name='model.layers.26.input_layernorm.weight'
通常の重み読み込み weight_name='model.layers.26.input_layernorm.weight'
重みを読み込み param.shape=torch.Size([1024]) loaded_weight.shape=torch.Size([1024])
重みを読み込み中 weight_name='model.layers.26.mlp.down_proj.weight'
通常の重み読み込み weight_name='model.layers.26.mlp.down_proj.weight'
RowParallelLinearの重みを読み込み開始 param.shape=torch.Size([1024, 3072]), loaded_weight.shape=torch.Size([1024, 3072])
RowParallelLinearの重みを読み込み完了
重みを読み込み中 weight_name='model.layers.26.mlp.gate_proj.weight'
マッピングによる重みを読み込み weight_name='model.layers.26.mlp.gate_proj.weight' k='gate_proj'
MergedColumnParallelLinearの重みを読み込み開始 param.shape=torch.Size([6144, 1024]), loaded_weight.shape=torch.Size([3072, 1024]), loaded_shard_id=0
MergedColumnParallelLinearの重みを読み込み完了
重みを読み込み中 weight_name='model.layers.26.mlp.up_proj.weight'
マッピングによる重みを読み込み weight_name='model.layers.26.mlp.up_proj.weight' k='up_proj'
MergedColumnParallelLinearの重みを読み込み開始 param.shape=torch.Size([6144, 1024]), loaded_weight.shape=torch.Size([3072, 1024]), loaded_shard_id=1
MergedColumnParallelLinearの重みを読み込み完了
重みを読み込み中 weight_name='model.layers.26.post_attention_layernorm.weight'
通常の重み読み込み weight_name='model.layers.26.post_attention_layernorm.weight'
重みを読み込み param.shape=torch.Size([1024]) loaded_weight.shape=torch.Size([1024])
重みを読み込み中 weight_name='model.layers.26.self_attn.k_norm.weight'
通常の重み読み込み weight_name='model.layers.26.self_attn.k_norm.weight'
重みを読み込み param.shape=torch.Size([128]) loaded_weight.shape=torch.Size([128])
重みを読み込み中 weight_name='model.layers.26.self_attn.k_proj.weight'
マッピングによる重みを読み込み weight_name='model.layers.26.self_attn.k_proj.weight' k='k_proj'
QKVParallelLinearの重みを読み込み開始 param.shape=torch.Size([4096, 1024]), loaded_weight.shape=torch.Size([1024, 1024]), loaded_shard_id='k'
QKVParallelLinearの重みを読み込み完了
重みを読み込み中 weight_name='model.layers.26.self_attn.o_proj.weight'
通常の重み読み込み weight_name='model.layers.26.self_attn.o_proj.weight'
RowParallelLinearの重みを読み込み開始 param.shape=torch.Size([1024, 2048]), loaded_weight.shape=torch.Size([1024, 2048])
RowParallelLinearの重みを読み込み完了
重みを読み込み中 weight_name='model.layers.26.self_attn.q_norm.weight'
通常の重み読み込み weight_name='model.layers.26.self_attn.q_norm.weight'
重みを読み込み param.shape=torch.Size([128]) loaded_weight.shape=torch.Size([128])
重みを読み込み中 weight_name='model.layers.26.self_attn.q_proj.weight'
マッピングによる重みを読み込み weight_name='model.layers.26.self_attn.q_proj.weight' k='q_proj'
QKVParallelLinearの重みを読み込み開始 param.shape=torch.Size([4096, 1024]), loaded_weight.shape=torch.Size([2048, 1024]), loaded_shard_id='q'
QKVParallelLinearの重みを読み込み完了
重みを読み込み中 weight_name='model.layers.26.self_attn.v_proj.weight'
マッピングによる重みを読み込み weight_name='model.layers.26.self_attn.v_proj.weight' k='v_proj'
QKVParallelLinearの重みを読み込み開始 param.shape=torch.Size([4096, 1024]), loaded_weight.shape=torch.Size([1024, 1024]), loaded_shard_id='v'
QKVParallelLinearの重みを読み込み完了
重みを読み込み中 weight_name='model.layers.27.input_layernorm.weight'
通常の重み読み込み weight_name='model.layers.27.input_layernorm.weight'
重みを読み込み param.shape=torch.Size([1024]) loaded_weight.shape=torch.Size([1024])
重みを読み込み中 weight_name='model.layers.27.mlp.down_proj.weight'
通常の重み読み込み weight_name='model.layers.27.mlp.down_proj.weight'
RowParallelLinearの重みを読み込み開始 param.shape=torch.Size([1024, 3072]), loaded_weight.shape=torch.Size([1024, 3072])
RowParallelLinearの重みを読み込み完了
重みを読み込み中 weight_name='model.layers.27.mlp.gate_proj.weight'
マッピングによる重みを読み込み weight_name='model.layers.27.mlp.gate_proj.weight' k='gate_proj'
MergedColumnParallelLinearの重みを読み込み開始 param.shape=torch.Size([6144, 1024]), loaded_weight.shape=torch.Size([3072, 1024]), loaded_shard_id=0
MergedColumnParallelLinearの重みを読み込み完了
重みを読み込み中 weight_name='model.layers.27.mlp.up_proj.weight'
マッピングによる重みを読み込み weight_name='model.layers.27.mlp.up_proj.weight' k='up_proj'
MergedColumnParallelLinearの重みを読み込み開始 param.shape=torch.Size([6144, 1024]), loaded_weight.shape=torch.Size([3072, 1024]), loaded_shard_id=1
MergedColumnParallelLinearの重みを読み込み完了
重みを読み込み中 weight_name='model.layers.27.post_attention_layernorm.weight'
通常の重み読み込み weight_name='model.layers.27.post_attention_layernorm.weight'
重みを読み込み param.shape=torch.Size([1024]) loaded_weight.shape=torch.Size([1024])
重みを読み込み中 weight_name='model.layers.27.self_attn.k_norm.weight'
通常の重み読み込み weight_name='model.layers.27.self_attn.k_norm.weight'
重みを読み込み param.shape=torch.Size([128]) loaded_weight.shape=torch.Size([128])
重みを読み込み中 weight_name='model.layers.27.self_attn.k_proj.weight'
マッピングによる重みを読み込み weight_name='model.layers.27.self_attn.k_proj.weight' k='k_proj'
QKVParallelLinearの重みを読み込み開始 param.shape=torch.Size([4096, 1024]), loaded_weight.shape=torch.Size([1024, 1024]), loaded_shard_id='k'
QKVParallelLinearの重みを読み込み完了
重みを読み込み中 weight_name='model.layers.27.self_attn.o_proj.weight'
通常の重み読み込み weight_name='model.layers.27.self_attn.o_proj.weight'
RowParallelLinearの重みを読み込み開始 param.shape=torch.Size([1024, 2048]), loaded_weight.shape=torch.Size([1024, 2048])
RowParallelLinearの重みを読み込み完了
重みを読み込み中 weight_name='model.layers.27.self_attn.q_norm.weight'
通常の重み読み込み weight_name='model.layers.27.self_attn.q_norm.weight'
重みを読み込み param.shape=torch.Size([128]) loaded_weight.shape=torch.Size([128])
重みを読み込み中 weight_name='model.layers.27.self_attn.q_proj.weight'
マッピングによる重みを読み込み weight_name='model.layers.27.self_attn.q_proj.weight' k='q_proj'
QKVParallelLinearの重みを読み込み開始 param.shape=torch.Size([4096, 1024]), loaded_weight.shape=torch.Size([2048, 1024]), loaded_shard_id='q'
QKVParallelLinearの重みを読み込み完了
重みを読み込み中 weight_name='model.layers.27.self_attn.v_proj.weight'
マッピングによる重みを読み込み weight_name='model.layers.27.self_attn.v_proj.weight' k='v_proj'
QKVParallelLinearの重みを読み込み開始 param.shape=torch.Size([4096, 1024]), loaded_weight.shape=torch.Size([1024, 1024]), loaded_shard_id='v'
QKVParallelLinearの重みを読み込み完了
重みを読み込み中 weight_name='model.layers.3.input_layernorm.weight'
通常の重み読み込み weight_name='model.layers.3.input_layernorm.weight'
重みを読み込み param.shape=torch.Size([1024]) loaded_weight.shape=torch.Size([1024])
重みを読み込み中 weight_name='model.layers.3.mlp.down_proj.weight'
通常の重み読み込み weight_name='model.layers.3.mlp.down_proj.weight'
RowParallelLinearの重みを読み込み開始 param.shape=torch.Size([1024, 3072]), loaded_weight.shape=torch.Size([1024, 3072])
RowParallelLinearの重みを読み込み完了
重みを読み込み中 weight_name='model.layers.3.mlp.gate_proj.weight'
マッピングによる重みを読み込み weight_name='model.layers.3.mlp.gate_proj.weight' k='gate_proj'
MergedColumnParallelLinearの重みを読み込み開始 param.shape=torch.Size([6144, 1024]), loaded_weight.shape=torch.Size([3072, 1024]), loaded_shard_id=0
MergedColumnParallelLinearの重みを読み込み完了
重みを読み込み中 weight_name='model.layers.3.mlp.up_proj.weight'
マッピングによる重みを読み込み weight_name='model.layers.3.mlp.up_proj.weight' k='up_proj'
MergedColumnParallelLinearの重みを読み込み開始 param.shape=torch.Size([6144, 1024]), loaded_weight.shape=torch.Size([3072, 1024]), loaded_shard_id=1
MergedColumnParallelLinearの重みを読み込み完了
重みを読み込み中 weight_name='model.layers.3.post_attention_layernorm.weight'
通常の重み読み込み weight_name='model.layers.3.post_attention_layernorm.weight'
重みを読み込み param.shape=torch.Size([1024]) loaded_weight.shape=torch.Size([1024])
重みを読み込み中 weight_name='model.layers.3.self_attn.k_norm.weight'
通常の重み読み込み weight_name='model.layers.3.self_attn.k_norm.weight'
重みを読み込み param.shape=torch.Size([128]) loaded_weight.shape=torch.Size([128])
重みを読み込み中 weight_name='model.layers.3.self_attn.k_proj.weight'
マッピングによる重みを読み込み weight_name='model.layers.3.self_attn.k_proj.weight' k='k_proj'
QKVParallelLinearの重みを読み込み開始 param.shape=torch.Size([4096, 1024]), loaded_weight.shape=torch.Size([1024, 1024]), loaded_shard_id='k'
QKVParallelLinearの重みを読み込み完了
重みを読み込み中 weight_name='model.layers.3.self_attn.o_proj.weight'
通常の重み読み込み weight_name='model.layers.3.self_attn.o_proj.weight'
RowParallelLinearの重みを読み込み開始 param.shape=torch.Size([1024, 2048]), loaded_weight.shape=torch.Size([1024, 2048])
RowParallelLinearの重みを読み込み完了
重みを読み込み中 weight_name='model.layers.3.self_attn.q_norm.weight'
通常の重み読み込み weight_name='model.layers.3.self_attn.q_norm.weight'
重みを読み込み param.shape=torch.Size([128]) loaded_weight.shape=torch.Size([128])
重みを読み込み中 weight_name='model.layers.3.self_attn.q_proj.weight'
マッピングによる重みを読み込み weight_name='model.layers.3.self_attn.q_proj.weight' k='q_proj'
QKVParallelLinearの重みを読み込み開始 param.shape=torch.Size([4096, 1024]), loaded_weight.shape=torch.Size([2048, 1024]), loaded_shard_id='q'
QKVParallelLinearの重みを読み込み完了
重みを読み込み中 weight_name='model.layers.3.self_attn.v_proj.weight'
マッピングによる重みを読み込み weight_name='model.layers.3.self_attn.v_proj.weight' k='v_proj'
QKVParallelLinearの重みを読み込み開始 param.shape=torch.Size([4096, 1024]), loaded_weight.shape=torch.Size([1024, 1024]), loaded_shard_id='v'
QKVParallelLinearの重みを読み込み完了
重みを読み込み中 weight_name='model.layers.4.input_layernorm.weight'
通常の重み読み込み weight_name='model.layers.4.input_layernorm.weight'
重みを読み込み param.shape=torch.Size([1024]) loaded_weight.shape=torch.Size([1024])
重みを読み込み中 weight_name='model.layers.4.mlp.down_proj.weight'
通常の重み読み込み weight_name='model.layers.4.mlp.down_proj.weight'
RowParallelLinearの重みを読み込み開始 param.shape=torch.Size([1024, 3072]), loaded_weight.shape=torch.Size([1024, 3072])
RowParallelLinearの重みを読み込み完了
重みを読み込み中 weight_name='model.layers.4.mlp.gate_proj.weight'
マッピングによる重みを読み込み weight_name='model.layers.4.mlp.gate_proj.weight' k='gate_proj'
MergedColumnParallelLinearの重みを読み込み開始 param.shape=torch.Size([6144, 1024]), loaded_weight.shape=torch.Size([3072, 1024]), loaded_shard_id=0
MergedColumnParallelLinearの重みを読み込み完了
重みを読み込み中 weight_name='model.layers.4.mlp.up_proj.weight'
マッピングによる重みを読み込み weight_name='model.layers.4.mlp.up_proj.weight' k='up_proj'
MergedColumnParallelLinearの重みを読み込み開始 param.shape=torch.Size([6144, 1024]), loaded_weight.shape=torch.Size([3072, 1024]), loaded_shard_id=1
MergedColumnParallelLinearの重みを読み込み完了
重みを読み込み中 weight_name='model.layers.4.post_attention_layernorm.weight'
通常の重み読み込み weight_name='model.layers.4.post_attention_layernorm.weight'
重みを読み込み param.shape=torch.Size([1024]) loaded_weight.shape=torch.Size([1024])
重みを読み込み中 weight_name='model.layers.4.self_attn.k_norm.weight'
通常の重み読み込み weight_name='model.layers.4.self_attn.k_norm.weight'
重みを読み込み param.shape=torch.Size([128]) loaded_weight.shape=torch.Size([128])
重みを読み込み中 weight_name='model.layers.4.self_attn.k_proj.weight'
マッピングによる重みを読み込み weight_name='model.layers.4.self_attn.k_proj.weight' k='k_proj'
QKVParallelLinearの重みを読み込み開始 param.shape=torch.Size([4096, 1024]), loaded_weight.shape=torch.Size([1024, 1024]), loaded_shard_id='k'
QKVParallelLinearの重みを読み込み完了
重みを読み込み中 weight_name='model.layers.4.self_attn.o_proj.weight'
通常の重み読み込み weight_name='model.layers.4.self_attn.o_proj.weight'
RowParallelLinearの重みを読み込み開始 param.shape=torch.Size([1024, 2048]), loaded_weight.shape=torch.Size([1024, 2048])
RowParallelLinearの重みを読み込み完了
重みを読み込み中 weight_name='model.layers.4.self_attn.q_norm.weight'
通常の重み読み込み weight_name='model.layers.4.self_attn.q_norm.weight'
重みを読み込み param.shape=torch.Size([128]) loaded_weight.shape=torch.Size([128])
重みを読み込み中 weight_name='model.layers.4.self_attn.q_proj.weight'
マッピングによる重みを読み込み weight_name='model.layers.4.self_attn.q_proj.weight' k='q_proj'
QKVParallelLinearの重みを読み込み開始 param.shape=torch.Size([4096, 1024]), loaded_weight.shape=torch.Size([2048, 1024]), loaded_shard_id='q'
QKVParallelLinearの重みを読み込み完了
重みを読み込み中 weight_name='model.layers.4.self_attn.v_proj.weight'
マッピングによる重みを読み込み weight_name='model.layers.4.self_attn.v_proj.weight' k='v_proj'
QKVParallelLinearの重みを読み込み開始 param.shape=torch.Size([4096, 1024]), loaded_weight.shape=torch.Size([1024, 1024]), loaded_shard_id='v'
QKVParallelLinearの重みを読み込み完了
重みを読み込み中 weight_name='model.layers.5.input_layernorm.weight'
通常の重み読み込み weight_name='model.layers.5.input_layernorm.weight'
重みを読み込み param.shape=torch.Size([1024]) loaded_weight.shape=torch.Size([1024])
重みを読み込み中 weight_name='model.layers.5.mlp.down_proj.weight'
通常の重み読み込み weight_name='model.layers.5.mlp.down_proj.weight'
RowParallelLinearの重みを読み込み開始 param.shape=torch.Size([1024, 3072]), loaded_weight.shape=torch.Size([1024, 3072])
RowParallelLinearの重みを読み込み完了
重みを読み込み中 weight_name='model.layers.5.mlp.gate_proj.weight'
マッピングによる重みを読み込み weight_name='model.layers.5.mlp.gate_proj.weight' k='gate_proj'
MergedColumnParallelLinearの重みを読み込み開始 param.shape=torch.Size([6144, 1024]), loaded_weight.shape=torch.Size([3072, 1024]), loaded_shard_id=0
MergedColumnParallelLinearの重みを読み込み完了
重みを読み込み中 weight_name='model.layers.5.mlp.up_proj.weight'
マッピングによる重みを読み込み weight_name='model.layers.5.mlp.up_proj.weight' k='up_proj'
MergedColumnParallelLinearの重みを読み込み開始 param.shape=torch.Size([6144, 1024]), loaded_weight.shape=torch.Size([3072, 1024]), loaded_shard_id=1
MergedColumnParallelLinearの重みを読み込み完了
重みを読み込み中 weight_name='model.layers.5.post_attention_layernorm.weight'
通常の重み読み込み weight_name='model.layers.5.post_attention_layernorm.weight'
重みを読み込み param.shape=torch.Size([1024]) loaded_weight.shape=torch.Size([1024])
重みを読み込み中 weight_name='model.layers.5.self_attn.k_norm.weight'
通常の重み読み込み weight_name='model.layers.5.self_attn.k_norm.weight'
重みを読み込み param.shape=torch.Size([128]) loaded_weight.shape=torch.Size([128])
重みを読み込み中 weight_name='model.layers.5.self_attn.k_proj.weight'
マッピングによる重みを読み込み weight_name='model.layers.5.self_attn.k_proj.weight' k='k_proj'
QKVParallelLinearの重みを読み込み開始 param.shape=torch.Size([4096, 1024]), loaded_weight.shape=torch.Size([1024, 1024]), loaded_shard_id='k'
QKVParallelLinearの重みを読み込み完了
重みを読み込み中 weight_name='model.layers.5.self_attn.o_proj.weight'
通常の重み読み込み weight_name='model.layers.5.self_attn.o_proj.weight'
RowParallelLinearの重みを読み込み開始 param.shape=torch.Size([1024, 2048]), loaded_weight.shape=torch.Size([1024, 2048])
RowParallelLinearの重みを読み込み完了
重みを読み込み中 weight_name='model.layers.5.self_attn.q_norm.weight'
通常の重み読み込み weight_name='model.layers.5.self_attn.q_norm.weight'
重みを読み込み param.shape=torch.Size([128]) loaded_weight.shape=torch.Size([128])
重みを読み込み中 weight_name='model.layers.5.self_attn.q_proj.weight'
マッピングによる重みを読み込み weight_name='model.layers.5.self_attn.q_proj.weight' k='q_proj'
QKVParallelLinearの重みを読み込み開始 param.shape=torch.Size([4096, 1024]), loaded_weight.shape=torch.Size([2048, 1024]), loaded_shard_id='q'
QKVParallelLinearの重みを読み込み完了
重みを読み込み中 weight_name='model.layers.5.self_attn.v_proj.weight'
マッピングによる重みを読み込み weight_name='model.layers.5.self_attn.v_proj.weight' k='v_proj'
QKVParallelLinearの重みを読み込み開始 param.shape=torch.Size([4096, 1024]), loaded_weight.shape=torch.Size([1024, 1024]), loaded_shard_id='v'
QKVParallelLinearの重みを読み込み完了
重みを読み込み中 weight_name='model.layers.6.input_layernorm.weight'
通常の重み読み込み weight_name='model.layers.6.input_layernorm.weight'
重みを読み込み param.shape=torch.Size([1024]) loaded_weight.shape=torch.Size([1024])
重みを読み込み中 weight_name='model.layers.6.mlp.down_proj.weight'
通常の重み読み込み weight_name='model.layers.6.mlp.down_proj.weight'
RowParallelLinearの重みを読み込み開始 param.shape=torch.Size([1024, 3072]), loaded_weight.shape=torch.Size([1024, 3072])
RowParallelLinearの重みを読み込み完了
重みを読み込み中 weight_name='model.layers.6.mlp.gate_proj.weight'
マッピングによる重みを読み込み weight_name='model.layers.6.mlp.gate_proj.weight' k='gate_proj'
MergedColumnParallelLinearの重みを読み込み開始 param.shape=torch.Size([6144, 1024]), loaded_weight.shape=torch.Size([3072, 1024]), loaded_shard_id=0
MergedColumnParallelLinearの重みを読み込み完了
重みを読み込み中 weight_name='model.layers.6.mlp.up_proj.weight'
マッピングによる重みを読み込み weight_name='model.layers.6.mlp.up_proj.weight' k='up_proj'
MergedColumnParallelLinearの重みを読み込み開始 param.shape=torch.Size([6144, 1024]), loaded_weight.shape=torch.Size([3072, 1024]), loaded_shard_id=1
MergedColumnParallelLinearの重みを読み込み完了
重みを読み込み中 weight_name='model.layers.6.post_attention_layernorm.weight'
通常の重み読み込み weight_name='model.layers.6.post_attention_layernorm.weight'
重みを読み込み param.shape=torch.Size([1024]) loaded_weight.shape=torch.Size([1024])
重みを読み込み中 weight_name='model.layers.6.self_attn.k_norm.weight'
通常の重み読み込み weight_name='model.layers.6.self_attn.k_norm.weight'
重みを読み込み param.shape=torch.Size([128]) loaded_weight.shape=torch.Size([128])
重みを読み込み中 weight_name='model.layers.6.self_attn.k_proj.weight'
マッピングによる重みを読み込み weight_name='model.layers.6.self_attn.k_proj.weight' k='k_proj'
QKVParallelLinearの重みを読み込み開始 param.shape=torch.Size([4096, 1024]), loaded_weight.shape=torch.Size([1024, 1024]), loaded_shard_id='k'
QKVParallelLinearの重みを読み込み完了
重みを読み込み中 weight_name='model.layers.6.self_attn.o_proj.weight'
通常の重み読み込み weight_name='model.layers.6.self_attn.o_proj.weight'
RowParallelLinearの重みを読み込み開始 param.shape=torch.Size([1024, 2048]), loaded_weight.shape=torch.Size([1024, 2048])
RowParallelLinearの重みを読み込み完了
重みを読み込み中 weight_name='model.layers.6.self_attn.q_norm.weight'
通常の重み読み込み weight_name='model.layers.6.self_attn.q_norm.weight'
重みを読み込み param.shape=torch.Size([128]) loaded_weight.shape=torch.Size([128])
重みを読み込み中 weight_name='model.layers.6.self_attn.q_proj.weight'
マッピングによる重みを読み込み weight_name='model.layers.6.self_attn.q_proj.weight' k='q_proj'
QKVParallelLinearの重みを読み込み開始 param.shape=torch.Size([4096, 1024]), loaded_weight.shape=torch.Size([2048, 1024]), loaded_shard_id='q'
QKVParallelLinearの重みを読み込み完了
重みを読み込み中 weight_name='model.layers.6.self_attn.v_proj.weight'
マッピングによる重みを読み込み weight_name='model.layers.6.self_attn.v_proj.weight' k='v_proj'
QKVParallelLinearの重みを読み込み開始 param.shape=torch.Size([4096, 1024]), loaded_weight.shape=torch.Size([1024, 1024]), loaded_shard_id='v'
QKVParallelLinearの重みを読み込み完了
重みを読み込み中 weight_name='model.layers.7.input_layernorm.weight'
通常の重み読み込み weight_name='model.layers.7.input_layernorm.weight'
重みを読み込み param.shape=torch.Size([1024]) loaded_weight.shape=torch.Size([1024])
重みを読み込み中 weight_name='model.layers.7.mlp.down_proj.weight'
通常の重み読み込み weight_name='model.layers.7.mlp.down_proj.weight'
RowParallelLinearの重みを読み込み開始 param.shape=torch.Size([1024, 3072]), loaded_weight.shape=torch.Size([1024, 3072])
RowParallelLinearの重みを読み込み完了
重みを読み込み中 weight_name='model.layers.7.mlp.gate_proj.weight'
マッピングによる重みを読み込み weight_name='model.layers.7.mlp.gate_proj.weight' k='gate_proj'
MergedColumnParallelLinearの重みを読み込み開始 param.shape=torch.Size([6144, 1024]), loaded_weight.shape=torch.Size([3072, 1024]), loaded_shard_id=0
MergedColumnParallelLinearの重みを読み込み完了
重みを読み込み中 weight_name='model.layers.7.mlp.up_proj.weight'
マッピングによる重みを読み込み weight_name='model.layers.7.mlp.up_proj.weight' k='up_proj'
MergedColumnParallelLinearの重みを読み込み開始 param.shape=torch.Size([6144, 1024]), loaded_weight.shape=torch.Size([3072, 1024]), loaded_shard_id=1
MergedColumnParallelLinearの重みを読み込み完了
重みを読み込み中 weight_name='model.layers.7.post_attention_layernorm.weight'
通常の重み読み込み weight_name='model.layers.7.post_attention_layernorm.weight'
重みを読み込み param.shape=torch.Size([1024]) loaded_weight.shape=torch.Size([1024])
重みを読み込み中 weight_name='model.layers.7.self_attn.k_norm.weight'
通常の重み読み込み weight_name='model.layers.7.self_attn.k_norm.weight'
重みを読み込み param.shape=torch.Size([128]) loaded_weight.shape=torch.Size([128])
重みを読み込み中 weight_name='model.layers.7.self_attn.k_proj.weight'
マッピングによる重みを読み込み weight_name='model.layers.7.self_attn.k_proj.weight' k='k_proj'
QKVParallelLinearの重みを読み込み開始 param.shape=torch.Size([4096, 1024]), loaded_weight.shape=torch.Size([1024, 1024]), loaded_shard_id='k'
QKVParallelLinearの重みを読み込み完了
重みを読み込み中 weight_name='model.layers.7.self_attn.o_proj.weight'
通常の重み読み込み weight_name='model.layers.7.self_attn.o_proj.weight'
RowParallelLinearの重みを読み込み開始 param.shape=torch.Size([1024, 2048]), loaded_weight.shape=torch.Size([1024, 2048])
RowParallelLinearの重みを読み込み完了
重みを読み込み中 weight_name='model.layers.7.self_attn.q_norm.weight'
通常の重み読み込み weight_name='model.layers.7.self_attn.q_norm.weight'
重みを読み込み param.shape=torch.Size([128]) loaded_weight.shape=torch.Size([128])
重みを読み込み中 weight_name='model.layers.7.self_attn.q_proj.weight'
マッピングによる重みを読み込み weight_name='model.layers.7.self_attn.q_proj.weight' k='q_proj'
QKVParallelLinearの重みを読み込み開始 param.shape=torch.Size([4096, 1024]), loaded_weight.shape=torch.Size([2048, 1024]), loaded_shard_id='q'
QKVParallelLinearの重みを読み込み完了
重みを読み込み中 weight_name='model.layers.7.self_attn.v_proj.weight'
マッピングによる重みを読み込み weight_name='model.layers.7.self_attn.v_proj.weight' k='v_proj'
QKVParallelLinearの重みを読み込み開始 param.shape=torch.Size([4096, 1024]), loaded_weight.shape=torch.Size([1024, 1024]), loaded_shard_id='v'
QKVParallelLinearの重みを読み込み完了
重みを読み込み中 weight_name='model.layers.8.input_layernorm.weight'
通常の重み読み込み weight_name='model.layers.8.input_layernorm.weight'
重みを読み込み param.shape=torch.Size([1024]) loaded_weight.shape=torch.Size([1024])
重みを読み込み中 weight_name='model.layers.8.mlp.down_proj.weight'
通常の重み読み込み weight_name='model.layers.8.mlp.down_proj.weight'
RowParallelLinearの重みを読み込み開始 param.shape=torch.Size([1024, 3072]), loaded_weight.shape=torch.Size([1024, 3072])
RowParallelLinearの重みを読み込み完了
重みを読み込み中 weight_name='model.layers.8.mlp.gate_proj.weight'
マッピングによる重みを読み込み weight_name='model.layers.8.mlp.gate_proj.weight' k='gate_proj'
MergedColumnParallelLinearの重みを読み込み開始 param.shape=torch.Size([6144, 1024]), loaded_weight.shape=torch.Size([3072, 1024]), loaded_shard_id=0
MergedColumnParallelLinearの重みを読み込み完了
重みを読み込み中 weight_name='model.layers.8.mlp.up_proj.weight'
マッピングによる重みを読み込み weight_name='model.layers.8.mlp.up_proj.weight' k='up_proj'
MergedColumnParallelLinearの重みを読み込み開始 param.shape=torch.Size([6144, 1024]), loaded_weight.shape=torch.Size([3072, 1024]), loaded_shard_id=1
MergedColumnParallelLinearの重みを読み込み完了
重みを読み込み中 weight_name='model.layers.8.post_attention_layernorm.weight'
通常の重み読み込み weight_name='model.layers.8.post_attention_layernorm.weight'
重みを読み込み param.shape=torch.Size([1024]) loaded_weight.shape=torch.Size([1024])
重みを読み込み中 weight_name='model.layers.8.self_attn.k_norm.weight'
通常の重み読み込み weight_name='model.layers.8.self_attn.k_norm.weight'
重みを読み込み param.shape=torch.Size([128]) loaded_weight.shape=torch.Size([128])
重みを読み込み中 weight_name='model.layers.8.self_attn.k_proj.weight'
マッピングによる重みを読み込み weight_name='model.layers.8.self_attn.k_proj.weight' k='k_proj'
QKVParallelLinearの重みを読み込み開始 param.shape=torch.Size([4096, 1024]), loaded_weight.shape=torch.Size([1024, 1024]), loaded_shard_id='k'
QKVParallelLinearの重みを読み込み完了
重みを読み込み中 weight_name='model.layers.8.self_attn.o_proj.weight'
通常の重み読み込み weight_name='model.layers.8.self_attn.o_proj.weight'
RowParallelLinearの重みを読み込み開始 param.shape=torch.Size([1024, 2048]), loaded_weight.shape=torch.Size([1024, 2048])
RowParallelLinearの重みを読み込み完了
重みを読み込み中 weight_name='model.layers.8.self_attn.q_norm.weight'
通常の重み読み込み weight_name='model.layers.8.self_attn.q_norm.weight'
重みを読み込み param.shape=torch.Size([128]) loaded_weight.shape=torch.Size([128])
重みを読み込み中 weight_name='model.layers.8.self_attn.q_proj.weight'
マッピングによる重みを読み込み weight_name='model.layers.8.self_attn.q_proj.weight' k='q_proj'
QKVParallelLinearの重みを読み込み開始 param.shape=torch.Size([4096, 1024]), loaded_weight.shape=torch.Size([2048, 1024]), loaded_shard_id='q'
QKVParallelLinearの重みを読み込み完了
重みを読み込み中 weight_name='model.layers.8.self_attn.v_proj.weight'
マッピングによる重みを読み込み weight_name='model.layers.8.self_attn.v_proj.weight' k='v_proj'
QKVParallelLinearの重みを読み込み開始 param.shape=torch.Size([4096, 1024]), loaded_weight.shape=torch.Size([1024, 1024]), loaded_shard_id='v'
QKVParallelLinearの重みを読み込み完了
重みを読み込み中 weight_name='model.layers.9.input_layernorm.weight'
通常の重み読み込み weight_name='model.layers.9.input_layernorm.weight'
重みを読み込み param.shape=torch.Size([1024]) loaded_weight.shape=torch.Size([1024])
重みを読み込み中 weight_name='model.layers.9.mlp.down_proj.weight'
通常の重み読み込み weight_name='model.layers.9.mlp.down_proj.weight'
RowParallelLinearの重みを読み込み開始 param.shape=torch.Size([1024, 3072]), loaded_weight.shape=torch.Size([1024, 3072])
RowParallelLinearの重みを読み込み完了
重みを読み込み中 weight_name='model.layers.9.mlp.gate_proj.weight'
マッピングによる重みを読み込み weight_name='model.layers.9.mlp.gate_proj.weight' k='gate_proj'
MergedColumnParallelLinearの重みを読み込み開始 param.shape=torch.Size([6144, 1024]), loaded_weight.shape=torch.Size([3072, 1024]), loaded_shard_id=0
MergedColumnParallelLinearの重みを読み込み完了
重みを読み込み中 weight_name='model.layers.9.mlp.up_proj.weight'
マッピングによる重みを読み込み weight_name='model.layers.9.mlp.up_proj.weight' k='up_proj'
MergedColumnParallelLinearの重みを読み込み開始 param.shape=torch.Size([6144, 1024]), loaded_weight.shape=torch.Size([3072, 1024]), loaded_shard_id=1
MergedColumnParallelLinearの重みを読み込み完了
重みを読み込み中 weight_name='model.layers.9.post_attention_layernorm.weight'
通常の重み読み込み weight_name='model.layers.9.post_attention_layernorm.weight'
重みを読み込み param.shape=torch.Size([1024]) loaded_weight.shape=torch.Size([1024])
重みを読み込み中 weight_name='model.layers.9.self_attn.k_norm.weight'
通常の重み読み込み weight_name='model.layers.9.self_attn.k_norm.weight'
重みを読み込み param.shape=torch.Size([128]) loaded_weight.shape=torch.Size([128])
重みを読み込み中 weight_name='model.layers.9.self_attn.k_proj.weight'
マッピングによる重みを読み込み weight_name='model.layers.9.self_attn.k_proj.weight' k='k_proj'
QKVParallelLinearの重みを読み込み開始 param.shape=torch.Size([4096, 1024]), loaded_weight.shape=torch.Size([1024, 1024]), loaded_shard_id='k'
QKVParallelLinearの重みを読み込み完了
重みを読み込み中 weight_name='model.layers.9.self_attn.o_proj.weight'
通常の重み読み込み weight_name='model.layers.9.self_attn.o_proj.weight'
RowParallelLinearの重みを読み込み開始 param.shape=torch.Size([1024, 2048]), loaded_weight.shape=torch.Size([1024, 2048])
RowParallelLinearの重みを読み込み完了
重みを読み込み中 weight_name='model.layers.9.self_attn.q_norm.weight'
通常の重み読み込み weight_name='model.layers.9.self_attn.q_norm.weight'
重みを読み込み param.shape=torch.Size([128]) loaded_weight.shape=torch.Size([128])
重みを読み込み中 weight_name='model.layers.9.self_attn.q_proj.weight'
マッピングによる重みを読み込み weight_name='model.layers.9.self_attn.q_proj.weight' k='q_proj'
QKVParallelLinearの重みを読み込み開始 param.shape=torch.Size([4096, 1024]), loaded_weight.shape=torch.Size([2048, 1024]), loaded_shard_id='q'
QKVParallelLinearの重みを読み込み完了
重みを読み込み中 weight_name='model.layers.9.self_attn.v_proj.weight'
マッピングによる重みを読み込み weight_name='model.layers.9.self_attn.v_proj.weight' k='v_proj'
QKVParallelLinearの重みを読み込み開始 param.shape=torch.Size([4096, 1024]), loaded_weight.shape=torch.Size([1024, 1024]), loaded_shard_id='v'
QKVParallelLinearの重みを読み込み完了
重みを読み込み中 weight_name='model.norm.weight'
通常の重み読み込み weight_name='model.norm.weight'
重みを読み込み param.shape=torch.Size([1024]) loaded_weight.shape=torch.Size([1024])
モデルの重みを読み込み完了
Samplerを初期化
Samplerの初期化完了
Warming up model on rank 0
シーケンスを初期化 len(token_ids)=4096, sampling_params=SamplingParams(temperature=1.0, max_tokens=64, ignore_eos=False)
シーケンスを初期化完了 self.seq_id=0
シーケンスを初期化 len(token_ids)=4096, sampling_params=SamplingParams(temperature=1.0, max_tokens=64, ignore_eos=False)
シーケンスを初期化完了 self.seq_id=1
シーケンスを初期化 len(token_ids)=4096, sampling_params=SamplingParams(temperature=1.0, max_tokens=64, ignore_eos=False)
シーケンスを初期化完了 self.seq_id=2
シーケンスを初期化 len(token_ids)=4096, sampling_params=SamplingParams(temperature=1.0, max_tokens=64, ignore_eos=False)
シーケンスを初期化完了 self.seq_id=3
推論の1サイクルを実行開始 len(seqs)=4 is_prefill=True
コンテキストを設定 is_prefill=True cu_seqlens_q=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32) cu_seqlens_k=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32) max_seqlen_q=4096 max_seqlen_k=4096 slot_mapping=tensor([], device='cuda:0', dtype=torch.int32) context_lens=None block_tables=None
サンプリングパラメータを準備開始 len(seqs)=4
サンプリングパラメータを準備完了 temperatures=tensor([1., 1., 1., 1.], device='cuda:0', dtype=torch.float32)
モデルの順伝搬を実行開始 input_ids.shape=torch.Size([16384]) positions.shape=torch.Size([16384]) is_prefill=True
Qwen3ForCausalLMの順伝播開始 input_ids.shape=torch.Size([16384]) positions.shape=torch.Size([16384])
Qwen3Modelの順伝播開始 input_ids.shape=torch.Size([16384]) positions.shape=torch.Size([16384])
埋め込み層の順伝播開始 x.shape=torch.Size([16384])
埋め込み層の順伝播完了 y.shape=torch.Size([16384, 1024])
Qwen3DecoderLayerの順伝播 positions.shape=torch.Size([16384]) hidden_states.shape=torch.Size([16384, 1024]) residual is None=True
RMSNormの順伝播開始 x.shape=torch.Size([16384, 1024]), residual.shape if residual is not None else None=None
正規化開始 x.shape=torch.Size([16384, 1024])
正規化完了 x.shape=torch.Size([16384, 1024])
Qwen3Attentionの順伝播 positions.shape=torch.Size([16384]) hidden_states.shape=torch.Size([16384, 1024])
出力次元を並列化する線形層の順伝播開始 x.shape=torch.Size([16384, 1024])
出力次元を並列化する線形層の順伝播完了 result.shape=torch.Size([16384, 4096])
RMSNormの順伝播開始 x.shape=torch.Size([16384, 16, 128]), residual.shape if residual is not None else None=None
正規化開始 x.shape=torch.Size([16384, 16, 128])
正規化完了 x.shape=torch.Size([16384, 16, 128])
RMSNormの順伝播開始 x.shape=torch.Size([16384, 8, 128]), residual.shape if residual is not None else None=None
正規化開始 x.shape=torch.Size([16384, 8, 128])
正規化完了 x.shape=torch.Size([16384, 8, 128])
RoPEの順伝播開始 positions.shape=torch.Size([16384]), query.shape=torch.Size([16384, 16, 128]), key.shape=torch.Size([16384, 8, 128])
RoPEを適用開始 x.shape=torch.Size([16384, 16, 128]), cos.shape=torch.Size([16384, 1, 64]), sin.shape=torch.Size([16384, 1, 64])
チャンク化 x1.shape=torch.Size([16384, 16, 64]), x2.shape=torch.Size([16384, 16, 64])
回転行列適用 y1.shape=torch.Size([16384, 16, 64]), y2.shape=torch.Size([16384, 16, 64])
RoPEを適用完了 result.shape=torch.Size([16384, 16, 128])
RoPEを適用開始 x.shape=torch.Size([16384, 8, 128]), cos.shape=torch.Size([16384, 1, 64]), sin.shape=torch.Size([16384, 1, 64])
チャンク化 x1.shape=torch.Size([16384, 8, 64]), x2.shape=torch.Size([16384, 8, 64])
回転行列適用 y1.shape=torch.Size([16384, 8, 64]), y2.shape=torch.Size([16384, 8, 64])
RoPEを適用完了 result.shape=torch.Size([16384, 8, 128])
RoPEの順伝播完了 query.shape=torch.Size([16384, 16, 128]), key.shape=torch.Size([16384, 8, 128])
Attention層の順伝播開始 q.shape=torch.Size([16384, 16, 128]), k.shape=torch.Size([16384, 8, 128]), v.shape=torch.Size([16384, 8, 128])
現在のコンテキスト取得 _CONTEXT=Context(is_prefill=True, cu_seqlens_q=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32), cu_seqlens_k=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32), max_seqlen_q=4096, max_seqlen_k=4096, slot_mapping=tensor([], device='cuda:0', dtype=torch.int32), context_lens=None, block_tables=None)
コンテキスト情報取得 context.is_prefill=True, context.slot_mapping=tensor([], device='cuda:0', dtype=torch.int32), context.max_seqlen_q=4096, context.cu_seqlens_q=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32), context.max_seqlen_k=4096, context.cu_seqlens_k=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32), context.context_lens=None, context.block_tables=None
PrefillモードでFlashAttentionを実行 context.max_seqlen_q=4096, context.cu_seqlens_q=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32), context.max_seqlen_k=4096, context.cu_seqlens_k=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32)
Attention層の順伝播完了 o.shape=torch.Size([16384, 16, 128])
RowParallelLinearの順伝播開始 入力形状: torch.Size([16384, 2048])
RowParallelLinearの順伝播完了 出力形状: torch.Size([16384, 1024])
Qwen3Attentionの順伝播完了 output.shape=torch.Size([16384, 1024])
RMSNormの順伝播開始 x.shape=torch.Size([16384, 1024]), residual.shape if residual is not None else None=torch.Size([16384, 1024])
残差接続を適用
残差接続を適用完了
Qwen3MLPの順伝播 x.shape=torch.Size([16384, 1024])
出力次元を並列化する線形層の順伝播開始 x.shape=torch.Size([16384, 1024])
出力次元を並列化する線形層の順伝播完了 result.shape=torch.Size([16384, 6144])
SwiGLUを順伝播開始 x.shape=torch.Size([16384, 6144])
チャンク化 x.shape=torch.Size([16384, 3072]), y.shape=torch.Size([16384, 3072])
SwiGLUを順伝播完了 result.shape=torch.Size([16384, 3072])
RowParallelLinearの順伝播開始 入力形状: torch.Size([16384, 3072])
RowParallelLinearの順伝播完了 出力形状: torch.Size([16384, 1024])
Qwen3MLPの順伝播完了 x.shape=torch.Size([16384, 1024])
Qwen3DecoderLayerの順伝播完了 hidden_states.shape=torch.Size([16384, 1024])
Qwen3DecoderLayerの順伝播 positions.shape=torch.Size([16384]) hidden_states.shape=torch.Size([16384, 1024]) residual is None=False
RMSNormの順伝播開始 x.shape=torch.Size([16384, 1024]), residual.shape if residual is not None else None=torch.Size([16384, 1024])
残差接続を適用
残差接続を適用完了
Qwen3Attentionの順伝播 positions.shape=torch.Size([16384]) hidden_states.shape=torch.Size([16384, 1024])
出力次元を並列化する線形層の順伝播開始 x.shape=torch.Size([16384, 1024])
出力次元を並列化する線形層の順伝播完了 result.shape=torch.Size([16384, 4096])
RMSNormの順伝播開始 x.shape=torch.Size([16384, 16, 128]), residual.shape if residual is not None else None=None
正規化開始 x.shape=torch.Size([16384, 16, 128])
正規化完了 x.shape=torch.Size([16384, 16, 128])
RMSNormの順伝播開始 x.shape=torch.Size([16384, 8, 128]), residual.shape if residual is not None else None=None
正規化開始 x.shape=torch.Size([16384, 8, 128])
正規化完了 x.shape=torch.Size([16384, 8, 128])
RoPEの順伝播開始 positions.shape=torch.Size([16384]), query.shape=torch.Size([16384, 16, 128]), key.shape=torch.Size([16384, 8, 128])
RoPEを適用開始 x.shape=torch.Size([16384, 16, 128]), cos.shape=torch.Size([16384, 1, 64]), sin.shape=torch.Size([16384, 1, 64])
チャンク化 x1.shape=torch.Size([16384, 16, 64]), x2.shape=torch.Size([16384, 16, 64])
回転行列適用 y1.shape=torch.Size([16384, 16, 64]), y2.shape=torch.Size([16384, 16, 64])
RoPEを適用完了 result.shape=torch.Size([16384, 16, 128])
RoPEを適用開始 x.shape=torch.Size([16384, 8, 128]), cos.shape=torch.Size([16384, 1, 64]), sin.shape=torch.Size([16384, 1, 64])
チャンク化 x1.shape=torch.Size([16384, 8, 64]), x2.shape=torch.Size([16384, 8, 64])
回転行列適用 y1.shape=torch.Size([16384, 8, 64]), y2.shape=torch.Size([16384, 8, 64])
RoPEを適用完了 result.shape=torch.Size([16384, 8, 128])
RoPEの順伝播完了 query.shape=torch.Size([16384, 16, 128]), key.shape=torch.Size([16384, 8, 128])
Attention層の順伝播開始 q.shape=torch.Size([16384, 16, 128]), k.shape=torch.Size([16384, 8, 128]), v.shape=torch.Size([16384, 8, 128])
現在のコンテキスト取得 _CONTEXT=Context(is_prefill=True, cu_seqlens_q=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32), cu_seqlens_k=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32), max_seqlen_q=4096, max_seqlen_k=4096, slot_mapping=tensor([], device='cuda:0', dtype=torch.int32), context_lens=None, block_tables=None)
コンテキスト情報取得 context.is_prefill=True, context.slot_mapping=tensor([], device='cuda:0', dtype=torch.int32), context.max_seqlen_q=4096, context.cu_seqlens_q=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32), context.max_seqlen_k=4096, context.cu_seqlens_k=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32), context.context_lens=None, context.block_tables=None
PrefillモードでFlashAttentionを実行 context.max_seqlen_q=4096, context.cu_seqlens_q=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32), context.max_seqlen_k=4096, context.cu_seqlens_k=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32)
Attention層の順伝播完了 o.shape=torch.Size([16384, 16, 128])
RowParallelLinearの順伝播開始 入力形状: torch.Size([16384, 2048])
RowParallelLinearの順伝播完了 出力形状: torch.Size([16384, 1024])
Qwen3Attentionの順伝播完了 output.shape=torch.Size([16384, 1024])
RMSNormの順伝播開始 x.shape=torch.Size([16384, 1024]), residual.shape if residual is not None else None=torch.Size([16384, 1024])
残差接続を適用
残差接続を適用完了
Qwen3MLPの順伝播 x.shape=torch.Size([16384, 1024])
出力次元を並列化する線形層の順伝播開始 x.shape=torch.Size([16384, 1024])
出力次元を並列化する線形層の順伝播完了 result.shape=torch.Size([16384, 6144])
SwiGLUを順伝播開始 x.shape=torch.Size([16384, 6144])
チャンク化 x.shape=torch.Size([16384, 3072]), y.shape=torch.Size([16384, 3072])
SwiGLUを順伝播完了 result.shape=torch.Size([16384, 3072])
RowParallelLinearの順伝播開始 入力形状: torch.Size([16384, 3072])
RowParallelLinearの順伝播完了 出力形状: torch.Size([16384, 1024])
Qwen3MLPの順伝播完了 x.shape=torch.Size([16384, 1024])
Qwen3DecoderLayerの順伝播完了 hidden_states.shape=torch.Size([16384, 1024])
Qwen3DecoderLayerの順伝播 positions.shape=torch.Size([16384]) hidden_states.shape=torch.Size([16384, 1024]) residual is None=False
RMSNormの順伝播開始 x.shape=torch.Size([16384, 1024]), residual.shape if residual is not None else None=torch.Size([16384, 1024])
残差接続を適用
残差接続を適用完了
Qwen3Attentionの順伝播 positions.shape=torch.Size([16384]) hidden_states.shape=torch.Size([16384, 1024])
出力次元を並列化する線形層の順伝播開始 x.shape=torch.Size([16384, 1024])
出力次元を並列化する線形層の順伝播完了 result.shape=torch.Size([16384, 4096])
RMSNormの順伝播開始 x.shape=torch.Size([16384, 16, 128]), residual.shape if residual is not None else None=None
正規化開始 x.shape=torch.Size([16384, 16, 128])
正規化完了 x.shape=torch.Size([16384, 16, 128])
RMSNormの順伝播開始 x.shape=torch.Size([16384, 8, 128]), residual.shape if residual is not None else None=None
正規化開始 x.shape=torch.Size([16384, 8, 128])
正規化完了 x.shape=torch.Size([16384, 8, 128])
RoPEの順伝播開始 positions.shape=torch.Size([16384]), query.shape=torch.Size([16384, 16, 128]), key.shape=torch.Size([16384, 8, 128])
RoPEを適用開始 x.shape=torch.Size([16384, 16, 128]), cos.shape=torch.Size([16384, 1, 64]), sin.shape=torch.Size([16384, 1, 64])
チャンク化 x1.shape=torch.Size([16384, 16, 64]), x2.shape=torch.Size([16384, 16, 64])
回転行列適用 y1.shape=torch.Size([16384, 16, 64]), y2.shape=torch.Size([16384, 16, 64])
RoPEを適用完了 result.shape=torch.Size([16384, 16, 128])
RoPEを適用開始 x.shape=torch.Size([16384, 8, 128]), cos.shape=torch.Size([16384, 1, 64]), sin.shape=torch.Size([16384, 1, 64])
チャンク化 x1.shape=torch.Size([16384, 8, 64]), x2.shape=torch.Size([16384, 8, 64])
回転行列適用 y1.shape=torch.Size([16384, 8, 64]), y2.shape=torch.Size([16384, 8, 64])
RoPEを適用完了 result.shape=torch.Size([16384, 8, 128])
RoPEの順伝播完了 query.shape=torch.Size([16384, 16, 128]), key.shape=torch.Size([16384, 8, 128])
Attention層の順伝播開始 q.shape=torch.Size([16384, 16, 128]), k.shape=torch.Size([16384, 8, 128]), v.shape=torch.Size([16384, 8, 128])
現在のコンテキスト取得 _CONTEXT=Context(is_prefill=True, cu_seqlens_q=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32), cu_seqlens_k=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32), max_seqlen_q=4096, max_seqlen_k=4096, slot_mapping=tensor([], device='cuda:0', dtype=torch.int32), context_lens=None, block_tables=None)
コンテキスト情報取得 context.is_prefill=True, context.slot_mapping=tensor([], device='cuda:0', dtype=torch.int32), context.max_seqlen_q=4096, context.cu_seqlens_q=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32), context.max_seqlen_k=4096, context.cu_seqlens_k=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32), context.context_lens=None, context.block_tables=None
PrefillモードでFlashAttentionを実行 context.max_seqlen_q=4096, context.cu_seqlens_q=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32), context.max_seqlen_k=4096, context.cu_seqlens_k=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32)
Attention層の順伝播完了 o.shape=torch.Size([16384, 16, 128])
RowParallelLinearの順伝播開始 入力形状: torch.Size([16384, 2048])
RowParallelLinearの順伝播完了 出力形状: torch.Size([16384, 1024])
Qwen3Attentionの順伝播完了 output.shape=torch.Size([16384, 1024])
RMSNormの順伝播開始 x.shape=torch.Size([16384, 1024]), residual.shape if residual is not None else None=torch.Size([16384, 1024])
残差接続を適用
残差接続を適用完了
Qwen3MLPの順伝播 x.shape=torch.Size([16384, 1024])
出力次元を並列化する線形層の順伝播開始 x.shape=torch.Size([16384, 1024])
出力次元を並列化する線形層の順伝播完了 result.shape=torch.Size([16384, 6144])
SwiGLUを順伝播開始 x.shape=torch.Size([16384, 6144])
チャンク化 x.shape=torch.Size([16384, 3072]), y.shape=torch.Size([16384, 3072])
SwiGLUを順伝播完了 result.shape=torch.Size([16384, 3072])
RowParallelLinearの順伝播開始 入力形状: torch.Size([16384, 3072])
RowParallelLinearの順伝播完了 出力形状: torch.Size([16384, 1024])
Qwen3MLPの順伝播完了 x.shape=torch.Size([16384, 1024])
Qwen3DecoderLayerの順伝播完了 hidden_states.shape=torch.Size([16384, 1024])
Qwen3DecoderLayerの順伝播 positions.shape=torch.Size([16384]) hidden_states.shape=torch.Size([16384, 1024]) residual is None=False
RMSNormの順伝播開始 x.shape=torch.Size([16384, 1024]), residual.shape if residual is not None else None=torch.Size([16384, 1024])
残差接続を適用
残差接続を適用完了
Qwen3Attentionの順伝播 positions.shape=torch.Size([16384]) hidden_states.shape=torch.Size([16384, 1024])
出力次元を並列化する線形層の順伝播開始 x.shape=torch.Size([16384, 1024])
出力次元を並列化する線形層の順伝播完了 result.shape=torch.Size([16384, 4096])
RMSNormの順伝播開始 x.shape=torch.Size([16384, 16, 128]), residual.shape if residual is not None else None=None
正規化開始 x.shape=torch.Size([16384, 16, 128])
正規化完了 x.shape=torch.Size([16384, 16, 128])
RMSNormの順伝播開始 x.shape=torch.Size([16384, 8, 128]), residual.shape if residual is not None else None=None
正規化開始 x.shape=torch.Size([16384, 8, 128])
正規化完了 x.shape=torch.Size([16384, 8, 128])
RoPEの順伝播開始 positions.shape=torch.Size([16384]), query.shape=torch.Size([16384, 16, 128]), key.shape=torch.Size([16384, 8, 128])
RoPEを適用開始 x.shape=torch.Size([16384, 16, 128]), cos.shape=torch.Size([16384, 1, 64]), sin.shape=torch.Size([16384, 1, 64])
チャンク化 x1.shape=torch.Size([16384, 16, 64]), x2.shape=torch.Size([16384, 16, 64])
回転行列適用 y1.shape=torch.Size([16384, 16, 64]), y2.shape=torch.Size([16384, 16, 64])
RoPEを適用完了 result.shape=torch.Size([16384, 16, 128])
RoPEを適用開始 x.shape=torch.Size([16384, 8, 128]), cos.shape=torch.Size([16384, 1, 64]), sin.shape=torch.Size([16384, 1, 64])
チャンク化 x1.shape=torch.Size([16384, 8, 64]), x2.shape=torch.Size([16384, 8, 64])
回転行列適用 y1.shape=torch.Size([16384, 8, 64]), y2.shape=torch.Size([16384, 8, 64])
RoPEを適用完了 result.shape=torch.Size([16384, 8, 128])
RoPEの順伝播完了 query.shape=torch.Size([16384, 16, 128]), key.shape=torch.Size([16384, 8, 128])
Attention層の順伝播開始 q.shape=torch.Size([16384, 16, 128]), k.shape=torch.Size([16384, 8, 128]), v.shape=torch.Size([16384, 8, 128])
現在のコンテキスト取得 _CONTEXT=Context(is_prefill=True, cu_seqlens_q=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32), cu_seqlens_k=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32), max_seqlen_q=4096, max_seqlen_k=4096, slot_mapping=tensor([], device='cuda:0', dtype=torch.int32), context_lens=None, block_tables=None)
コンテキスト情報取得 context.is_prefill=True, context.slot_mapping=tensor([], device='cuda:0', dtype=torch.int32), context.max_seqlen_q=4096, context.cu_seqlens_q=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32), context.max_seqlen_k=4096, context.cu_seqlens_k=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32), context.context_lens=None, context.block_tables=None
PrefillモードでFlashAttentionを実行 context.max_seqlen_q=4096, context.cu_seqlens_q=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32), context.max_seqlen_k=4096, context.cu_seqlens_k=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32)
Attention層の順伝播完了 o.shape=torch.Size([16384, 16, 128])
RowParallelLinearの順伝播開始 入力形状: torch.Size([16384, 2048])
RowParallelLinearの順伝播完了 出力形状: torch.Size([16384, 1024])
Qwen3Attentionの順伝播完了 output.shape=torch.Size([16384, 1024])
RMSNormの順伝播開始 x.shape=torch.Size([16384, 1024]), residual.shape if residual is not None else None=torch.Size([16384, 1024])
残差接続を適用
残差接続を適用完了
Qwen3MLPの順伝播 x.shape=torch.Size([16384, 1024])
出力次元を並列化する線形層の順伝播開始 x.shape=torch.Size([16384, 1024])
出力次元を並列化する線形層の順伝播完了 result.shape=torch.Size([16384, 6144])
SwiGLUを順伝播開始 x.shape=torch.Size([16384, 6144])
チャンク化 x.shape=torch.Size([16384, 3072]), y.shape=torch.Size([16384, 3072])
SwiGLUを順伝播完了 result.shape=torch.Size([16384, 3072])
RowParallelLinearの順伝播開始 入力形状: torch.Size([16384, 3072])
RowParallelLinearの順伝播完了 出力形状: torch.Size([16384, 1024])
Qwen3MLPの順伝播完了 x.shape=torch.Size([16384, 1024])
Qwen3DecoderLayerの順伝播完了 hidden_states.shape=torch.Size([16384, 1024])
Qwen3DecoderLayerの順伝播 positions.shape=torch.Size([16384]) hidden_states.shape=torch.Size([16384, 1024]) residual is None=False
RMSNormの順伝播開始 x.shape=torch.Size([16384, 1024]), residual.shape if residual is not None else None=torch.Size([16384, 1024])
残差接続を適用
残差接続を適用完了
Qwen3Attentionの順伝播 positions.shape=torch.Size([16384]) hidden_states.shape=torch.Size([16384, 1024])
出力次元を並列化する線形層の順伝播開始 x.shape=torch.Size([16384, 1024])
出力次元を並列化する線形層の順伝播完了 result.shape=torch.Size([16384, 4096])
RMSNormの順伝播開始 x.shape=torch.Size([16384, 16, 128]), residual.shape if residual is not None else None=None
正規化開始 x.shape=torch.Size([16384, 16, 128])
正規化完了 x.shape=torch.Size([16384, 16, 128])
RMSNormの順伝播開始 x.shape=torch.Size([16384, 8, 128]), residual.shape if residual is not None else None=None
正規化開始 x.shape=torch.Size([16384, 8, 128])
正規化完了 x.shape=torch.Size([16384, 8, 128])
RoPEの順伝播開始 positions.shape=torch.Size([16384]), query.shape=torch.Size([16384, 16, 128]), key.shape=torch.Size([16384, 8, 128])
RoPEを適用開始 x.shape=torch.Size([16384, 16, 128]), cos.shape=torch.Size([16384, 1, 64]), sin.shape=torch.Size([16384, 1, 64])
チャンク化 x1.shape=torch.Size([16384, 16, 64]), x2.shape=torch.Size([16384, 16, 64])
回転行列適用 y1.shape=torch.Size([16384, 16, 64]), y2.shape=torch.Size([16384, 16, 64])
RoPEを適用完了 result.shape=torch.Size([16384, 16, 128])
RoPEを適用開始 x.shape=torch.Size([16384, 8, 128]), cos.shape=torch.Size([16384, 1, 64]), sin.shape=torch.Size([16384, 1, 64])
チャンク化 x1.shape=torch.Size([16384, 8, 64]), x2.shape=torch.Size([16384, 8, 64])
回転行列適用 y1.shape=torch.Size([16384, 8, 64]), y2.shape=torch.Size([16384, 8, 64])
RoPEを適用完了 result.shape=torch.Size([16384, 8, 128])
RoPEの順伝播完了 query.shape=torch.Size([16384, 16, 128]), key.shape=torch.Size([16384, 8, 128])
Attention層の順伝播開始 q.shape=torch.Size([16384, 16, 128]), k.shape=torch.Size([16384, 8, 128]), v.shape=torch.Size([16384, 8, 128])
現在のコンテキスト取得 _CONTEXT=Context(is_prefill=True, cu_seqlens_q=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32), cu_seqlens_k=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32), max_seqlen_q=4096, max_seqlen_k=4096, slot_mapping=tensor([], device='cuda:0', dtype=torch.int32), context_lens=None, block_tables=None)
コンテキスト情報取得 context.is_prefill=True, context.slot_mapping=tensor([], device='cuda:0', dtype=torch.int32), context.max_seqlen_q=4096, context.cu_seqlens_q=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32), context.max_seqlen_k=4096, context.cu_seqlens_k=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32), context.context_lens=None, context.block_tables=None
PrefillモードでFlashAttentionを実行 context.max_seqlen_q=4096, context.cu_seqlens_q=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32), context.max_seqlen_k=4096, context.cu_seqlens_k=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32)
Attention層の順伝播完了 o.shape=torch.Size([16384, 16, 128])
RowParallelLinearの順伝播開始 入力形状: torch.Size([16384, 2048])
RowParallelLinearの順伝播完了 出力形状: torch.Size([16384, 1024])
Qwen3Attentionの順伝播完了 output.shape=torch.Size([16384, 1024])
RMSNormの順伝播開始 x.shape=torch.Size([16384, 1024]), residual.shape if residual is not None else None=torch.Size([16384, 1024])
残差接続を適用
残差接続を適用完了
Qwen3MLPの順伝播 x.shape=torch.Size([16384, 1024])
出力次元を並列化する線形層の順伝播開始 x.shape=torch.Size([16384, 1024])
出力次元を並列化する線形層の順伝播完了 result.shape=torch.Size([16384, 6144])
SwiGLUを順伝播開始 x.shape=torch.Size([16384, 6144])
チャンク化 x.shape=torch.Size([16384, 3072]), y.shape=torch.Size([16384, 3072])
SwiGLUを順伝播完了 result.shape=torch.Size([16384, 3072])
RowParallelLinearの順伝播開始 入力形状: torch.Size([16384, 3072])
RowParallelLinearの順伝播完了 出力形状: torch.Size([16384, 1024])
Qwen3MLPの順伝播完了 x.shape=torch.Size([16384, 1024])
Qwen3DecoderLayerの順伝播完了 hidden_states.shape=torch.Size([16384, 1024])
Qwen3DecoderLayerの順伝播 positions.shape=torch.Size([16384]) hidden_states.shape=torch.Size([16384, 1024]) residual is None=False
RMSNormの順伝播開始 x.shape=torch.Size([16384, 1024]), residual.shape if residual is not None else None=torch.Size([16384, 1024])
残差接続を適用
残差接続を適用完了
Qwen3Attentionの順伝播 positions.shape=torch.Size([16384]) hidden_states.shape=torch.Size([16384, 1024])
出力次元を並列化する線形層の順伝播開始 x.shape=torch.Size([16384, 1024])
出力次元を並列化する線形層の順伝播完了 result.shape=torch.Size([16384, 4096])
RMSNormの順伝播開始 x.shape=torch.Size([16384, 16, 128]), residual.shape if residual is not None else None=None
正規化開始 x.shape=torch.Size([16384, 16, 128])
正規化完了 x.shape=torch.Size([16384, 16, 128])
RMSNormの順伝播開始 x.shape=torch.Size([16384, 8, 128]), residual.shape if residual is not None else None=None
正規化開始 x.shape=torch.Size([16384, 8, 128])
正規化完了 x.shape=torch.Size([16384, 8, 128])
RoPEの順伝播開始 positions.shape=torch.Size([16384]), query.shape=torch.Size([16384, 16, 128]), key.shape=torch.Size([16384, 8, 128])
RoPEを適用開始 x.shape=torch.Size([16384, 16, 128]), cos.shape=torch.Size([16384, 1, 64]), sin.shape=torch.Size([16384, 1, 64])
チャンク化 x1.shape=torch.Size([16384, 16, 64]), x2.shape=torch.Size([16384, 16, 64])
回転行列適用 y1.shape=torch.Size([16384, 16, 64]), y2.shape=torch.Size([16384, 16, 64])
RoPEを適用完了 result.shape=torch.Size([16384, 16, 128])
RoPEを適用開始 x.shape=torch.Size([16384, 8, 128]), cos.shape=torch.Size([16384, 1, 64]), sin.shape=torch.Size([16384, 1, 64])
チャンク化 x1.shape=torch.Size([16384, 8, 64]), x2.shape=torch.Size([16384, 8, 64])
回転行列適用 y1.shape=torch.Size([16384, 8, 64]), y2.shape=torch.Size([16384, 8, 64])
RoPEを適用完了 result.shape=torch.Size([16384, 8, 128])
RoPEの順伝播完了 query.shape=torch.Size([16384, 16, 128]), key.shape=torch.Size([16384, 8, 128])
Attention層の順伝播開始 q.shape=torch.Size([16384, 16, 128]), k.shape=torch.Size([16384, 8, 128]), v.shape=torch.Size([16384, 8, 128])
現在のコンテキスト取得 _CONTEXT=Context(is_prefill=True, cu_seqlens_q=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32), cu_seqlens_k=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32), max_seqlen_q=4096, max_seqlen_k=4096, slot_mapping=tensor([], device='cuda:0', dtype=torch.int32), context_lens=None, block_tables=None)
コンテキスト情報取得 context.is_prefill=True, context.slot_mapping=tensor([], device='cuda:0', dtype=torch.int32), context.max_seqlen_q=4096, context.cu_seqlens_q=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32), context.max_seqlen_k=4096, context.cu_seqlens_k=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32), context.context_lens=None, context.block_tables=None
PrefillモードでFlashAttentionを実行 context.max_seqlen_q=4096, context.cu_seqlens_q=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32), context.max_seqlen_k=4096, context.cu_seqlens_k=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32)
Attention層の順伝播完了 o.shape=torch.Size([16384, 16, 128])
RowParallelLinearの順伝播開始 入力形状: torch.Size([16384, 2048])
RowParallelLinearの順伝播完了 出力形状: torch.Size([16384, 1024])
Qwen3Attentionの順伝播完了 output.shape=torch.Size([16384, 1024])
RMSNormの順伝播開始 x.shape=torch.Size([16384, 1024]), residual.shape if residual is not None else None=torch.Size([16384, 1024])
残差接続を適用
残差接続を適用完了
Qwen3MLPの順伝播 x.shape=torch.Size([16384, 1024])
出力次元を並列化する線形層の順伝播開始 x.shape=torch.Size([16384, 1024])
出力次元を並列化する線形層の順伝播完了 result.shape=torch.Size([16384, 6144])
SwiGLUを順伝播開始 x.shape=torch.Size([16384, 6144])
チャンク化 x.shape=torch.Size([16384, 3072]), y.shape=torch.Size([16384, 3072])
SwiGLUを順伝播完了 result.shape=torch.Size([16384, 3072])
RowParallelLinearの順伝播開始 入力形状: torch.Size([16384, 3072])
RowParallelLinearの順伝播完了 出力形状: torch.Size([16384, 1024])
Qwen3MLPの順伝播完了 x.shape=torch.Size([16384, 1024])
Qwen3DecoderLayerの順伝播完了 hidden_states.shape=torch.Size([16384, 1024])
Qwen3DecoderLayerの順伝播 positions.shape=torch.Size([16384]) hidden_states.shape=torch.Size([16384, 1024]) residual is None=False
RMSNormの順伝播開始 x.shape=torch.Size([16384, 1024]), residual.shape if residual is not None else None=torch.Size([16384, 1024])
残差接続を適用
残差接続を適用完了
Qwen3Attentionの順伝播 positions.shape=torch.Size([16384]) hidden_states.shape=torch.Size([16384, 1024])
出力次元を並列化する線形層の順伝播開始 x.shape=torch.Size([16384, 1024])
出力次元を並列化する線形層の順伝播完了 result.shape=torch.Size([16384, 4096])
RMSNormの順伝播開始 x.shape=torch.Size([16384, 16, 128]), residual.shape if residual is not None else None=None
正規化開始 x.shape=torch.Size([16384, 16, 128])
正規化完了 x.shape=torch.Size([16384, 16, 128])
RMSNormの順伝播開始 x.shape=torch.Size([16384, 8, 128]), residual.shape if residual is not None else None=None
正規化開始 x.shape=torch.Size([16384, 8, 128])
正規化完了 x.shape=torch.Size([16384, 8, 128])
RoPEの順伝播開始 positions.shape=torch.Size([16384]), query.shape=torch.Size([16384, 16, 128]), key.shape=torch.Size([16384, 8, 128])
RoPEを適用開始 x.shape=torch.Size([16384, 16, 128]), cos.shape=torch.Size([16384, 1, 64]), sin.shape=torch.Size([16384, 1, 64])
チャンク化 x1.shape=torch.Size([16384, 16, 64]), x2.shape=torch.Size([16384, 16, 64])
回転行列適用 y1.shape=torch.Size([16384, 16, 64]), y2.shape=torch.Size([16384, 16, 64])
RoPEを適用完了 result.shape=torch.Size([16384, 16, 128])
RoPEを適用開始 x.shape=torch.Size([16384, 8, 128]), cos.shape=torch.Size([16384, 1, 64]), sin.shape=torch.Size([16384, 1, 64])
チャンク化 x1.shape=torch.Size([16384, 8, 64]), x2.shape=torch.Size([16384, 8, 64])
回転行列適用 y1.shape=torch.Size([16384, 8, 64]), y2.shape=torch.Size([16384, 8, 64])
RoPEを適用完了 result.shape=torch.Size([16384, 8, 128])
RoPEの順伝播完了 query.shape=torch.Size([16384, 16, 128]), key.shape=torch.Size([16384, 8, 128])
Attention層の順伝播開始 q.shape=torch.Size([16384, 16, 128]), k.shape=torch.Size([16384, 8, 128]), v.shape=torch.Size([16384, 8, 128])
現在のコンテキスト取得 _CONTEXT=Context(is_prefill=True, cu_seqlens_q=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32), cu_seqlens_k=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32), max_seqlen_q=4096, max_seqlen_k=4096, slot_mapping=tensor([], device='cuda:0', dtype=torch.int32), context_lens=None, block_tables=None)
コンテキスト情報取得 context.is_prefill=True, context.slot_mapping=tensor([], device='cuda:0', dtype=torch.int32), context.max_seqlen_q=4096, context.cu_seqlens_q=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32), context.max_seqlen_k=4096, context.cu_seqlens_k=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32), context.context_lens=None, context.block_tables=None
PrefillモードでFlashAttentionを実行 context.max_seqlen_q=4096, context.cu_seqlens_q=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32), context.max_seqlen_k=4096, context.cu_seqlens_k=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32)
Attention層の順伝播完了 o.shape=torch.Size([16384, 16, 128])
RowParallelLinearの順伝播開始 入力形状: torch.Size([16384, 2048])
RowParallelLinearの順伝播完了 出力形状: torch.Size([16384, 1024])
Qwen3Attentionの順伝播完了 output.shape=torch.Size([16384, 1024])
RMSNormの順伝播開始 x.shape=torch.Size([16384, 1024]), residual.shape if residual is not None else None=torch.Size([16384, 1024])
残差接続を適用
残差接続を適用完了
Qwen3MLPの順伝播 x.shape=torch.Size([16384, 1024])
出力次元を並列化する線形層の順伝播開始 x.shape=torch.Size([16384, 1024])
出力次元を並列化する線形層の順伝播完了 result.shape=torch.Size([16384, 6144])
SwiGLUを順伝播開始 x.shape=torch.Size([16384, 6144])
チャンク化 x.shape=torch.Size([16384, 3072]), y.shape=torch.Size([16384, 3072])
SwiGLUを順伝播完了 result.shape=torch.Size([16384, 3072])
RowParallelLinearの順伝播開始 入力形状: torch.Size([16384, 3072])
RowParallelLinearの順伝播完了 出力形状: torch.Size([16384, 1024])
Qwen3MLPの順伝播完了 x.shape=torch.Size([16384, 1024])
Qwen3DecoderLayerの順伝播完了 hidden_states.shape=torch.Size([16384, 1024])
Qwen3DecoderLayerの順伝播 positions.shape=torch.Size([16384]) hidden_states.shape=torch.Size([16384, 1024]) residual is None=False
RMSNormの順伝播開始 x.shape=torch.Size([16384, 1024]), residual.shape if residual is not None else None=torch.Size([16384, 1024])
残差接続を適用
残差接続を適用完了
Qwen3Attentionの順伝播 positions.shape=torch.Size([16384]) hidden_states.shape=torch.Size([16384, 1024])
出力次元を並列化する線形層の順伝播開始 x.shape=torch.Size([16384, 1024])
出力次元を並列化する線形層の順伝播完了 result.shape=torch.Size([16384, 4096])
RMSNormの順伝播開始 x.shape=torch.Size([16384, 16, 128]), residual.shape if residual is not None else None=None
正規化開始 x.shape=torch.Size([16384, 16, 128])
正規化完了 x.shape=torch.Size([16384, 16, 128])
RMSNormの順伝播開始 x.shape=torch.Size([16384, 8, 128]), residual.shape if residual is not None else None=None
正規化開始 x.shape=torch.Size([16384, 8, 128])
正規化完了 x.shape=torch.Size([16384, 8, 128])
RoPEの順伝播開始 positions.shape=torch.Size([16384]), query.shape=torch.Size([16384, 16, 128]), key.shape=torch.Size([16384, 8, 128])
RoPEを適用開始 x.shape=torch.Size([16384, 16, 128]), cos.shape=torch.Size([16384, 1, 64]), sin.shape=torch.Size([16384, 1, 64])
チャンク化 x1.shape=torch.Size([16384, 16, 64]), x2.shape=torch.Size([16384, 16, 64])
回転行列適用 y1.shape=torch.Size([16384, 16, 64]), y2.shape=torch.Size([16384, 16, 64])
RoPEを適用完了 result.shape=torch.Size([16384, 16, 128])
RoPEを適用開始 x.shape=torch.Size([16384, 8, 128]), cos.shape=torch.Size([16384, 1, 64]), sin.shape=torch.Size([16384, 1, 64])
チャンク化 x1.shape=torch.Size([16384, 8, 64]), x2.shape=torch.Size([16384, 8, 64])
回転行列適用 y1.shape=torch.Size([16384, 8, 64]), y2.shape=torch.Size([16384, 8, 64])
RoPEを適用完了 result.shape=torch.Size([16384, 8, 128])
RoPEの順伝播完了 query.shape=torch.Size([16384, 16, 128]), key.shape=torch.Size([16384, 8, 128])
Attention層の順伝播開始 q.shape=torch.Size([16384, 16, 128]), k.shape=torch.Size([16384, 8, 128]), v.shape=torch.Size([16384, 8, 128])
現在のコンテキスト取得 _CONTEXT=Context(is_prefill=True, cu_seqlens_q=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32), cu_seqlens_k=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32), max_seqlen_q=4096, max_seqlen_k=4096, slot_mapping=tensor([], device='cuda:0', dtype=torch.int32), context_lens=None, block_tables=None)
コンテキスト情報取得 context.is_prefill=True, context.slot_mapping=tensor([], device='cuda:0', dtype=torch.int32), context.max_seqlen_q=4096, context.cu_seqlens_q=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32), context.max_seqlen_k=4096, context.cu_seqlens_k=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32), context.context_lens=None, context.block_tables=None
PrefillモードでFlashAttentionを実行 context.max_seqlen_q=4096, context.cu_seqlens_q=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32), context.max_seqlen_k=4096, context.cu_seqlens_k=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32)
Attention層の順伝播完了 o.shape=torch.Size([16384, 16, 128])
RowParallelLinearの順伝播開始 入力形状: torch.Size([16384, 2048])
RowParallelLinearの順伝播完了 出力形状: torch.Size([16384, 1024])
Qwen3Attentionの順伝播完了 output.shape=torch.Size([16384, 1024])
RMSNormの順伝播開始 x.shape=torch.Size([16384, 1024]), residual.shape if residual is not None else None=torch.Size([16384, 1024])
残差接続を適用
残差接続を適用完了
Qwen3MLPの順伝播 x.shape=torch.Size([16384, 1024])
出力次元を並列化する線形層の順伝播開始 x.shape=torch.Size([16384, 1024])
出力次元を並列化する線形層の順伝播完了 result.shape=torch.Size([16384, 6144])
SwiGLUを順伝播開始 x.shape=torch.Size([16384, 6144])
チャンク化 x.shape=torch.Size([16384, 3072]), y.shape=torch.Size([16384, 3072])
SwiGLUを順伝播完了 result.shape=torch.Size([16384, 3072])
RowParallelLinearの順伝播開始 入力形状: torch.Size([16384, 3072])
RowParallelLinearの順伝播完了 出力形状: torch.Size([16384, 1024])
Qwen3MLPの順伝播完了 x.shape=torch.Size([16384, 1024])
Qwen3DecoderLayerの順伝播完了 hidden_states.shape=torch.Size([16384, 1024])
Qwen3DecoderLayerの順伝播 positions.shape=torch.Size([16384]) hidden_states.shape=torch.Size([16384, 1024]) residual is None=False
RMSNormの順伝播開始 x.shape=torch.Size([16384, 1024]), residual.shape if residual is not None else None=torch.Size([16384, 1024])
残差接続を適用
残差接続を適用完了
Qwen3Attentionの順伝播 positions.shape=torch.Size([16384]) hidden_states.shape=torch.Size([16384, 1024])
出力次元を並列化する線形層の順伝播開始 x.shape=torch.Size([16384, 1024])
出力次元を並列化する線形層の順伝播完了 result.shape=torch.Size([16384, 4096])
RMSNormの順伝播開始 x.shape=torch.Size([16384, 16, 128]), residual.shape if residual is not None else None=None
正規化開始 x.shape=torch.Size([16384, 16, 128])
正規化完了 x.shape=torch.Size([16384, 16, 128])
RMSNormの順伝播開始 x.shape=torch.Size([16384, 8, 128]), residual.shape if residual is not None else None=None
正規化開始 x.shape=torch.Size([16384, 8, 128])
正規化完了 x.shape=torch.Size([16384, 8, 128])
RoPEの順伝播開始 positions.shape=torch.Size([16384]), query.shape=torch.Size([16384, 16, 128]), key.shape=torch.Size([16384, 8, 128])
RoPEを適用開始 x.shape=torch.Size([16384, 16, 128]), cos.shape=torch.Size([16384, 1, 64]), sin.shape=torch.Size([16384, 1, 64])
チャンク化 x1.shape=torch.Size([16384, 16, 64]), x2.shape=torch.Size([16384, 16, 64])
回転行列適用 y1.shape=torch.Size([16384, 16, 64]), y2.shape=torch.Size([16384, 16, 64])
RoPEを適用完了 result.shape=torch.Size([16384, 16, 128])
RoPEを適用開始 x.shape=torch.Size([16384, 8, 128]), cos.shape=torch.Size([16384, 1, 64]), sin.shape=torch.Size([16384, 1, 64])
チャンク化 x1.shape=torch.Size([16384, 8, 64]), x2.shape=torch.Size([16384, 8, 64])
回転行列適用 y1.shape=torch.Size([16384, 8, 64]), y2.shape=torch.Size([16384, 8, 64])
RoPEを適用完了 result.shape=torch.Size([16384, 8, 128])
RoPEの順伝播完了 query.shape=torch.Size([16384, 16, 128]), key.shape=torch.Size([16384, 8, 128])
Attention層の順伝播開始 q.shape=torch.Size([16384, 16, 128]), k.shape=torch.Size([16384, 8, 128]), v.shape=torch.Size([16384, 8, 128])
現在のコンテキスト取得 _CONTEXT=Context(is_prefill=True, cu_seqlens_q=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32), cu_seqlens_k=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32), max_seqlen_q=4096, max_seqlen_k=4096, slot_mapping=tensor([], device='cuda:0', dtype=torch.int32), context_lens=None, block_tables=None)
コンテキスト情報取得 context.is_prefill=True, context.slot_mapping=tensor([], device='cuda:0', dtype=torch.int32), context.max_seqlen_q=4096, context.cu_seqlens_q=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32), context.max_seqlen_k=4096, context.cu_seqlens_k=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32), context.context_lens=None, context.block_tables=None
PrefillモードでFlashAttentionを実行 context.max_seqlen_q=4096, context.cu_seqlens_q=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32), context.max_seqlen_k=4096, context.cu_seqlens_k=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32)
Attention層の順伝播完了 o.shape=torch.Size([16384, 16, 128])
RowParallelLinearの順伝播開始 入力形状: torch.Size([16384, 2048])
RowParallelLinearの順伝播完了 出力形状: torch.Size([16384, 1024])
Qwen3Attentionの順伝播完了 output.shape=torch.Size([16384, 1024])
RMSNormの順伝播開始 x.shape=torch.Size([16384, 1024]), residual.shape if residual is not None else None=torch.Size([16384, 1024])
残差接続を適用
残差接続を適用完了
Qwen3MLPの順伝播 x.shape=torch.Size([16384, 1024])
出力次元を並列化する線形層の順伝播開始 x.shape=torch.Size([16384, 1024])
出力次元を並列化する線形層の順伝播完了 result.shape=torch.Size([16384, 6144])
SwiGLUを順伝播開始 x.shape=torch.Size([16384, 6144])
チャンク化 x.shape=torch.Size([16384, 3072]), y.shape=torch.Size([16384, 3072])
SwiGLUを順伝播完了 result.shape=torch.Size([16384, 3072])
RowParallelLinearの順伝播開始 入力形状: torch.Size([16384, 3072])
RowParallelLinearの順伝播完了 出力形状: torch.Size([16384, 1024])
Qwen3MLPの順伝播完了 x.shape=torch.Size([16384, 1024])
Qwen3DecoderLayerの順伝播完了 hidden_states.shape=torch.Size([16384, 1024])
Qwen3DecoderLayerの順伝播 positions.shape=torch.Size([16384]) hidden_states.shape=torch.Size([16384, 1024]) residual is None=False
RMSNormの順伝播開始 x.shape=torch.Size([16384, 1024]), residual.shape if residual is not None else None=torch.Size([16384, 1024])
残差接続を適用
残差接続を適用完了
Qwen3Attentionの順伝播 positions.shape=torch.Size([16384]) hidden_states.shape=torch.Size([16384, 1024])
出力次元を並列化する線形層の順伝播開始 x.shape=torch.Size([16384, 1024])
出力次元を並列化する線形層の順伝播完了 result.shape=torch.Size([16384, 4096])
RMSNormの順伝播開始 x.shape=torch.Size([16384, 16, 128]), residual.shape if residual is not None else None=None
正規化開始 x.shape=torch.Size([16384, 16, 128])
正規化完了 x.shape=torch.Size([16384, 16, 128])
RMSNormの順伝播開始 x.shape=torch.Size([16384, 8, 128]), residual.shape if residual is not None else None=None
正規化開始 x.shape=torch.Size([16384, 8, 128])
正規化完了 x.shape=torch.Size([16384, 8, 128])
RoPEの順伝播開始 positions.shape=torch.Size([16384]), query.shape=torch.Size([16384, 16, 128]), key.shape=torch.Size([16384, 8, 128])
RoPEを適用開始 x.shape=torch.Size([16384, 16, 128]), cos.shape=torch.Size([16384, 1, 64]), sin.shape=torch.Size([16384, 1, 64])
チャンク化 x1.shape=torch.Size([16384, 16, 64]), x2.shape=torch.Size([16384, 16, 64])
回転行列適用 y1.shape=torch.Size([16384, 16, 64]), y2.shape=torch.Size([16384, 16, 64])
RoPEを適用完了 result.shape=torch.Size([16384, 16, 128])
RoPEを適用開始 x.shape=torch.Size([16384, 8, 128]), cos.shape=torch.Size([16384, 1, 64]), sin.shape=torch.Size([16384, 1, 64])
チャンク化 x1.shape=torch.Size([16384, 8, 64]), x2.shape=torch.Size([16384, 8, 64])
回転行列適用 y1.shape=torch.Size([16384, 8, 64]), y2.shape=torch.Size([16384, 8, 64])
RoPEを適用完了 result.shape=torch.Size([16384, 8, 128])
RoPEの順伝播完了 query.shape=torch.Size([16384, 16, 128]), key.shape=torch.Size([16384, 8, 128])
Attention層の順伝播開始 q.shape=torch.Size([16384, 16, 128]), k.shape=torch.Size([16384, 8, 128]), v.shape=torch.Size([16384, 8, 128])
現在のコンテキスト取得 _CONTEXT=Context(is_prefill=True, cu_seqlens_q=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32), cu_seqlens_k=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32), max_seqlen_q=4096, max_seqlen_k=4096, slot_mapping=tensor([], device='cuda:0', dtype=torch.int32), context_lens=None, block_tables=None)
コンテキスト情報取得 context.is_prefill=True, context.slot_mapping=tensor([], device='cuda:0', dtype=torch.int32), context.max_seqlen_q=4096, context.cu_seqlens_q=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32), context.max_seqlen_k=4096, context.cu_seqlens_k=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32), context.context_lens=None, context.block_tables=None
PrefillモードでFlashAttentionを実行 context.max_seqlen_q=4096, context.cu_seqlens_q=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32), context.max_seqlen_k=4096, context.cu_seqlens_k=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32)
Attention層の順伝播完了 o.shape=torch.Size([16384, 16, 128])
RowParallelLinearの順伝播開始 入力形状: torch.Size([16384, 2048])
RowParallelLinearの順伝播完了 出力形状: torch.Size([16384, 1024])
Qwen3Attentionの順伝播完了 output.shape=torch.Size([16384, 1024])
RMSNormの順伝播開始 x.shape=torch.Size([16384, 1024]), residual.shape if residual is not None else None=torch.Size([16384, 1024])
残差接続を適用
残差接続を適用完了
Qwen3MLPの順伝播 x.shape=torch.Size([16384, 1024])
出力次元を並列化する線形層の順伝播開始 x.shape=torch.Size([16384, 1024])
出力次元を並列化する線形層の順伝播完了 result.shape=torch.Size([16384, 6144])
SwiGLUを順伝播開始 x.shape=torch.Size([16384, 6144])
チャンク化 x.shape=torch.Size([16384, 3072]), y.shape=torch.Size([16384, 3072])
SwiGLUを順伝播完了 result.shape=torch.Size([16384, 3072])
RowParallelLinearの順伝播開始 入力形状: torch.Size([16384, 3072])
RowParallelLinearの順伝播完了 出力形状: torch.Size([16384, 1024])
Qwen3MLPの順伝播完了 x.shape=torch.Size([16384, 1024])
Qwen3DecoderLayerの順伝播完了 hidden_states.shape=torch.Size([16384, 1024])
Qwen3DecoderLayerの順伝播 positions.shape=torch.Size([16384]) hidden_states.shape=torch.Size([16384, 1024]) residual is None=False
RMSNormの順伝播開始 x.shape=torch.Size([16384, 1024]), residual.shape if residual is not None else None=torch.Size([16384, 1024])
残差接続を適用
残差接続を適用完了
Qwen3Attentionの順伝播 positions.shape=torch.Size([16384]) hidden_states.shape=torch.Size([16384, 1024])
出力次元を並列化する線形層の順伝播開始 x.shape=torch.Size([16384, 1024])
出力次元を並列化する線形層の順伝播完了 result.shape=torch.Size([16384, 4096])
RMSNormの順伝播開始 x.shape=torch.Size([16384, 16, 128]), residual.shape if residual is not None else None=None
正規化開始 x.shape=torch.Size([16384, 16, 128])
正規化完了 x.shape=torch.Size([16384, 16, 128])
RMSNormの順伝播開始 x.shape=torch.Size([16384, 8, 128]), residual.shape if residual is not None else None=None
正規化開始 x.shape=torch.Size([16384, 8, 128])
正規化完了 x.shape=torch.Size([16384, 8, 128])
RoPEの順伝播開始 positions.shape=torch.Size([16384]), query.shape=torch.Size([16384, 16, 128]), key.shape=torch.Size([16384, 8, 128])
RoPEを適用開始 x.shape=torch.Size([16384, 16, 128]), cos.shape=torch.Size([16384, 1, 64]), sin.shape=torch.Size([16384, 1, 64])
チャンク化 x1.shape=torch.Size([16384, 16, 64]), x2.shape=torch.Size([16384, 16, 64])
回転行列適用 y1.shape=torch.Size([16384, 16, 64]), y2.shape=torch.Size([16384, 16, 64])
RoPEを適用完了 result.shape=torch.Size([16384, 16, 128])
RoPEを適用開始 x.shape=torch.Size([16384, 8, 128]), cos.shape=torch.Size([16384, 1, 64]), sin.shape=torch.Size([16384, 1, 64])
チャンク化 x1.shape=torch.Size([16384, 8, 64]), x2.shape=torch.Size([16384, 8, 64])
回転行列適用 y1.shape=torch.Size([16384, 8, 64]), y2.shape=torch.Size([16384, 8, 64])
RoPEを適用完了 result.shape=torch.Size([16384, 8, 128])
RoPEの順伝播完了 query.shape=torch.Size([16384, 16, 128]), key.shape=torch.Size([16384, 8, 128])
Attention層の順伝播開始 q.shape=torch.Size([16384, 16, 128]), k.shape=torch.Size([16384, 8, 128]), v.shape=torch.Size([16384, 8, 128])
現在のコンテキスト取得 _CONTEXT=Context(is_prefill=True, cu_seqlens_q=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32), cu_seqlens_k=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32), max_seqlen_q=4096, max_seqlen_k=4096, slot_mapping=tensor([], device='cuda:0', dtype=torch.int32), context_lens=None, block_tables=None)
コンテキスト情報取得 context.is_prefill=True, context.slot_mapping=tensor([], device='cuda:0', dtype=torch.int32), context.max_seqlen_q=4096, context.cu_seqlens_q=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32), context.max_seqlen_k=4096, context.cu_seqlens_k=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32), context.context_lens=None, context.block_tables=None
PrefillモードでFlashAttentionを実行 context.max_seqlen_q=4096, context.cu_seqlens_q=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32), context.max_seqlen_k=4096, context.cu_seqlens_k=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32)
Attention層の順伝播完了 o.shape=torch.Size([16384, 16, 128])
RowParallelLinearの順伝播開始 入力形状: torch.Size([16384, 2048])
RowParallelLinearの順伝播完了 出力形状: torch.Size([16384, 1024])
Qwen3Attentionの順伝播完了 output.shape=torch.Size([16384, 1024])
RMSNormの順伝播開始 x.shape=torch.Size([16384, 1024]), residual.shape if residual is not None else None=torch.Size([16384, 1024])
残差接続を適用
残差接続を適用完了
Qwen3MLPの順伝播 x.shape=torch.Size([16384, 1024])
出力次元を並列化する線形層の順伝播開始 x.shape=torch.Size([16384, 1024])
出力次元を並列化する線形層の順伝播完了 result.shape=torch.Size([16384, 6144])
SwiGLUを順伝播開始 x.shape=torch.Size([16384, 6144])
チャンク化 x.shape=torch.Size([16384, 3072]), y.shape=torch.Size([16384, 3072])
SwiGLUを順伝播完了 result.shape=torch.Size([16384, 3072])
RowParallelLinearの順伝播開始 入力形状: torch.Size([16384, 3072])
RowParallelLinearの順伝播完了 出力形状: torch.Size([16384, 1024])
Qwen3MLPの順伝播完了 x.shape=torch.Size([16384, 1024])
Qwen3DecoderLayerの順伝播完了 hidden_states.shape=torch.Size([16384, 1024])
Qwen3DecoderLayerの順伝播 positions.shape=torch.Size([16384]) hidden_states.shape=torch.Size([16384, 1024]) residual is None=False
RMSNormの順伝播開始 x.shape=torch.Size([16384, 1024]), residual.shape if residual is not None else None=torch.Size([16384, 1024])
残差接続を適用
残差接続を適用完了
Qwen3Attentionの順伝播 positions.shape=torch.Size([16384]) hidden_states.shape=torch.Size([16384, 1024])
出力次元を並列化する線形層の順伝播開始 x.shape=torch.Size([16384, 1024])
出力次元を並列化する線形層の順伝播完了 result.shape=torch.Size([16384, 4096])
RMSNormの順伝播開始 x.shape=torch.Size([16384, 16, 128]), residual.shape if residual is not None else None=None
正規化開始 x.shape=torch.Size([16384, 16, 128])
正規化完了 x.shape=torch.Size([16384, 16, 128])
RMSNormの順伝播開始 x.shape=torch.Size([16384, 8, 128]), residual.shape if residual is not None else None=None
正規化開始 x.shape=torch.Size([16384, 8, 128])
正規化完了 x.shape=torch.Size([16384, 8, 128])
RoPEの順伝播開始 positions.shape=torch.Size([16384]), query.shape=torch.Size([16384, 16, 128]), key.shape=torch.Size([16384, 8, 128])
RoPEを適用開始 x.shape=torch.Size([16384, 16, 128]), cos.shape=torch.Size([16384, 1, 64]), sin.shape=torch.Size([16384, 1, 64])
チャンク化 x1.shape=torch.Size([16384, 16, 64]), x2.shape=torch.Size([16384, 16, 64])
回転行列適用 y1.shape=torch.Size([16384, 16, 64]), y2.shape=torch.Size([16384, 16, 64])
RoPEを適用完了 result.shape=torch.Size([16384, 16, 128])
RoPEを適用開始 x.shape=torch.Size([16384, 8, 128]), cos.shape=torch.Size([16384, 1, 64]), sin.shape=torch.Size([16384, 1, 64])
チャンク化 x1.shape=torch.Size([16384, 8, 64]), x2.shape=torch.Size([16384, 8, 64])
回転行列適用 y1.shape=torch.Size([16384, 8, 64]), y2.shape=torch.Size([16384, 8, 64])
RoPEを適用完了 result.shape=torch.Size([16384, 8, 128])
RoPEの順伝播完了 query.shape=torch.Size([16384, 16, 128]), key.shape=torch.Size([16384, 8, 128])
Attention層の順伝播開始 q.shape=torch.Size([16384, 16, 128]), k.shape=torch.Size([16384, 8, 128]), v.shape=torch.Size([16384, 8, 128])
現在のコンテキスト取得 _CONTEXT=Context(is_prefill=True, cu_seqlens_q=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32), cu_seqlens_k=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32), max_seqlen_q=4096, max_seqlen_k=4096, slot_mapping=tensor([], device='cuda:0', dtype=torch.int32), context_lens=None, block_tables=None)
コンテキスト情報取得 context.is_prefill=True, context.slot_mapping=tensor([], device='cuda:0', dtype=torch.int32), context.max_seqlen_q=4096, context.cu_seqlens_q=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32), context.max_seqlen_k=4096, context.cu_seqlens_k=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32), context.context_lens=None, context.block_tables=None
PrefillモードでFlashAttentionを実行 context.max_seqlen_q=4096, context.cu_seqlens_q=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32), context.max_seqlen_k=4096, context.cu_seqlens_k=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32)
Attention層の順伝播完了 o.shape=torch.Size([16384, 16, 128])
RowParallelLinearの順伝播開始 入力形状: torch.Size([16384, 2048])
RowParallelLinearの順伝播完了 出力形状: torch.Size([16384, 1024])
Qwen3Attentionの順伝播完了 output.shape=torch.Size([16384, 1024])
RMSNormの順伝播開始 x.shape=torch.Size([16384, 1024]), residual.shape if residual is not None else None=torch.Size([16384, 1024])
残差接続を適用
残差接続を適用完了
Qwen3MLPの順伝播 x.shape=torch.Size([16384, 1024])
出力次元を並列化する線形層の順伝播開始 x.shape=torch.Size([16384, 1024])
出力次元を並列化する線形層の順伝播完了 result.shape=torch.Size([16384, 6144])
SwiGLUを順伝播開始 x.shape=torch.Size([16384, 6144])
チャンク化 x.shape=torch.Size([16384, 3072]), y.shape=torch.Size([16384, 3072])
SwiGLUを順伝播完了 result.shape=torch.Size([16384, 3072])
RowParallelLinearの順伝播開始 入力形状: torch.Size([16384, 3072])
RowParallelLinearの順伝播完了 出力形状: torch.Size([16384, 1024])
Qwen3MLPの順伝播完了 x.shape=torch.Size([16384, 1024])
Qwen3DecoderLayerの順伝播完了 hidden_states.shape=torch.Size([16384, 1024])
Qwen3DecoderLayerの順伝播 positions.shape=torch.Size([16384]) hidden_states.shape=torch.Size([16384, 1024]) residual is None=False
RMSNormの順伝播開始 x.shape=torch.Size([16384, 1024]), residual.shape if residual is not None else None=torch.Size([16384, 1024])
残差接続を適用
残差接続を適用完了
Qwen3Attentionの順伝播 positions.shape=torch.Size([16384]) hidden_states.shape=torch.Size([16384, 1024])
出力次元を並列化する線形層の順伝播開始 x.shape=torch.Size([16384, 1024])
出力次元を並列化する線形層の順伝播完了 result.shape=torch.Size([16384, 4096])
RMSNormの順伝播開始 x.shape=torch.Size([16384, 16, 128]), residual.shape if residual is not None else None=None
正規化開始 x.shape=torch.Size([16384, 16, 128])
正規化完了 x.shape=torch.Size([16384, 16, 128])
RMSNormの順伝播開始 x.shape=torch.Size([16384, 8, 128]), residual.shape if residual is not None else None=None
正規化開始 x.shape=torch.Size([16384, 8, 128])
正規化完了 x.shape=torch.Size([16384, 8, 128])
RoPEの順伝播開始 positions.shape=torch.Size([16384]), query.shape=torch.Size([16384, 16, 128]), key.shape=torch.Size([16384, 8, 128])
RoPEを適用開始 x.shape=torch.Size([16384, 16, 128]), cos.shape=torch.Size([16384, 1, 64]), sin.shape=torch.Size([16384, 1, 64])
チャンク化 x1.shape=torch.Size([16384, 16, 64]), x2.shape=torch.Size([16384, 16, 64])
回転行列適用 y1.shape=torch.Size([16384, 16, 64]), y2.shape=torch.Size([16384, 16, 64])
RoPEを適用完了 result.shape=torch.Size([16384, 16, 128])
RoPEを適用開始 x.shape=torch.Size([16384, 8, 128]), cos.shape=torch.Size([16384, 1, 64]), sin.shape=torch.Size([16384, 1, 64])
チャンク化 x1.shape=torch.Size([16384, 8, 64]), x2.shape=torch.Size([16384, 8, 64])
回転行列適用 y1.shape=torch.Size([16384, 8, 64]), y2.shape=torch.Size([16384, 8, 64])
RoPEを適用完了 result.shape=torch.Size([16384, 8, 128])
RoPEの順伝播完了 query.shape=torch.Size([16384, 16, 128]), key.shape=torch.Size([16384, 8, 128])
Attention層の順伝播開始 q.shape=torch.Size([16384, 16, 128]), k.shape=torch.Size([16384, 8, 128]), v.shape=torch.Size([16384, 8, 128])
現在のコンテキスト取得 _CONTEXT=Context(is_prefill=True, cu_seqlens_q=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32), cu_seqlens_k=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32), max_seqlen_q=4096, max_seqlen_k=4096, slot_mapping=tensor([], device='cuda:0', dtype=torch.int32), context_lens=None, block_tables=None)
コンテキスト情報取得 context.is_prefill=True, context.slot_mapping=tensor([], device='cuda:0', dtype=torch.int32), context.max_seqlen_q=4096, context.cu_seqlens_q=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32), context.max_seqlen_k=4096, context.cu_seqlens_k=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32), context.context_lens=None, context.block_tables=None
PrefillモードでFlashAttentionを実行 context.max_seqlen_q=4096, context.cu_seqlens_q=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32), context.max_seqlen_k=4096, context.cu_seqlens_k=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32)
Attention層の順伝播完了 o.shape=torch.Size([16384, 16, 128])
RowParallelLinearの順伝播開始 入力形状: torch.Size([16384, 2048])
RowParallelLinearの順伝播完了 出力形状: torch.Size([16384, 1024])
Qwen3Attentionの順伝播完了 output.shape=torch.Size([16384, 1024])
RMSNormの順伝播開始 x.shape=torch.Size([16384, 1024]), residual.shape if residual is not None else None=torch.Size([16384, 1024])
残差接続を適用
残差接続を適用完了
Qwen3MLPの順伝播 x.shape=torch.Size([16384, 1024])
出力次元を並列化する線形層の順伝播開始 x.shape=torch.Size([16384, 1024])
出力次元を並列化する線形層の順伝播完了 result.shape=torch.Size([16384, 6144])
SwiGLUを順伝播開始 x.shape=torch.Size([16384, 6144])
チャンク化 x.shape=torch.Size([16384, 3072]), y.shape=torch.Size([16384, 3072])
SwiGLUを順伝播完了 result.shape=torch.Size([16384, 3072])
RowParallelLinearの順伝播開始 入力形状: torch.Size([16384, 3072])
RowParallelLinearの順伝播完了 出力形状: torch.Size([16384, 1024])
Qwen3MLPの順伝播完了 x.shape=torch.Size([16384, 1024])
Qwen3DecoderLayerの順伝播完了 hidden_states.shape=torch.Size([16384, 1024])
Qwen3DecoderLayerの順伝播 positions.shape=torch.Size([16384]) hidden_states.shape=torch.Size([16384, 1024]) residual is None=False
RMSNormの順伝播開始 x.shape=torch.Size([16384, 1024]), residual.shape if residual is not None else None=torch.Size([16384, 1024])
残差接続を適用
残差接続を適用完了
Qwen3Attentionの順伝播 positions.shape=torch.Size([16384]) hidden_states.shape=torch.Size([16384, 1024])
出力次元を並列化する線形層の順伝播開始 x.shape=torch.Size([16384, 1024])
出力次元を並列化する線形層の順伝播完了 result.shape=torch.Size([16384, 4096])
RMSNormの順伝播開始 x.shape=torch.Size([16384, 16, 128]), residual.shape if residual is not None else None=None
正規化開始 x.shape=torch.Size([16384, 16, 128])
正規化完了 x.shape=torch.Size([16384, 16, 128])
RMSNormの順伝播開始 x.shape=torch.Size([16384, 8, 128]), residual.shape if residual is not None else None=None
正規化開始 x.shape=torch.Size([16384, 8, 128])
正規化完了 x.shape=torch.Size([16384, 8, 128])
RoPEの順伝播開始 positions.shape=torch.Size([16384]), query.shape=torch.Size([16384, 16, 128]), key.shape=torch.Size([16384, 8, 128])
RoPEを適用開始 x.shape=torch.Size([16384, 16, 128]), cos.shape=torch.Size([16384, 1, 64]), sin.shape=torch.Size([16384, 1, 64])
チャンク化 x1.shape=torch.Size([16384, 16, 64]), x2.shape=torch.Size([16384, 16, 64])
回転行列適用 y1.shape=torch.Size([16384, 16, 64]), y2.shape=torch.Size([16384, 16, 64])
RoPEを適用完了 result.shape=torch.Size([16384, 16, 128])
RoPEを適用開始 x.shape=torch.Size([16384, 8, 128]), cos.shape=torch.Size([16384, 1, 64]), sin.shape=torch.Size([16384, 1, 64])
チャンク化 x1.shape=torch.Size([16384, 8, 64]), x2.shape=torch.Size([16384, 8, 64])
回転行列適用 y1.shape=torch.Size([16384, 8, 64]), y2.shape=torch.Size([16384, 8, 64])
RoPEを適用完了 result.shape=torch.Size([16384, 8, 128])
RoPEの順伝播完了 query.shape=torch.Size([16384, 16, 128]), key.shape=torch.Size([16384, 8, 128])
Attention層の順伝播開始 q.shape=torch.Size([16384, 16, 128]), k.shape=torch.Size([16384, 8, 128]), v.shape=torch.Size([16384, 8, 128])
現在のコンテキスト取得 _CONTEXT=Context(is_prefill=True, cu_seqlens_q=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32), cu_seqlens_k=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32), max_seqlen_q=4096, max_seqlen_k=4096, slot_mapping=tensor([], device='cuda:0', dtype=torch.int32), context_lens=None, block_tables=None)
コンテキスト情報取得 context.is_prefill=True, context.slot_mapping=tensor([], device='cuda:0', dtype=torch.int32), context.max_seqlen_q=4096, context.cu_seqlens_q=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32), context.max_seqlen_k=4096, context.cu_seqlens_k=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32), context.context_lens=None, context.block_tables=None
PrefillモードでFlashAttentionを実行 context.max_seqlen_q=4096, context.cu_seqlens_q=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32), context.max_seqlen_k=4096, context.cu_seqlens_k=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32)
Attention層の順伝播完了 o.shape=torch.Size([16384, 16, 128])
RowParallelLinearの順伝播開始 入力形状: torch.Size([16384, 2048])
RowParallelLinearの順伝播完了 出力形状: torch.Size([16384, 1024])
Qwen3Attentionの順伝播完了 output.shape=torch.Size([16384, 1024])
RMSNormの順伝播開始 x.shape=torch.Size([16384, 1024]), residual.shape if residual is not None else None=torch.Size([16384, 1024])
残差接続を適用
残差接続を適用完了
Qwen3MLPの順伝播 x.shape=torch.Size([16384, 1024])
出力次元を並列化する線形層の順伝播開始 x.shape=torch.Size([16384, 1024])
出力次元を並列化する線形層の順伝播完了 result.shape=torch.Size([16384, 6144])
SwiGLUを順伝播開始 x.shape=torch.Size([16384, 6144])
チャンク化 x.shape=torch.Size([16384, 3072]), y.shape=torch.Size([16384, 3072])
SwiGLUを順伝播完了 result.shape=torch.Size([16384, 3072])
RowParallelLinearの順伝播開始 入力形状: torch.Size([16384, 3072])
RowParallelLinearの順伝播完了 出力形状: torch.Size([16384, 1024])
Qwen3MLPの順伝播完了 x.shape=torch.Size([16384, 1024])
Qwen3DecoderLayerの順伝播完了 hidden_states.shape=torch.Size([16384, 1024])
Qwen3DecoderLayerの順伝播 positions.shape=torch.Size([16384]) hidden_states.shape=torch.Size([16384, 1024]) residual is None=False
RMSNormの順伝播開始 x.shape=torch.Size([16384, 1024]), residual.shape if residual is not None else None=torch.Size([16384, 1024])
残差接続を適用
残差接続を適用完了
Qwen3Attentionの順伝播 positions.shape=torch.Size([16384]) hidden_states.shape=torch.Size([16384, 1024])
出力次元を並列化する線形層の順伝播開始 x.shape=torch.Size([16384, 1024])
出力次元を並列化する線形層の順伝播完了 result.shape=torch.Size([16384, 4096])
RMSNormの順伝播開始 x.shape=torch.Size([16384, 16, 128]), residual.shape if residual is not None else None=None
正規化開始 x.shape=torch.Size([16384, 16, 128])
正規化完了 x.shape=torch.Size([16384, 16, 128])
RMSNormの順伝播開始 x.shape=torch.Size([16384, 8, 128]), residual.shape if residual is not None else None=None
正規化開始 x.shape=torch.Size([16384, 8, 128])
正規化完了 x.shape=torch.Size([16384, 8, 128])
RoPEの順伝播開始 positions.shape=torch.Size([16384]), query.shape=torch.Size([16384, 16, 128]), key.shape=torch.Size([16384, 8, 128])
RoPEを適用開始 x.shape=torch.Size([16384, 16, 128]), cos.shape=torch.Size([16384, 1, 64]), sin.shape=torch.Size([16384, 1, 64])
チャンク化 x1.shape=torch.Size([16384, 16, 64]), x2.shape=torch.Size([16384, 16, 64])
回転行列適用 y1.shape=torch.Size([16384, 16, 64]), y2.shape=torch.Size([16384, 16, 64])
RoPEを適用完了 result.shape=torch.Size([16384, 16, 128])
RoPEを適用開始 x.shape=torch.Size([16384, 8, 128]), cos.shape=torch.Size([16384, 1, 64]), sin.shape=torch.Size([16384, 1, 64])
チャンク化 x1.shape=torch.Size([16384, 8, 64]), x2.shape=torch.Size([16384, 8, 64])
回転行列適用 y1.shape=torch.Size([16384, 8, 64]), y2.shape=torch.Size([16384, 8, 64])
RoPEを適用完了 result.shape=torch.Size([16384, 8, 128])
RoPEの順伝播完了 query.shape=torch.Size([16384, 16, 128]), key.shape=torch.Size([16384, 8, 128])
Attention層の順伝播開始 q.shape=torch.Size([16384, 16, 128]), k.shape=torch.Size([16384, 8, 128]), v.shape=torch.Size([16384, 8, 128])
現在のコンテキスト取得 _CONTEXT=Context(is_prefill=True, cu_seqlens_q=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32), cu_seqlens_k=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32), max_seqlen_q=4096, max_seqlen_k=4096, slot_mapping=tensor([], device='cuda:0', dtype=torch.int32), context_lens=None, block_tables=None)
コンテキスト情報取得 context.is_prefill=True, context.slot_mapping=tensor([], device='cuda:0', dtype=torch.int32), context.max_seqlen_q=4096, context.cu_seqlens_q=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32), context.max_seqlen_k=4096, context.cu_seqlens_k=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32), context.context_lens=None, context.block_tables=None
PrefillモードでFlashAttentionを実行 context.max_seqlen_q=4096, context.cu_seqlens_q=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32), context.max_seqlen_k=4096, context.cu_seqlens_k=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32)
Attention層の順伝播完了 o.shape=torch.Size([16384, 16, 128])
RowParallelLinearの順伝播開始 入力形状: torch.Size([16384, 2048])
RowParallelLinearの順伝播完了 出力形状: torch.Size([16384, 1024])
Qwen3Attentionの順伝播完了 output.shape=torch.Size([16384, 1024])
RMSNormの順伝播開始 x.shape=torch.Size([16384, 1024]), residual.shape if residual is not None else None=torch.Size([16384, 1024])
残差接続を適用
残差接続を適用完了
Qwen3MLPの順伝播 x.shape=torch.Size([16384, 1024])
出力次元を並列化する線形層の順伝播開始 x.shape=torch.Size([16384, 1024])
出力次元を並列化する線形層の順伝播完了 result.shape=torch.Size([16384, 6144])
SwiGLUを順伝播開始 x.shape=torch.Size([16384, 6144])
チャンク化 x.shape=torch.Size([16384, 3072]), y.shape=torch.Size([16384, 3072])
SwiGLUを順伝播完了 result.shape=torch.Size([16384, 3072])
RowParallelLinearの順伝播開始 入力形状: torch.Size([16384, 3072])
RowParallelLinearの順伝播完了 出力形状: torch.Size([16384, 1024])
Qwen3MLPの順伝播完了 x.shape=torch.Size([16384, 1024])
Qwen3DecoderLayerの順伝播完了 hidden_states.shape=torch.Size([16384, 1024])
Qwen3DecoderLayerの順伝播 positions.shape=torch.Size([16384]) hidden_states.shape=torch.Size([16384, 1024]) residual is None=False
RMSNormの順伝播開始 x.shape=torch.Size([16384, 1024]), residual.shape if residual is not None else None=torch.Size([16384, 1024])
残差接続を適用
残差接続を適用完了
Qwen3Attentionの順伝播 positions.shape=torch.Size([16384]) hidden_states.shape=torch.Size([16384, 1024])
出力次元を並列化する線形層の順伝播開始 x.shape=torch.Size([16384, 1024])
出力次元を並列化する線形層の順伝播完了 result.shape=torch.Size([16384, 4096])
RMSNormの順伝播開始 x.shape=torch.Size([16384, 16, 128]), residual.shape if residual is not None else None=None
正規化開始 x.shape=torch.Size([16384, 16, 128])
正規化完了 x.shape=torch.Size([16384, 16, 128])
RMSNormの順伝播開始 x.shape=torch.Size([16384, 8, 128]), residual.shape if residual is not None else None=None
正規化開始 x.shape=torch.Size([16384, 8, 128])
正規化完了 x.shape=torch.Size([16384, 8, 128])
RoPEの順伝播開始 positions.shape=torch.Size([16384]), query.shape=torch.Size([16384, 16, 128]), key.shape=torch.Size([16384, 8, 128])
RoPEを適用開始 x.shape=torch.Size([16384, 16, 128]), cos.shape=torch.Size([16384, 1, 64]), sin.shape=torch.Size([16384, 1, 64])
チャンク化 x1.shape=torch.Size([16384, 16, 64]), x2.shape=torch.Size([16384, 16, 64])
回転行列適用 y1.shape=torch.Size([16384, 16, 64]), y2.shape=torch.Size([16384, 16, 64])
RoPEを適用完了 result.shape=torch.Size([16384, 16, 128])
RoPEを適用開始 x.shape=torch.Size([16384, 8, 128]), cos.shape=torch.Size([16384, 1, 64]), sin.shape=torch.Size([16384, 1, 64])
チャンク化 x1.shape=torch.Size([16384, 8, 64]), x2.shape=torch.Size([16384, 8, 64])
回転行列適用 y1.shape=torch.Size([16384, 8, 64]), y2.shape=torch.Size([16384, 8, 64])
RoPEを適用完了 result.shape=torch.Size([16384, 8, 128])
RoPEの順伝播完了 query.shape=torch.Size([16384, 16, 128]), key.shape=torch.Size([16384, 8, 128])
Attention層の順伝播開始 q.shape=torch.Size([16384, 16, 128]), k.shape=torch.Size([16384, 8, 128]), v.shape=torch.Size([16384, 8, 128])
現在のコンテキスト取得 _CONTEXT=Context(is_prefill=True, cu_seqlens_q=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32), cu_seqlens_k=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32), max_seqlen_q=4096, max_seqlen_k=4096, slot_mapping=tensor([], device='cuda:0', dtype=torch.int32), context_lens=None, block_tables=None)
コンテキスト情報取得 context.is_prefill=True, context.slot_mapping=tensor([], device='cuda:0', dtype=torch.int32), context.max_seqlen_q=4096, context.cu_seqlens_q=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32), context.max_seqlen_k=4096, context.cu_seqlens_k=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32), context.context_lens=None, context.block_tables=None
PrefillモードでFlashAttentionを実行 context.max_seqlen_q=4096, context.cu_seqlens_q=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32), context.max_seqlen_k=4096, context.cu_seqlens_k=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32)
Attention層の順伝播完了 o.shape=torch.Size([16384, 16, 128])
RowParallelLinearの順伝播開始 入力形状: torch.Size([16384, 2048])
RowParallelLinearの順伝播完了 出力形状: torch.Size([16384, 1024])
Qwen3Attentionの順伝播完了 output.shape=torch.Size([16384, 1024])
RMSNormの順伝播開始 x.shape=torch.Size([16384, 1024]), residual.shape if residual is not None else None=torch.Size([16384, 1024])
残差接続を適用
残差接続を適用完了
Qwen3MLPの順伝播 x.shape=torch.Size([16384, 1024])
出力次元を並列化する線形層の順伝播開始 x.shape=torch.Size([16384, 1024])
出力次元を並列化する線形層の順伝播完了 result.shape=torch.Size([16384, 6144])
SwiGLUを順伝播開始 x.shape=torch.Size([16384, 6144])
チャンク化 x.shape=torch.Size([16384, 3072]), y.shape=torch.Size([16384, 3072])
SwiGLUを順伝播完了 result.shape=torch.Size([16384, 3072])
RowParallelLinearの順伝播開始 入力形状: torch.Size([16384, 3072])
RowParallelLinearの順伝播完了 出力形状: torch.Size([16384, 1024])
Qwen3MLPの順伝播完了 x.shape=torch.Size([16384, 1024])
Qwen3DecoderLayerの順伝播完了 hidden_states.shape=torch.Size([16384, 1024])
Qwen3DecoderLayerの順伝播 positions.shape=torch.Size([16384]) hidden_states.shape=torch.Size([16384, 1024]) residual is None=False
RMSNormの順伝播開始 x.shape=torch.Size([16384, 1024]), residual.shape if residual is not None else None=torch.Size([16384, 1024])
残差接続を適用
残差接続を適用完了
Qwen3Attentionの順伝播 positions.shape=torch.Size([16384]) hidden_states.shape=torch.Size([16384, 1024])
出力次元を並列化する線形層の順伝播開始 x.shape=torch.Size([16384, 1024])
出力次元を並列化する線形層の順伝播完了 result.shape=torch.Size([16384, 4096])
RMSNormの順伝播開始 x.shape=torch.Size([16384, 16, 128]), residual.shape if residual is not None else None=None
正規化開始 x.shape=torch.Size([16384, 16, 128])
正規化完了 x.shape=torch.Size([16384, 16, 128])
RMSNormの順伝播開始 x.shape=torch.Size([16384, 8, 128]), residual.shape if residual is not None else None=None
正規化開始 x.shape=torch.Size([16384, 8, 128])
正規化完了 x.shape=torch.Size([16384, 8, 128])
RoPEの順伝播開始 positions.shape=torch.Size([16384]), query.shape=torch.Size([16384, 16, 128]), key.shape=torch.Size([16384, 8, 128])
RoPEを適用開始 x.shape=torch.Size([16384, 16, 128]), cos.shape=torch.Size([16384, 1, 64]), sin.shape=torch.Size([16384, 1, 64])
チャンク化 x1.shape=torch.Size([16384, 16, 64]), x2.shape=torch.Size([16384, 16, 64])
回転行列適用 y1.shape=torch.Size([16384, 16, 64]), y2.shape=torch.Size([16384, 16, 64])
RoPEを適用完了 result.shape=torch.Size([16384, 16, 128])
RoPEを適用開始 x.shape=torch.Size([16384, 8, 128]), cos.shape=torch.Size([16384, 1, 64]), sin.shape=torch.Size([16384, 1, 64])
チャンク化 x1.shape=torch.Size([16384, 8, 64]), x2.shape=torch.Size([16384, 8, 64])
回転行列適用 y1.shape=torch.Size([16384, 8, 64]), y2.shape=torch.Size([16384, 8, 64])
RoPEを適用完了 result.shape=torch.Size([16384, 8, 128])
RoPEの順伝播完了 query.shape=torch.Size([16384, 16, 128]), key.shape=torch.Size([16384, 8, 128])
Attention層の順伝播開始 q.shape=torch.Size([16384, 16, 128]), k.shape=torch.Size([16384, 8, 128]), v.shape=torch.Size([16384, 8, 128])
現在のコンテキスト取得 _CONTEXT=Context(is_prefill=True, cu_seqlens_q=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32), cu_seqlens_k=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32), max_seqlen_q=4096, max_seqlen_k=4096, slot_mapping=tensor([], device='cuda:0', dtype=torch.int32), context_lens=None, block_tables=None)
コンテキスト情報取得 context.is_prefill=True, context.slot_mapping=tensor([], device='cuda:0', dtype=torch.int32), context.max_seqlen_q=4096, context.cu_seqlens_q=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32), context.max_seqlen_k=4096, context.cu_seqlens_k=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32), context.context_lens=None, context.block_tables=None
PrefillモードでFlashAttentionを実行 context.max_seqlen_q=4096, context.cu_seqlens_q=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32), context.max_seqlen_k=4096, context.cu_seqlens_k=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32)
Attention層の順伝播完了 o.shape=torch.Size([16384, 16, 128])
RowParallelLinearの順伝播開始 入力形状: torch.Size([16384, 2048])
RowParallelLinearの順伝播完了 出力形状: torch.Size([16384, 1024])
Qwen3Attentionの順伝播完了 output.shape=torch.Size([16384, 1024])
RMSNormの順伝播開始 x.shape=torch.Size([16384, 1024]), residual.shape if residual is not None else None=torch.Size([16384, 1024])
残差接続を適用
残差接続を適用完了
Qwen3MLPの順伝播 x.shape=torch.Size([16384, 1024])
出力次元を並列化する線形層の順伝播開始 x.shape=torch.Size([16384, 1024])
出力次元を並列化する線形層の順伝播完了 result.shape=torch.Size([16384, 6144])
SwiGLUを順伝播開始 x.shape=torch.Size([16384, 6144])
チャンク化 x.shape=torch.Size([16384, 3072]), y.shape=torch.Size([16384, 3072])
SwiGLUを順伝播完了 result.shape=torch.Size([16384, 3072])
RowParallelLinearの順伝播開始 入力形状: torch.Size([16384, 3072])
RowParallelLinearの順伝播完了 出力形状: torch.Size([16384, 1024])
Qwen3MLPの順伝播完了 x.shape=torch.Size([16384, 1024])
Qwen3DecoderLayerの順伝播完了 hidden_states.shape=torch.Size([16384, 1024])
Qwen3DecoderLayerの順伝播 positions.shape=torch.Size([16384]) hidden_states.shape=torch.Size([16384, 1024]) residual is None=False
RMSNormの順伝播開始 x.shape=torch.Size([16384, 1024]), residual.shape if residual is not None else None=torch.Size([16384, 1024])
残差接続を適用
残差接続を適用完了
Qwen3Attentionの順伝播 positions.shape=torch.Size([16384]) hidden_states.shape=torch.Size([16384, 1024])
出力次元を並列化する線形層の順伝播開始 x.shape=torch.Size([16384, 1024])
出力次元を並列化する線形層の順伝播完了 result.shape=torch.Size([16384, 4096])
RMSNormの順伝播開始 x.shape=torch.Size([16384, 16, 128]), residual.shape if residual is not None else None=None
正規化開始 x.shape=torch.Size([16384, 16, 128])
正規化完了 x.shape=torch.Size([16384, 16, 128])
RMSNormの順伝播開始 x.shape=torch.Size([16384, 8, 128]), residual.shape if residual is not None else None=None
正規化開始 x.shape=torch.Size([16384, 8, 128])
正規化完了 x.shape=torch.Size([16384, 8, 128])
RoPEの順伝播開始 positions.shape=torch.Size([16384]), query.shape=torch.Size([16384, 16, 128]), key.shape=torch.Size([16384, 8, 128])
RoPEを適用開始 x.shape=torch.Size([16384, 16, 128]), cos.shape=torch.Size([16384, 1, 64]), sin.shape=torch.Size([16384, 1, 64])
チャンク化 x1.shape=torch.Size([16384, 16, 64]), x2.shape=torch.Size([16384, 16, 64])
回転行列適用 y1.shape=torch.Size([16384, 16, 64]), y2.shape=torch.Size([16384, 16, 64])
RoPEを適用完了 result.shape=torch.Size([16384, 16, 128])
RoPEを適用開始 x.shape=torch.Size([16384, 8, 128]), cos.shape=torch.Size([16384, 1, 64]), sin.shape=torch.Size([16384, 1, 64])
チャンク化 x1.shape=torch.Size([16384, 8, 64]), x2.shape=torch.Size([16384, 8, 64])
回転行列適用 y1.shape=torch.Size([16384, 8, 64]), y2.shape=torch.Size([16384, 8, 64])
RoPEを適用完了 result.shape=torch.Size([16384, 8, 128])
RoPEの順伝播完了 query.shape=torch.Size([16384, 16, 128]), key.shape=torch.Size([16384, 8, 128])
Attention層の順伝播開始 q.shape=torch.Size([16384, 16, 128]), k.shape=torch.Size([16384, 8, 128]), v.shape=torch.Size([16384, 8, 128])
現在のコンテキスト取得 _CONTEXT=Context(is_prefill=True, cu_seqlens_q=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32), cu_seqlens_k=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32), max_seqlen_q=4096, max_seqlen_k=4096, slot_mapping=tensor([], device='cuda:0', dtype=torch.int32), context_lens=None, block_tables=None)
コンテキスト情報取得 context.is_prefill=True, context.slot_mapping=tensor([], device='cuda:0', dtype=torch.int32), context.max_seqlen_q=4096, context.cu_seqlens_q=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32), context.max_seqlen_k=4096, context.cu_seqlens_k=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32), context.context_lens=None, context.block_tables=None
PrefillモードでFlashAttentionを実行 context.max_seqlen_q=4096, context.cu_seqlens_q=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32), context.max_seqlen_k=4096, context.cu_seqlens_k=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32)
Attention層の順伝播完了 o.shape=torch.Size([16384, 16, 128])
RowParallelLinearの順伝播開始 入力形状: torch.Size([16384, 2048])
RowParallelLinearの順伝播完了 出力形状: torch.Size([16384, 1024])
Qwen3Attentionの順伝播完了 output.shape=torch.Size([16384, 1024])
RMSNormの順伝播開始 x.shape=torch.Size([16384, 1024]), residual.shape if residual is not None else None=torch.Size([16384, 1024])
残差接続を適用
残差接続を適用完了
Qwen3MLPの順伝播 x.shape=torch.Size([16384, 1024])
出力次元を並列化する線形層の順伝播開始 x.shape=torch.Size([16384, 1024])
出力次元を並列化する線形層の順伝播完了 result.shape=torch.Size([16384, 6144])
SwiGLUを順伝播開始 x.shape=torch.Size([16384, 6144])
チャンク化 x.shape=torch.Size([16384, 3072]), y.shape=torch.Size([16384, 3072])
SwiGLUを順伝播完了 result.shape=torch.Size([16384, 3072])
RowParallelLinearの順伝播開始 入力形状: torch.Size([16384, 3072])
RowParallelLinearの順伝播完了 出力形状: torch.Size([16384, 1024])
Qwen3MLPの順伝播完了 x.shape=torch.Size([16384, 1024])
Qwen3DecoderLayerの順伝播完了 hidden_states.shape=torch.Size([16384, 1024])
Qwen3DecoderLayerの順伝播 positions.shape=torch.Size([16384]) hidden_states.shape=torch.Size([16384, 1024]) residual is None=False
RMSNormの順伝播開始 x.shape=torch.Size([16384, 1024]), residual.shape if residual is not None else None=torch.Size([16384, 1024])
残差接続を適用
残差接続を適用完了
Qwen3Attentionの順伝播 positions.shape=torch.Size([16384]) hidden_states.shape=torch.Size([16384, 1024])
出力次元を並列化する線形層の順伝播開始 x.shape=torch.Size([16384, 1024])
出力次元を並列化する線形層の順伝播完了 result.shape=torch.Size([16384, 4096])
RMSNormの順伝播開始 x.shape=torch.Size([16384, 16, 128]), residual.shape if residual is not None else None=None
正規化開始 x.shape=torch.Size([16384, 16, 128])
正規化完了 x.shape=torch.Size([16384, 16, 128])
RMSNormの順伝播開始 x.shape=torch.Size([16384, 8, 128]), residual.shape if residual is not None else None=None
正規化開始 x.shape=torch.Size([16384, 8, 128])
正規化完了 x.shape=torch.Size([16384, 8, 128])
RoPEの順伝播開始 positions.shape=torch.Size([16384]), query.shape=torch.Size([16384, 16, 128]), key.shape=torch.Size([16384, 8, 128])
RoPEを適用開始 x.shape=torch.Size([16384, 16, 128]), cos.shape=torch.Size([16384, 1, 64]), sin.shape=torch.Size([16384, 1, 64])
チャンク化 x1.shape=torch.Size([16384, 16, 64]), x2.shape=torch.Size([16384, 16, 64])
回転行列適用 y1.shape=torch.Size([16384, 16, 64]), y2.shape=torch.Size([16384, 16, 64])
RoPEを適用完了 result.shape=torch.Size([16384, 16, 128])
RoPEを適用開始 x.shape=torch.Size([16384, 8, 128]), cos.shape=torch.Size([16384, 1, 64]), sin.shape=torch.Size([16384, 1, 64])
チャンク化 x1.shape=torch.Size([16384, 8, 64]), x2.shape=torch.Size([16384, 8, 64])
回転行列適用 y1.shape=torch.Size([16384, 8, 64]), y2.shape=torch.Size([16384, 8, 64])
RoPEを適用完了 result.shape=torch.Size([16384, 8, 128])
RoPEの順伝播完了 query.shape=torch.Size([16384, 16, 128]), key.shape=torch.Size([16384, 8, 128])
Attention層の順伝播開始 q.shape=torch.Size([16384, 16, 128]), k.shape=torch.Size([16384, 8, 128]), v.shape=torch.Size([16384, 8, 128])
現在のコンテキスト取得 _CONTEXT=Context(is_prefill=True, cu_seqlens_q=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32), cu_seqlens_k=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32), max_seqlen_q=4096, max_seqlen_k=4096, slot_mapping=tensor([], device='cuda:0', dtype=torch.int32), context_lens=None, block_tables=None)
コンテキスト情報取得 context.is_prefill=True, context.slot_mapping=tensor([], device='cuda:0', dtype=torch.int32), context.max_seqlen_q=4096, context.cu_seqlens_q=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32), context.max_seqlen_k=4096, context.cu_seqlens_k=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32), context.context_lens=None, context.block_tables=None
PrefillモードでFlashAttentionを実行 context.max_seqlen_q=4096, context.cu_seqlens_q=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32), context.max_seqlen_k=4096, context.cu_seqlens_k=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32)
Attention層の順伝播完了 o.shape=torch.Size([16384, 16, 128])
RowParallelLinearの順伝播開始 入力形状: torch.Size([16384, 2048])
RowParallelLinearの順伝播完了 出力形状: torch.Size([16384, 1024])
Qwen3Attentionの順伝播完了 output.shape=torch.Size([16384, 1024])
RMSNormの順伝播開始 x.shape=torch.Size([16384, 1024]), residual.shape if residual is not None else None=torch.Size([16384, 1024])
残差接続を適用
残差接続を適用完了
Qwen3MLPの順伝播 x.shape=torch.Size([16384, 1024])
出力次元を並列化する線形層の順伝播開始 x.shape=torch.Size([16384, 1024])
出力次元を並列化する線形層の順伝播完了 result.shape=torch.Size([16384, 6144])
SwiGLUを順伝播開始 x.shape=torch.Size([16384, 6144])
チャンク化 x.shape=torch.Size([16384, 3072]), y.shape=torch.Size([16384, 3072])
SwiGLUを順伝播完了 result.shape=torch.Size([16384, 3072])
RowParallelLinearの順伝播開始 入力形状: torch.Size([16384, 3072])
RowParallelLinearの順伝播完了 出力形状: torch.Size([16384, 1024])
Qwen3MLPの順伝播完了 x.shape=torch.Size([16384, 1024])
Qwen3DecoderLayerの順伝播完了 hidden_states.shape=torch.Size([16384, 1024])
Qwen3DecoderLayerの順伝播 positions.shape=torch.Size([16384]) hidden_states.shape=torch.Size([16384, 1024]) residual is None=False
RMSNormの順伝播開始 x.shape=torch.Size([16384, 1024]), residual.shape if residual is not None else None=torch.Size([16384, 1024])
残差接続を適用
残差接続を適用完了
Qwen3Attentionの順伝播 positions.shape=torch.Size([16384]) hidden_states.shape=torch.Size([16384, 1024])
出力次元を並列化する線形層の順伝播開始 x.shape=torch.Size([16384, 1024])
出力次元を並列化する線形層の順伝播完了 result.shape=torch.Size([16384, 4096])
RMSNormの順伝播開始 x.shape=torch.Size([16384, 16, 128]), residual.shape if residual is not None else None=None
正規化開始 x.shape=torch.Size([16384, 16, 128])
正規化完了 x.shape=torch.Size([16384, 16, 128])
RMSNormの順伝播開始 x.shape=torch.Size([16384, 8, 128]), residual.shape if residual is not None else None=None
正規化開始 x.shape=torch.Size([16384, 8, 128])
正規化完了 x.shape=torch.Size([16384, 8, 128])
RoPEの順伝播開始 positions.shape=torch.Size([16384]), query.shape=torch.Size([16384, 16, 128]), key.shape=torch.Size([16384, 8, 128])
RoPEを適用開始 x.shape=torch.Size([16384, 16, 128]), cos.shape=torch.Size([16384, 1, 64]), sin.shape=torch.Size([16384, 1, 64])
チャンク化 x1.shape=torch.Size([16384, 16, 64]), x2.shape=torch.Size([16384, 16, 64])
回転行列適用 y1.shape=torch.Size([16384, 16, 64]), y2.shape=torch.Size([16384, 16, 64])
RoPEを適用完了 result.shape=torch.Size([16384, 16, 128])
RoPEを適用開始 x.shape=torch.Size([16384, 8, 128]), cos.shape=torch.Size([16384, 1, 64]), sin.shape=torch.Size([16384, 1, 64])
チャンク化 x1.shape=torch.Size([16384, 8, 64]), x2.shape=torch.Size([16384, 8, 64])
回転行列適用 y1.shape=torch.Size([16384, 8, 64]), y2.shape=torch.Size([16384, 8, 64])
RoPEを適用完了 result.shape=torch.Size([16384, 8, 128])
RoPEの順伝播完了 query.shape=torch.Size([16384, 16, 128]), key.shape=torch.Size([16384, 8, 128])
Attention層の順伝播開始 q.shape=torch.Size([16384, 16, 128]), k.shape=torch.Size([16384, 8, 128]), v.shape=torch.Size([16384, 8, 128])
現在のコンテキスト取得 _CONTEXT=Context(is_prefill=True, cu_seqlens_q=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32), cu_seqlens_k=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32), max_seqlen_q=4096, max_seqlen_k=4096, slot_mapping=tensor([], device='cuda:0', dtype=torch.int32), context_lens=None, block_tables=None)
コンテキスト情報取得 context.is_prefill=True, context.slot_mapping=tensor([], device='cuda:0', dtype=torch.int32), context.max_seqlen_q=4096, context.cu_seqlens_q=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32), context.max_seqlen_k=4096, context.cu_seqlens_k=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32), context.context_lens=None, context.block_tables=None
PrefillモードでFlashAttentionを実行 context.max_seqlen_q=4096, context.cu_seqlens_q=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32), context.max_seqlen_k=4096, context.cu_seqlens_k=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32)
Attention層の順伝播完了 o.shape=torch.Size([16384, 16, 128])
RowParallelLinearの順伝播開始 入力形状: torch.Size([16384, 2048])
RowParallelLinearの順伝播完了 出力形状: torch.Size([16384, 1024])
Qwen3Attentionの順伝播完了 output.shape=torch.Size([16384, 1024])
RMSNormの順伝播開始 x.shape=torch.Size([16384, 1024]), residual.shape if residual is not None else None=torch.Size([16384, 1024])
残差接続を適用
残差接続を適用完了
Qwen3MLPの順伝播 x.shape=torch.Size([16384, 1024])
出力次元を並列化する線形層の順伝播開始 x.shape=torch.Size([16384, 1024])
出力次元を並列化する線形層の順伝播完了 result.shape=torch.Size([16384, 6144])
SwiGLUを順伝播開始 x.shape=torch.Size([16384, 6144])
チャンク化 x.shape=torch.Size([16384, 3072]), y.shape=torch.Size([16384, 3072])
SwiGLUを順伝播完了 result.shape=torch.Size([16384, 3072])
RowParallelLinearの順伝播開始 入力形状: torch.Size([16384, 3072])
RowParallelLinearの順伝播完了 出力形状: torch.Size([16384, 1024])
Qwen3MLPの順伝播完了 x.shape=torch.Size([16384, 1024])
Qwen3DecoderLayerの順伝播完了 hidden_states.shape=torch.Size([16384, 1024])
Qwen3DecoderLayerの順伝播 positions.shape=torch.Size([16384]) hidden_states.shape=torch.Size([16384, 1024]) residual is None=False
RMSNormの順伝播開始 x.shape=torch.Size([16384, 1024]), residual.shape if residual is not None else None=torch.Size([16384, 1024])
残差接続を適用
残差接続を適用完了
Qwen3Attentionの順伝播 positions.shape=torch.Size([16384]) hidden_states.shape=torch.Size([16384, 1024])
出力次元を並列化する線形層の順伝播開始 x.shape=torch.Size([16384, 1024])
出力次元を並列化する線形層の順伝播完了 result.shape=torch.Size([16384, 4096])
RMSNormの順伝播開始 x.shape=torch.Size([16384, 16, 128]), residual.shape if residual is not None else None=None
正規化開始 x.shape=torch.Size([16384, 16, 128])
正規化完了 x.shape=torch.Size([16384, 16, 128])
RMSNormの順伝播開始 x.shape=torch.Size([16384, 8, 128]), residual.shape if residual is not None else None=None
正規化開始 x.shape=torch.Size([16384, 8, 128])
正規化完了 x.shape=torch.Size([16384, 8, 128])
RoPEの順伝播開始 positions.shape=torch.Size([16384]), query.shape=torch.Size([16384, 16, 128]), key.shape=torch.Size([16384, 8, 128])
RoPEを適用開始 x.shape=torch.Size([16384, 16, 128]), cos.shape=torch.Size([16384, 1, 64]), sin.shape=torch.Size([16384, 1, 64])
チャンク化 x1.shape=torch.Size([16384, 16, 64]), x2.shape=torch.Size([16384, 16, 64])
回転行列適用 y1.shape=torch.Size([16384, 16, 64]), y2.shape=torch.Size([16384, 16, 64])
RoPEを適用完了 result.shape=torch.Size([16384, 16, 128])
RoPEを適用開始 x.shape=torch.Size([16384, 8, 128]), cos.shape=torch.Size([16384, 1, 64]), sin.shape=torch.Size([16384, 1, 64])
チャンク化 x1.shape=torch.Size([16384, 8, 64]), x2.shape=torch.Size([16384, 8, 64])
回転行列適用 y1.shape=torch.Size([16384, 8, 64]), y2.shape=torch.Size([16384, 8, 64])
RoPEを適用完了 result.shape=torch.Size([16384, 8, 128])
RoPEの順伝播完了 query.shape=torch.Size([16384, 16, 128]), key.shape=torch.Size([16384, 8, 128])
Attention層の順伝播開始 q.shape=torch.Size([16384, 16, 128]), k.shape=torch.Size([16384, 8, 128]), v.shape=torch.Size([16384, 8, 128])
現在のコンテキスト取得 _CONTEXT=Context(is_prefill=True, cu_seqlens_q=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32), cu_seqlens_k=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32), max_seqlen_q=4096, max_seqlen_k=4096, slot_mapping=tensor([], device='cuda:0', dtype=torch.int32), context_lens=None, block_tables=None)
コンテキスト情報取得 context.is_prefill=True, context.slot_mapping=tensor([], device='cuda:0', dtype=torch.int32), context.max_seqlen_q=4096, context.cu_seqlens_q=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32), context.max_seqlen_k=4096, context.cu_seqlens_k=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32), context.context_lens=None, context.block_tables=None
PrefillモードでFlashAttentionを実行 context.max_seqlen_q=4096, context.cu_seqlens_q=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32), context.max_seqlen_k=4096, context.cu_seqlens_k=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32)
Attention層の順伝播完了 o.shape=torch.Size([16384, 16, 128])
RowParallelLinearの順伝播開始 入力形状: torch.Size([16384, 2048])
RowParallelLinearの順伝播完了 出力形状: torch.Size([16384, 1024])
Qwen3Attentionの順伝播完了 output.shape=torch.Size([16384, 1024])
RMSNormの順伝播開始 x.shape=torch.Size([16384, 1024]), residual.shape if residual is not None else None=torch.Size([16384, 1024])
残差接続を適用
残差接続を適用完了
Qwen3MLPの順伝播 x.shape=torch.Size([16384, 1024])
出力次元を並列化する線形層の順伝播開始 x.shape=torch.Size([16384, 1024])
出力次元を並列化する線形層の順伝播完了 result.shape=torch.Size([16384, 6144])
SwiGLUを順伝播開始 x.shape=torch.Size([16384, 6144])
チャンク化 x.shape=torch.Size([16384, 3072]), y.shape=torch.Size([16384, 3072])
SwiGLUを順伝播完了 result.shape=torch.Size([16384, 3072])
RowParallelLinearの順伝播開始 入力形状: torch.Size([16384, 3072])
RowParallelLinearの順伝播完了 出力形状: torch.Size([16384, 1024])
Qwen3MLPの順伝播完了 x.shape=torch.Size([16384, 1024])
Qwen3DecoderLayerの順伝播完了 hidden_states.shape=torch.Size([16384, 1024])
Qwen3DecoderLayerの順伝播 positions.shape=torch.Size([16384]) hidden_states.shape=torch.Size([16384, 1024]) residual is None=False
RMSNormの順伝播開始 x.shape=torch.Size([16384, 1024]), residual.shape if residual is not None else None=torch.Size([16384, 1024])
残差接続を適用
残差接続を適用完了
Qwen3Attentionの順伝播 positions.shape=torch.Size([16384]) hidden_states.shape=torch.Size([16384, 1024])
出力次元を並列化する線形層の順伝播開始 x.shape=torch.Size([16384, 1024])
出力次元を並列化する線形層の順伝播完了 result.shape=torch.Size([16384, 4096])
RMSNormの順伝播開始 x.shape=torch.Size([16384, 16, 128]), residual.shape if residual is not None else None=None
正規化開始 x.shape=torch.Size([16384, 16, 128])
正規化完了 x.shape=torch.Size([16384, 16, 128])
RMSNormの順伝播開始 x.shape=torch.Size([16384, 8, 128]), residual.shape if residual is not None else None=None
正規化開始 x.shape=torch.Size([16384, 8, 128])
正規化完了 x.shape=torch.Size([16384, 8, 128])
RoPEの順伝播開始 positions.shape=torch.Size([16384]), query.shape=torch.Size([16384, 16, 128]), key.shape=torch.Size([16384, 8, 128])
RoPEを適用開始 x.shape=torch.Size([16384, 16, 128]), cos.shape=torch.Size([16384, 1, 64]), sin.shape=torch.Size([16384, 1, 64])
チャンク化 x1.shape=torch.Size([16384, 16, 64]), x2.shape=torch.Size([16384, 16, 64])
回転行列適用 y1.shape=torch.Size([16384, 16, 64]), y2.shape=torch.Size([16384, 16, 64])
RoPEを適用完了 result.shape=torch.Size([16384, 16, 128])
RoPEを適用開始 x.shape=torch.Size([16384, 8, 128]), cos.shape=torch.Size([16384, 1, 64]), sin.shape=torch.Size([16384, 1, 64])
チャンク化 x1.shape=torch.Size([16384, 8, 64]), x2.shape=torch.Size([16384, 8, 64])
回転行列適用 y1.shape=torch.Size([16384, 8, 64]), y2.shape=torch.Size([16384, 8, 64])
RoPEを適用完了 result.shape=torch.Size([16384, 8, 128])
RoPEの順伝播完了 query.shape=torch.Size([16384, 16, 128]), key.shape=torch.Size([16384, 8, 128])
Attention層の順伝播開始 q.shape=torch.Size([16384, 16, 128]), k.shape=torch.Size([16384, 8, 128]), v.shape=torch.Size([16384, 8, 128])
現在のコンテキスト取得 _CONTEXT=Context(is_prefill=True, cu_seqlens_q=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32), cu_seqlens_k=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32), max_seqlen_q=4096, max_seqlen_k=4096, slot_mapping=tensor([], device='cuda:0', dtype=torch.int32), context_lens=None, block_tables=None)
コンテキスト情報取得 context.is_prefill=True, context.slot_mapping=tensor([], device='cuda:0', dtype=torch.int32), context.max_seqlen_q=4096, context.cu_seqlens_q=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32), context.max_seqlen_k=4096, context.cu_seqlens_k=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32), context.context_lens=None, context.block_tables=None
PrefillモードでFlashAttentionを実行 context.max_seqlen_q=4096, context.cu_seqlens_q=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32), context.max_seqlen_k=4096, context.cu_seqlens_k=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32)
Attention層の順伝播完了 o.shape=torch.Size([16384, 16, 128])
RowParallelLinearの順伝播開始 入力形状: torch.Size([16384, 2048])
RowParallelLinearの順伝播完了 出力形状: torch.Size([16384, 1024])
Qwen3Attentionの順伝播完了 output.shape=torch.Size([16384, 1024])
RMSNormの順伝播開始 x.shape=torch.Size([16384, 1024]), residual.shape if residual is not None else None=torch.Size([16384, 1024])
残差接続を適用
残差接続を適用完了
Qwen3MLPの順伝播 x.shape=torch.Size([16384, 1024])
出力次元を並列化する線形層の順伝播開始 x.shape=torch.Size([16384, 1024])
出力次元を並列化する線形層の順伝播完了 result.shape=torch.Size([16384, 6144])
SwiGLUを順伝播開始 x.shape=torch.Size([16384, 6144])
チャンク化 x.shape=torch.Size([16384, 3072]), y.shape=torch.Size([16384, 3072])
SwiGLUを順伝播完了 result.shape=torch.Size([16384, 3072])
RowParallelLinearの順伝播開始 入力形状: torch.Size([16384, 3072])
RowParallelLinearの順伝播完了 出力形状: torch.Size([16384, 1024])
Qwen3MLPの順伝播完了 x.shape=torch.Size([16384, 1024])
Qwen3DecoderLayerの順伝播完了 hidden_states.shape=torch.Size([16384, 1024])
Qwen3DecoderLayerの順伝播 positions.shape=torch.Size([16384]) hidden_states.shape=torch.Size([16384, 1024]) residual is None=False
RMSNormの順伝播開始 x.shape=torch.Size([16384, 1024]), residual.shape if residual is not None else None=torch.Size([16384, 1024])
残差接続を適用
残差接続を適用完了
Qwen3Attentionの順伝播 positions.shape=torch.Size([16384]) hidden_states.shape=torch.Size([16384, 1024])
出力次元を並列化する線形層の順伝播開始 x.shape=torch.Size([16384, 1024])
出力次元を並列化する線形層の順伝播完了 result.shape=torch.Size([16384, 4096])
RMSNormの順伝播開始 x.shape=torch.Size([16384, 16, 128]), residual.shape if residual is not None else None=None
正規化開始 x.shape=torch.Size([16384, 16, 128])
正規化完了 x.shape=torch.Size([16384, 16, 128])
RMSNormの順伝播開始 x.shape=torch.Size([16384, 8, 128]), residual.shape if residual is not None else None=None
正規化開始 x.shape=torch.Size([16384, 8, 128])
正規化完了 x.shape=torch.Size([16384, 8, 128])
RoPEの順伝播開始 positions.shape=torch.Size([16384]), query.shape=torch.Size([16384, 16, 128]), key.shape=torch.Size([16384, 8, 128])
RoPEを適用開始 x.shape=torch.Size([16384, 16, 128]), cos.shape=torch.Size([16384, 1, 64]), sin.shape=torch.Size([16384, 1, 64])
チャンク化 x1.shape=torch.Size([16384, 16, 64]), x2.shape=torch.Size([16384, 16, 64])
回転行列適用 y1.shape=torch.Size([16384, 16, 64]), y2.shape=torch.Size([16384, 16, 64])
RoPEを適用完了 result.shape=torch.Size([16384, 16, 128])
RoPEを適用開始 x.shape=torch.Size([16384, 8, 128]), cos.shape=torch.Size([16384, 1, 64]), sin.shape=torch.Size([16384, 1, 64])
チャンク化 x1.shape=torch.Size([16384, 8, 64]), x2.shape=torch.Size([16384, 8, 64])
回転行列適用 y1.shape=torch.Size([16384, 8, 64]), y2.shape=torch.Size([16384, 8, 64])
RoPEを適用完了 result.shape=torch.Size([16384, 8, 128])
RoPEの順伝播完了 query.shape=torch.Size([16384, 16, 128]), key.shape=torch.Size([16384, 8, 128])
Attention層の順伝播開始 q.shape=torch.Size([16384, 16, 128]), k.shape=torch.Size([16384, 8, 128]), v.shape=torch.Size([16384, 8, 128])
現在のコンテキスト取得 _CONTEXT=Context(is_prefill=True, cu_seqlens_q=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32), cu_seqlens_k=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32), max_seqlen_q=4096, max_seqlen_k=4096, slot_mapping=tensor([], device='cuda:0', dtype=torch.int32), context_lens=None, block_tables=None)
コンテキスト情報取得 context.is_prefill=True, context.slot_mapping=tensor([], device='cuda:0', dtype=torch.int32), context.max_seqlen_q=4096, context.cu_seqlens_q=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32), context.max_seqlen_k=4096, context.cu_seqlens_k=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32), context.context_lens=None, context.block_tables=None
PrefillモードでFlashAttentionを実行 context.max_seqlen_q=4096, context.cu_seqlens_q=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32), context.max_seqlen_k=4096, context.cu_seqlens_k=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32)
Attention層の順伝播完了 o.shape=torch.Size([16384, 16, 128])
RowParallelLinearの順伝播開始 入力形状: torch.Size([16384, 2048])
RowParallelLinearの順伝播完了 出力形状: torch.Size([16384, 1024])
Qwen3Attentionの順伝播完了 output.shape=torch.Size([16384, 1024])
RMSNormの順伝播開始 x.shape=torch.Size([16384, 1024]), residual.shape if residual is not None else None=torch.Size([16384, 1024])
残差接続を適用
残差接続を適用完了
Qwen3MLPの順伝播 x.shape=torch.Size([16384, 1024])
出力次元を並列化する線形層の順伝播開始 x.shape=torch.Size([16384, 1024])
出力次元を並列化する線形層の順伝播完了 result.shape=torch.Size([16384, 6144])
SwiGLUを順伝播開始 x.shape=torch.Size([16384, 6144])
チャンク化 x.shape=torch.Size([16384, 3072]), y.shape=torch.Size([16384, 3072])
SwiGLUを順伝播完了 result.shape=torch.Size([16384, 3072])
RowParallelLinearの順伝播開始 入力形状: torch.Size([16384, 3072])
RowParallelLinearの順伝播完了 出力形状: torch.Size([16384, 1024])
Qwen3MLPの順伝播完了 x.shape=torch.Size([16384, 1024])
Qwen3DecoderLayerの順伝播完了 hidden_states.shape=torch.Size([16384, 1024])
Qwen3DecoderLayerの順伝播 positions.shape=torch.Size([16384]) hidden_states.shape=torch.Size([16384, 1024]) residual is None=False
RMSNormの順伝播開始 x.shape=torch.Size([16384, 1024]), residual.shape if residual is not None else None=torch.Size([16384, 1024])
残差接続を適用
残差接続を適用完了
Qwen3Attentionの順伝播 positions.shape=torch.Size([16384]) hidden_states.shape=torch.Size([16384, 1024])
出力次元を並列化する線形層の順伝播開始 x.shape=torch.Size([16384, 1024])
出力次元を並列化する線形層の順伝播完了 result.shape=torch.Size([16384, 4096])
RMSNormの順伝播開始 x.shape=torch.Size([16384, 16, 128]), residual.shape if residual is not None else None=None
正規化開始 x.shape=torch.Size([16384, 16, 128])
正規化完了 x.shape=torch.Size([16384, 16, 128])
RMSNormの順伝播開始 x.shape=torch.Size([16384, 8, 128]), residual.shape if residual is not None else None=None
正規化開始 x.shape=torch.Size([16384, 8, 128])
正規化完了 x.shape=torch.Size([16384, 8, 128])
RoPEの順伝播開始 positions.shape=torch.Size([16384]), query.shape=torch.Size([16384, 16, 128]), key.shape=torch.Size([16384, 8, 128])
RoPEを適用開始 x.shape=torch.Size([16384, 16, 128]), cos.shape=torch.Size([16384, 1, 64]), sin.shape=torch.Size([16384, 1, 64])
チャンク化 x1.shape=torch.Size([16384, 16, 64]), x2.shape=torch.Size([16384, 16, 64])
回転行列適用 y1.shape=torch.Size([16384, 16, 64]), y2.shape=torch.Size([16384, 16, 64])
RoPEを適用完了 result.shape=torch.Size([16384, 16, 128])
RoPEを適用開始 x.shape=torch.Size([16384, 8, 128]), cos.shape=torch.Size([16384, 1, 64]), sin.shape=torch.Size([16384, 1, 64])
チャンク化 x1.shape=torch.Size([16384, 8, 64]), x2.shape=torch.Size([16384, 8, 64])
回転行列適用 y1.shape=torch.Size([16384, 8, 64]), y2.shape=torch.Size([16384, 8, 64])
RoPEを適用完了 result.shape=torch.Size([16384, 8, 128])
RoPEの順伝播完了 query.shape=torch.Size([16384, 16, 128]), key.shape=torch.Size([16384, 8, 128])
Attention層の順伝播開始 q.shape=torch.Size([16384, 16, 128]), k.shape=torch.Size([16384, 8, 128]), v.shape=torch.Size([16384, 8, 128])
現在のコンテキスト取得 _CONTEXT=Context(is_prefill=True, cu_seqlens_q=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32), cu_seqlens_k=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32), max_seqlen_q=4096, max_seqlen_k=4096, slot_mapping=tensor([], device='cuda:0', dtype=torch.int32), context_lens=None, block_tables=None)
コンテキスト情報取得 context.is_prefill=True, context.slot_mapping=tensor([], device='cuda:0', dtype=torch.int32), context.max_seqlen_q=4096, context.cu_seqlens_q=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32), context.max_seqlen_k=4096, context.cu_seqlens_k=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32), context.context_lens=None, context.block_tables=None
PrefillモードでFlashAttentionを実行 context.max_seqlen_q=4096, context.cu_seqlens_q=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32), context.max_seqlen_k=4096, context.cu_seqlens_k=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32)
Attention層の順伝播完了 o.shape=torch.Size([16384, 16, 128])
RowParallelLinearの順伝播開始 入力形状: torch.Size([16384, 2048])
RowParallelLinearの順伝播完了 出力形状: torch.Size([16384, 1024])
Qwen3Attentionの順伝播完了 output.shape=torch.Size([16384, 1024])
RMSNormの順伝播開始 x.shape=torch.Size([16384, 1024]), residual.shape if residual is not None else None=torch.Size([16384, 1024])
残差接続を適用
残差接続を適用完了
Qwen3MLPの順伝播 x.shape=torch.Size([16384, 1024])
出力次元を並列化する線形層の順伝播開始 x.shape=torch.Size([16384, 1024])
出力次元を並列化する線形層の順伝播完了 result.shape=torch.Size([16384, 6144])
SwiGLUを順伝播開始 x.shape=torch.Size([16384, 6144])
チャンク化 x.shape=torch.Size([16384, 3072]), y.shape=torch.Size([16384, 3072])
SwiGLUを順伝播完了 result.shape=torch.Size([16384, 3072])
RowParallelLinearの順伝播開始 入力形状: torch.Size([16384, 3072])
RowParallelLinearの順伝播完了 出力形状: torch.Size([16384, 1024])
Qwen3MLPの順伝播完了 x.shape=torch.Size([16384, 1024])
Qwen3DecoderLayerの順伝播完了 hidden_states.shape=torch.Size([16384, 1024])
Qwen3DecoderLayerの順伝播 positions.shape=torch.Size([16384]) hidden_states.shape=torch.Size([16384, 1024]) residual is None=False
RMSNormの順伝播開始 x.shape=torch.Size([16384, 1024]), residual.shape if residual is not None else None=torch.Size([16384, 1024])
残差接続を適用
残差接続を適用完了
Qwen3Attentionの順伝播 positions.shape=torch.Size([16384]) hidden_states.shape=torch.Size([16384, 1024])
出力次元を並列化する線形層の順伝播開始 x.shape=torch.Size([16384, 1024])
出力次元を並列化する線形層の順伝播完了 result.shape=torch.Size([16384, 4096])
RMSNormの順伝播開始 x.shape=torch.Size([16384, 16, 128]), residual.shape if residual is not None else None=None
正規化開始 x.shape=torch.Size([16384, 16, 128])
正規化完了 x.shape=torch.Size([16384, 16, 128])
RMSNormの順伝播開始 x.shape=torch.Size([16384, 8, 128]), residual.shape if residual is not None else None=None
正規化開始 x.shape=torch.Size([16384, 8, 128])
正規化完了 x.shape=torch.Size([16384, 8, 128])
RoPEの順伝播開始 positions.shape=torch.Size([16384]), query.shape=torch.Size([16384, 16, 128]), key.shape=torch.Size([16384, 8, 128])
RoPEを適用開始 x.shape=torch.Size([16384, 16, 128]), cos.shape=torch.Size([16384, 1, 64]), sin.shape=torch.Size([16384, 1, 64])
チャンク化 x1.shape=torch.Size([16384, 16, 64]), x2.shape=torch.Size([16384, 16, 64])
回転行列適用 y1.shape=torch.Size([16384, 16, 64]), y2.shape=torch.Size([16384, 16, 64])
RoPEを適用完了 result.shape=torch.Size([16384, 16, 128])
RoPEを適用開始 x.shape=torch.Size([16384, 8, 128]), cos.shape=torch.Size([16384, 1, 64]), sin.shape=torch.Size([16384, 1, 64])
チャンク化 x1.shape=torch.Size([16384, 8, 64]), x2.shape=torch.Size([16384, 8, 64])
回転行列適用 y1.shape=torch.Size([16384, 8, 64]), y2.shape=torch.Size([16384, 8, 64])
RoPEを適用完了 result.shape=torch.Size([16384, 8, 128])
RoPEの順伝播完了 query.shape=torch.Size([16384, 16, 128]), key.shape=torch.Size([16384, 8, 128])
Attention層の順伝播開始 q.shape=torch.Size([16384, 16, 128]), k.shape=torch.Size([16384, 8, 128]), v.shape=torch.Size([16384, 8, 128])
現在のコンテキスト取得 _CONTEXT=Context(is_prefill=True, cu_seqlens_q=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32), cu_seqlens_k=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32), max_seqlen_q=4096, max_seqlen_k=4096, slot_mapping=tensor([], device='cuda:0', dtype=torch.int32), context_lens=None, block_tables=None)
コンテキスト情報取得 context.is_prefill=True, context.slot_mapping=tensor([], device='cuda:0', dtype=torch.int32), context.max_seqlen_q=4096, context.cu_seqlens_q=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32), context.max_seqlen_k=4096, context.cu_seqlens_k=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32), context.context_lens=None, context.block_tables=None
PrefillモードでFlashAttentionを実行 context.max_seqlen_q=4096, context.cu_seqlens_q=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32), context.max_seqlen_k=4096, context.cu_seqlens_k=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32)
Attention層の順伝播完了 o.shape=torch.Size([16384, 16, 128])
RowParallelLinearの順伝播開始 入力形状: torch.Size([16384, 2048])
RowParallelLinearの順伝播完了 出力形状: torch.Size([16384, 1024])
Qwen3Attentionの順伝播完了 output.shape=torch.Size([16384, 1024])
RMSNormの順伝播開始 x.shape=torch.Size([16384, 1024]), residual.shape if residual is not None else None=torch.Size([16384, 1024])
残差接続を適用
残差接続を適用完了
Qwen3MLPの順伝播 x.shape=torch.Size([16384, 1024])
出力次元を並列化する線形層の順伝播開始 x.shape=torch.Size([16384, 1024])
出力次元を並列化する線形層の順伝播完了 result.shape=torch.Size([16384, 6144])
SwiGLUを順伝播開始 x.shape=torch.Size([16384, 6144])
チャンク化 x.shape=torch.Size([16384, 3072]), y.shape=torch.Size([16384, 3072])
SwiGLUを順伝播完了 result.shape=torch.Size([16384, 3072])
RowParallelLinearの順伝播開始 入力形状: torch.Size([16384, 3072])
RowParallelLinearの順伝播完了 出力形状: torch.Size([16384, 1024])
Qwen3MLPの順伝播完了 x.shape=torch.Size([16384, 1024])
Qwen3DecoderLayerの順伝播完了 hidden_states.shape=torch.Size([16384, 1024])
Qwen3DecoderLayerの順伝播 positions.shape=torch.Size([16384]) hidden_states.shape=torch.Size([16384, 1024]) residual is None=False
RMSNormの順伝播開始 x.shape=torch.Size([16384, 1024]), residual.shape if residual is not None else None=torch.Size([16384, 1024])
残差接続を適用
残差接続を適用完了
Qwen3Attentionの順伝播 positions.shape=torch.Size([16384]) hidden_states.shape=torch.Size([16384, 1024])
出力次元を並列化する線形層の順伝播開始 x.shape=torch.Size([16384, 1024])
出力次元を並列化する線形層の順伝播完了 result.shape=torch.Size([16384, 4096])
RMSNormの順伝播開始 x.shape=torch.Size([16384, 16, 128]), residual.shape if residual is not None else None=None
正規化開始 x.shape=torch.Size([16384, 16, 128])
正規化完了 x.shape=torch.Size([16384, 16, 128])
RMSNormの順伝播開始 x.shape=torch.Size([16384, 8, 128]), residual.shape if residual is not None else None=None
正規化開始 x.shape=torch.Size([16384, 8, 128])
正規化完了 x.shape=torch.Size([16384, 8, 128])
RoPEの順伝播開始 positions.shape=torch.Size([16384]), query.shape=torch.Size([16384, 16, 128]), key.shape=torch.Size([16384, 8, 128])
RoPEを適用開始 x.shape=torch.Size([16384, 16, 128]), cos.shape=torch.Size([16384, 1, 64]), sin.shape=torch.Size([16384, 1, 64])
チャンク化 x1.shape=torch.Size([16384, 16, 64]), x2.shape=torch.Size([16384, 16, 64])
回転行列適用 y1.shape=torch.Size([16384, 16, 64]), y2.shape=torch.Size([16384, 16, 64])
RoPEを適用完了 result.shape=torch.Size([16384, 16, 128])
RoPEを適用開始 x.shape=torch.Size([16384, 8, 128]), cos.shape=torch.Size([16384, 1, 64]), sin.shape=torch.Size([16384, 1, 64])
チャンク化 x1.shape=torch.Size([16384, 8, 64]), x2.shape=torch.Size([16384, 8, 64])
回転行列適用 y1.shape=torch.Size([16384, 8, 64]), y2.shape=torch.Size([16384, 8, 64])
RoPEを適用完了 result.shape=torch.Size([16384, 8, 128])
RoPEの順伝播完了 query.shape=torch.Size([16384, 16, 128]), key.shape=torch.Size([16384, 8, 128])
Attention層の順伝播開始 q.shape=torch.Size([16384, 16, 128]), k.shape=torch.Size([16384, 8, 128]), v.shape=torch.Size([16384, 8, 128])
現在のコンテキスト取得 _CONTEXT=Context(is_prefill=True, cu_seqlens_q=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32), cu_seqlens_k=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32), max_seqlen_q=4096, max_seqlen_k=4096, slot_mapping=tensor([], device='cuda:0', dtype=torch.int32), context_lens=None, block_tables=None)
コンテキスト情報取得 context.is_prefill=True, context.slot_mapping=tensor([], device='cuda:0', dtype=torch.int32), context.max_seqlen_q=4096, context.cu_seqlens_q=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32), context.max_seqlen_k=4096, context.cu_seqlens_k=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32), context.context_lens=None, context.block_tables=None
PrefillモードでFlashAttentionを実行 context.max_seqlen_q=4096, context.cu_seqlens_q=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32), context.max_seqlen_k=4096, context.cu_seqlens_k=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32)
Attention層の順伝播完了 o.shape=torch.Size([16384, 16, 128])
RowParallelLinearの順伝播開始 入力形状: torch.Size([16384, 2048])
RowParallelLinearの順伝播完了 出力形状: torch.Size([16384, 1024])
Qwen3Attentionの順伝播完了 output.shape=torch.Size([16384, 1024])
RMSNormの順伝播開始 x.shape=torch.Size([16384, 1024]), residual.shape if residual is not None else None=torch.Size([16384, 1024])
残差接続を適用
残差接続を適用完了
Qwen3MLPの順伝播 x.shape=torch.Size([16384, 1024])
出力次元を並列化する線形層の順伝播開始 x.shape=torch.Size([16384, 1024])
出力次元を並列化する線形層の順伝播完了 result.shape=torch.Size([16384, 6144])
SwiGLUを順伝播開始 x.shape=torch.Size([16384, 6144])
チャンク化 x.shape=torch.Size([16384, 3072]), y.shape=torch.Size([16384, 3072])
SwiGLUを順伝播完了 result.shape=torch.Size([16384, 3072])
RowParallelLinearの順伝播開始 入力形状: torch.Size([16384, 3072])
RowParallelLinearの順伝播完了 出力形状: torch.Size([16384, 1024])
Qwen3MLPの順伝播完了 x.shape=torch.Size([16384, 1024])
Qwen3DecoderLayerの順伝播完了 hidden_states.shape=torch.Size([16384, 1024])
Qwen3DecoderLayerの順伝播 positions.shape=torch.Size([16384]) hidden_states.shape=torch.Size([16384, 1024]) residual is None=False
RMSNormの順伝播開始 x.shape=torch.Size([16384, 1024]), residual.shape if residual is not None else None=torch.Size([16384, 1024])
残差接続を適用
残差接続を適用完了
Qwen3Attentionの順伝播 positions.shape=torch.Size([16384]) hidden_states.shape=torch.Size([16384, 1024])
出力次元を並列化する線形層の順伝播開始 x.shape=torch.Size([16384, 1024])
出力次元を並列化する線形層の順伝播完了 result.shape=torch.Size([16384, 4096])
RMSNormの順伝播開始 x.shape=torch.Size([16384, 16, 128]), residual.shape if residual is not None else None=None
正規化開始 x.shape=torch.Size([16384, 16, 128])
正規化完了 x.shape=torch.Size([16384, 16, 128])
RMSNormの順伝播開始 x.shape=torch.Size([16384, 8, 128]), residual.shape if residual is not None else None=None
正規化開始 x.shape=torch.Size([16384, 8, 128])
正規化完了 x.shape=torch.Size([16384, 8, 128])
RoPEの順伝播開始 positions.shape=torch.Size([16384]), query.shape=torch.Size([16384, 16, 128]), key.shape=torch.Size([16384, 8, 128])
RoPEを適用開始 x.shape=torch.Size([16384, 16, 128]), cos.shape=torch.Size([16384, 1, 64]), sin.shape=torch.Size([16384, 1, 64])
チャンク化 x1.shape=torch.Size([16384, 16, 64]), x2.shape=torch.Size([16384, 16, 64])
回転行列適用 y1.shape=torch.Size([16384, 16, 64]), y2.shape=torch.Size([16384, 16, 64])
RoPEを適用完了 result.shape=torch.Size([16384, 16, 128])
RoPEを適用開始 x.shape=torch.Size([16384, 8, 128]), cos.shape=torch.Size([16384, 1, 64]), sin.shape=torch.Size([16384, 1, 64])
チャンク化 x1.shape=torch.Size([16384, 8, 64]), x2.shape=torch.Size([16384, 8, 64])
回転行列適用 y1.shape=torch.Size([16384, 8, 64]), y2.shape=torch.Size([16384, 8, 64])
RoPEを適用完了 result.shape=torch.Size([16384, 8, 128])
RoPEの順伝播完了 query.shape=torch.Size([16384, 16, 128]), key.shape=torch.Size([16384, 8, 128])
Attention層の順伝播開始 q.shape=torch.Size([16384, 16, 128]), k.shape=torch.Size([16384, 8, 128]), v.shape=torch.Size([16384, 8, 128])
現在のコンテキスト取得 _CONTEXT=Context(is_prefill=True, cu_seqlens_q=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32), cu_seqlens_k=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32), max_seqlen_q=4096, max_seqlen_k=4096, slot_mapping=tensor([], device='cuda:0', dtype=torch.int32), context_lens=None, block_tables=None)
コンテキスト情報取得 context.is_prefill=True, context.slot_mapping=tensor([], device='cuda:0', dtype=torch.int32), context.max_seqlen_q=4096, context.cu_seqlens_q=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32), context.max_seqlen_k=4096, context.cu_seqlens_k=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32), context.context_lens=None, context.block_tables=None
PrefillモードでFlashAttentionを実行 context.max_seqlen_q=4096, context.cu_seqlens_q=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32), context.max_seqlen_k=4096, context.cu_seqlens_k=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32)
Attention層の順伝播完了 o.shape=torch.Size([16384, 16, 128])
RowParallelLinearの順伝播開始 入力形状: torch.Size([16384, 2048])
RowParallelLinearの順伝播完了 出力形状: torch.Size([16384, 1024])
Qwen3Attentionの順伝播完了 output.shape=torch.Size([16384, 1024])
RMSNormの順伝播開始 x.shape=torch.Size([16384, 1024]), residual.shape if residual is not None else None=torch.Size([16384, 1024])
残差接続を適用
残差接続を適用完了
Qwen3MLPの順伝播 x.shape=torch.Size([16384, 1024])
出力次元を並列化する線形層の順伝播開始 x.shape=torch.Size([16384, 1024])
出力次元を並列化する線形層の順伝播完了 result.shape=torch.Size([16384, 6144])
SwiGLUを順伝播開始 x.shape=torch.Size([16384, 6144])
チャンク化 x.shape=torch.Size([16384, 3072]), y.shape=torch.Size([16384, 3072])
SwiGLUを順伝播完了 result.shape=torch.Size([16384, 3072])
RowParallelLinearの順伝播開始 入力形状: torch.Size([16384, 3072])
RowParallelLinearの順伝播完了 出力形状: torch.Size([16384, 1024])
Qwen3MLPの順伝播完了 x.shape=torch.Size([16384, 1024])
Qwen3DecoderLayerの順伝播完了 hidden_states.shape=torch.Size([16384, 1024])
Qwen3DecoderLayerの順伝播 positions.shape=torch.Size([16384]) hidden_states.shape=torch.Size([16384, 1024]) residual is None=False
RMSNormの順伝播開始 x.shape=torch.Size([16384, 1024]), residual.shape if residual is not None else None=torch.Size([16384, 1024])
残差接続を適用
残差接続を適用完了
Qwen3Attentionの順伝播 positions.shape=torch.Size([16384]) hidden_states.shape=torch.Size([16384, 1024])
出力次元を並列化する線形層の順伝播開始 x.shape=torch.Size([16384, 1024])
出力次元を並列化する線形層の順伝播完了 result.shape=torch.Size([16384, 4096])
RMSNormの順伝播開始 x.shape=torch.Size([16384, 16, 128]), residual.shape if residual is not None else None=None
正規化開始 x.shape=torch.Size([16384, 16, 128])
正規化完了 x.shape=torch.Size([16384, 16, 128])
RMSNormの順伝播開始 x.shape=torch.Size([16384, 8, 128]), residual.shape if residual is not None else None=None
正規化開始 x.shape=torch.Size([16384, 8, 128])
正規化完了 x.shape=torch.Size([16384, 8, 128])
RoPEの順伝播開始 positions.shape=torch.Size([16384]), query.shape=torch.Size([16384, 16, 128]), key.shape=torch.Size([16384, 8, 128])
RoPEを適用開始 x.shape=torch.Size([16384, 16, 128]), cos.shape=torch.Size([16384, 1, 64]), sin.shape=torch.Size([16384, 1, 64])
チャンク化 x1.shape=torch.Size([16384, 16, 64]), x2.shape=torch.Size([16384, 16, 64])
回転行列適用 y1.shape=torch.Size([16384, 16, 64]), y2.shape=torch.Size([16384, 16, 64])
RoPEを適用完了 result.shape=torch.Size([16384, 16, 128])
RoPEを適用開始 x.shape=torch.Size([16384, 8, 128]), cos.shape=torch.Size([16384, 1, 64]), sin.shape=torch.Size([16384, 1, 64])
チャンク化 x1.shape=torch.Size([16384, 8, 64]), x2.shape=torch.Size([16384, 8, 64])
回転行列適用 y1.shape=torch.Size([16384, 8, 64]), y2.shape=torch.Size([16384, 8, 64])
RoPEを適用完了 result.shape=torch.Size([16384, 8, 128])
RoPEの順伝播完了 query.shape=torch.Size([16384, 16, 128]), key.shape=torch.Size([16384, 8, 128])
Attention層の順伝播開始 q.shape=torch.Size([16384, 16, 128]), k.shape=torch.Size([16384, 8, 128]), v.shape=torch.Size([16384, 8, 128])
現在のコンテキスト取得 _CONTEXT=Context(is_prefill=True, cu_seqlens_q=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32), cu_seqlens_k=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32), max_seqlen_q=4096, max_seqlen_k=4096, slot_mapping=tensor([], device='cuda:0', dtype=torch.int32), context_lens=None, block_tables=None)
コンテキスト情報取得 context.is_prefill=True, context.slot_mapping=tensor([], device='cuda:0', dtype=torch.int32), context.max_seqlen_q=4096, context.cu_seqlens_q=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32), context.max_seqlen_k=4096, context.cu_seqlens_k=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32), context.context_lens=None, context.block_tables=None
PrefillモードでFlashAttentionを実行 context.max_seqlen_q=4096, context.cu_seqlens_q=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32), context.max_seqlen_k=4096, context.cu_seqlens_k=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32)
Attention層の順伝播完了 o.shape=torch.Size([16384, 16, 128])
RowParallelLinearの順伝播開始 入力形状: torch.Size([16384, 2048])
RowParallelLinearの順伝播完了 出力形状: torch.Size([16384, 1024])
Qwen3Attentionの順伝播完了 output.shape=torch.Size([16384, 1024])
RMSNormの順伝播開始 x.shape=torch.Size([16384, 1024]), residual.shape if residual is not None else None=torch.Size([16384, 1024])
残差接続を適用
残差接続を適用完了
Qwen3MLPの順伝播 x.shape=torch.Size([16384, 1024])
出力次元を並列化する線形層の順伝播開始 x.shape=torch.Size([16384, 1024])
出力次元を並列化する線形層の順伝播完了 result.shape=torch.Size([16384, 6144])
SwiGLUを順伝播開始 x.shape=torch.Size([16384, 6144])
チャンク化 x.shape=torch.Size([16384, 3072]), y.shape=torch.Size([16384, 3072])
SwiGLUを順伝播完了 result.shape=torch.Size([16384, 3072])
RowParallelLinearの順伝播開始 入力形状: torch.Size([16384, 3072])
RowParallelLinearの順伝播完了 出力形状: torch.Size([16384, 1024])
Qwen3MLPの順伝播完了 x.shape=torch.Size([16384, 1024])
Qwen3DecoderLayerの順伝播完了 hidden_states.shape=torch.Size([16384, 1024])
RMSNormの順伝播開始 x.shape=torch.Size([16384, 1024]), residual.shape if residual is not None else None=torch.Size([16384, 1024])
残差接続を適用
残差接続を適用完了
Qwen3Modelの順伝播完了 hidden_states.shape=torch.Size([16384, 1024])
Qwen3ForCausalLMの順伝播完了 output.shape=torch.Size([16384, 1024])
Qwen3ForCausalLMのロジット計算開始 hidden_states.shape=torch.Size([16384, 1024])
出力の言語モデルヘッドの順伝播開始 x.shape=torch.Size([16384, 1024])
現在のコンテキスト取得 _CONTEXT=Context(is_prefill=True, cu_seqlens_q=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32), cu_seqlens_k=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32), max_seqlen_q=4096, max_seqlen_k=4096, slot_mapping=tensor([], device='cuda:0', dtype=torch.int32), context_lens=None, block_tables=None)
最後のトークンを抽出（Prefill） x.shape=torch.Size([4, 1024])
出力の言語モデルヘッドの順伝播完了 logits.shape if logits is not None else None=torch.Size([4, 151936])
Qwen3ForCausalLMのロジット計算完了 logits.shape=torch.Size([4, 151936])
モデルの順伝搬（prefill）を実行完了 result.shape=torch.Size([4, 151936])
サンプリング開始 logits.shape=torch.Size([4, 151936]), temperatures.shape=torch.Size([4])
サンプリング完了 sample_tokens.shape=torch.Size([4])
コンテキストをリセット
推論の1サイクルを実行完了 (len(token_ids) if token_ids else None)=4
Warmed up model on rank 0
Allocating KV cache on rank 0
Allocated KV cache with 893 blocks on rank 0
Initialized ModelRunner on rank 0
スケジューラを初期化 config.max_num_seqs=512, config.max_num_batched_tokens=16384 config.eos=151645, config.num_kvcache_blocks=893, config.kvcache_block_size=256
ブロックマネージャーを初期化開始 num_blocks=893 block_size=256
ブロックを初期化開始 block_id=0
ブロックを初期化完了 self.block_id=0
ブロックを初期化開始 block_id=1
ブロックを初期化完了 self.block_id=1
ブロックを初期化開始 block_id=2
ブロックを初期化完了 self.block_id=2
ブロックを初期化開始 block_id=3
ブロックを初期化完了 self.block_id=3
ブロックを初期化開始 block_id=4
ブロックを初期化完了 self.block_id=4
ブロックを初期化開始 block_id=5
ブロックを初期化完了 self.block_id=5
ブロックを初期化開始 block_id=6
ブロックを初期化完了 self.block_id=6
ブロックを初期化開始 block_id=7
ブロックを初期化完了 self.block_id=7
ブロックを初期化開始 block_id=8
ブロックを初期化完了 self.block_id=8
ブロックを初期化開始 block_id=9
ブロックを初期化完了 self.block_id=9
ブロックを初期化開始 block_id=10
ブロックを初期化完了 self.block_id=10
ブロックを初期化開始 block_id=11
ブロックを初期化完了 self.block_id=11
ブロックを初期化開始 block_id=12
ブロックを初期化完了 self.block_id=12
ブロックを初期化開始 block_id=13
ブロックを初期化完了 self.block_id=13
ブロックを初期化開始 block_id=14
ブロックを初期化完了 self.block_id=14
ブロックを初期化開始 block_id=15
ブロックを初期化完了 self.block_id=15
ブロックを初期化開始 block_id=16
ブロックを初期化完了 self.block_id=16
ブロックを初期化開始 block_id=17
ブロックを初期化完了 self.block_id=17
ブロックを初期化開始 block_id=18
ブロックを初期化完了 self.block_id=18
ブロックを初期化開始 block_id=19
ブロックを初期化完了 self.block_id=19
ブロックを初期化開始 block_id=20
ブロックを初期化完了 self.block_id=20
ブロックを初期化開始 block_id=21
ブロックを初期化完了 self.block_id=21
ブロックを初期化開始 block_id=22
ブロックを初期化完了 self.block_id=22
ブロックを初期化開始 block_id=23
ブロックを初期化完了 self.block_id=23
ブロックを初期化開始 block_id=24
ブロックを初期化完了 self.block_id=24
ブロックを初期化開始 block_id=25
ブロックを初期化完了 self.block_id=25
ブロックを初期化開始 block_id=26
ブロックを初期化完了 self.block_id=26
ブロックを初期化開始 block_id=27
ブロックを初期化完了 self.block_id=27
ブロックを初期化開始 block_id=28
ブロックを初期化完了 self.block_id=28
ブロックを初期化開始 block_id=29
ブロックを初期化完了 self.block_id=29
ブロックを初期化開始 block_id=30
ブロックを初期化完了 self.block_id=30
ブロックを初期化開始 block_id=31
ブロックを初期化完了 self.block_id=31
ブロックを初期化開始 block_id=32
ブロックを初期化完了 self.block_id=32
ブロックを初期化開始 block_id=33
ブロックを初期化完了 self.block_id=33
ブロックを初期化開始 block_id=34
ブロックを初期化完了 self.block_id=34
ブロックを初期化開始 block_id=35
ブロックを初期化完了 self.block_id=35
ブロックを初期化開始 block_id=36
ブロックを初期化完了 self.block_id=36
ブロックを初期化開始 block_id=37
ブロックを初期化完了 self.block_id=37
ブロックを初期化開始 block_id=38
ブロックを初期化完了 self.block_id=38
ブロックを初期化開始 block_id=39
ブロックを初期化完了 self.block_id=39
ブロックを初期化開始 block_id=40
ブロックを初期化完了 self.block_id=40
ブロックを初期化開始 block_id=41
ブロックを初期化完了 self.block_id=41
ブロックを初期化開始 block_id=42
ブロックを初期化完了 self.block_id=42
ブロックを初期化開始 block_id=43
ブロックを初期化完了 self.block_id=43
ブロックを初期化開始 block_id=44
ブロックを初期化完了 self.block_id=44
ブロックを初期化開始 block_id=45
ブロックを初期化完了 self.block_id=45
ブロックを初期化開始 block_id=46
ブロックを初期化完了 self.block_id=46
ブロックを初期化開始 block_id=47
ブロックを初期化完了 self.block_id=47
ブロックを初期化開始 block_id=48
ブロックを初期化完了 self.block_id=48
ブロックを初期化開始 block_id=49
ブロックを初期化完了 self.block_id=49
ブロックを初期化開始 block_id=50
ブロックを初期化完了 self.block_id=50
ブロックを初期化開始 block_id=51
ブロックを初期化完了 self.block_id=51
ブロックを初期化開始 block_id=52
ブロックを初期化完了 self.block_id=52
ブロックを初期化開始 block_id=53
ブロックを初期化完了 self.block_id=53
ブロックを初期化開始 block_id=54
ブロックを初期化完了 self.block_id=54
ブロックを初期化開始 block_id=55
ブロックを初期化完了 self.block_id=55
ブロックを初期化開始 block_id=56
ブロックを初期化完了 self.block_id=56
ブロックを初期化開始 block_id=57
ブロックを初期化完了 self.block_id=57
ブロックを初期化開始 block_id=58
ブロックを初期化完了 self.block_id=58
ブロックを初期化開始 block_id=59
ブロックを初期化完了 self.block_id=59
ブロックを初期化開始 block_id=60
ブロックを初期化完了 self.block_id=60
ブロックを初期化開始 block_id=61
ブロックを初期化完了 self.block_id=61
ブロックを初期化開始 block_id=62
ブロックを初期化完了 self.block_id=62
ブロックを初期化開始 block_id=63
ブロックを初期化完了 self.block_id=63
ブロックを初期化開始 block_id=64
ブロックを初期化完了 self.block_id=64
ブロックを初期化開始 block_id=65
ブロックを初期化完了 self.block_id=65
ブロックを初期化開始 block_id=66
ブロックを初期化完了 self.block_id=66
ブロックを初期化開始 block_id=67
ブロックを初期化完了 self.block_id=67
ブロックを初期化開始 block_id=68
ブロックを初期化完了 self.block_id=68
ブロックを初期化開始 block_id=69
ブロックを初期化完了 self.block_id=69
ブロックを初期化開始 block_id=70
ブロックを初期化完了 self.block_id=70
ブロックを初期化開始 block_id=71
ブロックを初期化完了 self.block_id=71
ブロックを初期化開始 block_id=72
ブロックを初期化完了 self.block_id=72
ブロックを初期化開始 block_id=73
ブロックを初期化完了 self.block_id=73
ブロックを初期化開始 block_id=74
ブロックを初期化完了 self.block_id=74
ブロックを初期化開始 block_id=75
ブロックを初期化完了 self.block_id=75
ブロックを初期化開始 block_id=76
ブロックを初期化完了 self.block_id=76
ブロックを初期化開始 block_id=77
ブロックを初期化完了 self.block_id=77
ブロックを初期化開始 block_id=78
ブロックを初期化完了 self.block_id=78
ブロックを初期化開始 block_id=79
ブロックを初期化完了 self.block_id=79
ブロックを初期化開始 block_id=80
ブロックを初期化完了 self.block_id=80
ブロックを初期化開始 block_id=81
ブロックを初期化完了 self.block_id=81
ブロックを初期化開始 block_id=82
ブロックを初期化完了 self.block_id=82
ブロックを初期化開始 block_id=83
ブロックを初期化完了 self.block_id=83
ブロックを初期化開始 block_id=84
ブロックを初期化完了 self.block_id=84
ブロックを初期化開始 block_id=85
ブロックを初期化完了 self.block_id=85
ブロックを初期化開始 block_id=86
ブロックを初期化完了 self.block_id=86
ブロックを初期化開始 block_id=87
ブロックを初期化完了 self.block_id=87
ブロックを初期化開始 block_id=88
ブロックを初期化完了 self.block_id=88
ブロックを初期化開始 block_id=89
ブロックを初期化完了 self.block_id=89
ブロックを初期化開始 block_id=90
ブロックを初期化完了 self.block_id=90
ブロックを初期化開始 block_id=91
ブロックを初期化完了 self.block_id=91
ブロックを初期化開始 block_id=92
ブロックを初期化完了 self.block_id=92
ブロックを初期化開始 block_id=93
ブロックを初期化完了 self.block_id=93
ブロックを初期化開始 block_id=94
ブロックを初期化完了 self.block_id=94
ブロックを初期化開始 block_id=95
ブロックを初期化完了 self.block_id=95
ブロックを初期化開始 block_id=96
ブロックを初期化完了 self.block_id=96
ブロックを初期化開始 block_id=97
ブロックを初期化完了 self.block_id=97
ブロックを初期化開始 block_id=98
ブロックを初期化完了 self.block_id=98
ブロックを初期化開始 block_id=99
ブロックを初期化完了 self.block_id=99
ブロックを初期化開始 block_id=100
ブロックを初期化完了 self.block_id=100
ブロックを初期化開始 block_id=101
ブロックを初期化完了 self.block_id=101
ブロックを初期化開始 block_id=102
ブロックを初期化完了 self.block_id=102
ブロックを初期化開始 block_id=103
ブロックを初期化完了 self.block_id=103
ブロックを初期化開始 block_id=104
ブロックを初期化完了 self.block_id=104
ブロックを初期化開始 block_id=105
ブロックを初期化完了 self.block_id=105
ブロックを初期化開始 block_id=106
ブロックを初期化完了 self.block_id=106
ブロックを初期化開始 block_id=107
ブロックを初期化完了 self.block_id=107
ブロックを初期化開始 block_id=108
ブロックを初期化完了 self.block_id=108
ブロックを初期化開始 block_id=109
ブロックを初期化完了 self.block_id=109
ブロックを初期化開始 block_id=110
ブロックを初期化完了 self.block_id=110
ブロックを初期化開始 block_id=111
ブロックを初期化完了 self.block_id=111
ブロックを初期化開始 block_id=112
ブロックを初期化完了 self.block_id=112
ブロックを初期化開始 block_id=113
ブロックを初期化完了 self.block_id=113
ブロックを初期化開始 block_id=114
ブロックを初期化完了 self.block_id=114
ブロックを初期化開始 block_id=115
ブロックを初期化完了 self.block_id=115
ブロックを初期化開始 block_id=116
ブロックを初期化完了 self.block_id=116
ブロックを初期化開始 block_id=117
ブロックを初期化完了 self.block_id=117
ブロックを初期化開始 block_id=118
ブロックを初期化完了 self.block_id=118
ブロックを初期化開始 block_id=119
ブロックを初期化完了 self.block_id=119
ブロックを初期化開始 block_id=120
ブロックを初期化完了 self.block_id=120
ブロックを初期化開始 block_id=121
ブロックを初期化完了 self.block_id=121
ブロックを初期化開始 block_id=122
ブロックを初期化完了 self.block_id=122
ブロックを初期化開始 block_id=123
ブロックを初期化完了 self.block_id=123
ブロックを初期化開始 block_id=124
ブロックを初期化完了 self.block_id=124
ブロックを初期化開始 block_id=125
ブロックを初期化完了 self.block_id=125
ブロックを初期化開始 block_id=126
ブロックを初期化完了 self.block_id=126
ブロックを初期化開始 block_id=127
ブロックを初期化完了 self.block_id=127
ブロックを初期化開始 block_id=128
ブロックを初期化完了 self.block_id=128
ブロックを初期化開始 block_id=129
ブロックを初期化完了 self.block_id=129
ブロックを初期化開始 block_id=130
ブロックを初期化完了 self.block_id=130
ブロックを初期化開始 block_id=131
ブロックを初期化完了 self.block_id=131
ブロックを初期化開始 block_id=132
ブロックを初期化完了 self.block_id=132
ブロックを初期化開始 block_id=133
ブロックを初期化完了 self.block_id=133
ブロックを初期化開始 block_id=134
ブロックを初期化完了 self.block_id=134
ブロックを初期化開始 block_id=135
ブロックを初期化完了 self.block_id=135
ブロックを初期化開始 block_id=136
ブロックを初期化完了 self.block_id=136
ブロックを初期化開始 block_id=137
ブロックを初期化完了 self.block_id=137
ブロックを初期化開始 block_id=138
ブロックを初期化完了 self.block_id=138
ブロックを初期化開始 block_id=139
ブロックを初期化完了 self.block_id=139
ブロックを初期化開始 block_id=140
ブロックを初期化完了 self.block_id=140
ブロックを初期化開始 block_id=141
ブロックを初期化完了 self.block_id=141
ブロックを初期化開始 block_id=142
ブロックを初期化完了 self.block_id=142
ブロックを初期化開始 block_id=143
ブロックを初期化完了 self.block_id=143
ブロックを初期化開始 block_id=144
ブロックを初期化完了 self.block_id=144
ブロックを初期化開始 block_id=145
ブロックを初期化完了 self.block_id=145
ブロックを初期化開始 block_id=146
ブロックを初期化完了 self.block_id=146
ブロックを初期化開始 block_id=147
ブロックを初期化完了 self.block_id=147
ブロックを初期化開始 block_id=148
ブロックを初期化完了 self.block_id=148
ブロックを初期化開始 block_id=149
ブロックを初期化完了 self.block_id=149
ブロックを初期化開始 block_id=150
ブロックを初期化完了 self.block_id=150
ブロックを初期化開始 block_id=151
ブロックを初期化完了 self.block_id=151
ブロックを初期化開始 block_id=152
ブロックを初期化完了 self.block_id=152
ブロックを初期化開始 block_id=153
ブロックを初期化完了 self.block_id=153
ブロックを初期化開始 block_id=154
ブロックを初期化完了 self.block_id=154
ブロックを初期化開始 block_id=155
ブロックを初期化完了 self.block_id=155
ブロックを初期化開始 block_id=156
ブロックを初期化完了 self.block_id=156
ブロックを初期化開始 block_id=157
ブロックを初期化完了 self.block_id=157
ブロックを初期化開始 block_id=158
ブロックを初期化完了 self.block_id=158
ブロックを初期化開始 block_id=159
ブロックを初期化完了 self.block_id=159
ブロックを初期化開始 block_id=160
ブロックを初期化完了 self.block_id=160
ブロックを初期化開始 block_id=161
ブロックを初期化完了 self.block_id=161
ブロックを初期化開始 block_id=162
ブロックを初期化完了 self.block_id=162
ブロックを初期化開始 block_id=163
ブロックを初期化完了 self.block_id=163
ブロックを初期化開始 block_id=164
ブロックを初期化完了 self.block_id=164
ブロックを初期化開始 block_id=165
ブロックを初期化完了 self.block_id=165
ブロックを初期化開始 block_id=166
ブロックを初期化完了 self.block_id=166
ブロックを初期化開始 block_id=167
ブロックを初期化完了 self.block_id=167
ブロックを初期化開始 block_id=168
ブロックを初期化完了 self.block_id=168
ブロックを初期化開始 block_id=169
ブロックを初期化完了 self.block_id=169
ブロックを初期化開始 block_id=170
ブロックを初期化完了 self.block_id=170
ブロックを初期化開始 block_id=171
ブロックを初期化完了 self.block_id=171
ブロックを初期化開始 block_id=172
ブロックを初期化完了 self.block_id=172
ブロックを初期化開始 block_id=173
ブロックを初期化完了 self.block_id=173
ブロックを初期化開始 block_id=174
ブロックを初期化完了 self.block_id=174
ブロックを初期化開始 block_id=175
ブロックを初期化完了 self.block_id=175
ブロックを初期化開始 block_id=176
ブロックを初期化完了 self.block_id=176
ブロックを初期化開始 block_id=177
ブロックを初期化完了 self.block_id=177
ブロックを初期化開始 block_id=178
ブロックを初期化完了 self.block_id=178
ブロックを初期化開始 block_id=179
ブロックを初期化完了 self.block_id=179
ブロックを初期化開始 block_id=180
ブロックを初期化完了 self.block_id=180
ブロックを初期化開始 block_id=181
ブロックを初期化完了 self.block_id=181
ブロックを初期化開始 block_id=182
ブロックを初期化完了 self.block_id=182
ブロックを初期化開始 block_id=183
ブロックを初期化完了 self.block_id=183
ブロックを初期化開始 block_id=184
ブロックを初期化完了 self.block_id=184
ブロックを初期化開始 block_id=185
ブロックを初期化完了 self.block_id=185
ブロックを初期化開始 block_id=186
ブロックを初期化完了 self.block_id=186
ブロックを初期化開始 block_id=187
ブロックを初期化完了 self.block_id=187
ブロックを初期化開始 block_id=188
ブロックを初期化完了 self.block_id=188
ブロックを初期化開始 block_id=189
ブロックを初期化完了 self.block_id=189
ブロックを初期化開始 block_id=190
ブロックを初期化完了 self.block_id=190
ブロックを初期化開始 block_id=191
ブロックを初期化完了 self.block_id=191
ブロックを初期化開始 block_id=192
ブロックを初期化完了 self.block_id=192
ブロックを初期化開始 block_id=193
ブロックを初期化完了 self.block_id=193
ブロックを初期化開始 block_id=194
ブロックを初期化完了 self.block_id=194
ブロックを初期化開始 block_id=195
ブロックを初期化完了 self.block_id=195
ブロックを初期化開始 block_id=196
ブロックを初期化完了 self.block_id=196
ブロックを初期化開始 block_id=197
ブロックを初期化完了 self.block_id=197
ブロックを初期化開始 block_id=198
ブロックを初期化完了 self.block_id=198
ブロックを初期化開始 block_id=199
ブロックを初期化完了 self.block_id=199
ブロックを初期化開始 block_id=200
ブロックを初期化完了 self.block_id=200
ブロックを初期化開始 block_id=201
ブロックを初期化完了 self.block_id=201
ブロックを初期化開始 block_id=202
ブロックを初期化完了 self.block_id=202
ブロックを初期化開始 block_id=203
ブロックを初期化完了 self.block_id=203
ブロックを初期化開始 block_id=204
ブロックを初期化完了 self.block_id=204
ブロックを初期化開始 block_id=205
ブロックを初期化完了 self.block_id=205
ブロックを初期化開始 block_id=206
ブロックを初期化完了 self.block_id=206
ブロックを初期化開始 block_id=207
ブロックを初期化完了 self.block_id=207
ブロックを初期化開始 block_id=208
ブロックを初期化完了 self.block_id=208
ブロックを初期化開始 block_id=209
ブロックを初期化完了 self.block_id=209
ブロックを初期化開始 block_id=210
ブロックを初期化完了 self.block_id=210
ブロックを初期化開始 block_id=211
ブロックを初期化完了 self.block_id=211
ブロックを初期化開始 block_id=212
ブロックを初期化完了 self.block_id=212
ブロックを初期化開始 block_id=213
ブロックを初期化完了 self.block_id=213
ブロックを初期化開始 block_id=214
ブロックを初期化完了 self.block_id=214
ブロックを初期化開始 block_id=215
ブロックを初期化完了 self.block_id=215
ブロックを初期化開始 block_id=216
ブロックを初期化完了 self.block_id=216
ブロックを初期化開始 block_id=217
ブロックを初期化完了 self.block_id=217
ブロックを初期化開始 block_id=218
ブロックを初期化完了 self.block_id=218
ブロックを初期化開始 block_id=219
ブロックを初期化完了 self.block_id=219
ブロックを初期化開始 block_id=220
ブロックを初期化完了 self.block_id=220
ブロックを初期化開始 block_id=221
ブロックを初期化完了 self.block_id=221
ブロックを初期化開始 block_id=222
ブロックを初期化完了 self.block_id=222
ブロックを初期化開始 block_id=223
ブロックを初期化完了 self.block_id=223
ブロックを初期化開始 block_id=224
ブロックを初期化完了 self.block_id=224
ブロックを初期化開始 block_id=225
ブロックを初期化完了 self.block_id=225
ブロックを初期化開始 block_id=226
ブロックを初期化完了 self.block_id=226
ブロックを初期化開始 block_id=227
ブロックを初期化完了 self.block_id=227
ブロックを初期化開始 block_id=228
ブロックを初期化完了 self.block_id=228
ブロックを初期化開始 block_id=229
ブロックを初期化完了 self.block_id=229
ブロックを初期化開始 block_id=230
ブロックを初期化完了 self.block_id=230
ブロックを初期化開始 block_id=231
ブロックを初期化完了 self.block_id=231
ブロックを初期化開始 block_id=232
ブロックを初期化完了 self.block_id=232
ブロックを初期化開始 block_id=233
ブロックを初期化完了 self.block_id=233
ブロックを初期化開始 block_id=234
ブロックを初期化完了 self.block_id=234
ブロックを初期化開始 block_id=235
ブロックを初期化完了 self.block_id=235
ブロックを初期化開始 block_id=236
ブロックを初期化完了 self.block_id=236
ブロックを初期化開始 block_id=237
ブロックを初期化完了 self.block_id=237
ブロックを初期化開始 block_id=238
ブロックを初期化完了 self.block_id=238
ブロックを初期化開始 block_id=239
ブロックを初期化完了 self.block_id=239
ブロックを初期化開始 block_id=240
ブロックを初期化完了 self.block_id=240
ブロックを初期化開始 block_id=241
ブロックを初期化完了 self.block_id=241
ブロックを初期化開始 block_id=242
ブロックを初期化完了 self.block_id=242
ブロックを初期化開始 block_id=243
ブロックを初期化完了 self.block_id=243
ブロックを初期化開始 block_id=244
ブロックを初期化完了 self.block_id=244
ブロックを初期化開始 block_id=245
ブロックを初期化完了 self.block_id=245
ブロックを初期化開始 block_id=246
ブロックを初期化完了 self.block_id=246
ブロックを初期化開始 block_id=247
ブロックを初期化完了 self.block_id=247
ブロックを初期化開始 block_id=248
ブロックを初期化完了 self.block_id=248
ブロックを初期化開始 block_id=249
ブロックを初期化完了 self.block_id=249
ブロックを初期化開始 block_id=250
ブロックを初期化完了 self.block_id=250
ブロックを初期化開始 block_id=251
ブロックを初期化完了 self.block_id=251
ブロックを初期化開始 block_id=252
ブロックを初期化完了 self.block_id=252
ブロックを初期化開始 block_id=253
ブロックを初期化完了 self.block_id=253
ブロックを初期化開始 block_id=254
ブロックを初期化完了 self.block_id=254
ブロックを初期化開始 block_id=255
ブロックを初期化完了 self.block_id=255
ブロックを初期化開始 block_id=256
ブロックを初期化完了 self.block_id=256
ブロックを初期化開始 block_id=257
ブロックを初期化完了 self.block_id=257
ブロックを初期化開始 block_id=258
ブロックを初期化完了 self.block_id=258
ブロックを初期化開始 block_id=259
ブロックを初期化完了 self.block_id=259
ブロックを初期化開始 block_id=260
ブロックを初期化完了 self.block_id=260
ブロックを初期化開始 block_id=261
ブロックを初期化完了 self.block_id=261
ブロックを初期化開始 block_id=262
ブロックを初期化完了 self.block_id=262
ブロックを初期化開始 block_id=263
ブロックを初期化完了 self.block_id=263
ブロックを初期化開始 block_id=264
ブロックを初期化完了 self.block_id=264
ブロックを初期化開始 block_id=265
ブロックを初期化完了 self.block_id=265
ブロックを初期化開始 block_id=266
ブロックを初期化完了 self.block_id=266
ブロックを初期化開始 block_id=267
ブロックを初期化完了 self.block_id=267
ブロックを初期化開始 block_id=268
ブロックを初期化完了 self.block_id=268
ブロックを初期化開始 block_id=269
ブロックを初期化完了 self.block_id=269
ブロックを初期化開始 block_id=270
ブロックを初期化完了 self.block_id=270
ブロックを初期化開始 block_id=271
ブロックを初期化完了 self.block_id=271
ブロックを初期化開始 block_id=272
ブロックを初期化完了 self.block_id=272
ブロックを初期化開始 block_id=273
ブロックを初期化完了 self.block_id=273
ブロックを初期化開始 block_id=274
ブロックを初期化完了 self.block_id=274
ブロックを初期化開始 block_id=275
ブロックを初期化完了 self.block_id=275
ブロックを初期化開始 block_id=276
ブロックを初期化完了 self.block_id=276
ブロックを初期化開始 block_id=277
ブロックを初期化完了 self.block_id=277
ブロックを初期化開始 block_id=278
ブロックを初期化完了 self.block_id=278
ブロックを初期化開始 block_id=279
ブロックを初期化完了 self.block_id=279
ブロックを初期化開始 block_id=280
ブロックを初期化完了 self.block_id=280
ブロックを初期化開始 block_id=281
ブロックを初期化完了 self.block_id=281
ブロックを初期化開始 block_id=282
ブロックを初期化完了 self.block_id=282
ブロックを初期化開始 block_id=283
ブロックを初期化完了 self.block_id=283
ブロックを初期化開始 block_id=284
ブロックを初期化完了 self.block_id=284
ブロックを初期化開始 block_id=285
ブロックを初期化完了 self.block_id=285
ブロックを初期化開始 block_id=286
ブロックを初期化完了 self.block_id=286
ブロックを初期化開始 block_id=287
ブロックを初期化完了 self.block_id=287
ブロックを初期化開始 block_id=288
ブロックを初期化完了 self.block_id=288
ブロックを初期化開始 block_id=289
ブロックを初期化完了 self.block_id=289
ブロックを初期化開始 block_id=290
ブロックを初期化完了 self.block_id=290
ブロックを初期化開始 block_id=291
ブロックを初期化完了 self.block_id=291
ブロックを初期化開始 block_id=292
ブロックを初期化完了 self.block_id=292
ブロックを初期化開始 block_id=293
ブロックを初期化完了 self.block_id=293
ブロックを初期化開始 block_id=294
ブロックを初期化完了 self.block_id=294
ブロックを初期化開始 block_id=295
ブロックを初期化完了 self.block_id=295
ブロックを初期化開始 block_id=296
ブロックを初期化完了 self.block_id=296
ブロックを初期化開始 block_id=297
ブロックを初期化完了 self.block_id=297
ブロックを初期化開始 block_id=298
ブロックを初期化完了 self.block_id=298
ブロックを初期化開始 block_id=299
ブロックを初期化完了 self.block_id=299
ブロックを初期化開始 block_id=300
ブロックを初期化完了 self.block_id=300
ブロックを初期化開始 block_id=301
ブロックを初期化完了 self.block_id=301
ブロックを初期化開始 block_id=302
ブロックを初期化完了 self.block_id=302
ブロックを初期化開始 block_id=303
ブロックを初期化完了 self.block_id=303
ブロックを初期化開始 block_id=304
ブロックを初期化完了 self.block_id=304
ブロックを初期化開始 block_id=305
ブロックを初期化完了 self.block_id=305
ブロックを初期化開始 block_id=306
ブロックを初期化完了 self.block_id=306
ブロックを初期化開始 block_id=307
ブロックを初期化完了 self.block_id=307
ブロックを初期化開始 block_id=308
ブロックを初期化完了 self.block_id=308
ブロックを初期化開始 block_id=309
ブロックを初期化完了 self.block_id=309
ブロックを初期化開始 block_id=310
ブロックを初期化完了 self.block_id=310
ブロックを初期化開始 block_id=311
ブロックを初期化完了 self.block_id=311
ブロックを初期化開始 block_id=312
ブロックを初期化完了 self.block_id=312
ブロックを初期化開始 block_id=313
ブロックを初期化完了 self.block_id=313
ブロックを初期化開始 block_id=314
ブロックを初期化完了 self.block_id=314
ブロックを初期化開始 block_id=315
ブロックを初期化完了 self.block_id=315
ブロックを初期化開始 block_id=316
ブロックを初期化完了 self.block_id=316
ブロックを初期化開始 block_id=317
ブロックを初期化完了 self.block_id=317
ブロックを初期化開始 block_id=318
ブロックを初期化完了 self.block_id=318
ブロックを初期化開始 block_id=319
ブロックを初期化完了 self.block_id=319
ブロックを初期化開始 block_id=320
ブロックを初期化完了 self.block_id=320
ブロックを初期化開始 block_id=321
ブロックを初期化完了 self.block_id=321
ブロックを初期化開始 block_id=322
ブロックを初期化完了 self.block_id=322
ブロックを初期化開始 block_id=323
ブロックを初期化完了 self.block_id=323
ブロックを初期化開始 block_id=324
ブロックを初期化完了 self.block_id=324
ブロックを初期化開始 block_id=325
ブロックを初期化完了 self.block_id=325
ブロックを初期化開始 block_id=326
ブロックを初期化完了 self.block_id=326
ブロックを初期化開始 block_id=327
ブロックを初期化完了 self.block_id=327
ブロックを初期化開始 block_id=328
ブロックを初期化完了 self.block_id=328
ブロックを初期化開始 block_id=329
ブロックを初期化完了 self.block_id=329
ブロックを初期化開始 block_id=330
ブロックを初期化完了 self.block_id=330
ブロックを初期化開始 block_id=331
ブロックを初期化完了 self.block_id=331
ブロックを初期化開始 block_id=332
ブロックを初期化完了 self.block_id=332
ブロックを初期化開始 block_id=333
ブロックを初期化完了 self.block_id=333
ブロックを初期化開始 block_id=334
ブロックを初期化完了 self.block_id=334
ブロックを初期化開始 block_id=335
ブロックを初期化完了 self.block_id=335
ブロックを初期化開始 block_id=336
ブロックを初期化完了 self.block_id=336
ブロックを初期化開始 block_id=337
ブロックを初期化完了 self.block_id=337
ブロックを初期化開始 block_id=338
ブロックを初期化完了 self.block_id=338
ブロックを初期化開始 block_id=339
ブロックを初期化完了 self.block_id=339
ブロックを初期化開始 block_id=340
ブロックを初期化完了 self.block_id=340
ブロックを初期化開始 block_id=341
ブロックを初期化完了 self.block_id=341
ブロックを初期化開始 block_id=342
ブロックを初期化完了 self.block_id=342
ブロックを初期化開始 block_id=343
ブロックを初期化完了 self.block_id=343
ブロックを初期化開始 block_id=344
ブロックを初期化完了 self.block_id=344
ブロックを初期化開始 block_id=345
ブロックを初期化完了 self.block_id=345
ブロックを初期化開始 block_id=346
ブロックを初期化完了 self.block_id=346
ブロックを初期化開始 block_id=347
ブロックを初期化完了 self.block_id=347
ブロックを初期化開始 block_id=348
ブロックを初期化完了 self.block_id=348
ブロックを初期化開始 block_id=349
ブロックを初期化完了 self.block_id=349
ブロックを初期化開始 block_id=350
ブロックを初期化完了 self.block_id=350
ブロックを初期化開始 block_id=351
ブロックを初期化完了 self.block_id=351
ブロックを初期化開始 block_id=352
ブロックを初期化完了 self.block_id=352
ブロックを初期化開始 block_id=353
ブロックを初期化完了 self.block_id=353
ブロックを初期化開始 block_id=354
ブロックを初期化完了 self.block_id=354
ブロックを初期化開始 block_id=355
ブロックを初期化完了 self.block_id=355
ブロックを初期化開始 block_id=356
ブロックを初期化完了 self.block_id=356
ブロックを初期化開始 block_id=357
ブロックを初期化完了 self.block_id=357
ブロックを初期化開始 block_id=358
ブロックを初期化完了 self.block_id=358
ブロックを初期化開始 block_id=359
ブロックを初期化完了 self.block_id=359
ブロックを初期化開始 block_id=360
ブロックを初期化完了 self.block_id=360
ブロックを初期化開始 block_id=361
ブロックを初期化完了 self.block_id=361
ブロックを初期化開始 block_id=362
ブロックを初期化完了 self.block_id=362
ブロックを初期化開始 block_id=363
ブロックを初期化完了 self.block_id=363
ブロックを初期化開始 block_id=364
ブロックを初期化完了 self.block_id=364
ブロックを初期化開始 block_id=365
ブロックを初期化完了 self.block_id=365
ブロックを初期化開始 block_id=366
ブロックを初期化完了 self.block_id=366
ブロックを初期化開始 block_id=367
ブロックを初期化完了 self.block_id=367
ブロックを初期化開始 block_id=368
ブロックを初期化完了 self.block_id=368
ブロックを初期化開始 block_id=369
ブロックを初期化完了 self.block_id=369
ブロックを初期化開始 block_id=370
ブロックを初期化完了 self.block_id=370
ブロックを初期化開始 block_id=371
ブロックを初期化完了 self.block_id=371
ブロックを初期化開始 block_id=372
ブロックを初期化完了 self.block_id=372
ブロックを初期化開始 block_id=373
ブロックを初期化完了 self.block_id=373
ブロックを初期化開始 block_id=374
ブロックを初期化完了 self.block_id=374
ブロックを初期化開始 block_id=375
ブロックを初期化完了 self.block_id=375
ブロックを初期化開始 block_id=376
ブロックを初期化完了 self.block_id=376
ブロックを初期化開始 block_id=377
ブロックを初期化完了 self.block_id=377
ブロックを初期化開始 block_id=378
ブロックを初期化完了 self.block_id=378
ブロックを初期化開始 block_id=379
ブロックを初期化完了 self.block_id=379
ブロックを初期化開始 block_id=380
ブロックを初期化完了 self.block_id=380
ブロックを初期化開始 block_id=381
ブロックを初期化完了 self.block_id=381
ブロックを初期化開始 block_id=382
ブロックを初期化完了 self.block_id=382
ブロックを初期化開始 block_id=383
ブロックを初期化完了 self.block_id=383
ブロックを初期化開始 block_id=384
ブロックを初期化完了 self.block_id=384
ブロックを初期化開始 block_id=385
ブロックを初期化完了 self.block_id=385
ブロックを初期化開始 block_id=386
ブロックを初期化完了 self.block_id=386
ブロックを初期化開始 block_id=387
ブロックを初期化完了 self.block_id=387
ブロックを初期化開始 block_id=388
ブロックを初期化完了 self.block_id=388
ブロックを初期化開始 block_id=389
ブロックを初期化完了 self.block_id=389
ブロックを初期化開始 block_id=390
ブロックを初期化完了 self.block_id=390
ブロックを初期化開始 block_id=391
ブロックを初期化完了 self.block_id=391
ブロックを初期化開始 block_id=392
ブロックを初期化完了 self.block_id=392
ブロックを初期化開始 block_id=393
ブロックを初期化完了 self.block_id=393
ブロックを初期化開始 block_id=394
ブロックを初期化完了 self.block_id=394
ブロックを初期化開始 block_id=395
ブロックを初期化完了 self.block_id=395
ブロックを初期化開始 block_id=396
ブロックを初期化完了 self.block_id=396
ブロックを初期化開始 block_id=397
ブロックを初期化完了 self.block_id=397
ブロックを初期化開始 block_id=398
ブロックを初期化完了 self.block_id=398
ブロックを初期化開始 block_id=399
ブロックを初期化完了 self.block_id=399
ブロックを初期化開始 block_id=400
ブロックを初期化完了 self.block_id=400
ブロックを初期化開始 block_id=401
ブロックを初期化完了 self.block_id=401
ブロックを初期化開始 block_id=402
ブロックを初期化完了 self.block_id=402
ブロックを初期化開始 block_id=403
ブロックを初期化完了 self.block_id=403
ブロックを初期化開始 block_id=404
ブロックを初期化完了 self.block_id=404
ブロックを初期化開始 block_id=405
ブロックを初期化完了 self.block_id=405
ブロックを初期化開始 block_id=406
ブロックを初期化完了 self.block_id=406
ブロックを初期化開始 block_id=407
ブロックを初期化完了 self.block_id=407
ブロックを初期化開始 block_id=408
ブロックを初期化完了 self.block_id=408
ブロックを初期化開始 block_id=409
ブロックを初期化完了 self.block_id=409
ブロックを初期化開始 block_id=410
ブロックを初期化完了 self.block_id=410
ブロックを初期化開始 block_id=411
ブロックを初期化完了 self.block_id=411
ブロックを初期化開始 block_id=412
ブロックを初期化完了 self.block_id=412
ブロックを初期化開始 block_id=413
ブロックを初期化完了 self.block_id=413
ブロックを初期化開始 block_id=414
ブロックを初期化完了 self.block_id=414
ブロックを初期化開始 block_id=415
ブロックを初期化完了 self.block_id=415
ブロックを初期化開始 block_id=416
ブロックを初期化完了 self.block_id=416
ブロックを初期化開始 block_id=417
ブロックを初期化完了 self.block_id=417
ブロックを初期化開始 block_id=418
ブロックを初期化完了 self.block_id=418
ブロックを初期化開始 block_id=419
ブロックを初期化完了 self.block_id=419
ブロックを初期化開始 block_id=420
ブロックを初期化完了 self.block_id=420
ブロックを初期化開始 block_id=421
ブロックを初期化完了 self.block_id=421
ブロックを初期化開始 block_id=422
ブロックを初期化完了 self.block_id=422
ブロックを初期化開始 block_id=423
ブロックを初期化完了 self.block_id=423
ブロックを初期化開始 block_id=424
ブロックを初期化完了 self.block_id=424
ブロックを初期化開始 block_id=425
ブロックを初期化完了 self.block_id=425
ブロックを初期化開始 block_id=426
ブロックを初期化完了 self.block_id=426
ブロックを初期化開始 block_id=427
ブロックを初期化完了 self.block_id=427
ブロックを初期化開始 block_id=428
ブロックを初期化完了 self.block_id=428
ブロックを初期化開始 block_id=429
ブロックを初期化完了 self.block_id=429
ブロックを初期化開始 block_id=430
ブロックを初期化完了 self.block_id=430
ブロックを初期化開始 block_id=431
ブロックを初期化完了 self.block_id=431
ブロックを初期化開始 block_id=432
ブロックを初期化完了 self.block_id=432
ブロックを初期化開始 block_id=433
ブロックを初期化完了 self.block_id=433
ブロックを初期化開始 block_id=434
ブロックを初期化完了 self.block_id=434
ブロックを初期化開始 block_id=435
ブロックを初期化完了 self.block_id=435
ブロックを初期化開始 block_id=436
ブロックを初期化完了 self.block_id=436
ブロックを初期化開始 block_id=437
ブロックを初期化完了 self.block_id=437
ブロックを初期化開始 block_id=438
ブロックを初期化完了 self.block_id=438
ブロックを初期化開始 block_id=439
ブロックを初期化完了 self.block_id=439
ブロックを初期化開始 block_id=440
ブロックを初期化完了 self.block_id=440
ブロックを初期化開始 block_id=441
ブロックを初期化完了 self.block_id=441
ブロックを初期化開始 block_id=442
ブロックを初期化完了 self.block_id=442
ブロックを初期化開始 block_id=443
ブロックを初期化完了 self.block_id=443
ブロックを初期化開始 block_id=444
ブロックを初期化完了 self.block_id=444
ブロックを初期化開始 block_id=445
ブロックを初期化完了 self.block_id=445
ブロックを初期化開始 block_id=446
ブロックを初期化完了 self.block_id=446
ブロックを初期化開始 block_id=447
ブロックを初期化完了 self.block_id=447
ブロックを初期化開始 block_id=448
ブロックを初期化完了 self.block_id=448
ブロックを初期化開始 block_id=449
ブロックを初期化完了 self.block_id=449
ブロックを初期化開始 block_id=450
ブロックを初期化完了 self.block_id=450
ブロックを初期化開始 block_id=451
ブロックを初期化完了 self.block_id=451
ブロックを初期化開始 block_id=452
ブロックを初期化完了 self.block_id=452
ブロックを初期化開始 block_id=453
ブロックを初期化完了 self.block_id=453
ブロックを初期化開始 block_id=454
ブロックを初期化完了 self.block_id=454
ブロックを初期化開始 block_id=455
ブロックを初期化完了 self.block_id=455
ブロックを初期化開始 block_id=456
ブロックを初期化完了 self.block_id=456
ブロックを初期化開始 block_id=457
ブロックを初期化完了 self.block_id=457
ブロックを初期化開始 block_id=458
ブロックを初期化完了 self.block_id=458
ブロックを初期化開始 block_id=459
ブロックを初期化完了 self.block_id=459
ブロックを初期化開始 block_id=460
ブロックを初期化完了 self.block_id=460
ブロックを初期化開始 block_id=461
ブロックを初期化完了 self.block_id=461
ブロックを初期化開始 block_id=462
ブロックを初期化完了 self.block_id=462
ブロックを初期化開始 block_id=463
ブロックを初期化完了 self.block_id=463
ブロックを初期化開始 block_id=464
ブロックを初期化完了 self.block_id=464
ブロックを初期化開始 block_id=465
ブロックを初期化完了 self.block_id=465
ブロックを初期化開始 block_id=466
ブロックを初期化完了 self.block_id=466
ブロックを初期化開始 block_id=467
ブロックを初期化完了 self.block_id=467
ブロックを初期化開始 block_id=468
ブロックを初期化完了 self.block_id=468
ブロックを初期化開始 block_id=469
ブロックを初期化完了 self.block_id=469
ブロックを初期化開始 block_id=470
ブロックを初期化完了 self.block_id=470
ブロックを初期化開始 block_id=471
ブロックを初期化完了 self.block_id=471
ブロックを初期化開始 block_id=472
ブロックを初期化完了 self.block_id=472
ブロックを初期化開始 block_id=473
ブロックを初期化完了 self.block_id=473
ブロックを初期化開始 block_id=474
ブロックを初期化完了 self.block_id=474
ブロックを初期化開始 block_id=475
ブロックを初期化完了 self.block_id=475
ブロックを初期化開始 block_id=476
ブロックを初期化完了 self.block_id=476
ブロックを初期化開始 block_id=477
ブロックを初期化完了 self.block_id=477
ブロックを初期化開始 block_id=478
ブロックを初期化完了 self.block_id=478
ブロックを初期化開始 block_id=479
ブロックを初期化完了 self.block_id=479
ブロックを初期化開始 block_id=480
ブロックを初期化完了 self.block_id=480
ブロックを初期化開始 block_id=481
ブロックを初期化完了 self.block_id=481
ブロックを初期化開始 block_id=482
ブロックを初期化完了 self.block_id=482
ブロックを初期化開始 block_id=483
ブロックを初期化完了 self.block_id=483
ブロックを初期化開始 block_id=484
ブロックを初期化完了 self.block_id=484
ブロックを初期化開始 block_id=485
ブロックを初期化完了 self.block_id=485
ブロックを初期化開始 block_id=486
ブロックを初期化完了 self.block_id=486
ブロックを初期化開始 block_id=487
ブロックを初期化完了 self.block_id=487
ブロックを初期化開始 block_id=488
ブロックを初期化完了 self.block_id=488
ブロックを初期化開始 block_id=489
ブロックを初期化完了 self.block_id=489
ブロックを初期化開始 block_id=490
ブロックを初期化完了 self.block_id=490
ブロックを初期化開始 block_id=491
ブロックを初期化完了 self.block_id=491
ブロックを初期化開始 block_id=492
ブロックを初期化完了 self.block_id=492
ブロックを初期化開始 block_id=493
ブロックを初期化完了 self.block_id=493
ブロックを初期化開始 block_id=494
ブロックを初期化完了 self.block_id=494
ブロックを初期化開始 block_id=495
ブロックを初期化完了 self.block_id=495
ブロックを初期化開始 block_id=496
ブロックを初期化完了 self.block_id=496
ブロックを初期化開始 block_id=497
ブロックを初期化完了 self.block_id=497
ブロックを初期化開始 block_id=498
ブロックを初期化完了 self.block_id=498
ブロックを初期化開始 block_id=499
ブロックを初期化完了 self.block_id=499
ブロックを初期化開始 block_id=500
ブロックを初期化完了 self.block_id=500
ブロックを初期化開始 block_id=501
ブロックを初期化完了 self.block_id=501
ブロックを初期化開始 block_id=502
ブロックを初期化完了 self.block_id=502
ブロックを初期化開始 block_id=503
ブロックを初期化完了 self.block_id=503
ブロックを初期化開始 block_id=504
ブロックを初期化完了 self.block_id=504
ブロックを初期化開始 block_id=505
ブロックを初期化完了 self.block_id=505
ブロックを初期化開始 block_id=506
ブロックを初期化完了 self.block_id=506
ブロックを初期化開始 block_id=507
ブロックを初期化完了 self.block_id=507
ブロックを初期化開始 block_id=508
ブロックを初期化完了 self.block_id=508
ブロックを初期化開始 block_id=509
ブロックを初期化完了 self.block_id=509
ブロックを初期化開始 block_id=510
ブロックを初期化完了 self.block_id=510
ブロックを初期化開始 block_id=511
ブロックを初期化完了 self.block_id=511
ブロックを初期化開始 block_id=512
ブロックを初期化完了 self.block_id=512
ブロックを初期化開始 block_id=513
ブロックを初期化完了 self.block_id=513
ブロックを初期化開始 block_id=514
ブロックを初期化完了 self.block_id=514
ブロックを初期化開始 block_id=515
ブロックを初期化完了 self.block_id=515
ブロックを初期化開始 block_id=516
ブロックを初期化完了 self.block_id=516
ブロックを初期化開始 block_id=517
ブロックを初期化完了 self.block_id=517
ブロックを初期化開始 block_id=518
ブロックを初期化完了 self.block_id=518
ブロックを初期化開始 block_id=519
ブロックを初期化完了 self.block_id=519
ブロックを初期化開始 block_id=520
ブロックを初期化完了 self.block_id=520
ブロックを初期化開始 block_id=521
ブロックを初期化完了 self.block_id=521
ブロックを初期化開始 block_id=522
ブロックを初期化完了 self.block_id=522
ブロックを初期化開始 block_id=523
ブロックを初期化完了 self.block_id=523
ブロックを初期化開始 block_id=524
ブロックを初期化完了 self.block_id=524
ブロックを初期化開始 block_id=525
ブロックを初期化完了 self.block_id=525
ブロックを初期化開始 block_id=526
ブロックを初期化完了 self.block_id=526
ブロックを初期化開始 block_id=527
ブロックを初期化完了 self.block_id=527
ブロックを初期化開始 block_id=528
ブロックを初期化完了 self.block_id=528
ブロックを初期化開始 block_id=529
ブロックを初期化完了 self.block_id=529
ブロックを初期化開始 block_id=530
ブロックを初期化完了 self.block_id=530
ブロックを初期化開始 block_id=531
ブロックを初期化完了 self.block_id=531
ブロックを初期化開始 block_id=532
ブロックを初期化完了 self.block_id=532
ブロックを初期化開始 block_id=533
ブロックを初期化完了 self.block_id=533
ブロックを初期化開始 block_id=534
ブロックを初期化完了 self.block_id=534
ブロックを初期化開始 block_id=535
ブロックを初期化完了 self.block_id=535
ブロックを初期化開始 block_id=536
ブロックを初期化完了 self.block_id=536
ブロックを初期化開始 block_id=537
ブロックを初期化完了 self.block_id=537
ブロックを初期化開始 block_id=538
ブロックを初期化完了 self.block_id=538
ブロックを初期化開始 block_id=539
ブロックを初期化完了 self.block_id=539
ブロックを初期化開始 block_id=540
ブロックを初期化完了 self.block_id=540
ブロックを初期化開始 block_id=541
ブロックを初期化完了 self.block_id=541
ブロックを初期化開始 block_id=542
ブロックを初期化完了 self.block_id=542
ブロックを初期化開始 block_id=543
ブロックを初期化完了 self.block_id=543
ブロックを初期化開始 block_id=544
ブロックを初期化完了 self.block_id=544
ブロックを初期化開始 block_id=545
ブロックを初期化完了 self.block_id=545
ブロックを初期化開始 block_id=546
ブロックを初期化完了 self.block_id=546
ブロックを初期化開始 block_id=547
ブロックを初期化完了 self.block_id=547
ブロックを初期化開始 block_id=548
ブロックを初期化完了 self.block_id=548
ブロックを初期化開始 block_id=549
ブロックを初期化完了 self.block_id=549
ブロックを初期化開始 block_id=550
ブロックを初期化完了 self.block_id=550
ブロックを初期化開始 block_id=551
ブロックを初期化完了 self.block_id=551
ブロックを初期化開始 block_id=552
ブロックを初期化完了 self.block_id=552
ブロックを初期化開始 block_id=553
ブロックを初期化完了 self.block_id=553
ブロックを初期化開始 block_id=554
ブロックを初期化完了 self.block_id=554
ブロックを初期化開始 block_id=555
ブロックを初期化完了 self.block_id=555
ブロックを初期化開始 block_id=556
ブロックを初期化完了 self.block_id=556
ブロックを初期化開始 block_id=557
ブロックを初期化完了 self.block_id=557
ブロックを初期化開始 block_id=558
ブロックを初期化完了 self.block_id=558
ブロックを初期化開始 block_id=559
ブロックを初期化完了 self.block_id=559
ブロックを初期化開始 block_id=560
ブロックを初期化完了 self.block_id=560
ブロックを初期化開始 block_id=561
ブロックを初期化完了 self.block_id=561
ブロックを初期化開始 block_id=562
ブロックを初期化完了 self.block_id=562
ブロックを初期化開始 block_id=563
ブロックを初期化完了 self.block_id=563
ブロックを初期化開始 block_id=564
ブロックを初期化完了 self.block_id=564
ブロックを初期化開始 block_id=565
ブロックを初期化完了 self.block_id=565
ブロックを初期化開始 block_id=566
ブロックを初期化完了 self.block_id=566
ブロックを初期化開始 block_id=567
ブロックを初期化完了 self.block_id=567
ブロックを初期化開始 block_id=568
ブロックを初期化完了 self.block_id=568
ブロックを初期化開始 block_id=569
ブロックを初期化完了 self.block_id=569
ブロックを初期化開始 block_id=570
ブロックを初期化完了 self.block_id=570
ブロックを初期化開始 block_id=571
ブロックを初期化完了 self.block_id=571
ブロックを初期化開始 block_id=572
ブロックを初期化完了 self.block_id=572
ブロックを初期化開始 block_id=573
ブロックを初期化完了 self.block_id=573
ブロックを初期化開始 block_id=574
ブロックを初期化完了 self.block_id=574
ブロックを初期化開始 block_id=575
ブロックを初期化完了 self.block_id=575
ブロックを初期化開始 block_id=576
ブロックを初期化完了 self.block_id=576
ブロックを初期化開始 block_id=577
ブロックを初期化完了 self.block_id=577
ブロックを初期化開始 block_id=578
ブロックを初期化完了 self.block_id=578
ブロックを初期化開始 block_id=579
ブロックを初期化完了 self.block_id=579
ブロックを初期化開始 block_id=580
ブロックを初期化完了 self.block_id=580
ブロックを初期化開始 block_id=581
ブロックを初期化完了 self.block_id=581
ブロックを初期化開始 block_id=582
ブロックを初期化完了 self.block_id=582
ブロックを初期化開始 block_id=583
ブロックを初期化完了 self.block_id=583
ブロックを初期化開始 block_id=584
ブロックを初期化完了 self.block_id=584
ブロックを初期化開始 block_id=585
ブロックを初期化完了 self.block_id=585
ブロックを初期化開始 block_id=586
ブロックを初期化完了 self.block_id=586
ブロックを初期化開始 block_id=587
ブロックを初期化完了 self.block_id=587
ブロックを初期化開始 block_id=588
ブロックを初期化完了 self.block_id=588
ブロックを初期化開始 block_id=589
ブロックを初期化完了 self.block_id=589
ブロックを初期化開始 block_id=590
ブロックを初期化完了 self.block_id=590
ブロックを初期化開始 block_id=591
ブロックを初期化完了 self.block_id=591
ブロックを初期化開始 block_id=592
ブロックを初期化完了 self.block_id=592
ブロックを初期化開始 block_id=593
ブロックを初期化完了 self.block_id=593
ブロックを初期化開始 block_id=594
ブロックを初期化完了 self.block_id=594
ブロックを初期化開始 block_id=595
ブロックを初期化完了 self.block_id=595
ブロックを初期化開始 block_id=596
ブロックを初期化完了 self.block_id=596
ブロックを初期化開始 block_id=597
ブロックを初期化完了 self.block_id=597
ブロックを初期化開始 block_id=598
ブロックを初期化完了 self.block_id=598
ブロックを初期化開始 block_id=599
ブロックを初期化完了 self.block_id=599
ブロックを初期化開始 block_id=600
ブロックを初期化完了 self.block_id=600
ブロックを初期化開始 block_id=601
ブロックを初期化完了 self.block_id=601
ブロックを初期化開始 block_id=602
ブロックを初期化完了 self.block_id=602
ブロックを初期化開始 block_id=603
ブロックを初期化完了 self.block_id=603
ブロックを初期化開始 block_id=604
ブロックを初期化完了 self.block_id=604
ブロックを初期化開始 block_id=605
ブロックを初期化完了 self.block_id=605
ブロックを初期化開始 block_id=606
ブロックを初期化完了 self.block_id=606
ブロックを初期化開始 block_id=607
ブロックを初期化完了 self.block_id=607
ブロックを初期化開始 block_id=608
ブロックを初期化完了 self.block_id=608
ブロックを初期化開始 block_id=609
ブロックを初期化完了 self.block_id=609
ブロックを初期化開始 block_id=610
ブロックを初期化完了 self.block_id=610
ブロックを初期化開始 block_id=611
ブロックを初期化完了 self.block_id=611
ブロックを初期化開始 block_id=612
ブロックを初期化完了 self.block_id=612
ブロックを初期化開始 block_id=613
ブロックを初期化完了 self.block_id=613
ブロックを初期化開始 block_id=614
ブロックを初期化完了 self.block_id=614
ブロックを初期化開始 block_id=615
ブロックを初期化完了 self.block_id=615
ブロックを初期化開始 block_id=616
ブロックを初期化完了 self.block_id=616
ブロックを初期化開始 block_id=617
ブロックを初期化完了 self.block_id=617
ブロックを初期化開始 block_id=618
ブロックを初期化完了 self.block_id=618
ブロックを初期化開始 block_id=619
ブロックを初期化完了 self.block_id=619
ブロックを初期化開始 block_id=620
ブロックを初期化完了 self.block_id=620
ブロックを初期化開始 block_id=621
ブロックを初期化完了 self.block_id=621
ブロックを初期化開始 block_id=622
ブロックを初期化完了 self.block_id=622
ブロックを初期化開始 block_id=623
ブロックを初期化完了 self.block_id=623
ブロックを初期化開始 block_id=624
ブロックを初期化完了 self.block_id=624
ブロックを初期化開始 block_id=625
ブロックを初期化完了 self.block_id=625
ブロックを初期化開始 block_id=626
ブロックを初期化完了 self.block_id=626
ブロックを初期化開始 block_id=627
ブロックを初期化完了 self.block_id=627
ブロックを初期化開始 block_id=628
ブロックを初期化完了 self.block_id=628
ブロックを初期化開始 block_id=629
ブロックを初期化完了 self.block_id=629
ブロックを初期化開始 block_id=630
ブロックを初期化完了 self.block_id=630
ブロックを初期化開始 block_id=631
ブロックを初期化完了 self.block_id=631
ブロックを初期化開始 block_id=632
ブロックを初期化完了 self.block_id=632
ブロックを初期化開始 block_id=633
ブロックを初期化完了 self.block_id=633
ブロックを初期化開始 block_id=634
ブロックを初期化完了 self.block_id=634
ブロックを初期化開始 block_id=635
ブロックを初期化完了 self.block_id=635
ブロックを初期化開始 block_id=636
ブロックを初期化完了 self.block_id=636
ブロックを初期化開始 block_id=637
ブロックを初期化完了 self.block_id=637
ブロックを初期化開始 block_id=638
ブロックを初期化完了 self.block_id=638
ブロックを初期化開始 block_id=639
ブロックを初期化完了 self.block_id=639
ブロックを初期化開始 block_id=640
ブロックを初期化完了 self.block_id=640
ブロックを初期化開始 block_id=641
ブロックを初期化完了 self.block_id=641
ブロックを初期化開始 block_id=642
ブロックを初期化完了 self.block_id=642
ブロックを初期化開始 block_id=643
ブロックを初期化完了 self.block_id=643
ブロックを初期化開始 block_id=644
ブロックを初期化完了 self.block_id=644
ブロックを初期化開始 block_id=645
ブロックを初期化完了 self.block_id=645
ブロックを初期化開始 block_id=646
ブロックを初期化完了 self.block_id=646
ブロックを初期化開始 block_id=647
ブロックを初期化完了 self.block_id=647
ブロックを初期化開始 block_id=648
ブロックを初期化完了 self.block_id=648
ブロックを初期化開始 block_id=649
ブロックを初期化完了 self.block_id=649
ブロックを初期化開始 block_id=650
ブロックを初期化完了 self.block_id=650
ブロックを初期化開始 block_id=651
ブロックを初期化完了 self.block_id=651
ブロックを初期化開始 block_id=652
ブロックを初期化完了 self.block_id=652
ブロックを初期化開始 block_id=653
ブロックを初期化完了 self.block_id=653
ブロックを初期化開始 block_id=654
ブロックを初期化完了 self.block_id=654
ブロックを初期化開始 block_id=655
ブロックを初期化完了 self.block_id=655
ブロックを初期化開始 block_id=656
ブロックを初期化完了 self.block_id=656
ブロックを初期化開始 block_id=657
ブロックを初期化完了 self.block_id=657
ブロックを初期化開始 block_id=658
ブロックを初期化完了 self.block_id=658
ブロックを初期化開始 block_id=659
ブロックを初期化完了 self.block_id=659
ブロックを初期化開始 block_id=660
ブロックを初期化完了 self.block_id=660
ブロックを初期化開始 block_id=661
ブロックを初期化完了 self.block_id=661
ブロックを初期化開始 block_id=662
ブロックを初期化完了 self.block_id=662
ブロックを初期化開始 block_id=663
ブロックを初期化完了 self.block_id=663
ブロックを初期化開始 block_id=664
ブロックを初期化完了 self.block_id=664
ブロックを初期化開始 block_id=665
ブロックを初期化完了 self.block_id=665
ブロックを初期化開始 block_id=666
ブロックを初期化完了 self.block_id=666
ブロックを初期化開始 block_id=667
ブロックを初期化完了 self.block_id=667
ブロックを初期化開始 block_id=668
ブロックを初期化完了 self.block_id=668
ブロックを初期化開始 block_id=669
ブロックを初期化完了 self.block_id=669
ブロックを初期化開始 block_id=670
ブロックを初期化完了 self.block_id=670
ブロックを初期化開始 block_id=671
ブロックを初期化完了 self.block_id=671
ブロックを初期化開始 block_id=672
ブロックを初期化完了 self.block_id=672
ブロックを初期化開始 block_id=673
ブロックを初期化完了 self.block_id=673
ブロックを初期化開始 block_id=674
ブロックを初期化完了 self.block_id=674
ブロックを初期化開始 block_id=675
ブロックを初期化完了 self.block_id=675
ブロックを初期化開始 block_id=676
ブロックを初期化完了 self.block_id=676
ブロックを初期化開始 block_id=677
ブロックを初期化完了 self.block_id=677
ブロックを初期化開始 block_id=678
ブロックを初期化完了 self.block_id=678
ブロックを初期化開始 block_id=679
ブロックを初期化完了 self.block_id=679
ブロックを初期化開始 block_id=680
ブロックを初期化完了 self.block_id=680
ブロックを初期化開始 block_id=681
ブロックを初期化完了 self.block_id=681
ブロックを初期化開始 block_id=682
ブロックを初期化完了 self.block_id=682
ブロックを初期化開始 block_id=683
ブロックを初期化完了 self.block_id=683
ブロックを初期化開始 block_id=684
ブロックを初期化完了 self.block_id=684
ブロックを初期化開始 block_id=685
ブロックを初期化完了 self.block_id=685
ブロックを初期化開始 block_id=686
ブロックを初期化完了 self.block_id=686
ブロックを初期化開始 block_id=687
ブロックを初期化完了 self.block_id=687
ブロックを初期化開始 block_id=688
ブロックを初期化完了 self.block_id=688
ブロックを初期化開始 block_id=689
ブロックを初期化完了 self.block_id=689
ブロックを初期化開始 block_id=690
ブロックを初期化完了 self.block_id=690
ブロックを初期化開始 block_id=691
ブロックを初期化完了 self.block_id=691
ブロックを初期化開始 block_id=692
ブロックを初期化完了 self.block_id=692
ブロックを初期化開始 block_id=693
ブロックを初期化完了 self.block_id=693
ブロックを初期化開始 block_id=694
ブロックを初期化完了 self.block_id=694
ブロックを初期化開始 block_id=695
ブロックを初期化完了 self.block_id=695
ブロックを初期化開始 block_id=696
ブロックを初期化完了 self.block_id=696
ブロックを初期化開始 block_id=697
ブロックを初期化完了 self.block_id=697
ブロックを初期化開始 block_id=698
ブロックを初期化完了 self.block_id=698
ブロックを初期化開始 block_id=699
ブロックを初期化完了 self.block_id=699
ブロックを初期化開始 block_id=700
ブロックを初期化完了 self.block_id=700
ブロックを初期化開始 block_id=701
ブロックを初期化完了 self.block_id=701
ブロックを初期化開始 block_id=702
ブロックを初期化完了 self.block_id=702
ブロックを初期化開始 block_id=703
ブロックを初期化完了 self.block_id=703
ブロックを初期化開始 block_id=704
ブロックを初期化完了 self.block_id=704
ブロックを初期化開始 block_id=705
ブロックを初期化完了 self.block_id=705
ブロックを初期化開始 block_id=706
ブロックを初期化完了 self.block_id=706
ブロックを初期化開始 block_id=707
ブロックを初期化完了 self.block_id=707
ブロックを初期化開始 block_id=708
ブロックを初期化完了 self.block_id=708
ブロックを初期化開始 block_id=709
ブロックを初期化完了 self.block_id=709
ブロックを初期化開始 block_id=710
ブロックを初期化完了 self.block_id=710
ブロックを初期化開始 block_id=711
ブロックを初期化完了 self.block_id=711
ブロックを初期化開始 block_id=712
ブロックを初期化完了 self.block_id=712
ブロックを初期化開始 block_id=713
ブロックを初期化完了 self.block_id=713
ブロックを初期化開始 block_id=714
ブロックを初期化完了 self.block_id=714
ブロックを初期化開始 block_id=715
ブロックを初期化完了 self.block_id=715
ブロックを初期化開始 block_id=716
ブロックを初期化完了 self.block_id=716
ブロックを初期化開始 block_id=717
ブロックを初期化完了 self.block_id=717
ブロックを初期化開始 block_id=718
ブロックを初期化完了 self.block_id=718
ブロックを初期化開始 block_id=719
ブロックを初期化完了 self.block_id=719
ブロックを初期化開始 block_id=720
ブロックを初期化完了 self.block_id=720
ブロックを初期化開始 block_id=721
ブロックを初期化完了 self.block_id=721
ブロックを初期化開始 block_id=722
ブロックを初期化完了 self.block_id=722
ブロックを初期化開始 block_id=723
ブロックを初期化完了 self.block_id=723
ブロックを初期化開始 block_id=724
ブロックを初期化完了 self.block_id=724
ブロックを初期化開始 block_id=725
ブロックを初期化完了 self.block_id=725
ブロックを初期化開始 block_id=726
ブロックを初期化完了 self.block_id=726
ブロックを初期化開始 block_id=727
ブロックを初期化完了 self.block_id=727
ブロックを初期化開始 block_id=728
ブロックを初期化完了 self.block_id=728
ブロックを初期化開始 block_id=729
ブロックを初期化完了 self.block_id=729
ブロックを初期化開始 block_id=730
ブロックを初期化完了 self.block_id=730
ブロックを初期化開始 block_id=731
ブロックを初期化完了 self.block_id=731
ブロックを初期化開始 block_id=732
ブロックを初期化完了 self.block_id=732
ブロックを初期化開始 block_id=733
ブロックを初期化完了 self.block_id=733
ブロックを初期化開始 block_id=734
ブロックを初期化完了 self.block_id=734
ブロックを初期化開始 block_id=735
ブロックを初期化完了 self.block_id=735
ブロックを初期化開始 block_id=736
ブロックを初期化完了 self.block_id=736
ブロックを初期化開始 block_id=737
ブロックを初期化完了 self.block_id=737
ブロックを初期化開始 block_id=738
ブロックを初期化完了 self.block_id=738
ブロックを初期化開始 block_id=739
ブロックを初期化完了 self.block_id=739
ブロックを初期化開始 block_id=740
ブロックを初期化完了 self.block_id=740
ブロックを初期化開始 block_id=741
ブロックを初期化完了 self.block_id=741
ブロックを初期化開始 block_id=742
ブロックを初期化完了 self.block_id=742
ブロックを初期化開始 block_id=743
ブロックを初期化完了 self.block_id=743
ブロックを初期化開始 block_id=744
ブロックを初期化完了 self.block_id=744
ブロックを初期化開始 block_id=745
ブロックを初期化完了 self.block_id=745
ブロックを初期化開始 block_id=746
ブロックを初期化完了 self.block_id=746
ブロックを初期化開始 block_id=747
ブロックを初期化完了 self.block_id=747
ブロックを初期化開始 block_id=748
ブロックを初期化完了 self.block_id=748
ブロックを初期化開始 block_id=749
ブロックを初期化完了 self.block_id=749
ブロックを初期化開始 block_id=750
ブロックを初期化完了 self.block_id=750
ブロックを初期化開始 block_id=751
ブロックを初期化完了 self.block_id=751
ブロックを初期化開始 block_id=752
ブロックを初期化完了 self.block_id=752
ブロックを初期化開始 block_id=753
ブロックを初期化完了 self.block_id=753
ブロックを初期化開始 block_id=754
ブロックを初期化完了 self.block_id=754
ブロックを初期化開始 block_id=755
ブロックを初期化完了 self.block_id=755
ブロックを初期化開始 block_id=756
ブロックを初期化完了 self.block_id=756
ブロックを初期化開始 block_id=757
ブロックを初期化完了 self.block_id=757
ブロックを初期化開始 block_id=758
ブロックを初期化完了 self.block_id=758
ブロックを初期化開始 block_id=759
ブロックを初期化完了 self.block_id=759
ブロックを初期化開始 block_id=760
ブロックを初期化完了 self.block_id=760
ブロックを初期化開始 block_id=761
ブロックを初期化完了 self.block_id=761
ブロックを初期化開始 block_id=762
ブロックを初期化完了 self.block_id=762
ブロックを初期化開始 block_id=763
ブロックを初期化完了 self.block_id=763
ブロックを初期化開始 block_id=764
ブロックを初期化完了 self.block_id=764
ブロックを初期化開始 block_id=765
ブロックを初期化完了 self.block_id=765
ブロックを初期化開始 block_id=766
ブロックを初期化完了 self.block_id=766
ブロックを初期化開始 block_id=767
ブロックを初期化完了 self.block_id=767
ブロックを初期化開始 block_id=768
ブロックを初期化完了 self.block_id=768
ブロックを初期化開始 block_id=769
ブロックを初期化完了 self.block_id=769
ブロックを初期化開始 block_id=770
ブロックを初期化完了 self.block_id=770
ブロックを初期化開始 block_id=771
ブロックを初期化完了 self.block_id=771
ブロックを初期化開始 block_id=772
ブロックを初期化完了 self.block_id=772
ブロックを初期化開始 block_id=773
ブロックを初期化完了 self.block_id=773
ブロックを初期化開始 block_id=774
ブロックを初期化完了 self.block_id=774
ブロックを初期化開始 block_id=775
ブロックを初期化完了 self.block_id=775
ブロックを初期化開始 block_id=776
ブロックを初期化完了 self.block_id=776
ブロックを初期化開始 block_id=777
ブロックを初期化完了 self.block_id=777
ブロックを初期化開始 block_id=778
ブロックを初期化完了 self.block_id=778
ブロックを初期化開始 block_id=779
ブロックを初期化完了 self.block_id=779
ブロックを初期化開始 block_id=780
ブロックを初期化完了 self.block_id=780
ブロックを初期化開始 block_id=781
ブロックを初期化完了 self.block_id=781
ブロックを初期化開始 block_id=782
ブロックを初期化完了 self.block_id=782
ブロックを初期化開始 block_id=783
ブロックを初期化完了 self.block_id=783
ブロックを初期化開始 block_id=784
ブロックを初期化完了 self.block_id=784
ブロックを初期化開始 block_id=785
ブロックを初期化完了 self.block_id=785
ブロックを初期化開始 block_id=786
ブロックを初期化完了 self.block_id=786
ブロックを初期化開始 block_id=787
ブロックを初期化完了 self.block_id=787
ブロックを初期化開始 block_id=788
ブロックを初期化完了 self.block_id=788
ブロックを初期化開始 block_id=789
ブロックを初期化完了 self.block_id=789
ブロックを初期化開始 block_id=790
ブロックを初期化完了 self.block_id=790
ブロックを初期化開始 block_id=791
ブロックを初期化完了 self.block_id=791
ブロックを初期化開始 block_id=792
ブロックを初期化完了 self.block_id=792
ブロックを初期化開始 block_id=793
ブロックを初期化完了 self.block_id=793
ブロックを初期化開始 block_id=794
ブロックを初期化完了 self.block_id=794
ブロックを初期化開始 block_id=795
ブロックを初期化完了 self.block_id=795
ブロックを初期化開始 block_id=796
ブロックを初期化完了 self.block_id=796
ブロックを初期化開始 block_id=797
ブロックを初期化完了 self.block_id=797
ブロックを初期化開始 block_id=798
ブロックを初期化完了 self.block_id=798
ブロックを初期化開始 block_id=799
ブロックを初期化完了 self.block_id=799
ブロックを初期化開始 block_id=800
ブロックを初期化完了 self.block_id=800
ブロックを初期化開始 block_id=801
ブロックを初期化完了 self.block_id=801
ブロックを初期化開始 block_id=802
ブロックを初期化完了 self.block_id=802
ブロックを初期化開始 block_id=803
ブロックを初期化完了 self.block_id=803
ブロックを初期化開始 block_id=804
ブロックを初期化完了 self.block_id=804
ブロックを初期化開始 block_id=805
ブロックを初期化完了 self.block_id=805
ブロックを初期化開始 block_id=806
ブロックを初期化完了 self.block_id=806
ブロックを初期化開始 block_id=807
ブロックを初期化完了 self.block_id=807
ブロックを初期化開始 block_id=808
ブロックを初期化完了 self.block_id=808
ブロックを初期化開始 block_id=809
ブロックを初期化完了 self.block_id=809
ブロックを初期化開始 block_id=810
ブロックを初期化完了 self.block_id=810
ブロックを初期化開始 block_id=811
ブロックを初期化完了 self.block_id=811
ブロックを初期化開始 block_id=812
ブロックを初期化完了 self.block_id=812
ブロックを初期化開始 block_id=813
ブロックを初期化完了 self.block_id=813
ブロックを初期化開始 block_id=814
ブロックを初期化完了 self.block_id=814
ブロックを初期化開始 block_id=815
ブロックを初期化完了 self.block_id=815
ブロックを初期化開始 block_id=816
ブロックを初期化完了 self.block_id=816
ブロックを初期化開始 block_id=817
ブロックを初期化完了 self.block_id=817
ブロックを初期化開始 block_id=818
ブロックを初期化完了 self.block_id=818
ブロックを初期化開始 block_id=819
ブロックを初期化完了 self.block_id=819
ブロックを初期化開始 block_id=820
ブロックを初期化完了 self.block_id=820
ブロックを初期化開始 block_id=821
ブロックを初期化完了 self.block_id=821
ブロックを初期化開始 block_id=822
ブロックを初期化完了 self.block_id=822
ブロックを初期化開始 block_id=823
ブロックを初期化完了 self.block_id=823
ブロックを初期化開始 block_id=824
ブロックを初期化完了 self.block_id=824
ブロックを初期化開始 block_id=825
ブロックを初期化完了 self.block_id=825
ブロックを初期化開始 block_id=826
ブロックを初期化完了 self.block_id=826
ブロックを初期化開始 block_id=827
ブロックを初期化完了 self.block_id=827
ブロックを初期化開始 block_id=828
ブロックを初期化完了 self.block_id=828
ブロックを初期化開始 block_id=829
ブロックを初期化完了 self.block_id=829
ブロックを初期化開始 block_id=830
ブロックを初期化完了 self.block_id=830
ブロックを初期化開始 block_id=831
ブロックを初期化完了 self.block_id=831
ブロックを初期化開始 block_id=832
ブロックを初期化完了 self.block_id=832
ブロックを初期化開始 block_id=833
ブロックを初期化完了 self.block_id=833
ブロックを初期化開始 block_id=834
ブロックを初期化完了 self.block_id=834
ブロックを初期化開始 block_id=835
ブロックを初期化完了 self.block_id=835
ブロックを初期化開始 block_id=836
ブロックを初期化完了 self.block_id=836
ブロックを初期化開始 block_id=837
ブロックを初期化完了 self.block_id=837
ブロックを初期化開始 block_id=838
ブロックを初期化完了 self.block_id=838
ブロックを初期化開始 block_id=839
ブロックを初期化完了 self.block_id=839
ブロックを初期化開始 block_id=840
ブロックを初期化完了 self.block_id=840
ブロックを初期化開始 block_id=841
ブロックを初期化完了 self.block_id=841
ブロックを初期化開始 block_id=842
ブロックを初期化完了 self.block_id=842
ブロックを初期化開始 block_id=843
ブロックを初期化完了 self.block_id=843
ブロックを初期化開始 block_id=844
ブロックを初期化完了 self.block_id=844
ブロックを初期化開始 block_id=845
ブロックを初期化完了 self.block_id=845
ブロックを初期化開始 block_id=846
ブロックを初期化完了 self.block_id=846
ブロックを初期化開始 block_id=847
ブロックを初期化完了 self.block_id=847
ブロックを初期化開始 block_id=848
ブロックを初期化完了 self.block_id=848
ブロックを初期化開始 block_id=849
ブロックを初期化完了 self.block_id=849
ブロックを初期化開始 block_id=850
ブロックを初期化完了 self.block_id=850
ブロックを初期化開始 block_id=851
ブロックを初期化完了 self.block_id=851
ブロックを初期化開始 block_id=852
ブロックを初期化完了 self.block_id=852
ブロックを初期化開始 block_id=853
ブロックを初期化完了 self.block_id=853
ブロックを初期化開始 block_id=854
ブロックを初期化完了 self.block_id=854
ブロックを初期化開始 block_id=855
ブロックを初期化完了 self.block_id=855
ブロックを初期化開始 block_id=856
ブロックを初期化完了 self.block_id=856
ブロックを初期化開始 block_id=857
ブロックを初期化完了 self.block_id=857
ブロックを初期化開始 block_id=858
ブロックを初期化完了 self.block_id=858
ブロックを初期化開始 block_id=859
ブロックを初期化完了 self.block_id=859
ブロックを初期化開始 block_id=860
ブロックを初期化完了 self.block_id=860
ブロックを初期化開始 block_id=861
ブロックを初期化完了 self.block_id=861
ブロックを初期化開始 block_id=862
ブロックを初期化完了 self.block_id=862
ブロックを初期化開始 block_id=863
ブロックを初期化完了 self.block_id=863
ブロックを初期化開始 block_id=864
ブロックを初期化完了 self.block_id=864
ブロックを初期化開始 block_id=865
ブロックを初期化完了 self.block_id=865
ブロックを初期化開始 block_id=866
ブロックを初期化完了 self.block_id=866
ブロックを初期化開始 block_id=867
ブロックを初期化完了 self.block_id=867
ブロックを初期化開始 block_id=868
ブロックを初期化完了 self.block_id=868
ブロックを初期化開始 block_id=869
ブロックを初期化完了 self.block_id=869
ブロックを初期化開始 block_id=870
ブロックを初期化完了 self.block_id=870
ブロックを初期化開始 block_id=871
ブロックを初期化完了 self.block_id=871
ブロックを初期化開始 block_id=872
ブロックを初期化完了 self.block_id=872
ブロックを初期化開始 block_id=873
ブロックを初期化完了 self.block_id=873
ブロックを初期化開始 block_id=874
ブロックを初期化完了 self.block_id=874
ブロックを初期化開始 block_id=875
ブロックを初期化完了 self.block_id=875
ブロックを初期化開始 block_id=876
ブロックを初期化完了 self.block_id=876
ブロックを初期化開始 block_id=877
ブロックを初期化完了 self.block_id=877
ブロックを初期化開始 block_id=878
ブロックを初期化完了 self.block_id=878
ブロックを初期化開始 block_id=879
ブロックを初期化完了 self.block_id=879
ブロックを初期化開始 block_id=880
ブロックを初期化完了 self.block_id=880
ブロックを初期化開始 block_id=881
ブロックを初期化完了 self.block_id=881
ブロックを初期化開始 block_id=882
ブロックを初期化完了 self.block_id=882
ブロックを初期化開始 block_id=883
ブロックを初期化完了 self.block_id=883
ブロックを初期化開始 block_id=884
ブロックを初期化完了 self.block_id=884
ブロックを初期化開始 block_id=885
ブロックを初期化完了 self.block_id=885
ブロックを初期化開始 block_id=886
ブロックを初期化完了 self.block_id=886
ブロックを初期化開始 block_id=887
ブロックを初期化完了 self.block_id=887
ブロックを初期化開始 block_id=888
ブロックを初期化完了 self.block_id=888
ブロックを初期化開始 block_id=889
ブロックを初期化完了 self.block_id=889
ブロックを初期化開始 block_id=890
ブロックを初期化完了 self.block_id=890
ブロックを初期化開始 block_id=891
ブロックを初期化完了 self.block_id=891
ブロックを初期化開始 block_id=892
ブロックを初期化完了 self.block_id=892
ブロックマネージャーを初期化完了
スケジューラを初期化完了
エンジンの初期化完了
SamplingParams初期化の後処理 self=SamplingParams(temperature=0.6, max_tokens=1, ignore_eos=False)
テキスト生成を開始 len(prompts)=1 sampling_params=SamplingParams(temperature=0.6, max_tokens=1, ignore_eos=False)
リクエストをキューに登録開始 prompt='Hello' sampling_params=SamplingParams(temperature=0.6, max_tokens=1, ignore_eos=False)
シーケンスを初期化 len(token_ids)=1, sampling_params=SamplingParams(temperature=0.6, max_tokens=1, ignore_eos=False)
シーケンスを初期化完了 self.seq_id=4
スケジューラに新しいリクエストを追加 seq.seq_id=4
リクエストをキューに登録完了 seq.seq_id=4
スケジューラの完了状態を確認 result=False
スケジューラに処理すべきタスクが残っているか確認 res=False
推論エンジンの1サイクルを実行開始
スケジューリングを開始
ブロックを割り当て可能か確認 4 seq.num_blocks=1 result=True
シーケンスにブロックを割り当て開始 4
論理ブロックを取得 i=0
論理ブロックを取得完了 i=0 result=[9707]
空きブロックを割り当て開始 block_id=0
ブロックをリセット 0
空きブロックを割り当て完了 block_id=0
シーケンスにブロックを割り当て完了 4 seq.block_table=[0]
スケジューリング完了（Prefill） len(scheduled_seqs)=1
推論の1サイクルを実行開始 len(seqs)=1 is_prefill=True
コンテキストを設定 is_prefill=True cu_seqlens_q=tensor([0, 1], device='cuda:0', dtype=torch.int32) cu_seqlens_k=tensor([0, 1], device='cuda:0', dtype=torch.int32) max_seqlen_q=1 max_seqlen_k=1 slot_mapping=tensor([0], device='cuda:0', dtype=torch.int32) context_lens=None block_tables=None
サンプリングパラメータを準備開始 len(seqs)=1
サンプリングパラメータを準備完了 temperatures=tensor([0.6000], device='cuda:0')
モデルの順伝搬を実行開始 input_ids.shape=torch.Size([1]) positions.shape=torch.Size([1]) is_prefill=True
Qwen3ForCausalLMの順伝播開始 input_ids.shape=torch.Size([1]) positions.shape=torch.Size([1])
Qwen3Modelの順伝播開始 input_ids.shape=torch.Size([1]) positions.shape=torch.Size([1])
埋め込み層の順伝播開始 x.shape=torch.Size([1])
埋め込み層の順伝播完了 y.shape=torch.Size([1, 1024])
Qwen3DecoderLayerの順伝播 positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 1024]) residual is None=True
RMSNormの順伝播開始 x.shape=torch.Size([1, 1024]), residual.shape if residual is not None else None=None
正規化開始 x.shape=torch.Size([1, 1024])
正規化完了 x.shape=torch.Size([1, 1024])
Qwen3Attentionの順伝播 positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 1024])
出力次元を並列化する線形層の順伝播開始 x.shape=torch.Size([1, 1024])
出力次元を並列化する線形層の順伝播完了 result.shape=torch.Size([1, 4096])
RMSNormの順伝播開始 x.shape=torch.Size([1, 16, 128]), residual.shape if residual is not None else None=None
正規化開始 x.shape=torch.Size([1, 16, 128])
正規化完了 x.shape=torch.Size([1, 16, 128])
RMSNormの順伝播開始 x.shape=torch.Size([1, 8, 128]), residual.shape if residual is not None else None=None
正規化開始 x.shape=torch.Size([1, 8, 128])
正規化完了 x.shape=torch.Size([1, 8, 128])
RoPEの順伝播開始 positions.shape=torch.Size([1]), query.shape=torch.Size([1, 16, 128]), key.shape=torch.Size([1, 8, 128])
RoPEを適用開始 x.shape=torch.Size([1, 16, 128]), cos.shape=torch.Size([1, 1, 64]), sin.shape=torch.Size([1, 1, 64])
チャンク化 x1.shape=torch.Size([1, 16, 64]), x2.shape=torch.Size([1, 16, 64])
回転行列適用 y1.shape=torch.Size([1, 16, 64]), y2.shape=torch.Size([1, 16, 64])
RoPEを適用完了 result.shape=torch.Size([1, 16, 128])
RoPEを適用開始 x.shape=torch.Size([1, 8, 128]), cos.shape=torch.Size([1, 1, 64]), sin.shape=torch.Size([1, 1, 64])
チャンク化 x1.shape=torch.Size([1, 8, 64]), x2.shape=torch.Size([1, 8, 64])
回転行列適用 y1.shape=torch.Size([1, 8, 64]), y2.shape=torch.Size([1, 8, 64])
RoPEを適用完了 result.shape=torch.Size([1, 8, 128])
RoPEの順伝播完了 query.shape=torch.Size([1, 16, 128]), key.shape=torch.Size([1, 8, 128])
Attention層の順伝播開始 q.shape=torch.Size([1, 16, 128]), k.shape=torch.Size([1, 8, 128]), v.shape=torch.Size([1, 8, 128])
現在のコンテキスト取得 _CONTEXT=Context(is_prefill=True, cu_seqlens_q=tensor([0, 1], device='cuda:0', dtype=torch.int32), cu_seqlens_k=tensor([0, 1], device='cuda:0', dtype=torch.int32), max_seqlen_q=1, max_seqlen_k=1, slot_mapping=tensor([0], device='cuda:0', dtype=torch.int32), context_lens=None, block_tables=None)
コンテキスト情報取得 context.is_prefill=True, context.slot_mapping=tensor([0], device='cuda:0', dtype=torch.int32), context.max_seqlen_q=1, context.cu_seqlens_q=tensor([0, 1], device='cuda:0', dtype=torch.int32), context.max_seqlen_k=1, context.cu_seqlens_k=tensor([0, 1], device='cuda:0', dtype=torch.int32), context.context_lens=None, context.block_tables=None
KVキャッシュにキーとバリューを保存開始 key.shape=torch.Size([1, 8, 128]), value.shape=torch.Size([1, 8, 128]), k_cache.shape=torch.Size([893, 256, 8, 128]), v_cache.shape=torch.Size([893, 256, 8, 128]), slot_mapping.shape=torch.Size([1])
KVキャッシュにキーとバリューを保存完了
PrefillモードでFlashAttentionを実行 context.max_seqlen_q=1, context.cu_seqlens_q=tensor([0, 1], device='cuda:0', dtype=torch.int32), context.max_seqlen_k=1, context.cu_seqlens_k=tensor([0, 1], device='cuda:0', dtype=torch.int32)
Attention層の順伝播完了 o.shape=torch.Size([1, 16, 128])
RowParallelLinearの順伝播開始 入力形状: torch.Size([1, 2048])
RowParallelLinearの順伝播完了 出力形状: torch.Size([1, 1024])
Qwen3Attentionの順伝播完了 output.shape=torch.Size([1, 1024])
RMSNormの順伝播開始 x.shape=torch.Size([1, 1024]), residual.shape if residual is not None else None=torch.Size([1, 1024])
残差接続を適用
残差接続を適用完了
Qwen3MLPの順伝播 x.shape=torch.Size([1, 1024])
出力次元を並列化する線形層の順伝播開始 x.shape=torch.Size([1, 1024])
出力次元を並列化する線形層の順伝播完了 result.shape=torch.Size([1, 6144])
SwiGLUを順伝播開始 x.shape=torch.Size([1, 6144])
チャンク化 x.shape=torch.Size([1, 3072]), y.shape=torch.Size([1, 3072])
SwiGLUを順伝播完了 result.shape=torch.Size([1, 3072])
RowParallelLinearの順伝播開始 入力形状: torch.Size([1, 3072])
RowParallelLinearの順伝播完了 出力形状: torch.Size([1, 1024])
Qwen3MLPの順伝播完了 x.shape=torch.Size([1, 1024])
Qwen3DecoderLayerの順伝播完了 hidden_states.shape=torch.Size([1, 1024])
Qwen3DecoderLayerの順伝播 positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 1024]) residual is None=False
RMSNormの順伝播開始 x.shape=torch.Size([1, 1024]), residual.shape if residual is not None else None=torch.Size([1, 1024])
残差接続を適用
残差接続を適用完了
Qwen3Attentionの順伝播 positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 1024])
出力次元を並列化する線形層の順伝播開始 x.shape=torch.Size([1, 1024])
出力次元を並列化する線形層の順伝播完了 result.shape=torch.Size([1, 4096])
RMSNormの順伝播開始 x.shape=torch.Size([1, 16, 128]), residual.shape if residual is not None else None=None
正規化開始 x.shape=torch.Size([1, 16, 128])
正規化完了 x.shape=torch.Size([1, 16, 128])
RMSNormの順伝播開始 x.shape=torch.Size([1, 8, 128]), residual.shape if residual is not None else None=None
正規化開始 x.shape=torch.Size([1, 8, 128])
正規化完了 x.shape=torch.Size([1, 8, 128])
RoPEの順伝播開始 positions.shape=torch.Size([1]), query.shape=torch.Size([1, 16, 128]), key.shape=torch.Size([1, 8, 128])
RoPEを適用開始 x.shape=torch.Size([1, 16, 128]), cos.shape=torch.Size([1, 1, 64]), sin.shape=torch.Size([1, 1, 64])
チャンク化 x1.shape=torch.Size([1, 16, 64]), x2.shape=torch.Size([1, 16, 64])
回転行列適用 y1.shape=torch.Size([1, 16, 64]), y2.shape=torch.Size([1, 16, 64])
RoPEを適用完了 result.shape=torch.Size([1, 16, 128])
RoPEを適用開始 x.shape=torch.Size([1, 8, 128]), cos.shape=torch.Size([1, 1, 64]), sin.shape=torch.Size([1, 1, 64])
チャンク化 x1.shape=torch.Size([1, 8, 64]), x2.shape=torch.Size([1, 8, 64])
回転行列適用 y1.shape=torch.Size([1, 8, 64]), y2.shape=torch.Size([1, 8, 64])
RoPEを適用完了 result.shape=torch.Size([1, 8, 128])
RoPEの順伝播完了 query.shape=torch.Size([1, 16, 128]), key.shape=torch.Size([1, 8, 128])
Attention層の順伝播開始 q.shape=torch.Size([1, 16, 128]), k.shape=torch.Size([1, 8, 128]), v.shape=torch.Size([1, 8, 128])
現在のコンテキスト取得 _CONTEXT=Context(is_prefill=True, cu_seqlens_q=tensor([0, 1], device='cuda:0', dtype=torch.int32), cu_seqlens_k=tensor([0, 1], device='cuda:0', dtype=torch.int32), max_seqlen_q=1, max_seqlen_k=1, slot_mapping=tensor([0], device='cuda:0', dtype=torch.int32), context_lens=None, block_tables=None)
コンテキスト情報取得 context.is_prefill=True, context.slot_mapping=tensor([0], device='cuda:0', dtype=torch.int32), context.max_seqlen_q=1, context.cu_seqlens_q=tensor([0, 1], device='cuda:0', dtype=torch.int32), context.max_seqlen_k=1, context.cu_seqlens_k=tensor([0, 1], device='cuda:0', dtype=torch.int32), context.context_lens=None, context.block_tables=None
KVキャッシュにキーとバリューを保存開始 key.shape=torch.Size([1, 8, 128]), value.shape=torch.Size([1, 8, 128]), k_cache.shape=torch.Size([893, 256, 8, 128]), v_cache.shape=torch.Size([893, 256, 8, 128]), slot_mapping.shape=torch.Size([1])
KVキャッシュにキーとバリューを保存完了
PrefillモードでFlashAttentionを実行 context.max_seqlen_q=1, context.cu_seqlens_q=tensor([0, 1], device='cuda:0', dtype=torch.int32), context.max_seqlen_k=1, context.cu_seqlens_k=tensor([0, 1], device='cuda:0', dtype=torch.int32)
Attention層の順伝播完了 o.shape=torch.Size([1, 16, 128])
RowParallelLinearの順伝播開始 入力形状: torch.Size([1, 2048])
RowParallelLinearの順伝播完了 出力形状: torch.Size([1, 1024])
Qwen3Attentionの順伝播完了 output.shape=torch.Size([1, 1024])
RMSNormの順伝播開始 x.shape=torch.Size([1, 1024]), residual.shape if residual is not None else None=torch.Size([1, 1024])
残差接続を適用
残差接続を適用完了
Qwen3MLPの順伝播 x.shape=torch.Size([1, 1024])
出力次元を並列化する線形層の順伝播開始 x.shape=torch.Size([1, 1024])
出力次元を並列化する線形層の順伝播完了 result.shape=torch.Size([1, 6144])
SwiGLUを順伝播開始 x.shape=torch.Size([1, 6144])
チャンク化 x.shape=torch.Size([1, 3072]), y.shape=torch.Size([1, 3072])
SwiGLUを順伝播完了 result.shape=torch.Size([1, 3072])
RowParallelLinearの順伝播開始 入力形状: torch.Size([1, 3072])
RowParallelLinearの順伝播完了 出力形状: torch.Size([1, 1024])
Qwen3MLPの順伝播完了 x.shape=torch.Size([1, 1024])
Qwen3DecoderLayerの順伝播完了 hidden_states.shape=torch.Size([1, 1024])
Qwen3DecoderLayerの順伝播 positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 1024]) residual is None=False
RMSNormの順伝播開始 x.shape=torch.Size([1, 1024]), residual.shape if residual is not None else None=torch.Size([1, 1024])
残差接続を適用
残差接続を適用完了
Qwen3Attentionの順伝播 positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 1024])
出力次元を並列化する線形層の順伝播開始 x.shape=torch.Size([1, 1024])
出力次元を並列化する線形層の順伝播完了 result.shape=torch.Size([1, 4096])
RMSNormの順伝播開始 x.shape=torch.Size([1, 16, 128]), residual.shape if residual is not None else None=None
正規化開始 x.shape=torch.Size([1, 16, 128])
正規化完了 x.shape=torch.Size([1, 16, 128])
RMSNormの順伝播開始 x.shape=torch.Size([1, 8, 128]), residual.shape if residual is not None else None=None
正規化開始 x.shape=torch.Size([1, 8, 128])
正規化完了 x.shape=torch.Size([1, 8, 128])
RoPEの順伝播開始 positions.shape=torch.Size([1]), query.shape=torch.Size([1, 16, 128]), key.shape=torch.Size([1, 8, 128])
RoPEを適用開始 x.shape=torch.Size([1, 16, 128]), cos.shape=torch.Size([1, 1, 64]), sin.shape=torch.Size([1, 1, 64])
チャンク化 x1.shape=torch.Size([1, 16, 64]), x2.shape=torch.Size([1, 16, 64])
回転行列適用 y1.shape=torch.Size([1, 16, 64]), y2.shape=torch.Size([1, 16, 64])
RoPEを適用完了 result.shape=torch.Size([1, 16, 128])
RoPEを適用開始 x.shape=torch.Size([1, 8, 128]), cos.shape=torch.Size([1, 1, 64]), sin.shape=torch.Size([1, 1, 64])
チャンク化 x1.shape=torch.Size([1, 8, 64]), x2.shape=torch.Size([1, 8, 64])
回転行列適用 y1.shape=torch.Size([1, 8, 64]), y2.shape=torch.Size([1, 8, 64])
RoPEを適用完了 result.shape=torch.Size([1, 8, 128])
RoPEの順伝播完了 query.shape=torch.Size([1, 16, 128]), key.shape=torch.Size([1, 8, 128])
Attention層の順伝播開始 q.shape=torch.Size([1, 16, 128]), k.shape=torch.Size([1, 8, 128]), v.shape=torch.Size([1, 8, 128])
現在のコンテキスト取得 _CONTEXT=Context(is_prefill=True, cu_seqlens_q=tensor([0, 1], device='cuda:0', dtype=torch.int32), cu_seqlens_k=tensor([0, 1], device='cuda:0', dtype=torch.int32), max_seqlen_q=1, max_seqlen_k=1, slot_mapping=tensor([0], device='cuda:0', dtype=torch.int32), context_lens=None, block_tables=None)
コンテキスト情報取得 context.is_prefill=True, context.slot_mapping=tensor([0], device='cuda:0', dtype=torch.int32), context.max_seqlen_q=1, context.cu_seqlens_q=tensor([0, 1], device='cuda:0', dtype=torch.int32), context.max_seqlen_k=1, context.cu_seqlens_k=tensor([0, 1], device='cuda:0', dtype=torch.int32), context.context_lens=None, context.block_tables=None
KVキャッシュにキーとバリューを保存開始 key.shape=torch.Size([1, 8, 128]), value.shape=torch.Size([1, 8, 128]), k_cache.shape=torch.Size([893, 256, 8, 128]), v_cache.shape=torch.Size([893, 256, 8, 128]), slot_mapping.shape=torch.Size([1])
KVキャッシュにキーとバリューを保存完了
PrefillモードでFlashAttentionを実行 context.max_seqlen_q=1, context.cu_seqlens_q=tensor([0, 1], device='cuda:0', dtype=torch.int32), context.max_seqlen_k=1, context.cu_seqlens_k=tensor([0, 1], device='cuda:0', dtype=torch.int32)
Attention層の順伝播完了 o.shape=torch.Size([1, 16, 128])
RowParallelLinearの順伝播開始 入力形状: torch.Size([1, 2048])
RowParallelLinearの順伝播完了 出力形状: torch.Size([1, 1024])
Qwen3Attentionの順伝播完了 output.shape=torch.Size([1, 1024])
RMSNormの順伝播開始 x.shape=torch.Size([1, 1024]), residual.shape if residual is not None else None=torch.Size([1, 1024])
残差接続を適用
残差接続を適用完了
Qwen3MLPの順伝播 x.shape=torch.Size([1, 1024])
出力次元を並列化する線形層の順伝播開始 x.shape=torch.Size([1, 1024])
出力次元を並列化する線形層の順伝播完了 result.shape=torch.Size([1, 6144])
SwiGLUを順伝播開始 x.shape=torch.Size([1, 6144])
チャンク化 x.shape=torch.Size([1, 3072]), y.shape=torch.Size([1, 3072])
SwiGLUを順伝播完了 result.shape=torch.Size([1, 3072])
RowParallelLinearの順伝播開始 入力形状: torch.Size([1, 3072])
RowParallelLinearの順伝播完了 出力形状: torch.Size([1, 1024])
Qwen3MLPの順伝播完了 x.shape=torch.Size([1, 1024])
Qwen3DecoderLayerの順伝播完了 hidden_states.shape=torch.Size([1, 1024])
Qwen3DecoderLayerの順伝播 positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 1024]) residual is None=False
RMSNormの順伝播開始 x.shape=torch.Size([1, 1024]), residual.shape if residual is not None else None=torch.Size([1, 1024])
残差接続を適用
残差接続を適用完了
Qwen3Attentionの順伝播 positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 1024])
出力次元を並列化する線形層の順伝播開始 x.shape=torch.Size([1, 1024])
出力次元を並列化する線形層の順伝播完了 result.shape=torch.Size([1, 4096])
RMSNormの順伝播開始 x.shape=torch.Size([1, 16, 128]), residual.shape if residual is not None else None=None
正規化開始 x.shape=torch.Size([1, 16, 128])
正規化完了 x.shape=torch.Size([1, 16, 128])
RMSNormの順伝播開始 x.shape=torch.Size([1, 8, 128]), residual.shape if residual is not None else None=None
正規化開始 x.shape=torch.Size([1, 8, 128])
正規化完了 x.shape=torch.Size([1, 8, 128])
RoPEの順伝播開始 positions.shape=torch.Size([1]), query.shape=torch.Size([1, 16, 128]), key.shape=torch.Size([1, 8, 128])
RoPEを適用開始 x.shape=torch.Size([1, 16, 128]), cos.shape=torch.Size([1, 1, 64]), sin.shape=torch.Size([1, 1, 64])
チャンク化 x1.shape=torch.Size([1, 16, 64]), x2.shape=torch.Size([1, 16, 64])
回転行列適用 y1.shape=torch.Size([1, 16, 64]), y2.shape=torch.Size([1, 16, 64])
RoPEを適用完了 result.shape=torch.Size([1, 16, 128])
RoPEを適用開始 x.shape=torch.Size([1, 8, 128]), cos.shape=torch.Size([1, 1, 64]), sin.shape=torch.Size([1, 1, 64])
チャンク化 x1.shape=torch.Size([1, 8, 64]), x2.shape=torch.Size([1, 8, 64])
回転行列適用 y1.shape=torch.Size([1, 8, 64]), y2.shape=torch.Size([1, 8, 64])
RoPEを適用完了 result.shape=torch.Size([1, 8, 128])
RoPEの順伝播完了 query.shape=torch.Size([1, 16, 128]), key.shape=torch.Size([1, 8, 128])
Attention層の順伝播開始 q.shape=torch.Size([1, 16, 128]), k.shape=torch.Size([1, 8, 128]), v.shape=torch.Size([1, 8, 128])
現在のコンテキスト取得 _CONTEXT=Context(is_prefill=True, cu_seqlens_q=tensor([0, 1], device='cuda:0', dtype=torch.int32), cu_seqlens_k=tensor([0, 1], device='cuda:0', dtype=torch.int32), max_seqlen_q=1, max_seqlen_k=1, slot_mapping=tensor([0], device='cuda:0', dtype=torch.int32), context_lens=None, block_tables=None)
コンテキスト情報取得 context.is_prefill=True, context.slot_mapping=tensor([0], device='cuda:0', dtype=torch.int32), context.max_seqlen_q=1, context.cu_seqlens_q=tensor([0, 1], device='cuda:0', dtype=torch.int32), context.max_seqlen_k=1, context.cu_seqlens_k=tensor([0, 1], device='cuda:0', dtype=torch.int32), context.context_lens=None, context.block_tables=None
KVキャッシュにキーとバリューを保存開始 key.shape=torch.Size([1, 8, 128]), value.shape=torch.Size([1, 8, 128]), k_cache.shape=torch.Size([893, 256, 8, 128]), v_cache.shape=torch.Size([893, 256, 8, 128]), slot_mapping.shape=torch.Size([1])
KVキャッシュにキーとバリューを保存完了
PrefillモードでFlashAttentionを実行 context.max_seqlen_q=1, context.cu_seqlens_q=tensor([0, 1], device='cuda:0', dtype=torch.int32), context.max_seqlen_k=1, context.cu_seqlens_k=tensor([0, 1], device='cuda:0', dtype=torch.int32)
Attention層の順伝播完了 o.shape=torch.Size([1, 16, 128])
RowParallelLinearの順伝播開始 入力形状: torch.Size([1, 2048])
RowParallelLinearの順伝播完了 出力形状: torch.Size([1, 1024])
Qwen3Attentionの順伝播完了 output.shape=torch.Size([1, 1024])
RMSNormの順伝播開始 x.shape=torch.Size([1, 1024]), residual.shape if residual is not None else None=torch.Size([1, 1024])
残差接続を適用
残差接続を適用完了
Qwen3MLPの順伝播 x.shape=torch.Size([1, 1024])
出力次元を並列化する線形層の順伝播開始 x.shape=torch.Size([1, 1024])
出力次元を並列化する線形層の順伝播完了 result.shape=torch.Size([1, 6144])
SwiGLUを順伝播開始 x.shape=torch.Size([1, 6144])
チャンク化 x.shape=torch.Size([1, 3072]), y.shape=torch.Size([1, 3072])
SwiGLUを順伝播完了 result.shape=torch.Size([1, 3072])
RowParallelLinearの順伝播開始 入力形状: torch.Size([1, 3072])
RowParallelLinearの順伝播完了 出力形状: torch.Size([1, 1024])
Qwen3MLPの順伝播完了 x.shape=torch.Size([1, 1024])
Qwen3DecoderLayerの順伝播完了 hidden_states.shape=torch.Size([1, 1024])
Qwen3DecoderLayerの順伝播 positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 1024]) residual is None=False
RMSNormの順伝播開始 x.shape=torch.Size([1, 1024]), residual.shape if residual is not None else None=torch.Size([1, 1024])
残差接続を適用
残差接続を適用完了
Qwen3Attentionの順伝播 positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 1024])
出力次元を並列化する線形層の順伝播開始 x.shape=torch.Size([1, 1024])
出力次元を並列化する線形層の順伝播完了 result.shape=torch.Size([1, 4096])
RMSNormの順伝播開始 x.shape=torch.Size([1, 16, 128]), residual.shape if residual is not None else None=None
正規化開始 x.shape=torch.Size([1, 16, 128])
正規化完了 x.shape=torch.Size([1, 16, 128])
RMSNormの順伝播開始 x.shape=torch.Size([1, 8, 128]), residual.shape if residual is not None else None=None
正規化開始 x.shape=torch.Size([1, 8, 128])
正規化完了 x.shape=torch.Size([1, 8, 128])
RoPEの順伝播開始 positions.shape=torch.Size([1]), query.shape=torch.Size([1, 16, 128]), key.shape=torch.Size([1, 8, 128])
RoPEを適用開始 x.shape=torch.Size([1, 16, 128]), cos.shape=torch.Size([1, 1, 64]), sin.shape=torch.Size([1, 1, 64])
チャンク化 x1.shape=torch.Size([1, 16, 64]), x2.shape=torch.Size([1, 16, 64])
回転行列適用 y1.shape=torch.Size([1, 16, 64]), y2.shape=torch.Size([1, 16, 64])
RoPEを適用完了 result.shape=torch.Size([1, 16, 128])
RoPEを適用開始 x.shape=torch.Size([1, 8, 128]), cos.shape=torch.Size([1, 1, 64]), sin.shape=torch.Size([1, 1, 64])
チャンク化 x1.shape=torch.Size([1, 8, 64]), x2.shape=torch.Size([1, 8, 64])
回転行列適用 y1.shape=torch.Size([1, 8, 64]), y2.shape=torch.Size([1, 8, 64])
RoPEを適用完了 result.shape=torch.Size([1, 8, 128])
RoPEの順伝播完了 query.shape=torch.Size([1, 16, 128]), key.shape=torch.Size([1, 8, 128])
Attention層の順伝播開始 q.shape=torch.Size([1, 16, 128]), k.shape=torch.Size([1, 8, 128]), v.shape=torch.Size([1, 8, 128])
現在のコンテキスト取得 _CONTEXT=Context(is_prefill=True, cu_seqlens_q=tensor([0, 1], device='cuda:0', dtype=torch.int32), cu_seqlens_k=tensor([0, 1], device='cuda:0', dtype=torch.int32), max_seqlen_q=1, max_seqlen_k=1, slot_mapping=tensor([0], device='cuda:0', dtype=torch.int32), context_lens=None, block_tables=None)
コンテキスト情報取得 context.is_prefill=True, context.slot_mapping=tensor([0], device='cuda:0', dtype=torch.int32), context.max_seqlen_q=1, context.cu_seqlens_q=tensor([0, 1], device='cuda:0', dtype=torch.int32), context.max_seqlen_k=1, context.cu_seqlens_k=tensor([0, 1], device='cuda:0', dtype=torch.int32), context.context_lens=None, context.block_tables=None
KVキャッシュにキーとバリューを保存開始 key.shape=torch.Size([1, 8, 128]), value.shape=torch.Size([1, 8, 128]), k_cache.shape=torch.Size([893, 256, 8, 128]), v_cache.shape=torch.Size([893, 256, 8, 128]), slot_mapping.shape=torch.Size([1])
KVキャッシュにキーとバリューを保存完了
PrefillモードでFlashAttentionを実行 context.max_seqlen_q=1, context.cu_seqlens_q=tensor([0, 1], device='cuda:0', dtype=torch.int32), context.max_seqlen_k=1, context.cu_seqlens_k=tensor([0, 1], device='cuda:0', dtype=torch.int32)
Attention層の順伝播完了 o.shape=torch.Size([1, 16, 128])
RowParallelLinearの順伝播開始 入力形状: torch.Size([1, 2048])
RowParallelLinearの順伝播完了 出力形状: torch.Size([1, 1024])
Qwen3Attentionの順伝播完了 output.shape=torch.Size([1, 1024])
RMSNormの順伝播開始 x.shape=torch.Size([1, 1024]), residual.shape if residual is not None else None=torch.Size([1, 1024])
残差接続を適用
残差接続を適用完了
Qwen3MLPの順伝播 x.shape=torch.Size([1, 1024])
出力次元を並列化する線形層の順伝播開始 x.shape=torch.Size([1, 1024])
出力次元を並列化する線形層の順伝播完了 result.shape=torch.Size([1, 6144])
SwiGLUを順伝播開始 x.shape=torch.Size([1, 6144])
チャンク化 x.shape=torch.Size([1, 3072]), y.shape=torch.Size([1, 3072])
SwiGLUを順伝播完了 result.shape=torch.Size([1, 3072])
RowParallelLinearの順伝播開始 入力形状: torch.Size([1, 3072])
RowParallelLinearの順伝播完了 出力形状: torch.Size([1, 1024])
Qwen3MLPの順伝播完了 x.shape=torch.Size([1, 1024])
Qwen3DecoderLayerの順伝播完了 hidden_states.shape=torch.Size([1, 1024])
Qwen3DecoderLayerの順伝播 positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 1024]) residual is None=False
RMSNormの順伝播開始 x.shape=torch.Size([1, 1024]), residual.shape if residual is not None else None=torch.Size([1, 1024])
残差接続を適用
残差接続を適用完了
Qwen3Attentionの順伝播 positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 1024])
出力次元を並列化する線形層の順伝播開始 x.shape=torch.Size([1, 1024])
出力次元を並列化する線形層の順伝播完了 result.shape=torch.Size([1, 4096])
RMSNormの順伝播開始 x.shape=torch.Size([1, 16, 128]), residual.shape if residual is not None else None=None
正規化開始 x.shape=torch.Size([1, 16, 128])
正規化完了 x.shape=torch.Size([1, 16, 128])
RMSNormの順伝播開始 x.shape=torch.Size([1, 8, 128]), residual.shape if residual is not None else None=None
正規化開始 x.shape=torch.Size([1, 8, 128])
正規化完了 x.shape=torch.Size([1, 8, 128])
RoPEの順伝播開始 positions.shape=torch.Size([1]), query.shape=torch.Size([1, 16, 128]), key.shape=torch.Size([1, 8, 128])
RoPEを適用開始 x.shape=torch.Size([1, 16, 128]), cos.shape=torch.Size([1, 1, 64]), sin.shape=torch.Size([1, 1, 64])
チャンク化 x1.shape=torch.Size([1, 16, 64]), x2.shape=torch.Size([1, 16, 64])
回転行列適用 y1.shape=torch.Size([1, 16, 64]), y2.shape=torch.Size([1, 16, 64])
RoPEを適用完了 result.shape=torch.Size([1, 16, 128])
RoPEを適用開始 x.shape=torch.Size([1, 8, 128]), cos.shape=torch.Size([1, 1, 64]), sin.shape=torch.Size([1, 1, 64])
チャンク化 x1.shape=torch.Size([1, 8, 64]), x2.shape=torch.Size([1, 8, 64])
回転行列適用 y1.shape=torch.Size([1, 8, 64]), y2.shape=torch.Size([1, 8, 64])
RoPEを適用完了 result.shape=torch.Size([1, 8, 128])
RoPEの順伝播完了 query.shape=torch.Size([1, 16, 128]), key.shape=torch.Size([1, 8, 128])
Attention層の順伝播開始 q.shape=torch.Size([1, 16, 128]), k.shape=torch.Size([1, 8, 128]), v.shape=torch.Size([1, 8, 128])
現在のコンテキスト取得 _CONTEXT=Context(is_prefill=True, cu_seqlens_q=tensor([0, 1], device='cuda:0', dtype=torch.int32), cu_seqlens_k=tensor([0, 1], device='cuda:0', dtype=torch.int32), max_seqlen_q=1, max_seqlen_k=1, slot_mapping=tensor([0], device='cuda:0', dtype=torch.int32), context_lens=None, block_tables=None)
コンテキスト情報取得 context.is_prefill=True, context.slot_mapping=tensor([0], device='cuda:0', dtype=torch.int32), context.max_seqlen_q=1, context.cu_seqlens_q=tensor([0, 1], device='cuda:0', dtype=torch.int32), context.max_seqlen_k=1, context.cu_seqlens_k=tensor([0, 1], device='cuda:0', dtype=torch.int32), context.context_lens=None, context.block_tables=None
KVキャッシュにキーとバリューを保存開始 key.shape=torch.Size([1, 8, 128]), value.shape=torch.Size([1, 8, 128]), k_cache.shape=torch.Size([893, 256, 8, 128]), v_cache.shape=torch.Size([893, 256, 8, 128]), slot_mapping.shape=torch.Size([1])
KVキャッシュにキーとバリューを保存完了
PrefillモードでFlashAttentionを実行 context.max_seqlen_q=1, context.cu_seqlens_q=tensor([0, 1], device='cuda:0', dtype=torch.int32), context.max_seqlen_k=1, context.cu_seqlens_k=tensor([0, 1], device='cuda:0', dtype=torch.int32)
Attention層の順伝播完了 o.shape=torch.Size([1, 16, 128])
RowParallelLinearの順伝播開始 入力形状: torch.Size([1, 2048])
RowParallelLinearの順伝播完了 出力形状: torch.Size([1, 1024])
Qwen3Attentionの順伝播完了 output.shape=torch.Size([1, 1024])
RMSNormの順伝播開始 x.shape=torch.Size([1, 1024]), residual.shape if residual is not None else None=torch.Size([1, 1024])
残差接続を適用
残差接続を適用完了
Qwen3MLPの順伝播 x.shape=torch.Size([1, 1024])
出力次元を並列化する線形層の順伝播開始 x.shape=torch.Size([1, 1024])
出力次元を並列化する線形層の順伝播完了 result.shape=torch.Size([1, 6144])
SwiGLUを順伝播開始 x.shape=torch.Size([1, 6144])
チャンク化 x.shape=torch.Size([1, 3072]), y.shape=torch.Size([1, 3072])
SwiGLUを順伝播完了 result.shape=torch.Size([1, 3072])
RowParallelLinearの順伝播開始 入力形状: torch.Size([1, 3072])
RowParallelLinearの順伝播完了 出力形状: torch.Size([1, 1024])
Qwen3MLPの順伝播完了 x.shape=torch.Size([1, 1024])
Qwen3DecoderLayerの順伝播完了 hidden_states.shape=torch.Size([1, 1024])
Qwen3DecoderLayerの順伝播 positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 1024]) residual is None=False
RMSNormの順伝播開始 x.shape=torch.Size([1, 1024]), residual.shape if residual is not None else None=torch.Size([1, 1024])
残差接続を適用
残差接続を適用完了
Qwen3Attentionの順伝播 positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 1024])
出力次元を並列化する線形層の順伝播開始 x.shape=torch.Size([1, 1024])
出力次元を並列化する線形層の順伝播完了 result.shape=torch.Size([1, 4096])
RMSNormの順伝播開始 x.shape=torch.Size([1, 16, 128]), residual.shape if residual is not None else None=None
正規化開始 x.shape=torch.Size([1, 16, 128])
正規化完了 x.shape=torch.Size([1, 16, 128])
RMSNormの順伝播開始 x.shape=torch.Size([1, 8, 128]), residual.shape if residual is not None else None=None
正規化開始 x.shape=torch.Size([1, 8, 128])
正規化完了 x.shape=torch.Size([1, 8, 128])
RoPEの順伝播開始 positions.shape=torch.Size([1]), query.shape=torch.Size([1, 16, 128]), key.shape=torch.Size([1, 8, 128])
RoPEを適用開始 x.shape=torch.Size([1, 16, 128]), cos.shape=torch.Size([1, 1, 64]), sin.shape=torch.Size([1, 1, 64])
チャンク化 x1.shape=torch.Size([1, 16, 64]), x2.shape=torch.Size([1, 16, 64])
回転行列適用 y1.shape=torch.Size([1, 16, 64]), y2.shape=torch.Size([1, 16, 64])
RoPEを適用完了 result.shape=torch.Size([1, 16, 128])
RoPEを適用開始 x.shape=torch.Size([1, 8, 128]), cos.shape=torch.Size([1, 1, 64]), sin.shape=torch.Size([1, 1, 64])
チャンク化 x1.shape=torch.Size([1, 8, 64]), x2.shape=torch.Size([1, 8, 64])
回転行列適用 y1.shape=torch.Size([1, 8, 64]), y2.shape=torch.Size([1, 8, 64])
RoPEを適用完了 result.shape=torch.Size([1, 8, 128])
RoPEの順伝播完了 query.shape=torch.Size([1, 16, 128]), key.shape=torch.Size([1, 8, 128])
Attention層の順伝播開始 q.shape=torch.Size([1, 16, 128]), k.shape=torch.Size([1, 8, 128]), v.shape=torch.Size([1, 8, 128])
現在のコンテキスト取得 _CONTEXT=Context(is_prefill=True, cu_seqlens_q=tensor([0, 1], device='cuda:0', dtype=torch.int32), cu_seqlens_k=tensor([0, 1], device='cuda:0', dtype=torch.int32), max_seqlen_q=1, max_seqlen_k=1, slot_mapping=tensor([0], device='cuda:0', dtype=torch.int32), context_lens=None, block_tables=None)
コンテキスト情報取得 context.is_prefill=True, context.slot_mapping=tensor([0], device='cuda:0', dtype=torch.int32), context.max_seqlen_q=1, context.cu_seqlens_q=tensor([0, 1], device='cuda:0', dtype=torch.int32), context.max_seqlen_k=1, context.cu_seqlens_k=tensor([0, 1], device='cuda:0', dtype=torch.int32), context.context_lens=None, context.block_tables=None
KVキャッシュにキーとバリューを保存開始 key.shape=torch.Size([1, 8, 128]), value.shape=torch.Size([1, 8, 128]), k_cache.shape=torch.Size([893, 256, 8, 128]), v_cache.shape=torch.Size([893, 256, 8, 128]), slot_mapping.shape=torch.Size([1])
KVキャッシュにキーとバリューを保存完了
PrefillモードでFlashAttentionを実行 context.max_seqlen_q=1, context.cu_seqlens_q=tensor([0, 1], device='cuda:0', dtype=torch.int32), context.max_seqlen_k=1, context.cu_seqlens_k=tensor([0, 1], device='cuda:0', dtype=torch.int32)
Attention層の順伝播完了 o.shape=torch.Size([1, 16, 128])
RowParallelLinearの順伝播開始 入力形状: torch.Size([1, 2048])
RowParallelLinearの順伝播完了 出力形状: torch.Size([1, 1024])
Qwen3Attentionの順伝播完了 output.shape=torch.Size([1, 1024])
RMSNormの順伝播開始 x.shape=torch.Size([1, 1024]), residual.shape if residual is not None else None=torch.Size([1, 1024])
残差接続を適用
残差接続を適用完了
Qwen3MLPの順伝播 x.shape=torch.Size([1, 1024])
出力次元を並列化する線形層の順伝播開始 x.shape=torch.Size([1, 1024])
出力次元を並列化する線形層の順伝播完了 result.shape=torch.Size([1, 6144])
SwiGLUを順伝播開始 x.shape=torch.Size([1, 6144])
チャンク化 x.shape=torch.Size([1, 3072]), y.shape=torch.Size([1, 3072])
SwiGLUを順伝播完了 result.shape=torch.Size([1, 3072])
RowParallelLinearの順伝播開始 入力形状: torch.Size([1, 3072])
RowParallelLinearの順伝播完了 出力形状: torch.Size([1, 1024])
Qwen3MLPの順伝播完了 x.shape=torch.Size([1, 1024])
Qwen3DecoderLayerの順伝播完了 hidden_states.shape=torch.Size([1, 1024])
Qwen3DecoderLayerの順伝播 positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 1024]) residual is None=False
RMSNormの順伝播開始 x.shape=torch.Size([1, 1024]), residual.shape if residual is not None else None=torch.Size([1, 1024])
残差接続を適用
残差接続を適用完了
Qwen3Attentionの順伝播 positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 1024])
出力次元を並列化する線形層の順伝播開始 x.shape=torch.Size([1, 1024])
出力次元を並列化する線形層の順伝播完了 result.shape=torch.Size([1, 4096])
RMSNormの順伝播開始 x.shape=torch.Size([1, 16, 128]), residual.shape if residual is not None else None=None
正規化開始 x.shape=torch.Size([1, 16, 128])
正規化完了 x.shape=torch.Size([1, 16, 128])
RMSNormの順伝播開始 x.shape=torch.Size([1, 8, 128]), residual.shape if residual is not None else None=None
正規化開始 x.shape=torch.Size([1, 8, 128])
正規化完了 x.shape=torch.Size([1, 8, 128])
RoPEの順伝播開始 positions.shape=torch.Size([1]), query.shape=torch.Size([1, 16, 128]), key.shape=torch.Size([1, 8, 128])
RoPEを適用開始 x.shape=torch.Size([1, 16, 128]), cos.shape=torch.Size([1, 1, 64]), sin.shape=torch.Size([1, 1, 64])
チャンク化 x1.shape=torch.Size([1, 16, 64]), x2.shape=torch.Size([1, 16, 64])
回転行列適用 y1.shape=torch.Size([1, 16, 64]), y2.shape=torch.Size([1, 16, 64])
RoPEを適用完了 result.shape=torch.Size([1, 16, 128])
RoPEを適用開始 x.shape=torch.Size([1, 8, 128]), cos.shape=torch.Size([1, 1, 64]), sin.shape=torch.Size([1, 1, 64])
チャンク化 x1.shape=torch.Size([1, 8, 64]), x2.shape=torch.Size([1, 8, 64])
回転行列適用 y1.shape=torch.Size([1, 8, 64]), y2.shape=torch.Size([1, 8, 64])
RoPEを適用完了 result.shape=torch.Size([1, 8, 128])
RoPEの順伝播完了 query.shape=torch.Size([1, 16, 128]), key.shape=torch.Size([1, 8, 128])
Attention層の順伝播開始 q.shape=torch.Size([1, 16, 128]), k.shape=torch.Size([1, 8, 128]), v.shape=torch.Size([1, 8, 128])
現在のコンテキスト取得 _CONTEXT=Context(is_prefill=True, cu_seqlens_q=tensor([0, 1], device='cuda:0', dtype=torch.int32), cu_seqlens_k=tensor([0, 1], device='cuda:0', dtype=torch.int32), max_seqlen_q=1, max_seqlen_k=1, slot_mapping=tensor([0], device='cuda:0', dtype=torch.int32), context_lens=None, block_tables=None)
コンテキスト情報取得 context.is_prefill=True, context.slot_mapping=tensor([0], device='cuda:0', dtype=torch.int32), context.max_seqlen_q=1, context.cu_seqlens_q=tensor([0, 1], device='cuda:0', dtype=torch.int32), context.max_seqlen_k=1, context.cu_seqlens_k=tensor([0, 1], device='cuda:0', dtype=torch.int32), context.context_lens=None, context.block_tables=None
KVキャッシュにキーとバリューを保存開始 key.shape=torch.Size([1, 8, 128]), value.shape=torch.Size([1, 8, 128]), k_cache.shape=torch.Size([893, 256, 8, 128]), v_cache.shape=torch.Size([893, 256, 8, 128]), slot_mapping.shape=torch.Size([1])
KVキャッシュにキーとバリューを保存完了
PrefillモードでFlashAttentionを実行 context.max_seqlen_q=1, context.cu_seqlens_q=tensor([0, 1], device='cuda:0', dtype=torch.int32), context.max_seqlen_k=1, context.cu_seqlens_k=tensor([0, 1], device='cuda:0', dtype=torch.int32)
Attention層の順伝播完了 o.shape=torch.Size([1, 16, 128])
RowParallelLinearの順伝播開始 入力形状: torch.Size([1, 2048])
RowParallelLinearの順伝播完了 出力形状: torch.Size([1, 1024])
Qwen3Attentionの順伝播完了 output.shape=torch.Size([1, 1024])
RMSNormの順伝播開始 x.shape=torch.Size([1, 1024]), residual.shape if residual is not None else None=torch.Size([1, 1024])
残差接続を適用
残差接続を適用完了
Qwen3MLPの順伝播 x.shape=torch.Size([1, 1024])
出力次元を並列化する線形層の順伝播開始 x.shape=torch.Size([1, 1024])
出力次元を並列化する線形層の順伝播完了 result.shape=torch.Size([1, 6144])
SwiGLUを順伝播開始 x.shape=torch.Size([1, 6144])
チャンク化 x.shape=torch.Size([1, 3072]), y.shape=torch.Size([1, 3072])
SwiGLUを順伝播完了 result.shape=torch.Size([1, 3072])
RowParallelLinearの順伝播開始 入力形状: torch.Size([1, 3072])
RowParallelLinearの順伝播完了 出力形状: torch.Size([1, 1024])
Qwen3MLPの順伝播完了 x.shape=torch.Size([1, 1024])
Qwen3DecoderLayerの順伝播完了 hidden_states.shape=torch.Size([1, 1024])
Qwen3DecoderLayerの順伝播 positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 1024]) residual is None=False
RMSNormの順伝播開始 x.shape=torch.Size([1, 1024]), residual.shape if residual is not None else None=torch.Size([1, 1024])
残差接続を適用
残差接続を適用完了
Qwen3Attentionの順伝播 positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 1024])
出力次元を並列化する線形層の順伝播開始 x.shape=torch.Size([1, 1024])
出力次元を並列化する線形層の順伝播完了 result.shape=torch.Size([1, 4096])
RMSNormの順伝播開始 x.shape=torch.Size([1, 16, 128]), residual.shape if residual is not None else None=None
正規化開始 x.shape=torch.Size([1, 16, 128])
正規化完了 x.shape=torch.Size([1, 16, 128])
RMSNormの順伝播開始 x.shape=torch.Size([1, 8, 128]), residual.shape if residual is not None else None=None
正規化開始 x.shape=torch.Size([1, 8, 128])
正規化完了 x.shape=torch.Size([1, 8, 128])
RoPEの順伝播開始 positions.shape=torch.Size([1]), query.shape=torch.Size([1, 16, 128]), key.shape=torch.Size([1, 8, 128])
RoPEを適用開始 x.shape=torch.Size([1, 16, 128]), cos.shape=torch.Size([1, 1, 64]), sin.shape=torch.Size([1, 1, 64])
チャンク化 x1.shape=torch.Size([1, 16, 64]), x2.shape=torch.Size([1, 16, 64])
回転行列適用 y1.shape=torch.Size([1, 16, 64]), y2.shape=torch.Size([1, 16, 64])
RoPEを適用完了 result.shape=torch.Size([1, 16, 128])
RoPEを適用開始 x.shape=torch.Size([1, 8, 128]), cos.shape=torch.Size([1, 1, 64]), sin.shape=torch.Size([1, 1, 64])
チャンク化 x1.shape=torch.Size([1, 8, 64]), x2.shape=torch.Size([1, 8, 64])
回転行列適用 y1.shape=torch.Size([1, 8, 64]), y2.shape=torch.Size([1, 8, 64])
RoPEを適用完了 result.shape=torch.Size([1, 8, 128])
RoPEの順伝播完了 query.shape=torch.Size([1, 16, 128]), key.shape=torch.Size([1, 8, 128])
Attention層の順伝播開始 q.shape=torch.Size([1, 16, 128]), k.shape=torch.Size([1, 8, 128]), v.shape=torch.Size([1, 8, 128])
現在のコンテキスト取得 _CONTEXT=Context(is_prefill=True, cu_seqlens_q=tensor([0, 1], device='cuda:0', dtype=torch.int32), cu_seqlens_k=tensor([0, 1], device='cuda:0', dtype=torch.int32), max_seqlen_q=1, max_seqlen_k=1, slot_mapping=tensor([0], device='cuda:0', dtype=torch.int32), context_lens=None, block_tables=None)
コンテキスト情報取得 context.is_prefill=True, context.slot_mapping=tensor([0], device='cuda:0', dtype=torch.int32), context.max_seqlen_q=1, context.cu_seqlens_q=tensor([0, 1], device='cuda:0', dtype=torch.int32), context.max_seqlen_k=1, context.cu_seqlens_k=tensor([0, 1], device='cuda:0', dtype=torch.int32), context.context_lens=None, context.block_tables=None
KVキャッシュにキーとバリューを保存開始 key.shape=torch.Size([1, 8, 128]), value.shape=torch.Size([1, 8, 128]), k_cache.shape=torch.Size([893, 256, 8, 128]), v_cache.shape=torch.Size([893, 256, 8, 128]), slot_mapping.shape=torch.Size([1])
KVキャッシュにキーとバリューを保存完了
PrefillモードでFlashAttentionを実行 context.max_seqlen_q=1, context.cu_seqlens_q=tensor([0, 1], device='cuda:0', dtype=torch.int32), context.max_seqlen_k=1, context.cu_seqlens_k=tensor([0, 1], device='cuda:0', dtype=torch.int32)
Attention層の順伝播完了 o.shape=torch.Size([1, 16, 128])
RowParallelLinearの順伝播開始 入力形状: torch.Size([1, 2048])
RowParallelLinearの順伝播完了 出力形状: torch.Size([1, 1024])
Qwen3Attentionの順伝播完了 output.shape=torch.Size([1, 1024])
RMSNormの順伝播開始 x.shape=torch.Size([1, 1024]), residual.shape if residual is not None else None=torch.Size([1, 1024])
残差接続を適用
残差接続を適用完了
Qwen3MLPの順伝播 x.shape=torch.Size([1, 1024])
出力次元を並列化する線形層の順伝播開始 x.shape=torch.Size([1, 1024])
出力次元を並列化する線形層の順伝播完了 result.shape=torch.Size([1, 6144])
SwiGLUを順伝播開始 x.shape=torch.Size([1, 6144])
チャンク化 x.shape=torch.Size([1, 3072]), y.shape=torch.Size([1, 3072])
SwiGLUを順伝播完了 result.shape=torch.Size([1, 3072])
RowParallelLinearの順伝播開始 入力形状: torch.Size([1, 3072])
RowParallelLinearの順伝播完了 出力形状: torch.Size([1, 1024])
Qwen3MLPの順伝播完了 x.shape=torch.Size([1, 1024])
Qwen3DecoderLayerの順伝播完了 hidden_states.shape=torch.Size([1, 1024])
Qwen3DecoderLayerの順伝播 positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 1024]) residual is None=False
RMSNormの順伝播開始 x.shape=torch.Size([1, 1024]), residual.shape if residual is not None else None=torch.Size([1, 1024])
残差接続を適用
残差接続を適用完了
Qwen3Attentionの順伝播 positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 1024])
出力次元を並列化する線形層の順伝播開始 x.shape=torch.Size([1, 1024])
出力次元を並列化する線形層の順伝播完了 result.shape=torch.Size([1, 4096])
RMSNormの順伝播開始 x.shape=torch.Size([1, 16, 128]), residual.shape if residual is not None else None=None
正規化開始 x.shape=torch.Size([1, 16, 128])
正規化完了 x.shape=torch.Size([1, 16, 128])
RMSNormの順伝播開始 x.shape=torch.Size([1, 8, 128]), residual.shape if residual is not None else None=None
正規化開始 x.shape=torch.Size([1, 8, 128])
正規化完了 x.shape=torch.Size([1, 8, 128])
RoPEの順伝播開始 positions.shape=torch.Size([1]), query.shape=torch.Size([1, 16, 128]), key.shape=torch.Size([1, 8, 128])
RoPEを適用開始 x.shape=torch.Size([1, 16, 128]), cos.shape=torch.Size([1, 1, 64]), sin.shape=torch.Size([1, 1, 64])
チャンク化 x1.shape=torch.Size([1, 16, 64]), x2.shape=torch.Size([1, 16, 64])
回転行列適用 y1.shape=torch.Size([1, 16, 64]), y2.shape=torch.Size([1, 16, 64])
RoPEを適用完了 result.shape=torch.Size([1, 16, 128])
RoPEを適用開始 x.shape=torch.Size([1, 8, 128]), cos.shape=torch.Size([1, 1, 64]), sin.shape=torch.Size([1, 1, 64])
チャンク化 x1.shape=torch.Size([1, 8, 64]), x2.shape=torch.Size([1, 8, 64])
回転行列適用 y1.shape=torch.Size([1, 8, 64]), y2.shape=torch.Size([1, 8, 64])
RoPEを適用完了 result.shape=torch.Size([1, 8, 128])
RoPEの順伝播完了 query.shape=torch.Size([1, 16, 128]), key.shape=torch.Size([1, 8, 128])
Attention層の順伝播開始 q.shape=torch.Size([1, 16, 128]), k.shape=torch.Size([1, 8, 128]), v.shape=torch.Size([1, 8, 128])
現在のコンテキスト取得 _CONTEXT=Context(is_prefill=True, cu_seqlens_q=tensor([0, 1], device='cuda:0', dtype=torch.int32), cu_seqlens_k=tensor([0, 1], device='cuda:0', dtype=torch.int32), max_seqlen_q=1, max_seqlen_k=1, slot_mapping=tensor([0], device='cuda:0', dtype=torch.int32), context_lens=None, block_tables=None)
コンテキスト情報取得 context.is_prefill=True, context.slot_mapping=tensor([0], device='cuda:0', dtype=torch.int32), context.max_seqlen_q=1, context.cu_seqlens_q=tensor([0, 1], device='cuda:0', dtype=torch.int32), context.max_seqlen_k=1, context.cu_seqlens_k=tensor([0, 1], device='cuda:0', dtype=torch.int32), context.context_lens=None, context.block_tables=None
KVキャッシュにキーとバリューを保存開始 key.shape=torch.Size([1, 8, 128]), value.shape=torch.Size([1, 8, 128]), k_cache.shape=torch.Size([893, 256, 8, 128]), v_cache.shape=torch.Size([893, 256, 8, 128]), slot_mapping.shape=torch.Size([1])
KVキャッシュにキーとバリューを保存完了
PrefillモードでFlashAttentionを実行 context.max_seqlen_q=1, context.cu_seqlens_q=tensor([0, 1], device='cuda:0', dtype=torch.int32), context.max_seqlen_k=1, context.cu_seqlens_k=tensor([0, 1], device='cuda:0', dtype=torch.int32)
Attention層の順伝播完了 o.shape=torch.Size([1, 16, 128])
RowParallelLinearの順伝播開始 入力形状: torch.Size([1, 2048])
RowParallelLinearの順伝播完了 出力形状: torch.Size([1, 1024])
Qwen3Attentionの順伝播完了 output.shape=torch.Size([1, 1024])
RMSNormの順伝播開始 x.shape=torch.Size([1, 1024]), residual.shape if residual is not None else None=torch.Size([1, 1024])
残差接続を適用
残差接続を適用完了
Qwen3MLPの順伝播 x.shape=torch.Size([1, 1024])
出力次元を並列化する線形層の順伝播開始 x.shape=torch.Size([1, 1024])
出力次元を並列化する線形層の順伝播完了 result.shape=torch.Size([1, 6144])
SwiGLUを順伝播開始 x.shape=torch.Size([1, 6144])
チャンク化 x.shape=torch.Size([1, 3072]), y.shape=torch.Size([1, 3072])
SwiGLUを順伝播完了 result.shape=torch.Size([1, 3072])
RowParallelLinearの順伝播開始 入力形状: torch.Size([1, 3072])
RowParallelLinearの順伝播完了 出力形状: torch.Size([1, 1024])
Qwen3MLPの順伝播完了 x.shape=torch.Size([1, 1024])
Qwen3DecoderLayerの順伝播完了 hidden_states.shape=torch.Size([1, 1024])
Qwen3DecoderLayerの順伝播 positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 1024]) residual is None=False
RMSNormの順伝播開始 x.shape=torch.Size([1, 1024]), residual.shape if residual is not None else None=torch.Size([1, 1024])
残差接続を適用
残差接続を適用完了
Qwen3Attentionの順伝播 positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 1024])
出力次元を並列化する線形層の順伝播開始 x.shape=torch.Size([1, 1024])
出力次元を並列化する線形層の順伝播完了 result.shape=torch.Size([1, 4096])
RMSNormの順伝播開始 x.shape=torch.Size([1, 16, 128]), residual.shape if residual is not None else None=None
正規化開始 x.shape=torch.Size([1, 16, 128])
正規化完了 x.shape=torch.Size([1, 16, 128])
RMSNormの順伝播開始 x.shape=torch.Size([1, 8, 128]), residual.shape if residual is not None else None=None
正規化開始 x.shape=torch.Size([1, 8, 128])
正規化完了 x.shape=torch.Size([1, 8, 128])
RoPEの順伝播開始 positions.shape=torch.Size([1]), query.shape=torch.Size([1, 16, 128]), key.shape=torch.Size([1, 8, 128])
RoPEを適用開始 x.shape=torch.Size([1, 16, 128]), cos.shape=torch.Size([1, 1, 64]), sin.shape=torch.Size([1, 1, 64])
チャンク化 x1.shape=torch.Size([1, 16, 64]), x2.shape=torch.Size([1, 16, 64])
回転行列適用 y1.shape=torch.Size([1, 16, 64]), y2.shape=torch.Size([1, 16, 64])
RoPEを適用完了 result.shape=torch.Size([1, 16, 128])
RoPEを適用開始 x.shape=torch.Size([1, 8, 128]), cos.shape=torch.Size([1, 1, 64]), sin.shape=torch.Size([1, 1, 64])
チャンク化 x1.shape=torch.Size([1, 8, 64]), x2.shape=torch.Size([1, 8, 64])
回転行列適用 y1.shape=torch.Size([1, 8, 64]), y2.shape=torch.Size([1, 8, 64])
RoPEを適用完了 result.shape=torch.Size([1, 8, 128])
RoPEの順伝播完了 query.shape=torch.Size([1, 16, 128]), key.shape=torch.Size([1, 8, 128])
Attention層の順伝播開始 q.shape=torch.Size([1, 16, 128]), k.shape=torch.Size([1, 8, 128]), v.shape=torch.Size([1, 8, 128])
現在のコンテキスト取得 _CONTEXT=Context(is_prefill=True, cu_seqlens_q=tensor([0, 1], device='cuda:0', dtype=torch.int32), cu_seqlens_k=tensor([0, 1], device='cuda:0', dtype=torch.int32), max_seqlen_q=1, max_seqlen_k=1, slot_mapping=tensor([0], device='cuda:0', dtype=torch.int32), context_lens=None, block_tables=None)
コンテキスト情報取得 context.is_prefill=True, context.slot_mapping=tensor([0], device='cuda:0', dtype=torch.int32), context.max_seqlen_q=1, context.cu_seqlens_q=tensor([0, 1], device='cuda:0', dtype=torch.int32), context.max_seqlen_k=1, context.cu_seqlens_k=tensor([0, 1], device='cuda:0', dtype=torch.int32), context.context_lens=None, context.block_tables=None
KVキャッシュにキーとバリューを保存開始 key.shape=torch.Size([1, 8, 128]), value.shape=torch.Size([1, 8, 128]), k_cache.shape=torch.Size([893, 256, 8, 128]), v_cache.shape=torch.Size([893, 256, 8, 128]), slot_mapping.shape=torch.Size([1])
KVキャッシュにキーとバリューを保存完了
PrefillモードでFlashAttentionを実行 context.max_seqlen_q=1, context.cu_seqlens_q=tensor([0, 1], device='cuda:0', dtype=torch.int32), context.max_seqlen_k=1, context.cu_seqlens_k=tensor([0, 1], device='cuda:0', dtype=torch.int32)
Attention層の順伝播完了 o.shape=torch.Size([1, 16, 128])
RowParallelLinearの順伝播開始 入力形状: torch.Size([1, 2048])
RowParallelLinearの順伝播完了 出力形状: torch.Size([1, 1024])
Qwen3Attentionの順伝播完了 output.shape=torch.Size([1, 1024])
RMSNormの順伝播開始 x.shape=torch.Size([1, 1024]), residual.shape if residual is not None else None=torch.Size([1, 1024])
残差接続を適用
残差接続を適用完了
Qwen3MLPの順伝播 x.shape=torch.Size([1, 1024])
出力次元を並列化する線形層の順伝播開始 x.shape=torch.Size([1, 1024])
出力次元を並列化する線形層の順伝播完了 result.shape=torch.Size([1, 6144])
SwiGLUを順伝播開始 x.shape=torch.Size([1, 6144])
チャンク化 x.shape=torch.Size([1, 3072]), y.shape=torch.Size([1, 3072])
SwiGLUを順伝播完了 result.shape=torch.Size([1, 3072])
RowParallelLinearの順伝播開始 入力形状: torch.Size([1, 3072])
RowParallelLinearの順伝播完了 出力形状: torch.Size([1, 1024])
Qwen3MLPの順伝播完了 x.shape=torch.Size([1, 1024])
Qwen3DecoderLayerの順伝播完了 hidden_states.shape=torch.Size([1, 1024])
Qwen3DecoderLayerの順伝播 positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 1024]) residual is None=False
RMSNormの順伝播開始 x.shape=torch.Size([1, 1024]), residual.shape if residual is not None else None=torch.Size([1, 1024])
残差接続を適用
残差接続を適用完了
Qwen3Attentionの順伝播 positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 1024])
出力次元を並列化する線形層の順伝播開始 x.shape=torch.Size([1, 1024])
出力次元を並列化する線形層の順伝播完了 result.shape=torch.Size([1, 4096])
RMSNormの順伝播開始 x.shape=torch.Size([1, 16, 128]), residual.shape if residual is not None else None=None
正規化開始 x.shape=torch.Size([1, 16, 128])
正規化完了 x.shape=torch.Size([1, 16, 128])
RMSNormの順伝播開始 x.shape=torch.Size([1, 8, 128]), residual.shape if residual is not None else None=None
正規化開始 x.shape=torch.Size([1, 8, 128])
正規化完了 x.shape=torch.Size([1, 8, 128])
RoPEの順伝播開始 positions.shape=torch.Size([1]), query.shape=torch.Size([1, 16, 128]), key.shape=torch.Size([1, 8, 128])
RoPEを適用開始 x.shape=torch.Size([1, 16, 128]), cos.shape=torch.Size([1, 1, 64]), sin.shape=torch.Size([1, 1, 64])
チャンク化 x1.shape=torch.Size([1, 16, 64]), x2.shape=torch.Size([1, 16, 64])
回転行列適用 y1.shape=torch.Size([1, 16, 64]), y2.shape=torch.Size([1, 16, 64])
RoPEを適用完了 result.shape=torch.Size([1, 16, 128])
RoPEを適用開始 x.shape=torch.Size([1, 8, 128]), cos.shape=torch.Size([1, 1, 64]), sin.shape=torch.Size([1, 1, 64])
チャンク化 x1.shape=torch.Size([1, 8, 64]), x2.shape=torch.Size([1, 8, 64])
回転行列適用 y1.shape=torch.Size([1, 8, 64]), y2.shape=torch.Size([1, 8, 64])
RoPEを適用完了 result.shape=torch.Size([1, 8, 128])
RoPEの順伝播完了 query.shape=torch.Size([1, 16, 128]), key.shape=torch.Size([1, 8, 128])
Attention層の順伝播開始 q.shape=torch.Size([1, 16, 128]), k.shape=torch.Size([1, 8, 128]), v.shape=torch.Size([1, 8, 128])
現在のコンテキスト取得 _CONTEXT=Context(is_prefill=True, cu_seqlens_q=tensor([0, 1], device='cuda:0', dtype=torch.int32), cu_seqlens_k=tensor([0, 1], device='cuda:0', dtype=torch.int32), max_seqlen_q=1, max_seqlen_k=1, slot_mapping=tensor([0], device='cuda:0', dtype=torch.int32), context_lens=None, block_tables=None)
コンテキスト情報取得 context.is_prefill=True, context.slot_mapping=tensor([0], device='cuda:0', dtype=torch.int32), context.max_seqlen_q=1, context.cu_seqlens_q=tensor([0, 1], device='cuda:0', dtype=torch.int32), context.max_seqlen_k=1, context.cu_seqlens_k=tensor([0, 1], device='cuda:0', dtype=torch.int32), context.context_lens=None, context.block_tables=None
KVキャッシュにキーとバリューを保存開始 key.shape=torch.Size([1, 8, 128]), value.shape=torch.Size([1, 8, 128]), k_cache.shape=torch.Size([893, 256, 8, 128]), v_cache.shape=torch.Size([893, 256, 8, 128]), slot_mapping.shape=torch.Size([1])
KVキャッシュにキーとバリューを保存完了
PrefillモードでFlashAttentionを実行 context.max_seqlen_q=1, context.cu_seqlens_q=tensor([0, 1], device='cuda:0', dtype=torch.int32), context.max_seqlen_k=1, context.cu_seqlens_k=tensor([0, 1], device='cuda:0', dtype=torch.int32)
Attention層の順伝播完了 o.shape=torch.Size([1, 16, 128])
RowParallelLinearの順伝播開始 入力形状: torch.Size([1, 2048])
RowParallelLinearの順伝播完了 出力形状: torch.Size([1, 1024])
Qwen3Attentionの順伝播完了 output.shape=torch.Size([1, 1024])
RMSNormの順伝播開始 x.shape=torch.Size([1, 1024]), residual.shape if residual is not None else None=torch.Size([1, 1024])
残差接続を適用
残差接続を適用完了
Qwen3MLPの順伝播 x.shape=torch.Size([1, 1024])
出力次元を並列化する線形層の順伝播開始 x.shape=torch.Size([1, 1024])
出力次元を並列化する線形層の順伝播完了 result.shape=torch.Size([1, 6144])
SwiGLUを順伝播開始 x.shape=torch.Size([1, 6144])
チャンク化 x.shape=torch.Size([1, 3072]), y.shape=torch.Size([1, 3072])
SwiGLUを順伝播完了 result.shape=torch.Size([1, 3072])
RowParallelLinearの順伝播開始 入力形状: torch.Size([1, 3072])
RowParallelLinearの順伝播完了 出力形状: torch.Size([1, 1024])
Qwen3MLPの順伝播完了 x.shape=torch.Size([1, 1024])
Qwen3DecoderLayerの順伝播完了 hidden_states.shape=torch.Size([1, 1024])
Qwen3DecoderLayerの順伝播 positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 1024]) residual is None=False
RMSNormの順伝播開始 x.shape=torch.Size([1, 1024]), residual.shape if residual is not None else None=torch.Size([1, 1024])
残差接続を適用
残差接続を適用完了
Qwen3Attentionの順伝播 positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 1024])
出力次元を並列化する線形層の順伝播開始 x.shape=torch.Size([1, 1024])
出力次元を並列化する線形層の順伝播完了 result.shape=torch.Size([1, 4096])
RMSNormの順伝播開始 x.shape=torch.Size([1, 16, 128]), residual.shape if residual is not None else None=None
正規化開始 x.shape=torch.Size([1, 16, 128])
正規化完了 x.shape=torch.Size([1, 16, 128])
RMSNormの順伝播開始 x.shape=torch.Size([1, 8, 128]), residual.shape if residual is not None else None=None
正規化開始 x.shape=torch.Size([1, 8, 128])
正規化完了 x.shape=torch.Size([1, 8, 128])
RoPEの順伝播開始 positions.shape=torch.Size([1]), query.shape=torch.Size([1, 16, 128]), key.shape=torch.Size([1, 8, 128])
RoPEを適用開始 x.shape=torch.Size([1, 16, 128]), cos.shape=torch.Size([1, 1, 64]), sin.shape=torch.Size([1, 1, 64])
チャンク化 x1.shape=torch.Size([1, 16, 64]), x2.shape=torch.Size([1, 16, 64])
回転行列適用 y1.shape=torch.Size([1, 16, 64]), y2.shape=torch.Size([1, 16, 64])
RoPEを適用完了 result.shape=torch.Size([1, 16, 128])
RoPEを適用開始 x.shape=torch.Size([1, 8, 128]), cos.shape=torch.Size([1, 1, 64]), sin.shape=torch.Size([1, 1, 64])
チャンク化 x1.shape=torch.Size([1, 8, 64]), x2.shape=torch.Size([1, 8, 64])
回転行列適用 y1.shape=torch.Size([1, 8, 64]), y2.shape=torch.Size([1, 8, 64])
RoPEを適用完了 result.shape=torch.Size([1, 8, 128])
RoPEの順伝播完了 query.shape=torch.Size([1, 16, 128]), key.shape=torch.Size([1, 8, 128])
Attention層の順伝播開始 q.shape=torch.Size([1, 16, 128]), k.shape=torch.Size([1, 8, 128]), v.shape=torch.Size([1, 8, 128])
現在のコンテキスト取得 _CONTEXT=Context(is_prefill=True, cu_seqlens_q=tensor([0, 1], device='cuda:0', dtype=torch.int32), cu_seqlens_k=tensor([0, 1], device='cuda:0', dtype=torch.int32), max_seqlen_q=1, max_seqlen_k=1, slot_mapping=tensor([0], device='cuda:0', dtype=torch.int32), context_lens=None, block_tables=None)
コンテキスト情報取得 context.is_prefill=True, context.slot_mapping=tensor([0], device='cuda:0', dtype=torch.int32), context.max_seqlen_q=1, context.cu_seqlens_q=tensor([0, 1], device='cuda:0', dtype=torch.int32), context.max_seqlen_k=1, context.cu_seqlens_k=tensor([0, 1], device='cuda:0', dtype=torch.int32), context.context_lens=None, context.block_tables=None
KVキャッシュにキーとバリューを保存開始 key.shape=torch.Size([1, 8, 128]), value.shape=torch.Size([1, 8, 128]), k_cache.shape=torch.Size([893, 256, 8, 128]), v_cache.shape=torch.Size([893, 256, 8, 128]), slot_mapping.shape=torch.Size([1])
KVキャッシュにキーとバリューを保存完了
PrefillモードでFlashAttentionを実行 context.max_seqlen_q=1, context.cu_seqlens_q=tensor([0, 1], device='cuda:0', dtype=torch.int32), context.max_seqlen_k=1, context.cu_seqlens_k=tensor([0, 1], device='cuda:0', dtype=torch.int32)
Attention層の順伝播完了 o.shape=torch.Size([1, 16, 128])
RowParallelLinearの順伝播開始 入力形状: torch.Size([1, 2048])
RowParallelLinearの順伝播完了 出力形状: torch.Size([1, 1024])
Qwen3Attentionの順伝播完了 output.shape=torch.Size([1, 1024])
RMSNormの順伝播開始 x.shape=torch.Size([1, 1024]), residual.shape if residual is not None else None=torch.Size([1, 1024])
残差接続を適用
残差接続を適用完了
Qwen3MLPの順伝播 x.shape=torch.Size([1, 1024])
出力次元を並列化する線形層の順伝播開始 x.shape=torch.Size([1, 1024])
出力次元を並列化する線形層の順伝播完了 result.shape=torch.Size([1, 6144])
SwiGLUを順伝播開始 x.shape=torch.Size([1, 6144])
チャンク化 x.shape=torch.Size([1, 3072]), y.shape=torch.Size([1, 3072])
SwiGLUを順伝播完了 result.shape=torch.Size([1, 3072])
RowParallelLinearの順伝播開始 入力形状: torch.Size([1, 3072])
RowParallelLinearの順伝播完了 出力形状: torch.Size([1, 1024])
Qwen3MLPの順伝播完了 x.shape=torch.Size([1, 1024])
Qwen3DecoderLayerの順伝播完了 hidden_states.shape=torch.Size([1, 1024])
Qwen3DecoderLayerの順伝播 positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 1024]) residual is None=False
RMSNormの順伝播開始 x.shape=torch.Size([1, 1024]), residual.shape if residual is not None else None=torch.Size([1, 1024])
残差接続を適用
残差接続を適用完了
Qwen3Attentionの順伝播 positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 1024])
出力次元を並列化する線形層の順伝播開始 x.shape=torch.Size([1, 1024])
出力次元を並列化する線形層の順伝播完了 result.shape=torch.Size([1, 4096])
RMSNormの順伝播開始 x.shape=torch.Size([1, 16, 128]), residual.shape if residual is not None else None=None
正規化開始 x.shape=torch.Size([1, 16, 128])
正規化完了 x.shape=torch.Size([1, 16, 128])
RMSNormの順伝播開始 x.shape=torch.Size([1, 8, 128]), residual.shape if residual is not None else None=None
正規化開始 x.shape=torch.Size([1, 8, 128])
正規化完了 x.shape=torch.Size([1, 8, 128])
RoPEの順伝播開始 positions.shape=torch.Size([1]), query.shape=torch.Size([1, 16, 128]), key.shape=torch.Size([1, 8, 128])
RoPEを適用開始 x.shape=torch.Size([1, 16, 128]), cos.shape=torch.Size([1, 1, 64]), sin.shape=torch.Size([1, 1, 64])
チャンク化 x1.shape=torch.Size([1, 16, 64]), x2.shape=torch.Size([1, 16, 64])
回転行列適用 y1.shape=torch.Size([1, 16, 64]), y2.shape=torch.Size([1, 16, 64])
RoPEを適用完了 result.shape=torch.Size([1, 16, 128])
RoPEを適用開始 x.shape=torch.Size([1, 8, 128]), cos.shape=torch.Size([1, 1, 64]), sin.shape=torch.Size([1, 1, 64])
チャンク化 x1.shape=torch.Size([1, 8, 64]), x2.shape=torch.Size([1, 8, 64])
回転行列適用 y1.shape=torch.Size([1, 8, 64]), y2.shape=torch.Size([1, 8, 64])
RoPEを適用完了 result.shape=torch.Size([1, 8, 128])
RoPEの順伝播完了 query.shape=torch.Size([1, 16, 128]), key.shape=torch.Size([1, 8, 128])
Attention層の順伝播開始 q.shape=torch.Size([1, 16, 128]), k.shape=torch.Size([1, 8, 128]), v.shape=torch.Size([1, 8, 128])
現在のコンテキスト取得 _CONTEXT=Context(is_prefill=True, cu_seqlens_q=tensor([0, 1], device='cuda:0', dtype=torch.int32), cu_seqlens_k=tensor([0, 1], device='cuda:0', dtype=torch.int32), max_seqlen_q=1, max_seqlen_k=1, slot_mapping=tensor([0], device='cuda:0', dtype=torch.int32), context_lens=None, block_tables=None)
コンテキスト情報取得 context.is_prefill=True, context.slot_mapping=tensor([0], device='cuda:0', dtype=torch.int32), context.max_seqlen_q=1, context.cu_seqlens_q=tensor([0, 1], device='cuda:0', dtype=torch.int32), context.max_seqlen_k=1, context.cu_seqlens_k=tensor([0, 1], device='cuda:0', dtype=torch.int32), context.context_lens=None, context.block_tables=None
KVキャッシュにキーとバリューを保存開始 key.shape=torch.Size([1, 8, 128]), value.shape=torch.Size([1, 8, 128]), k_cache.shape=torch.Size([893, 256, 8, 128]), v_cache.shape=torch.Size([893, 256, 8, 128]), slot_mapping.shape=torch.Size([1])
KVキャッシュにキーとバリューを保存完了
PrefillモードでFlashAttentionを実行 context.max_seqlen_q=1, context.cu_seqlens_q=tensor([0, 1], device='cuda:0', dtype=torch.int32), context.max_seqlen_k=1, context.cu_seqlens_k=tensor([0, 1], device='cuda:0', dtype=torch.int32)
Attention層の順伝播完了 o.shape=torch.Size([1, 16, 128])
RowParallelLinearの順伝播開始 入力形状: torch.Size([1, 2048])
RowParallelLinearの順伝播完了 出力形状: torch.Size([1, 1024])
Qwen3Attentionの順伝播完了 output.shape=torch.Size([1, 1024])
RMSNormの順伝播開始 x.shape=torch.Size([1, 1024]), residual.shape if residual is not None else None=torch.Size([1, 1024])
残差接続を適用
残差接続を適用完了
Qwen3MLPの順伝播 x.shape=torch.Size([1, 1024])
出力次元を並列化する線形層の順伝播開始 x.shape=torch.Size([1, 1024])
出力次元を並列化する線形層の順伝播完了 result.shape=torch.Size([1, 6144])
SwiGLUを順伝播開始 x.shape=torch.Size([1, 6144])
チャンク化 x.shape=torch.Size([1, 3072]), y.shape=torch.Size([1, 3072])
SwiGLUを順伝播完了 result.shape=torch.Size([1, 3072])
RowParallelLinearの順伝播開始 入力形状: torch.Size([1, 3072])
RowParallelLinearの順伝播完了 出力形状: torch.Size([1, 1024])
Qwen3MLPの順伝播完了 x.shape=torch.Size([1, 1024])
Qwen3DecoderLayerの順伝播完了 hidden_states.shape=torch.Size([1, 1024])
Qwen3DecoderLayerの順伝播 positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 1024]) residual is None=False
RMSNormの順伝播開始 x.shape=torch.Size([1, 1024]), residual.shape if residual is not None else None=torch.Size([1, 1024])
残差接続を適用
残差接続を適用完了
Qwen3Attentionの順伝播 positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 1024])
出力次元を並列化する線形層の順伝播開始 x.shape=torch.Size([1, 1024])
出力次元を並列化する線形層の順伝播完了 result.shape=torch.Size([1, 4096])
RMSNormの順伝播開始 x.shape=torch.Size([1, 16, 128]), residual.shape if residual is not None else None=None
正規化開始 x.shape=torch.Size([1, 16, 128])
正規化完了 x.shape=torch.Size([1, 16, 128])
RMSNormの順伝播開始 x.shape=torch.Size([1, 8, 128]), residual.shape if residual is not None else None=None
正規化開始 x.shape=torch.Size([1, 8, 128])
正規化完了 x.shape=torch.Size([1, 8, 128])
RoPEの順伝播開始 positions.shape=torch.Size([1]), query.shape=torch.Size([1, 16, 128]), key.shape=torch.Size([1, 8, 128])
RoPEを適用開始 x.shape=torch.Size([1, 16, 128]), cos.shape=torch.Size([1, 1, 64]), sin.shape=torch.Size([1, 1, 64])
チャンク化 x1.shape=torch.Size([1, 16, 64]), x2.shape=torch.Size([1, 16, 64])
回転行列適用 y1.shape=torch.Size([1, 16, 64]), y2.shape=torch.Size([1, 16, 64])
RoPEを適用完了 result.shape=torch.Size([1, 16, 128])
RoPEを適用開始 x.shape=torch.Size([1, 8, 128]), cos.shape=torch.Size([1, 1, 64]), sin.shape=torch.Size([1, 1, 64])
チャンク化 x1.shape=torch.Size([1, 8, 64]), x2.shape=torch.Size([1, 8, 64])
回転行列適用 y1.shape=torch.Size([1, 8, 64]), y2.shape=torch.Size([1, 8, 64])
RoPEを適用完了 result.shape=torch.Size([1, 8, 128])
RoPEの順伝播完了 query.shape=torch.Size([1, 16, 128]), key.shape=torch.Size([1, 8, 128])
Attention層の順伝播開始 q.shape=torch.Size([1, 16, 128]), k.shape=torch.Size([1, 8, 128]), v.shape=torch.Size([1, 8, 128])
現在のコンテキスト取得 _CONTEXT=Context(is_prefill=True, cu_seqlens_q=tensor([0, 1], device='cuda:0', dtype=torch.int32), cu_seqlens_k=tensor([0, 1], device='cuda:0', dtype=torch.int32), max_seqlen_q=1, max_seqlen_k=1, slot_mapping=tensor([0], device='cuda:0', dtype=torch.int32), context_lens=None, block_tables=None)
コンテキスト情報取得 context.is_prefill=True, context.slot_mapping=tensor([0], device='cuda:0', dtype=torch.int32), context.max_seqlen_q=1, context.cu_seqlens_q=tensor([0, 1], device='cuda:0', dtype=torch.int32), context.max_seqlen_k=1, context.cu_seqlens_k=tensor([0, 1], device='cuda:0', dtype=torch.int32), context.context_lens=None, context.block_tables=None
KVキャッシュにキーとバリューを保存開始 key.shape=torch.Size([1, 8, 128]), value.shape=torch.Size([1, 8, 128]), k_cache.shape=torch.Size([893, 256, 8, 128]), v_cache.shape=torch.Size([893, 256, 8, 128]), slot_mapping.shape=torch.Size([1])
KVキャッシュにキーとバリューを保存完了
PrefillモードでFlashAttentionを実行 context.max_seqlen_q=1, context.cu_seqlens_q=tensor([0, 1], device='cuda:0', dtype=torch.int32), context.max_seqlen_k=1, context.cu_seqlens_k=tensor([0, 1], device='cuda:0', dtype=torch.int32)
Attention層の順伝播完了 o.shape=torch.Size([1, 16, 128])
RowParallelLinearの順伝播開始 入力形状: torch.Size([1, 2048])
RowParallelLinearの順伝播完了 出力形状: torch.Size([1, 1024])
Qwen3Attentionの順伝播完了 output.shape=torch.Size([1, 1024])
RMSNormの順伝播開始 x.shape=torch.Size([1, 1024]), residual.shape if residual is not None else None=torch.Size([1, 1024])
残差接続を適用
残差接続を適用完了
Qwen3MLPの順伝播 x.shape=torch.Size([1, 1024])
出力次元を並列化する線形層の順伝播開始 x.shape=torch.Size([1, 1024])
出力次元を並列化する線形層の順伝播完了 result.shape=torch.Size([1, 6144])
SwiGLUを順伝播開始 x.shape=torch.Size([1, 6144])
チャンク化 x.shape=torch.Size([1, 3072]), y.shape=torch.Size([1, 3072])
SwiGLUを順伝播完了 result.shape=torch.Size([1, 3072])
RowParallelLinearの順伝播開始 入力形状: torch.Size([1, 3072])
RowParallelLinearの順伝播完了 出力形状: torch.Size([1, 1024])
Qwen3MLPの順伝播完了 x.shape=torch.Size([1, 1024])
Qwen3DecoderLayerの順伝播完了 hidden_states.shape=torch.Size([1, 1024])
Qwen3DecoderLayerの順伝播 positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 1024]) residual is None=False
RMSNormの順伝播開始 x.shape=torch.Size([1, 1024]), residual.shape if residual is not None else None=torch.Size([1, 1024])
残差接続を適用
残差接続を適用完了
Qwen3Attentionの順伝播 positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 1024])
出力次元を並列化する線形層の順伝播開始 x.shape=torch.Size([1, 1024])
出力次元を並列化する線形層の順伝播完了 result.shape=torch.Size([1, 4096])
RMSNormの順伝播開始 x.shape=torch.Size([1, 16, 128]), residual.shape if residual is not None else None=None
正規化開始 x.shape=torch.Size([1, 16, 128])
正規化完了 x.shape=torch.Size([1, 16, 128])
RMSNormの順伝播開始 x.shape=torch.Size([1, 8, 128]), residual.shape if residual is not None else None=None
正規化開始 x.shape=torch.Size([1, 8, 128])
正規化完了 x.shape=torch.Size([1, 8, 128])
RoPEの順伝播開始 positions.shape=torch.Size([1]), query.shape=torch.Size([1, 16, 128]), key.shape=torch.Size([1, 8, 128])
RoPEを適用開始 x.shape=torch.Size([1, 16, 128]), cos.shape=torch.Size([1, 1, 64]), sin.shape=torch.Size([1, 1, 64])
チャンク化 x1.shape=torch.Size([1, 16, 64]), x2.shape=torch.Size([1, 16, 64])
回転行列適用 y1.shape=torch.Size([1, 16, 64]), y2.shape=torch.Size([1, 16, 64])
RoPEを適用完了 result.shape=torch.Size([1, 16, 128])
RoPEを適用開始 x.shape=torch.Size([1, 8, 128]), cos.shape=torch.Size([1, 1, 64]), sin.shape=torch.Size([1, 1, 64])
チャンク化 x1.shape=torch.Size([1, 8, 64]), x2.shape=torch.Size([1, 8, 64])
回転行列適用 y1.shape=torch.Size([1, 8, 64]), y2.shape=torch.Size([1, 8, 64])
RoPEを適用完了 result.shape=torch.Size([1, 8, 128])
RoPEの順伝播完了 query.shape=torch.Size([1, 16, 128]), key.shape=torch.Size([1, 8, 128])
Attention層の順伝播開始 q.shape=torch.Size([1, 16, 128]), k.shape=torch.Size([1, 8, 128]), v.shape=torch.Size([1, 8, 128])
現在のコンテキスト取得 _CONTEXT=Context(is_prefill=True, cu_seqlens_q=tensor([0, 1], device='cuda:0', dtype=torch.int32), cu_seqlens_k=tensor([0, 1], device='cuda:0', dtype=torch.int32), max_seqlen_q=1, max_seqlen_k=1, slot_mapping=tensor([0], device='cuda:0', dtype=torch.int32), context_lens=None, block_tables=None)
コンテキスト情報取得 context.is_prefill=True, context.slot_mapping=tensor([0], device='cuda:0', dtype=torch.int32), context.max_seqlen_q=1, context.cu_seqlens_q=tensor([0, 1], device='cuda:0', dtype=torch.int32), context.max_seqlen_k=1, context.cu_seqlens_k=tensor([0, 1], device='cuda:0', dtype=torch.int32), context.context_lens=None, context.block_tables=None
KVキャッシュにキーとバリューを保存開始 key.shape=torch.Size([1, 8, 128]), value.shape=torch.Size([1, 8, 128]), k_cache.shape=torch.Size([893, 256, 8, 128]), v_cache.shape=torch.Size([893, 256, 8, 128]), slot_mapping.shape=torch.Size([1])
KVキャッシュにキーとバリューを保存完了
PrefillモードでFlashAttentionを実行 context.max_seqlen_q=1, context.cu_seqlens_q=tensor([0, 1], device='cuda:0', dtype=torch.int32), context.max_seqlen_k=1, context.cu_seqlens_k=tensor([0, 1], device='cuda:0', dtype=torch.int32)
Attention層の順伝播完了 o.shape=torch.Size([1, 16, 128])
RowParallelLinearの順伝播開始 入力形状: torch.Size([1, 2048])
RowParallelLinearの順伝播完了 出力形状: torch.Size([1, 1024])
Qwen3Attentionの順伝播完了 output.shape=torch.Size([1, 1024])
RMSNormの順伝播開始 x.shape=torch.Size([1, 1024]), residual.shape if residual is not None else None=torch.Size([1, 1024])
残差接続を適用
残差接続を適用完了
Qwen3MLPの順伝播 x.shape=torch.Size([1, 1024])
出力次元を並列化する線形層の順伝播開始 x.shape=torch.Size([1, 1024])
出力次元を並列化する線形層の順伝播完了 result.shape=torch.Size([1, 6144])
SwiGLUを順伝播開始 x.shape=torch.Size([1, 6144])
チャンク化 x.shape=torch.Size([1, 3072]), y.shape=torch.Size([1, 3072])
SwiGLUを順伝播完了 result.shape=torch.Size([1, 3072])
RowParallelLinearの順伝播開始 入力形状: torch.Size([1, 3072])
RowParallelLinearの順伝播完了 出力形状: torch.Size([1, 1024])
Qwen3MLPの順伝播完了 x.shape=torch.Size([1, 1024])
Qwen3DecoderLayerの順伝播完了 hidden_states.shape=torch.Size([1, 1024])
Qwen3DecoderLayerの順伝播 positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 1024]) residual is None=False
RMSNormの順伝播開始 x.shape=torch.Size([1, 1024]), residual.shape if residual is not None else None=torch.Size([1, 1024])
残差接続を適用
残差接続を適用完了
Qwen3Attentionの順伝播 positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 1024])
出力次元を並列化する線形層の順伝播開始 x.shape=torch.Size([1, 1024])
出力次元を並列化する線形層の順伝播完了 result.shape=torch.Size([1, 4096])
RMSNormの順伝播開始 x.shape=torch.Size([1, 16, 128]), residual.shape if residual is not None else None=None
正規化開始 x.shape=torch.Size([1, 16, 128])
正規化完了 x.shape=torch.Size([1, 16, 128])
RMSNormの順伝播開始 x.shape=torch.Size([1, 8, 128]), residual.shape if residual is not None else None=None
正規化開始 x.shape=torch.Size([1, 8, 128])
正規化完了 x.shape=torch.Size([1, 8, 128])
RoPEの順伝播開始 positions.shape=torch.Size([1]), query.shape=torch.Size([1, 16, 128]), key.shape=torch.Size([1, 8, 128])
RoPEを適用開始 x.shape=torch.Size([1, 16, 128]), cos.shape=torch.Size([1, 1, 64]), sin.shape=torch.Size([1, 1, 64])
チャンク化 x1.shape=torch.Size([1, 16, 64]), x2.shape=torch.Size([1, 16, 64])
回転行列適用 y1.shape=torch.Size([1, 16, 64]), y2.shape=torch.Size([1, 16, 64])
RoPEを適用完了 result.shape=torch.Size([1, 16, 128])
RoPEを適用開始 x.shape=torch.Size([1, 8, 128]), cos.shape=torch.Size([1, 1, 64]), sin.shape=torch.Size([1, 1, 64])
チャンク化 x1.shape=torch.Size([1, 8, 64]), x2.shape=torch.Size([1, 8, 64])
回転行列適用 y1.shape=torch.Size([1, 8, 64]), y2.shape=torch.Size([1, 8, 64])
RoPEを適用完了 result.shape=torch.Size([1, 8, 128])
RoPEの順伝播完了 query.shape=torch.Size([1, 16, 128]), key.shape=torch.Size([1, 8, 128])
Attention層の順伝播開始 q.shape=torch.Size([1, 16, 128]), k.shape=torch.Size([1, 8, 128]), v.shape=torch.Size([1, 8, 128])
現在のコンテキスト取得 _CONTEXT=Context(is_prefill=True, cu_seqlens_q=tensor([0, 1], device='cuda:0', dtype=torch.int32), cu_seqlens_k=tensor([0, 1], device='cuda:0', dtype=torch.int32), max_seqlen_q=1, max_seqlen_k=1, slot_mapping=tensor([0], device='cuda:0', dtype=torch.int32), context_lens=None, block_tables=None)
コンテキスト情報取得 context.is_prefill=True, context.slot_mapping=tensor([0], device='cuda:0', dtype=torch.int32), context.max_seqlen_q=1, context.cu_seqlens_q=tensor([0, 1], device='cuda:0', dtype=torch.int32), context.max_seqlen_k=1, context.cu_seqlens_k=tensor([0, 1], device='cuda:0', dtype=torch.int32), context.context_lens=None, context.block_tables=None
KVキャッシュにキーとバリューを保存開始 key.shape=torch.Size([1, 8, 128]), value.shape=torch.Size([1, 8, 128]), k_cache.shape=torch.Size([893, 256, 8, 128]), v_cache.shape=torch.Size([893, 256, 8, 128]), slot_mapping.shape=torch.Size([1])
KVキャッシュにキーとバリューを保存完了
PrefillモードでFlashAttentionを実行 context.max_seqlen_q=1, context.cu_seqlens_q=tensor([0, 1], device='cuda:0', dtype=torch.int32), context.max_seqlen_k=1, context.cu_seqlens_k=tensor([0, 1], device='cuda:0', dtype=torch.int32)
Attention層の順伝播完了 o.shape=torch.Size([1, 16, 128])
RowParallelLinearの順伝播開始 入力形状: torch.Size([1, 2048])
RowParallelLinearの順伝播完了 出力形状: torch.Size([1, 1024])
Qwen3Attentionの順伝播完了 output.shape=torch.Size([1, 1024])
RMSNormの順伝播開始 x.shape=torch.Size([1, 1024]), residual.shape if residual is not None else None=torch.Size([1, 1024])
残差接続を適用
残差接続を適用完了
Qwen3MLPの順伝播 x.shape=torch.Size([1, 1024])
出力次元を並列化する線形層の順伝播開始 x.shape=torch.Size([1, 1024])
出力次元を並列化する線形層の順伝播完了 result.shape=torch.Size([1, 6144])
SwiGLUを順伝播開始 x.shape=torch.Size([1, 6144])
チャンク化 x.shape=torch.Size([1, 3072]), y.shape=torch.Size([1, 3072])
SwiGLUを順伝播完了 result.shape=torch.Size([1, 3072])
RowParallelLinearの順伝播開始 入力形状: torch.Size([1, 3072])
RowParallelLinearの順伝播完了 出力形状: torch.Size([1, 1024])
Qwen3MLPの順伝播完了 x.shape=torch.Size([1, 1024])
Qwen3DecoderLayerの順伝播完了 hidden_states.shape=torch.Size([1, 1024])
Qwen3DecoderLayerの順伝播 positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 1024]) residual is None=False
RMSNormの順伝播開始 x.shape=torch.Size([1, 1024]), residual.shape if residual is not None else None=torch.Size([1, 1024])
残差接続を適用
残差接続を適用完了
Qwen3Attentionの順伝播 positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 1024])
出力次元を並列化する線形層の順伝播開始 x.shape=torch.Size([1, 1024])
出力次元を並列化する線形層の順伝播完了 result.shape=torch.Size([1, 4096])
RMSNormの順伝播開始 x.shape=torch.Size([1, 16, 128]), residual.shape if residual is not None else None=None
正規化開始 x.shape=torch.Size([1, 16, 128])
正規化完了 x.shape=torch.Size([1, 16, 128])
RMSNormの順伝播開始 x.shape=torch.Size([1, 8, 128]), residual.shape if residual is not None else None=None
正規化開始 x.shape=torch.Size([1, 8, 128])
正規化完了 x.shape=torch.Size([1, 8, 128])
RoPEの順伝播開始 positions.shape=torch.Size([1]), query.shape=torch.Size([1, 16, 128]), key.shape=torch.Size([1, 8, 128])
RoPEを適用開始 x.shape=torch.Size([1, 16, 128]), cos.shape=torch.Size([1, 1, 64]), sin.shape=torch.Size([1, 1, 64])
チャンク化 x1.shape=torch.Size([1, 16, 64]), x2.shape=torch.Size([1, 16, 64])
回転行列適用 y1.shape=torch.Size([1, 16, 64]), y2.shape=torch.Size([1, 16, 64])
RoPEを適用完了 result.shape=torch.Size([1, 16, 128])
RoPEを適用開始 x.shape=torch.Size([1, 8, 128]), cos.shape=torch.Size([1, 1, 64]), sin.shape=torch.Size([1, 1, 64])
チャンク化 x1.shape=torch.Size([1, 8, 64]), x2.shape=torch.Size([1, 8, 64])
回転行列適用 y1.shape=torch.Size([1, 8, 64]), y2.shape=torch.Size([1, 8, 64])
RoPEを適用完了 result.shape=torch.Size([1, 8, 128])
RoPEの順伝播完了 query.shape=torch.Size([1, 16, 128]), key.shape=torch.Size([1, 8, 128])
Attention層の順伝播開始 q.shape=torch.Size([1, 16, 128]), k.shape=torch.Size([1, 8, 128]), v.shape=torch.Size([1, 8, 128])
現在のコンテキスト取得 _CONTEXT=Context(is_prefill=True, cu_seqlens_q=tensor([0, 1], device='cuda:0', dtype=torch.int32), cu_seqlens_k=tensor([0, 1], device='cuda:0', dtype=torch.int32), max_seqlen_q=1, max_seqlen_k=1, slot_mapping=tensor([0], device='cuda:0', dtype=torch.int32), context_lens=None, block_tables=None)
コンテキスト情報取得 context.is_prefill=True, context.slot_mapping=tensor([0], device='cuda:0', dtype=torch.int32), context.max_seqlen_q=1, context.cu_seqlens_q=tensor([0, 1], device='cuda:0', dtype=torch.int32), context.max_seqlen_k=1, context.cu_seqlens_k=tensor([0, 1], device='cuda:0', dtype=torch.int32), context.context_lens=None, context.block_tables=None
KVキャッシュにキーとバリューを保存開始 key.shape=torch.Size([1, 8, 128]), value.shape=torch.Size([1, 8, 128]), k_cache.shape=torch.Size([893, 256, 8, 128]), v_cache.shape=torch.Size([893, 256, 8, 128]), slot_mapping.shape=torch.Size([1])
KVキャッシュにキーとバリューを保存完了
PrefillモードでFlashAttentionを実行 context.max_seqlen_q=1, context.cu_seqlens_q=tensor([0, 1], device='cuda:0', dtype=torch.int32), context.max_seqlen_k=1, context.cu_seqlens_k=tensor([0, 1], device='cuda:0', dtype=torch.int32)
Attention層の順伝播完了 o.shape=torch.Size([1, 16, 128])
RowParallelLinearの順伝播開始 入力形状: torch.Size([1, 2048])
RowParallelLinearの順伝播完了 出力形状: torch.Size([1, 1024])
Qwen3Attentionの順伝播完了 output.shape=torch.Size([1, 1024])
RMSNormの順伝播開始 x.shape=torch.Size([1, 1024]), residual.shape if residual is not None else None=torch.Size([1, 1024])
残差接続を適用
残差接続を適用完了
Qwen3MLPの順伝播 x.shape=torch.Size([1, 1024])
出力次元を並列化する線形層の順伝播開始 x.shape=torch.Size([1, 1024])
出力次元を並列化する線形層の順伝播完了 result.shape=torch.Size([1, 6144])
SwiGLUを順伝播開始 x.shape=torch.Size([1, 6144])
チャンク化 x.shape=torch.Size([1, 3072]), y.shape=torch.Size([1, 3072])
SwiGLUを順伝播完了 result.shape=torch.Size([1, 3072])
RowParallelLinearの順伝播開始 入力形状: torch.Size([1, 3072])
RowParallelLinearの順伝播完了 出力形状: torch.Size([1, 1024])
Qwen3MLPの順伝播完了 x.shape=torch.Size([1, 1024])
Qwen3DecoderLayerの順伝播完了 hidden_states.shape=torch.Size([1, 1024])
Qwen3DecoderLayerの順伝播 positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 1024]) residual is None=False
RMSNormの順伝播開始 x.shape=torch.Size([1, 1024]), residual.shape if residual is not None else None=torch.Size([1, 1024])
残差接続を適用
残差接続を適用完了
Qwen3Attentionの順伝播 positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 1024])
出力次元を並列化する線形層の順伝播開始 x.shape=torch.Size([1, 1024])
出力次元を並列化する線形層の順伝播完了 result.shape=torch.Size([1, 4096])
RMSNormの順伝播開始 x.shape=torch.Size([1, 16, 128]), residual.shape if residual is not None else None=None
正規化開始 x.shape=torch.Size([1, 16, 128])
正規化完了 x.shape=torch.Size([1, 16, 128])
RMSNormの順伝播開始 x.shape=torch.Size([1, 8, 128]), residual.shape if residual is not None else None=None
正規化開始 x.shape=torch.Size([1, 8, 128])
正規化完了 x.shape=torch.Size([1, 8, 128])
RoPEの順伝播開始 positions.shape=torch.Size([1]), query.shape=torch.Size([1, 16, 128]), key.shape=torch.Size([1, 8, 128])
RoPEを適用開始 x.shape=torch.Size([1, 16, 128]), cos.shape=torch.Size([1, 1, 64]), sin.shape=torch.Size([1, 1, 64])
チャンク化 x1.shape=torch.Size([1, 16, 64]), x2.shape=torch.Size([1, 16, 64])
回転行列適用 y1.shape=torch.Size([1, 16, 64]), y2.shape=torch.Size([1, 16, 64])
RoPEを適用完了 result.shape=torch.Size([1, 16, 128])
RoPEを適用開始 x.shape=torch.Size([1, 8, 128]), cos.shape=torch.Size([1, 1, 64]), sin.shape=torch.Size([1, 1, 64])
チャンク化 x1.shape=torch.Size([1, 8, 64]), x2.shape=torch.Size([1, 8, 64])
回転行列適用 y1.shape=torch.Size([1, 8, 64]), y2.shape=torch.Size([1, 8, 64])
RoPEを適用完了 result.shape=torch.Size([1, 8, 128])
RoPEの順伝播完了 query.shape=torch.Size([1, 16, 128]), key.shape=torch.Size([1, 8, 128])
Attention層の順伝播開始 q.shape=torch.Size([1, 16, 128]), k.shape=torch.Size([1, 8, 128]), v.shape=torch.Size([1, 8, 128])
現在のコンテキスト取得 _CONTEXT=Context(is_prefill=True, cu_seqlens_q=tensor([0, 1], device='cuda:0', dtype=torch.int32), cu_seqlens_k=tensor([0, 1], device='cuda:0', dtype=torch.int32), max_seqlen_q=1, max_seqlen_k=1, slot_mapping=tensor([0], device='cuda:0', dtype=torch.int32), context_lens=None, block_tables=None)
コンテキスト情報取得 context.is_prefill=True, context.slot_mapping=tensor([0], device='cuda:0', dtype=torch.int32), context.max_seqlen_q=1, context.cu_seqlens_q=tensor([0, 1], device='cuda:0', dtype=torch.int32), context.max_seqlen_k=1, context.cu_seqlens_k=tensor([0, 1], device='cuda:0', dtype=torch.int32), context.context_lens=None, context.block_tables=None
KVキャッシュにキーとバリューを保存開始 key.shape=torch.Size([1, 8, 128]), value.shape=torch.Size([1, 8, 128]), k_cache.shape=torch.Size([893, 256, 8, 128]), v_cache.shape=torch.Size([893, 256, 8, 128]), slot_mapping.shape=torch.Size([1])
KVキャッシュにキーとバリューを保存完了
PrefillモードでFlashAttentionを実行 context.max_seqlen_q=1, context.cu_seqlens_q=tensor([0, 1], device='cuda:0', dtype=torch.int32), context.max_seqlen_k=1, context.cu_seqlens_k=tensor([0, 1], device='cuda:0', dtype=torch.int32)
Attention層の順伝播完了 o.shape=torch.Size([1, 16, 128])
RowParallelLinearの順伝播開始 入力形状: torch.Size([1, 2048])
RowParallelLinearの順伝播完了 出力形状: torch.Size([1, 1024])
Qwen3Attentionの順伝播完了 output.shape=torch.Size([1, 1024])
RMSNormの順伝播開始 x.shape=torch.Size([1, 1024]), residual.shape if residual is not None else None=torch.Size([1, 1024])
残差接続を適用
残差接続を適用完了
Qwen3MLPの順伝播 x.shape=torch.Size([1, 1024])
出力次元を並列化する線形層の順伝播開始 x.shape=torch.Size([1, 1024])
出力次元を並列化する線形層の順伝播完了 result.shape=torch.Size([1, 6144])
SwiGLUを順伝播開始 x.shape=torch.Size([1, 6144])
チャンク化 x.shape=torch.Size([1, 3072]), y.shape=torch.Size([1, 3072])
SwiGLUを順伝播完了 result.shape=torch.Size([1, 3072])
RowParallelLinearの順伝播開始 入力形状: torch.Size([1, 3072])
RowParallelLinearの順伝播完了 出力形状: torch.Size([1, 1024])
Qwen3MLPの順伝播完了 x.shape=torch.Size([1, 1024])
Qwen3DecoderLayerの順伝播完了 hidden_states.shape=torch.Size([1, 1024])
Qwen3DecoderLayerの順伝播 positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 1024]) residual is None=False
RMSNormの順伝播開始 x.shape=torch.Size([1, 1024]), residual.shape if residual is not None else None=torch.Size([1, 1024])
残差接続を適用
残差接続を適用完了
Qwen3Attentionの順伝播 positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 1024])
出力次元を並列化する線形層の順伝播開始 x.shape=torch.Size([1, 1024])
出力次元を並列化する線形層の順伝播完了 result.shape=torch.Size([1, 4096])
RMSNormの順伝播開始 x.shape=torch.Size([1, 16, 128]), residual.shape if residual is not None else None=None
正規化開始 x.shape=torch.Size([1, 16, 128])
正規化完了 x.shape=torch.Size([1, 16, 128])
RMSNormの順伝播開始 x.shape=torch.Size([1, 8, 128]), residual.shape if residual is not None else None=None
正規化開始 x.shape=torch.Size([1, 8, 128])
正規化完了 x.shape=torch.Size([1, 8, 128])
RoPEの順伝播開始 positions.shape=torch.Size([1]), query.shape=torch.Size([1, 16, 128]), key.shape=torch.Size([1, 8, 128])
RoPEを適用開始 x.shape=torch.Size([1, 16, 128]), cos.shape=torch.Size([1, 1, 64]), sin.shape=torch.Size([1, 1, 64])
チャンク化 x1.shape=torch.Size([1, 16, 64]), x2.shape=torch.Size([1, 16, 64])
回転行列適用 y1.shape=torch.Size([1, 16, 64]), y2.shape=torch.Size([1, 16, 64])
RoPEを適用完了 result.shape=torch.Size([1, 16, 128])
RoPEを適用開始 x.shape=torch.Size([1, 8, 128]), cos.shape=torch.Size([1, 1, 64]), sin.shape=torch.Size([1, 1, 64])
チャンク化 x1.shape=torch.Size([1, 8, 64]), x2.shape=torch.Size([1, 8, 64])
回転行列適用 y1.shape=torch.Size([1, 8, 64]), y2.shape=torch.Size([1, 8, 64])
RoPEを適用完了 result.shape=torch.Size([1, 8, 128])
RoPEの順伝播完了 query.shape=torch.Size([1, 16, 128]), key.shape=torch.Size([1, 8, 128])
Attention層の順伝播開始 q.shape=torch.Size([1, 16, 128]), k.shape=torch.Size([1, 8, 128]), v.shape=torch.Size([1, 8, 128])
現在のコンテキスト取得 _CONTEXT=Context(is_prefill=True, cu_seqlens_q=tensor([0, 1], device='cuda:0', dtype=torch.int32), cu_seqlens_k=tensor([0, 1], device='cuda:0', dtype=torch.int32), max_seqlen_q=1, max_seqlen_k=1, slot_mapping=tensor([0], device='cuda:0', dtype=torch.int32), context_lens=None, block_tables=None)
コンテキスト情報取得 context.is_prefill=True, context.slot_mapping=tensor([0], device='cuda:0', dtype=torch.int32), context.max_seqlen_q=1, context.cu_seqlens_q=tensor([0, 1], device='cuda:0', dtype=torch.int32), context.max_seqlen_k=1, context.cu_seqlens_k=tensor([0, 1], device='cuda:0', dtype=torch.int32), context.context_lens=None, context.block_tables=None
KVキャッシュにキーとバリューを保存開始 key.shape=torch.Size([1, 8, 128]), value.shape=torch.Size([1, 8, 128]), k_cache.shape=torch.Size([893, 256, 8, 128]), v_cache.shape=torch.Size([893, 256, 8, 128]), slot_mapping.shape=torch.Size([1])
KVキャッシュにキーとバリューを保存完了
PrefillモードでFlashAttentionを実行 context.max_seqlen_q=1, context.cu_seqlens_q=tensor([0, 1], device='cuda:0', dtype=torch.int32), context.max_seqlen_k=1, context.cu_seqlens_k=tensor([0, 1], device='cuda:0', dtype=torch.int32)
Attention層の順伝播完了 o.shape=torch.Size([1, 16, 128])
RowParallelLinearの順伝播開始 入力形状: torch.Size([1, 2048])
RowParallelLinearの順伝播完了 出力形状: torch.Size([1, 1024])
Qwen3Attentionの順伝播完了 output.shape=torch.Size([1, 1024])
RMSNormの順伝播開始 x.shape=torch.Size([1, 1024]), residual.shape if residual is not None else None=torch.Size([1, 1024])
残差接続を適用
残差接続を適用完了
Qwen3MLPの順伝播 x.shape=torch.Size([1, 1024])
出力次元を並列化する線形層の順伝播開始 x.shape=torch.Size([1, 1024])
出力次元を並列化する線形層の順伝播完了 result.shape=torch.Size([1, 6144])
SwiGLUを順伝播開始 x.shape=torch.Size([1, 6144])
チャンク化 x.shape=torch.Size([1, 3072]), y.shape=torch.Size([1, 3072])
SwiGLUを順伝播完了 result.shape=torch.Size([1, 3072])
RowParallelLinearの順伝播開始 入力形状: torch.Size([1, 3072])
RowParallelLinearの順伝播完了 出力形状: torch.Size([1, 1024])
Qwen3MLPの順伝播完了 x.shape=torch.Size([1, 1024])
Qwen3DecoderLayerの順伝播完了 hidden_states.shape=torch.Size([1, 1024])
Qwen3DecoderLayerの順伝播 positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 1024]) residual is None=False
RMSNormの順伝播開始 x.shape=torch.Size([1, 1024]), residual.shape if residual is not None else None=torch.Size([1, 1024])
残差接続を適用
残差接続を適用完了
Qwen3Attentionの順伝播 positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 1024])
出力次元を並列化する線形層の順伝播開始 x.shape=torch.Size([1, 1024])
出力次元を並列化する線形層の順伝播完了 result.shape=torch.Size([1, 4096])
RMSNormの順伝播開始 x.shape=torch.Size([1, 16, 128]), residual.shape if residual is not None else None=None
正規化開始 x.shape=torch.Size([1, 16, 128])
正規化完了 x.shape=torch.Size([1, 16, 128])
RMSNormの順伝播開始 x.shape=torch.Size([1, 8, 128]), residual.shape if residual is not None else None=None
正規化開始 x.shape=torch.Size([1, 8, 128])
正規化完了 x.shape=torch.Size([1, 8, 128])
RoPEの順伝播開始 positions.shape=torch.Size([1]), query.shape=torch.Size([1, 16, 128]), key.shape=torch.Size([1, 8, 128])
RoPEを適用開始 x.shape=torch.Size([1, 16, 128]), cos.shape=torch.Size([1, 1, 64]), sin.shape=torch.Size([1, 1, 64])
チャンク化 x1.shape=torch.Size([1, 16, 64]), x2.shape=torch.Size([1, 16, 64])
回転行列適用 y1.shape=torch.Size([1, 16, 64]), y2.shape=torch.Size([1, 16, 64])
RoPEを適用完了 result.shape=torch.Size([1, 16, 128])
RoPEを適用開始 x.shape=torch.Size([1, 8, 128]), cos.shape=torch.Size([1, 1, 64]), sin.shape=torch.Size([1, 1, 64])
チャンク化 x1.shape=torch.Size([1, 8, 64]), x2.shape=torch.Size([1, 8, 64])
回転行列適用 y1.shape=torch.Size([1, 8, 64]), y2.shape=torch.Size([1, 8, 64])
RoPEを適用完了 result.shape=torch.Size([1, 8, 128])
RoPEの順伝播完了 query.shape=torch.Size([1, 16, 128]), key.shape=torch.Size([1, 8, 128])
Attention層の順伝播開始 q.shape=torch.Size([1, 16, 128]), k.shape=torch.Size([1, 8, 128]), v.shape=torch.Size([1, 8, 128])
現在のコンテキスト取得 _CONTEXT=Context(is_prefill=True, cu_seqlens_q=tensor([0, 1], device='cuda:0', dtype=torch.int32), cu_seqlens_k=tensor([0, 1], device='cuda:0', dtype=torch.int32), max_seqlen_q=1, max_seqlen_k=1, slot_mapping=tensor([0], device='cuda:0', dtype=torch.int32), context_lens=None, block_tables=None)
コンテキスト情報取得 context.is_prefill=True, context.slot_mapping=tensor([0], device='cuda:0', dtype=torch.int32), context.max_seqlen_q=1, context.cu_seqlens_q=tensor([0, 1], device='cuda:0', dtype=torch.int32), context.max_seqlen_k=1, context.cu_seqlens_k=tensor([0, 1], device='cuda:0', dtype=torch.int32), context.context_lens=None, context.block_tables=None
KVキャッシュにキーとバリューを保存開始 key.shape=torch.Size([1, 8, 128]), value.shape=torch.Size([1, 8, 128]), k_cache.shape=torch.Size([893, 256, 8, 128]), v_cache.shape=torch.Size([893, 256, 8, 128]), slot_mapping.shape=torch.Size([1])
KVキャッシュにキーとバリューを保存完了
PrefillモードでFlashAttentionを実行 context.max_seqlen_q=1, context.cu_seqlens_q=tensor([0, 1], device='cuda:0', dtype=torch.int32), context.max_seqlen_k=1, context.cu_seqlens_k=tensor([0, 1], device='cuda:0', dtype=torch.int32)
Attention層の順伝播完了 o.shape=torch.Size([1, 16, 128])
RowParallelLinearの順伝播開始 入力形状: torch.Size([1, 2048])
RowParallelLinearの順伝播完了 出力形状: torch.Size([1, 1024])
Qwen3Attentionの順伝播完了 output.shape=torch.Size([1, 1024])
RMSNormの順伝播開始 x.shape=torch.Size([1, 1024]), residual.shape if residual is not None else None=torch.Size([1, 1024])
残差接続を適用
残差接続を適用完了
Qwen3MLPの順伝播 x.shape=torch.Size([1, 1024])
出力次元を並列化する線形層の順伝播開始 x.shape=torch.Size([1, 1024])
出力次元を並列化する線形層の順伝播完了 result.shape=torch.Size([1, 6144])
SwiGLUを順伝播開始 x.shape=torch.Size([1, 6144])
チャンク化 x.shape=torch.Size([1, 3072]), y.shape=torch.Size([1, 3072])
SwiGLUを順伝播完了 result.shape=torch.Size([1, 3072])
RowParallelLinearの順伝播開始 入力形状: torch.Size([1, 3072])
RowParallelLinearの順伝播完了 出力形状: torch.Size([1, 1024])
Qwen3MLPの順伝播完了 x.shape=torch.Size([1, 1024])
Qwen3DecoderLayerの順伝播完了 hidden_states.shape=torch.Size([1, 1024])
Qwen3DecoderLayerの順伝播 positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 1024]) residual is None=False
RMSNormの順伝播開始 x.shape=torch.Size([1, 1024]), residual.shape if residual is not None else None=torch.Size([1, 1024])
残差接続を適用
残差接続を適用完了
Qwen3Attentionの順伝播 positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 1024])
出力次元を並列化する線形層の順伝播開始 x.shape=torch.Size([1, 1024])
出力次元を並列化する線形層の順伝播完了 result.shape=torch.Size([1, 4096])
RMSNormの順伝播開始 x.shape=torch.Size([1, 16, 128]), residual.shape if residual is not None else None=None
正規化開始 x.shape=torch.Size([1, 16, 128])
正規化完了 x.shape=torch.Size([1, 16, 128])
RMSNormの順伝播開始 x.shape=torch.Size([1, 8, 128]), residual.shape if residual is not None else None=None
正規化開始 x.shape=torch.Size([1, 8, 128])
正規化完了 x.shape=torch.Size([1, 8, 128])
RoPEの順伝播開始 positions.shape=torch.Size([1]), query.shape=torch.Size([1, 16, 128]), key.shape=torch.Size([1, 8, 128])
RoPEを適用開始 x.shape=torch.Size([1, 16, 128]), cos.shape=torch.Size([1, 1, 64]), sin.shape=torch.Size([1, 1, 64])
チャンク化 x1.shape=torch.Size([1, 16, 64]), x2.shape=torch.Size([1, 16, 64])
回転行列適用 y1.shape=torch.Size([1, 16, 64]), y2.shape=torch.Size([1, 16, 64])
RoPEを適用完了 result.shape=torch.Size([1, 16, 128])
RoPEを適用開始 x.shape=torch.Size([1, 8, 128]), cos.shape=torch.Size([1, 1, 64]), sin.shape=torch.Size([1, 1, 64])
チャンク化 x1.shape=torch.Size([1, 8, 64]), x2.shape=torch.Size([1, 8, 64])
回転行列適用 y1.shape=torch.Size([1, 8, 64]), y2.shape=torch.Size([1, 8, 64])
RoPEを適用完了 result.shape=torch.Size([1, 8, 128])
RoPEの順伝播完了 query.shape=torch.Size([1, 16, 128]), key.shape=torch.Size([1, 8, 128])
Attention層の順伝播開始 q.shape=torch.Size([1, 16, 128]), k.shape=torch.Size([1, 8, 128]), v.shape=torch.Size([1, 8, 128])
現在のコンテキスト取得 _CONTEXT=Context(is_prefill=True, cu_seqlens_q=tensor([0, 1], device='cuda:0', dtype=torch.int32), cu_seqlens_k=tensor([0, 1], device='cuda:0', dtype=torch.int32), max_seqlen_q=1, max_seqlen_k=1, slot_mapping=tensor([0], device='cuda:0', dtype=torch.int32), context_lens=None, block_tables=None)
コンテキスト情報取得 context.is_prefill=True, context.slot_mapping=tensor([0], device='cuda:0', dtype=torch.int32), context.max_seqlen_q=1, context.cu_seqlens_q=tensor([0, 1], device='cuda:0', dtype=torch.int32), context.max_seqlen_k=1, context.cu_seqlens_k=tensor([0, 1], device='cuda:0', dtype=torch.int32), context.context_lens=None, context.block_tables=None
KVキャッシュにキーとバリューを保存開始 key.shape=torch.Size([1, 8, 128]), value.shape=torch.Size([1, 8, 128]), k_cache.shape=torch.Size([893, 256, 8, 128]), v_cache.shape=torch.Size([893, 256, 8, 128]), slot_mapping.shape=torch.Size([1])
KVキャッシュにキーとバリューを保存完了
PrefillモードでFlashAttentionを実行 context.max_seqlen_q=1, context.cu_seqlens_q=tensor([0, 1], device='cuda:0', dtype=torch.int32), context.max_seqlen_k=1, context.cu_seqlens_k=tensor([0, 1], device='cuda:0', dtype=torch.int32)
Attention層の順伝播完了 o.shape=torch.Size([1, 16, 128])
RowParallelLinearの順伝播開始 入力形状: torch.Size([1, 2048])
RowParallelLinearの順伝播完了 出力形状: torch.Size([1, 1024])
Qwen3Attentionの順伝播完了 output.shape=torch.Size([1, 1024])
RMSNormの順伝播開始 x.shape=torch.Size([1, 1024]), residual.shape if residual is not None else None=torch.Size([1, 1024])
残差接続を適用
残差接続を適用完了
Qwen3MLPの順伝播 x.shape=torch.Size([1, 1024])
出力次元を並列化する線形層の順伝播開始 x.shape=torch.Size([1, 1024])
出力次元を並列化する線形層の順伝播完了 result.shape=torch.Size([1, 6144])
SwiGLUを順伝播開始 x.shape=torch.Size([1, 6144])
チャンク化 x.shape=torch.Size([1, 3072]), y.shape=torch.Size([1, 3072])
SwiGLUを順伝播完了 result.shape=torch.Size([1, 3072])
RowParallelLinearの順伝播開始 入力形状: torch.Size([1, 3072])
RowParallelLinearの順伝播完了 出力形状: torch.Size([1, 1024])
Qwen3MLPの順伝播完了 x.shape=torch.Size([1, 1024])
Qwen3DecoderLayerの順伝播完了 hidden_states.shape=torch.Size([1, 1024])
Qwen3DecoderLayerの順伝播 positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 1024]) residual is None=False
RMSNormの順伝播開始 x.shape=torch.Size([1, 1024]), residual.shape if residual is not None else None=torch.Size([1, 1024])
残差接続を適用
残差接続を適用完了
Qwen3Attentionの順伝播 positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 1024])
出力次元を並列化する線形層の順伝播開始 x.shape=torch.Size([1, 1024])
出力次元を並列化する線形層の順伝播完了 result.shape=torch.Size([1, 4096])
RMSNormの順伝播開始 x.shape=torch.Size([1, 16, 128]), residual.shape if residual is not None else None=None
正規化開始 x.shape=torch.Size([1, 16, 128])
正規化完了 x.shape=torch.Size([1, 16, 128])
RMSNormの順伝播開始 x.shape=torch.Size([1, 8, 128]), residual.shape if residual is not None else None=None
正規化開始 x.shape=torch.Size([1, 8, 128])
正規化完了 x.shape=torch.Size([1, 8, 128])
RoPEの順伝播開始 positions.shape=torch.Size([1]), query.shape=torch.Size([1, 16, 128]), key.shape=torch.Size([1, 8, 128])
RoPEを適用開始 x.shape=torch.Size([1, 16, 128]), cos.shape=torch.Size([1, 1, 64]), sin.shape=torch.Size([1, 1, 64])
チャンク化 x1.shape=torch.Size([1, 16, 64]), x2.shape=torch.Size([1, 16, 64])
回転行列適用 y1.shape=torch.Size([1, 16, 64]), y2.shape=torch.Size([1, 16, 64])
RoPEを適用完了 result.shape=torch.Size([1, 16, 128])
RoPEを適用開始 x.shape=torch.Size([1, 8, 128]), cos.shape=torch.Size([1, 1, 64]), sin.shape=torch.Size([1, 1, 64])
チャンク化 x1.shape=torch.Size([1, 8, 64]), x2.shape=torch.Size([1, 8, 64])
回転行列適用 y1.shape=torch.Size([1, 8, 64]), y2.shape=torch.Size([1, 8, 64])
RoPEを適用完了 result.shape=torch.Size([1, 8, 128])
RoPEの順伝播完了 query.shape=torch.Size([1, 16, 128]), key.shape=torch.Size([1, 8, 128])
Attention層の順伝播開始 q.shape=torch.Size([1, 16, 128]), k.shape=torch.Size([1, 8, 128]), v.shape=torch.Size([1, 8, 128])
現在のコンテキスト取得 _CONTEXT=Context(is_prefill=True, cu_seqlens_q=tensor([0, 1], device='cuda:0', dtype=torch.int32), cu_seqlens_k=tensor([0, 1], device='cuda:0', dtype=torch.int32), max_seqlen_q=1, max_seqlen_k=1, slot_mapping=tensor([0], device='cuda:0', dtype=torch.int32), context_lens=None, block_tables=None)
コンテキスト情報取得 context.is_prefill=True, context.slot_mapping=tensor([0], device='cuda:0', dtype=torch.int32), context.max_seqlen_q=1, context.cu_seqlens_q=tensor([0, 1], device='cuda:0', dtype=torch.int32), context.max_seqlen_k=1, context.cu_seqlens_k=tensor([0, 1], device='cuda:0', dtype=torch.int32), context.context_lens=None, context.block_tables=None
KVキャッシュにキーとバリューを保存開始 key.shape=torch.Size([1, 8, 128]), value.shape=torch.Size([1, 8, 128]), k_cache.shape=torch.Size([893, 256, 8, 128]), v_cache.shape=torch.Size([893, 256, 8, 128]), slot_mapping.shape=torch.Size([1])
KVキャッシュにキーとバリューを保存完了
PrefillモードでFlashAttentionを実行 context.max_seqlen_q=1, context.cu_seqlens_q=tensor([0, 1], device='cuda:0', dtype=torch.int32), context.max_seqlen_k=1, context.cu_seqlens_k=tensor([0, 1], device='cuda:0', dtype=torch.int32)
Attention層の順伝播完了 o.shape=torch.Size([1, 16, 128])
RowParallelLinearの順伝播開始 入力形状: torch.Size([1, 2048])
RowParallelLinearの順伝播完了 出力形状: torch.Size([1, 1024])
Qwen3Attentionの順伝播完了 output.shape=torch.Size([1, 1024])
RMSNormの順伝播開始 x.shape=torch.Size([1, 1024]), residual.shape if residual is not None else None=torch.Size([1, 1024])
残差接続を適用
残差接続を適用完了
Qwen3MLPの順伝播 x.shape=torch.Size([1, 1024])
出力次元を並列化する線形層の順伝播開始 x.shape=torch.Size([1, 1024])
出力次元を並列化する線形層の順伝播完了 result.shape=torch.Size([1, 6144])
SwiGLUを順伝播開始 x.shape=torch.Size([1, 6144])
チャンク化 x.shape=torch.Size([1, 3072]), y.shape=torch.Size([1, 3072])
SwiGLUを順伝播完了 result.shape=torch.Size([1, 3072])
RowParallelLinearの順伝播開始 入力形状: torch.Size([1, 3072])
RowParallelLinearの順伝播完了 出力形状: torch.Size([1, 1024])
Qwen3MLPの順伝播完了 x.shape=torch.Size([1, 1024])
Qwen3DecoderLayerの順伝播完了 hidden_states.shape=torch.Size([1, 1024])
Qwen3DecoderLayerの順伝播 positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 1024]) residual is None=False
RMSNormの順伝播開始 x.shape=torch.Size([1, 1024]), residual.shape if residual is not None else None=torch.Size([1, 1024])
残差接続を適用
残差接続を適用完了
Qwen3Attentionの順伝播 positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 1024])
出力次元を並列化する線形層の順伝播開始 x.shape=torch.Size([1, 1024])
出力次元を並列化する線形層の順伝播完了 result.shape=torch.Size([1, 4096])
RMSNormの順伝播開始 x.shape=torch.Size([1, 16, 128]), residual.shape if residual is not None else None=None
正規化開始 x.shape=torch.Size([1, 16, 128])
正規化完了 x.shape=torch.Size([1, 16, 128])
RMSNormの順伝播開始 x.shape=torch.Size([1, 8, 128]), residual.shape if residual is not None else None=None
正規化開始 x.shape=torch.Size([1, 8, 128])
正規化完了 x.shape=torch.Size([1, 8, 128])
RoPEの順伝播開始 positions.shape=torch.Size([1]), query.shape=torch.Size([1, 16, 128]), key.shape=torch.Size([1, 8, 128])
RoPEを適用開始 x.shape=torch.Size([1, 16, 128]), cos.shape=torch.Size([1, 1, 64]), sin.shape=torch.Size([1, 1, 64])
チャンク化 x1.shape=torch.Size([1, 16, 64]), x2.shape=torch.Size([1, 16, 64])
回転行列適用 y1.shape=torch.Size([1, 16, 64]), y2.shape=torch.Size([1, 16, 64])
RoPEを適用完了 result.shape=torch.Size([1, 16, 128])
RoPEを適用開始 x.shape=torch.Size([1, 8, 128]), cos.shape=torch.Size([1, 1, 64]), sin.shape=torch.Size([1, 1, 64])
チャンク化 x1.shape=torch.Size([1, 8, 64]), x2.shape=torch.Size([1, 8, 64])
回転行列適用 y1.shape=torch.Size([1, 8, 64]), y2.shape=torch.Size([1, 8, 64])
RoPEを適用完了 result.shape=torch.Size([1, 8, 128])
RoPEの順伝播完了 query.shape=torch.Size([1, 16, 128]), key.shape=torch.Size([1, 8, 128])
Attention層の順伝播開始 q.shape=torch.Size([1, 16, 128]), k.shape=torch.Size([1, 8, 128]), v.shape=torch.Size([1, 8, 128])
現在のコンテキスト取得 _CONTEXT=Context(is_prefill=True, cu_seqlens_q=tensor([0, 1], device='cuda:0', dtype=torch.int32), cu_seqlens_k=tensor([0, 1], device='cuda:0', dtype=torch.int32), max_seqlen_q=1, max_seqlen_k=1, slot_mapping=tensor([0], device='cuda:0', dtype=torch.int32), context_lens=None, block_tables=None)
コンテキスト情報取得 context.is_prefill=True, context.slot_mapping=tensor([0], device='cuda:0', dtype=torch.int32), context.max_seqlen_q=1, context.cu_seqlens_q=tensor([0, 1], device='cuda:0', dtype=torch.int32), context.max_seqlen_k=1, context.cu_seqlens_k=tensor([0, 1], device='cuda:0', dtype=torch.int32), context.context_lens=None, context.block_tables=None
KVキャッシュにキーとバリューを保存開始 key.shape=torch.Size([1, 8, 128]), value.shape=torch.Size([1, 8, 128]), k_cache.shape=torch.Size([893, 256, 8, 128]), v_cache.shape=torch.Size([893, 256, 8, 128]), slot_mapping.shape=torch.Size([1])
KVキャッシュにキーとバリューを保存完了
PrefillモードでFlashAttentionを実行 context.max_seqlen_q=1, context.cu_seqlens_q=tensor([0, 1], device='cuda:0', dtype=torch.int32), context.max_seqlen_k=1, context.cu_seqlens_k=tensor([0, 1], device='cuda:0', dtype=torch.int32)
Attention層の順伝播完了 o.shape=torch.Size([1, 16, 128])
RowParallelLinearの順伝播開始 入力形状: torch.Size([1, 2048])
RowParallelLinearの順伝播完了 出力形状: torch.Size([1, 1024])
Qwen3Attentionの順伝播完了 output.shape=torch.Size([1, 1024])
RMSNormの順伝播開始 x.shape=torch.Size([1, 1024]), residual.shape if residual is not None else None=torch.Size([1, 1024])
残差接続を適用
残差接続を適用完了
Qwen3MLPの順伝播 x.shape=torch.Size([1, 1024])
出力次元を並列化する線形層の順伝播開始 x.shape=torch.Size([1, 1024])
出力次元を並列化する線形層の順伝播完了 result.shape=torch.Size([1, 6144])
SwiGLUを順伝播開始 x.shape=torch.Size([1, 6144])
チャンク化 x.shape=torch.Size([1, 3072]), y.shape=torch.Size([1, 3072])
SwiGLUを順伝播完了 result.shape=torch.Size([1, 3072])
RowParallelLinearの順伝播開始 入力形状: torch.Size([1, 3072])
RowParallelLinearの順伝播完了 出力形状: torch.Size([1, 1024])
Qwen3MLPの順伝播完了 x.shape=torch.Size([1, 1024])
Qwen3DecoderLayerの順伝播完了 hidden_states.shape=torch.Size([1, 1024])
Qwen3DecoderLayerの順伝播 positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 1024]) residual is None=False
RMSNormの順伝播開始 x.shape=torch.Size([1, 1024]), residual.shape if residual is not None else None=torch.Size([1, 1024])
残差接続を適用
残差接続を適用完了
Qwen3Attentionの順伝播 positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 1024])
出力次元を並列化する線形層の順伝播開始 x.shape=torch.Size([1, 1024])
出力次元を並列化する線形層の順伝播完了 result.shape=torch.Size([1, 4096])
RMSNormの順伝播開始 x.shape=torch.Size([1, 16, 128]), residual.shape if residual is not None else None=None
正規化開始 x.shape=torch.Size([1, 16, 128])
正規化完了 x.shape=torch.Size([1, 16, 128])
RMSNormの順伝播開始 x.shape=torch.Size([1, 8, 128]), residual.shape if residual is not None else None=None
正規化開始 x.shape=torch.Size([1, 8, 128])
正規化完了 x.shape=torch.Size([1, 8, 128])
RoPEの順伝播開始 positions.shape=torch.Size([1]), query.shape=torch.Size([1, 16, 128]), key.shape=torch.Size([1, 8, 128])
RoPEを適用開始 x.shape=torch.Size([1, 16, 128]), cos.shape=torch.Size([1, 1, 64]), sin.shape=torch.Size([1, 1, 64])
チャンク化 x1.shape=torch.Size([1, 16, 64]), x2.shape=torch.Size([1, 16, 64])
回転行列適用 y1.shape=torch.Size([1, 16, 64]), y2.shape=torch.Size([1, 16, 64])
RoPEを適用完了 result.shape=torch.Size([1, 16, 128])
RoPEを適用開始 x.shape=torch.Size([1, 8, 128]), cos.shape=torch.Size([1, 1, 64]), sin.shape=torch.Size([1, 1, 64])
チャンク化 x1.shape=torch.Size([1, 8, 64]), x2.shape=torch.Size([1, 8, 64])
回転行列適用 y1.shape=torch.Size([1, 8, 64]), y2.shape=torch.Size([1, 8, 64])
RoPEを適用完了 result.shape=torch.Size([1, 8, 128])
RoPEの順伝播完了 query.shape=torch.Size([1, 16, 128]), key.shape=torch.Size([1, 8, 128])
Attention層の順伝播開始 q.shape=torch.Size([1, 16, 128]), k.shape=torch.Size([1, 8, 128]), v.shape=torch.Size([1, 8, 128])
現在のコンテキスト取得 _CONTEXT=Context(is_prefill=True, cu_seqlens_q=tensor([0, 1], device='cuda:0', dtype=torch.int32), cu_seqlens_k=tensor([0, 1], device='cuda:0', dtype=torch.int32), max_seqlen_q=1, max_seqlen_k=1, slot_mapping=tensor([0], device='cuda:0', dtype=torch.int32), context_lens=None, block_tables=None)
コンテキスト情報取得 context.is_prefill=True, context.slot_mapping=tensor([0], device='cuda:0', dtype=torch.int32), context.max_seqlen_q=1, context.cu_seqlens_q=tensor([0, 1], device='cuda:0', dtype=torch.int32), context.max_seqlen_k=1, context.cu_seqlens_k=tensor([0, 1], device='cuda:0', dtype=torch.int32), context.context_lens=None, context.block_tables=None
KVキャッシュにキーとバリューを保存開始 key.shape=torch.Size([1, 8, 128]), value.shape=torch.Size([1, 8, 128]), k_cache.shape=torch.Size([893, 256, 8, 128]), v_cache.shape=torch.Size([893, 256, 8, 128]), slot_mapping.shape=torch.Size([1])
KVキャッシュにキーとバリューを保存完了
PrefillモードでFlashAttentionを実行 context.max_seqlen_q=1, context.cu_seqlens_q=tensor([0, 1], device='cuda:0', dtype=torch.int32), context.max_seqlen_k=1, context.cu_seqlens_k=tensor([0, 1], device='cuda:0', dtype=torch.int32)
Attention層の順伝播完了 o.shape=torch.Size([1, 16, 128])
RowParallelLinearの順伝播開始 入力形状: torch.Size([1, 2048])
RowParallelLinearの順伝播完了 出力形状: torch.Size([1, 1024])
Qwen3Attentionの順伝播完了 output.shape=torch.Size([1, 1024])
RMSNormの順伝播開始 x.shape=torch.Size([1, 1024]), residual.shape if residual is not None else None=torch.Size([1, 1024])
残差接続を適用
残差接続を適用完了
Qwen3MLPの順伝播 x.shape=torch.Size([1, 1024])
出力次元を並列化する線形層の順伝播開始 x.shape=torch.Size([1, 1024])
出力次元を並列化する線形層の順伝播完了 result.shape=torch.Size([1, 6144])
SwiGLUを順伝播開始 x.shape=torch.Size([1, 6144])
チャンク化 x.shape=torch.Size([1, 3072]), y.shape=torch.Size([1, 3072])
SwiGLUを順伝播完了 result.shape=torch.Size([1, 3072])
RowParallelLinearの順伝播開始 入力形状: torch.Size([1, 3072])
RowParallelLinearの順伝播完了 出力形状: torch.Size([1, 1024])
Qwen3MLPの順伝播完了 x.shape=torch.Size([1, 1024])
Qwen3DecoderLayerの順伝播完了 hidden_states.shape=torch.Size([1, 1024])
Qwen3DecoderLayerの順伝播 positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 1024]) residual is None=False
RMSNormの順伝播開始 x.shape=torch.Size([1, 1024]), residual.shape if residual is not None else None=torch.Size([1, 1024])
残差接続を適用
残差接続を適用完了
Qwen3Attentionの順伝播 positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 1024])
出力次元を並列化する線形層の順伝播開始 x.shape=torch.Size([1, 1024])
出力次元を並列化する線形層の順伝播完了 result.shape=torch.Size([1, 4096])
RMSNormの順伝播開始 x.shape=torch.Size([1, 16, 128]), residual.shape if residual is not None else None=None
正規化開始 x.shape=torch.Size([1, 16, 128])
正規化完了 x.shape=torch.Size([1, 16, 128])
RMSNormの順伝播開始 x.shape=torch.Size([1, 8, 128]), residual.shape if residual is not None else None=None
正規化開始 x.shape=torch.Size([1, 8, 128])
正規化完了 x.shape=torch.Size([1, 8, 128])
RoPEの順伝播開始 positions.shape=torch.Size([1]), query.shape=torch.Size([1, 16, 128]), key.shape=torch.Size([1, 8, 128])
RoPEを適用開始 x.shape=torch.Size([1, 16, 128]), cos.shape=torch.Size([1, 1, 64]), sin.shape=torch.Size([1, 1, 64])
チャンク化 x1.shape=torch.Size([1, 16, 64]), x2.shape=torch.Size([1, 16, 64])
回転行列適用 y1.shape=torch.Size([1, 16, 64]), y2.shape=torch.Size([1, 16, 64])
RoPEを適用完了 result.shape=torch.Size([1, 16, 128])
RoPEを適用開始 x.shape=torch.Size([1, 8, 128]), cos.shape=torch.Size([1, 1, 64]), sin.shape=torch.Size([1, 1, 64])
チャンク化 x1.shape=torch.Size([1, 8, 64]), x2.shape=torch.Size([1, 8, 64])
回転行列適用 y1.shape=torch.Size([1, 8, 64]), y2.shape=torch.Size([1, 8, 64])
RoPEを適用完了 result.shape=torch.Size([1, 8, 128])
RoPEの順伝播完了 query.shape=torch.Size([1, 16, 128]), key.shape=torch.Size([1, 8, 128])
Attention層の順伝播開始 q.shape=torch.Size([1, 16, 128]), k.shape=torch.Size([1, 8, 128]), v.shape=torch.Size([1, 8, 128])
現在のコンテキスト取得 _CONTEXT=Context(is_prefill=True, cu_seqlens_q=tensor([0, 1], device='cuda:0', dtype=torch.int32), cu_seqlens_k=tensor([0, 1], device='cuda:0', dtype=torch.int32), max_seqlen_q=1, max_seqlen_k=1, slot_mapping=tensor([0], device='cuda:0', dtype=torch.int32), context_lens=None, block_tables=None)
コンテキスト情報取得 context.is_prefill=True, context.slot_mapping=tensor([0], device='cuda:0', dtype=torch.int32), context.max_seqlen_q=1, context.cu_seqlens_q=tensor([0, 1], device='cuda:0', dtype=torch.int32), context.max_seqlen_k=1, context.cu_seqlens_k=tensor([0, 1], device='cuda:0', dtype=torch.int32), context.context_lens=None, context.block_tables=None
KVキャッシュにキーとバリューを保存開始 key.shape=torch.Size([1, 8, 128]), value.shape=torch.Size([1, 8, 128]), k_cache.shape=torch.Size([893, 256, 8, 128]), v_cache.shape=torch.Size([893, 256, 8, 128]), slot_mapping.shape=torch.Size([1])
KVキャッシュにキーとバリューを保存完了
PrefillモードでFlashAttentionを実行 context.max_seqlen_q=1, context.cu_seqlens_q=tensor([0, 1], device='cuda:0', dtype=torch.int32), context.max_seqlen_k=1, context.cu_seqlens_k=tensor([0, 1], device='cuda:0', dtype=torch.int32)
Attention層の順伝播完了 o.shape=torch.Size([1, 16, 128])
RowParallelLinearの順伝播開始 入力形状: torch.Size([1, 2048])
RowParallelLinearの順伝播完了 出力形状: torch.Size([1, 1024])
Qwen3Attentionの順伝播完了 output.shape=torch.Size([1, 1024])
RMSNormの順伝播開始 x.shape=torch.Size([1, 1024]), residual.shape if residual is not None else None=torch.Size([1, 1024])
残差接続を適用
残差接続を適用完了
Qwen3MLPの順伝播 x.shape=torch.Size([1, 1024])
出力次元を並列化する線形層の順伝播開始 x.shape=torch.Size([1, 1024])
出力次元を並列化する線形層の順伝播完了 result.shape=torch.Size([1, 6144])
SwiGLUを順伝播開始 x.shape=torch.Size([1, 6144])
チャンク化 x.shape=torch.Size([1, 3072]), y.shape=torch.Size([1, 3072])
SwiGLUを順伝播完了 result.shape=torch.Size([1, 3072])
RowParallelLinearの順伝播開始 入力形状: torch.Size([1, 3072])
RowParallelLinearの順伝播完了 出力形状: torch.Size([1, 1024])
Qwen3MLPの順伝播完了 x.shape=torch.Size([1, 1024])
Qwen3DecoderLayerの順伝播完了 hidden_states.shape=torch.Size([1, 1024])
Qwen3DecoderLayerの順伝播 positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 1024]) residual is None=False
RMSNormの順伝播開始 x.shape=torch.Size([1, 1024]), residual.shape if residual is not None else None=torch.Size([1, 1024])
残差接続を適用
残差接続を適用完了
Qwen3Attentionの順伝播 positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 1024])
出力次元を並列化する線形層の順伝播開始 x.shape=torch.Size([1, 1024])
出力次元を並列化する線形層の順伝播完了 result.shape=torch.Size([1, 4096])
RMSNormの順伝播開始 x.shape=torch.Size([1, 16, 128]), residual.shape if residual is not None else None=None
正規化開始 x.shape=torch.Size([1, 16, 128])
正規化完了 x.shape=torch.Size([1, 16, 128])
RMSNormの順伝播開始 x.shape=torch.Size([1, 8, 128]), residual.shape if residual is not None else None=None
正規化開始 x.shape=torch.Size([1, 8, 128])
正規化完了 x.shape=torch.Size([1, 8, 128])
RoPEの順伝播開始 positions.shape=torch.Size([1]), query.shape=torch.Size([1, 16, 128]), key.shape=torch.Size([1, 8, 128])
RoPEを適用開始 x.shape=torch.Size([1, 16, 128]), cos.shape=torch.Size([1, 1, 64]), sin.shape=torch.Size([1, 1, 64])
チャンク化 x1.shape=torch.Size([1, 16, 64]), x2.shape=torch.Size([1, 16, 64])
回転行列適用 y1.shape=torch.Size([1, 16, 64]), y2.shape=torch.Size([1, 16, 64])
RoPEを適用完了 result.shape=torch.Size([1, 16, 128])
RoPEを適用開始 x.shape=torch.Size([1, 8, 128]), cos.shape=torch.Size([1, 1, 64]), sin.shape=torch.Size([1, 1, 64])
チャンク化 x1.shape=torch.Size([1, 8, 64]), x2.shape=torch.Size([1, 8, 64])
回転行列適用 y1.shape=torch.Size([1, 8, 64]), y2.shape=torch.Size([1, 8, 64])
RoPEを適用完了 result.shape=torch.Size([1, 8, 128])
RoPEの順伝播完了 query.shape=torch.Size([1, 16, 128]), key.shape=torch.Size([1, 8, 128])
Attention層の順伝播開始 q.shape=torch.Size([1, 16, 128]), k.shape=torch.Size([1, 8, 128]), v.shape=torch.Size([1, 8, 128])
現在のコンテキスト取得 _CONTEXT=Context(is_prefill=True, cu_seqlens_q=tensor([0, 1], device='cuda:0', dtype=torch.int32), cu_seqlens_k=tensor([0, 1], device='cuda:0', dtype=torch.int32), max_seqlen_q=1, max_seqlen_k=1, slot_mapping=tensor([0], device='cuda:0', dtype=torch.int32), context_lens=None, block_tables=None)
コンテキスト情報取得 context.is_prefill=True, context.slot_mapping=tensor([0], device='cuda:0', dtype=torch.int32), context.max_seqlen_q=1, context.cu_seqlens_q=tensor([0, 1], device='cuda:0', dtype=torch.int32), context.max_seqlen_k=1, context.cu_seqlens_k=tensor([0, 1], device='cuda:0', dtype=torch.int32), context.context_lens=None, context.block_tables=None
KVキャッシュにキーとバリューを保存開始 key.shape=torch.Size([1, 8, 128]), value.shape=torch.Size([1, 8, 128]), k_cache.shape=torch.Size([893, 256, 8, 128]), v_cache.shape=torch.Size([893, 256, 8, 128]), slot_mapping.shape=torch.Size([1])
KVキャッシュにキーとバリューを保存完了
PrefillモードでFlashAttentionを実行 context.max_seqlen_q=1, context.cu_seqlens_q=tensor([0, 1], device='cuda:0', dtype=torch.int32), context.max_seqlen_k=1, context.cu_seqlens_k=tensor([0, 1], device='cuda:0', dtype=torch.int32)
Attention層の順伝播完了 o.shape=torch.Size([1, 16, 128])
RowParallelLinearの順伝播開始 入力形状: torch.Size([1, 2048])
RowParallelLinearの順伝播完了 出力形状: torch.Size([1, 1024])
Qwen3Attentionの順伝播完了 output.shape=torch.Size([1, 1024])
RMSNormの順伝播開始 x.shape=torch.Size([1, 1024]), residual.shape if residual is not None else None=torch.Size([1, 1024])
残差接続を適用
残差接続を適用完了
Qwen3MLPの順伝播 x.shape=torch.Size([1, 1024])
出力次元を並列化する線形層の順伝播開始 x.shape=torch.Size([1, 1024])
出力次元を並列化する線形層の順伝播完了 result.shape=torch.Size([1, 6144])
SwiGLUを順伝播開始 x.shape=torch.Size([1, 6144])
チャンク化 x.shape=torch.Size([1, 3072]), y.shape=torch.Size([1, 3072])
SwiGLUを順伝播完了 result.shape=torch.Size([1, 3072])
RowParallelLinearの順伝播開始 入力形状: torch.Size([1, 3072])
RowParallelLinearの順伝播完了 出力形状: torch.Size([1, 1024])
Qwen3MLPの順伝播完了 x.shape=torch.Size([1, 1024])
Qwen3DecoderLayerの順伝播完了 hidden_states.shape=torch.Size([1, 1024])
Qwen3DecoderLayerの順伝播 positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 1024]) residual is None=False
RMSNormの順伝播開始 x.shape=torch.Size([1, 1024]), residual.shape if residual is not None else None=torch.Size([1, 1024])
残差接続を適用
残差接続を適用完了
Qwen3Attentionの順伝播 positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 1024])
出力次元を並列化する線形層の順伝播開始 x.shape=torch.Size([1, 1024])
出力次元を並列化する線形層の順伝播完了 result.shape=torch.Size([1, 4096])
RMSNormの順伝播開始 x.shape=torch.Size([1, 16, 128]), residual.shape if residual is not None else None=None
正規化開始 x.shape=torch.Size([1, 16, 128])
正規化完了 x.shape=torch.Size([1, 16, 128])
RMSNormの順伝播開始 x.shape=torch.Size([1, 8, 128]), residual.shape if residual is not None else None=None
正規化開始 x.shape=torch.Size([1, 8, 128])
正規化完了 x.shape=torch.Size([1, 8, 128])
RoPEの順伝播開始 positions.shape=torch.Size([1]), query.shape=torch.Size([1, 16, 128]), key.shape=torch.Size([1, 8, 128])
RoPEを適用開始 x.shape=torch.Size([1, 16, 128]), cos.shape=torch.Size([1, 1, 64]), sin.shape=torch.Size([1, 1, 64])
チャンク化 x1.shape=torch.Size([1, 16, 64]), x2.shape=torch.Size([1, 16, 64])
回転行列適用 y1.shape=torch.Size([1, 16, 64]), y2.shape=torch.Size([1, 16, 64])
RoPEを適用完了 result.shape=torch.Size([1, 16, 128])
RoPEを適用開始 x.shape=torch.Size([1, 8, 128]), cos.shape=torch.Size([1, 1, 64]), sin.shape=torch.Size([1, 1, 64])
チャンク化 x1.shape=torch.Size([1, 8, 64]), x2.shape=torch.Size([1, 8, 64])
回転行列適用 y1.shape=torch.Size([1, 8, 64]), y2.shape=torch.Size([1, 8, 64])
RoPEを適用完了 result.shape=torch.Size([1, 8, 128])
RoPEの順伝播完了 query.shape=torch.Size([1, 16, 128]), key.shape=torch.Size([1, 8, 128])
Attention層の順伝播開始 q.shape=torch.Size([1, 16, 128]), k.shape=torch.Size([1, 8, 128]), v.shape=torch.Size([1, 8, 128])
現在のコンテキスト取得 _CONTEXT=Context(is_prefill=True, cu_seqlens_q=tensor([0, 1], device='cuda:0', dtype=torch.int32), cu_seqlens_k=tensor([0, 1], device='cuda:0', dtype=torch.int32), max_seqlen_q=1, max_seqlen_k=1, slot_mapping=tensor([0], device='cuda:0', dtype=torch.int32), context_lens=None, block_tables=None)
コンテキスト情報取得 context.is_prefill=True, context.slot_mapping=tensor([0], device='cuda:0', dtype=torch.int32), context.max_seqlen_q=1, context.cu_seqlens_q=tensor([0, 1], device='cuda:0', dtype=torch.int32), context.max_seqlen_k=1, context.cu_seqlens_k=tensor([0, 1], device='cuda:0', dtype=torch.int32), context.context_lens=None, context.block_tables=None
KVキャッシュにキーとバリューを保存開始 key.shape=torch.Size([1, 8, 128]), value.shape=torch.Size([1, 8, 128]), k_cache.shape=torch.Size([893, 256, 8, 128]), v_cache.shape=torch.Size([893, 256, 8, 128]), slot_mapping.shape=torch.Size([1])
KVキャッシュにキーとバリューを保存完了
PrefillモードでFlashAttentionを実行 context.max_seqlen_q=1, context.cu_seqlens_q=tensor([0, 1], device='cuda:0', dtype=torch.int32), context.max_seqlen_k=1, context.cu_seqlens_k=tensor([0, 1], device='cuda:0', dtype=torch.int32)
Attention層の順伝播完了 o.shape=torch.Size([1, 16, 128])
RowParallelLinearの順伝播開始 入力形状: torch.Size([1, 2048])
RowParallelLinearの順伝播完了 出力形状: torch.Size([1, 1024])
Qwen3Attentionの順伝播完了 output.shape=torch.Size([1, 1024])
RMSNormの順伝播開始 x.shape=torch.Size([1, 1024]), residual.shape if residual is not None else None=torch.Size([1, 1024])
残差接続を適用
残差接続を適用完了
Qwen3MLPの順伝播 x.shape=torch.Size([1, 1024])
出力次元を並列化する線形層の順伝播開始 x.shape=torch.Size([1, 1024])
出力次元を並列化する線形層の順伝播完了 result.shape=torch.Size([1, 6144])
SwiGLUを順伝播開始 x.shape=torch.Size([1, 6144])
チャンク化 x.shape=torch.Size([1, 3072]), y.shape=torch.Size([1, 3072])
SwiGLUを順伝播完了 result.shape=torch.Size([1, 3072])
RowParallelLinearの順伝播開始 入力形状: torch.Size([1, 3072])
RowParallelLinearの順伝播完了 出力形状: torch.Size([1, 1024])
Qwen3MLPの順伝播完了 x.shape=torch.Size([1, 1024])
Qwen3DecoderLayerの順伝播完了 hidden_states.shape=torch.Size([1, 1024])
RMSNormの順伝播開始 x.shape=torch.Size([1, 1024]), residual.shape if residual is not None else None=torch.Size([1, 1024])
残差接続を適用
残差接続を適用完了
Qwen3Modelの順伝播完了 hidden_states.shape=torch.Size([1, 1024])
Qwen3ForCausalLMの順伝播完了 output.shape=torch.Size([1, 1024])
Qwen3ForCausalLMのロジット計算開始 hidden_states.shape=torch.Size([1, 1024])
出力の言語モデルヘッドの順伝播開始 x.shape=torch.Size([1, 1024])
現在のコンテキスト取得 _CONTEXT=Context(is_prefill=True, cu_seqlens_q=tensor([0, 1], device='cuda:0', dtype=torch.int32), cu_seqlens_k=tensor([0, 1], device='cuda:0', dtype=torch.int32), max_seqlen_q=1, max_seqlen_k=1, slot_mapping=tensor([0], device='cuda:0', dtype=torch.int32), context_lens=None, block_tables=None)
最後のトークンを抽出（Prefill） x.shape=torch.Size([1, 1024])
出力の言語モデルヘッドの順伝播完了 logits.shape if logits is not None else None=torch.Size([1, 151936])
Qwen3ForCausalLMのロジット計算完了 logits.shape=torch.Size([1, 151936])
モデルの順伝搬（prefill）を実行完了 result.shape=torch.Size([1, 151936])
サンプリング開始 logits.shape=torch.Size([1, 151936]), temperatures.shape=torch.Size([1])
サンプリング完了 sample_tokens.shape=torch.Size([1])
コンテキストをリセット
推論の1サイクルを実行完了 (len(token_ids) if token_ids else None)=1
生成トークンの後処理を開始 len(seqs)=1 len(token_ids)=1
トークンを追加 token_id=70030
トークンを追加完了 token_id=70030 self.num_tokens=2
シーケンスのブロックを解放開始 4
ブロックを解放開始 block_id=0
ブロックを解放完了 block_id=0
シーケンスのブロックを解放完了 4
生成トークンの後処理完了 len(seqs)=1
推論エンジンの1サイクルを実行完了 outputs=[(4, [70030])] num_tokens=2
スケジューラの完了状態を確認 result=True
スケジューラに処理すべきタスクが残っているか確認 res=True
テキスト生成を完了 len(outputs)=1
