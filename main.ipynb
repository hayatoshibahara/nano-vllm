{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff311100",
   "metadata": {},
   "source": [
    "# Nano-vLLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "318655b6",
   "metadata": {},
   "source": [
    "### 環境構築"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b367e634",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "import os\n",
    "from dataclasses import dataclass\n",
    "from transformers import AutoConfig\n",
    "from dataclasses import dataclass\n",
    "import torch\n",
    "\n",
    "import logging\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.DEBUG)\n",
    "logger.addHandler(logging.StreamHandler())\n",
    "logger.addHandler(logging.FileHandler(\"debug.log\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c415eed4",
   "metadata": {},
   "source": [
    "## Qwen3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1579046c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SiluAndMul(nn.Module):\n",
    "    \"\"\"\n",
    "    SwiGLU（Swish-Gated Linear Unit）活性化関数の実装\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        logger.info(f\"SwiGLUを初期化\")\n",
    "        super().__init__()\n",
    "\n",
    "    @torch.compile\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        logger.info(f\"SwiGLUを順伝播開始 {x.shape=}\")\n",
    "\n",
    "        # 入力を2つのチャンクに分割\n",
    "        x, y = x.chunk(2, -1)\n",
    "        logger.debug(f\"チャンク化 {x.shape=}, {y.shape=}\")\n",
    "\n",
    "        # SwiGLU活性化関数の適用\n",
    "        result = F.silu(x) * y\n",
    "\n",
    "        logger.info(f\"SwiGLUを順伝播完了 {result.shape=}\")\n",
    "        return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2cb0ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import triton\n",
    "import triton.language as tl\n",
    "\n",
    "from flash_attn import flash_attn_varlen_func, flash_attn_with_kvcache\n",
    "from nanovllm.utils.context import get_context\n",
    "\n",
    "@triton.jit\n",
    "def store_kvcache_kernel(\n",
    "    key_ptr,\n",
    "    key_stride,\n",
    "    value_ptr,\n",
    "    value_stride,\n",
    "    k_cache_ptr,\n",
    "    v_cache_ptr,\n",
    "    slot_mapping_ptr,\n",
    "    D: tl.constexpr,\n",
    "):\n",
    "    \"\"\"\n",
    "    計算したキーとバリューをKVキャッシュに保存するTritonカーネル\n",
    "    スロットマッピングを参照して適切な物理メモリにキーとバリューを保存する\n",
    "\n",
    "    Args:\n",
    "        key_ptr: キーのポインタ\n",
    "        key_stride: キーのストライド\n",
    "        value_ptr: バリューのポインタ\n",
    "        value_stride: バリューのストライド\n",
    "        k_cache_ptr: キーキャッシュの先頭ポインタ\n",
    "        v_cache_ptr: バリューキャッシュの先頭ポインタ\n",
    "        slot_mapping_ptr: スロットマッピングのポインタ\n",
    "        D: キーとバリューの次元数\n",
    "    \"\"\"\n",
    "\n",
    "    # 1) 担当するインデックスを取得\n",
    "\n",
    "    # 担当するトークンのインデックスを取得\n",
    "    idx = tl.program_id(0)\n",
    "\n",
    "    # 2) 保存先スロットを特定\n",
    "\n",
    "    # スロットマッピングからトークンに対応する物理スロットを取得\n",
    "    slot = tl.load(slot_mapping_ptr + idx)\n",
    "\n",
    "    # 割り当てがない場合は何もしない\n",
    "    if slot == -1: return\n",
    "\n",
    "    # 3) 入力データを読み込み\n",
    "\n",
    "    # キーの読み込みオフセットを作成（D次元分）\n",
    "    key_offsets = idx * key_stride + tl.arange(0, D)\n",
    "\n",
    "    # バリューの読み込みオフセットを作成（D次元分）\n",
    "    value_offsets = idx * value_stride + tl.arange(0, D)\n",
    "\n",
    "    # 単一のキーを読み込み\n",
    "    key = tl.load(key_ptr + key_offsets)\n",
    "\n",
    "    # 単一のバリューを読み込み\n",
    "    value = tl.load(value_ptr + value_offsets)\n",
    "\n",
    "    # 4) KVキャッシュに書き込み\n",
    "\n",
    "    # KVキャッシュの書き込みオフセットを作成（D次元分）\n",
    "    cache_offsets = slot * D + tl.arange(0, D)\n",
    "\n",
    "    # 単一のキーをKVキャッシュに書き込み\n",
    "    tl.store(k_cache_ptr + cache_offsets, key)\n",
    "\n",
    "    # 単一のバリューをKVキャッシュに書き込み\n",
    "    tl.store(v_cache_ptr + cache_offsets, value)\n",
    "\n",
    "\n",
    "def store_kvcache(key: torch.Tensor, value: torch.Tensor, k_cache: torch.Tensor, v_cache: torch.Tensor, slot_mapping: torch.Tensor):\n",
    "    \"\"\"\n",
    "    計算したキーとバリューをKVキャッシュに保存する\n",
    "    store_kvcache_kernelのラッパー\n",
    "\n",
    "    Args:\n",
    "        key (torch.Tensor): 計算したキーのテンソル\n",
    "        value (torch.Tensor): 計算したバリューのテンソル\n",
    "        k_cache (torch.Tensor): キーキャッシュのテンソル\n",
    "        v_cache (torch.Tensor): バリューキャッシュのテンソル\n",
    "        slot_mapping (torch.Tensor): スロットマッピングのテンソル\n",
    "    \"\"\"\n",
    "    logger.info(f\"KVキャッシュにキーとバリューを保存開始 {key.shape=}, {value.shape=}, {k_cache.shape=}, {v_cache.shape=}, {slot_mapping.shape=}\")\n",
    "\n",
    "    # 1) 入力を検証\n",
    "\n",
    "    # トークン数N、ヘッド数num_heads、ヘッド次元head_dim\n",
    "    N, num_heads, head_dim = key.shape\n",
    "\n",
    "    # キーとバリューの次元数D\n",
    "    D = num_heads * head_dim\n",
    "\n",
    "    assert key.stride(-1) == 1 and value.stride(-1) == 1\n",
    "    assert key.stride(1) == head_dim and value.stride(1) == head_dim\n",
    "    assert k_cache.stride(1) == D and v_cache.stride(1) == D\n",
    "    assert slot_mapping.numel() == N\n",
    "\n",
    "    # 2) Tritonカーネルを起動\n",
    "    store_kvcache_kernel[(N,)](\n",
    "        key,\n",
    "        key.stride(0),\n",
    "        value,\n",
    "        value.stride(0),\n",
    "        k_cache, v_cache,\n",
    "        slot_mapping,\n",
    "        D)\n",
    "\n",
    "    logger.info(f\"KVキャッシュにキーとバリューを保存完了\")\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    \"\"\"\n",
    "    FlashAttentionのラッパー\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_heads,\n",
    "        head_dim,\n",
    "        scale,\n",
    "        num_kv_heads,\n",
    "    ):\n",
    "        logger.info(f\"Attention層を初期化 {num_heads=}, {head_dim=}, {scale=}, {num_kv_heads=}\")\n",
    "        super().__init__()\n",
    "\n",
    "        # ヘッド数\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        # ヘッドの次元数\n",
    "        self.head_dim = head_dim\n",
    "\n",
    "        # QKのスケーリング係数\n",
    "        self.scale = scale\n",
    "\n",
    "        # KVヘッド数（GQA用）\n",
    "        self.num_kv_heads = num_kv_heads\n",
    "\n",
    "        # KVキャッシュ領域への参照を初期化\n",
    "        self.k_cache = self.v_cache = torch.tensor([])\n",
    "\n",
    "        logger.info(f\"Attention層を初期化完了\")\n",
    "\n",
    "    def forward(self, q: torch.Tensor, k: torch.Tensor, v: torch.Tensor):\n",
    "        \"\"\"\n",
    "        FlashAttentionの順伝播\n",
    "\n",
    "        Args:\n",
    "            q (torch.Tensor): 計算したばかりのクエリ\n",
    "            k (torch.Tensor): 計算したばかりのキー\n",
    "            v (torch.Tensor): 計算したばかりのバリュー\n",
    "        Returns:\n",
    "            torch.Tensor: アテンション\n",
    "        \"\"\"\n",
    "        logger.info(f\"Attention層の順伝播開始 {q.shape=}, {k.shape=}, {v.shape=}\")\n",
    "\n",
    "        # 1) コンテキストを取得\n",
    "\n",
    "        context = get_context()\n",
    "\n",
    "        logger.debug(f\"コンテキスト情報取得 {context.is_prefill=}, {context.slot_mapping=}, {context.max_seqlen_q=}, {context.cu_seqlens_q=}, {context.max_seqlen_k=}, {context.cu_seqlens_k=}, {context.context_lens=}, {context.block_tables=}\")\n",
    "\n",
    "        # 2) KVキャッシュへの保存\n",
    "\n",
    "        # KVキャッシュへの参照を取得\n",
    "        k_cache, v_cache = self.k_cache, self.v_cache\n",
    "\n",
    "        # KVキャッシュが存在する場合\n",
    "        if k_cache.numel() and v_cache.numel():\n",
    "\n",
    "            # KVキャッシュにキーとバリューを保存\n",
    "            store_kvcache(k, v, k_cache, v_cache, context.slot_mapping)\n",
    "\n",
    "        # 3) FlashAttentionの実行\n",
    "\n",
    "        # Prefillの場合\n",
    "        if context.is_prefill:\n",
    "\n",
    "            # PrefixCachingの場合\n",
    "            if context.block_tables is not None:\n",
    "\n",
    "                # キャッシュからキーとバリューを取得\n",
    "                k, v = k_cache, v_cache\n",
    "\n",
    "            logger.debug(f\"PrefillモードでFlashAttentionを実行 {context.max_seqlen_q=}, {context.cu_seqlens_q=}, {context.max_seqlen_k=}, {context.cu_seqlens_k=}\")\n",
    "\n",
    "            # 可変長に対応したFlashAttentionを実行\n",
    "            o = flash_attn_varlen_func(\n",
    "                q,\n",
    "                k,\n",
    "                v,\n",
    "                max_seqlen_q=context.max_seqlen_q,\n",
    "                cu_seqlens_q=context.cu_seqlens_q,\n",
    "                max_seqlen_k=context.max_seqlen_k,\n",
    "                cu_seqlens_k=context.cu_seqlens_k,\n",
    "                softmax_scale=self.scale,\n",
    "                causal=True,\n",
    "                block_table=context.block_tables)\n",
    "        \n",
    "        # Decodeの場合\n",
    "        else:\n",
    "\n",
    "            logger.debug(f\"DecodeモードでFlashAttentionを実行 {context.context_lens=}, {context.block_tables=}\")\n",
    "\n",
    "            # KVキャッシュを用いたFlashAttentionを実行\n",
    "            o = flash_attn_with_kvcache(\n",
    "                q.unsqueeze(1),\n",
    "                k_cache,\n",
    "                v_cache,\n",
    "                cache_seqlens=context.context_lens,\n",
    "                block_table=context.block_tables, \n",
    "                softmax_scale=self.scale,\n",
    "                causal=True)\n",
    "\n",
    "        logger.info(f\"Attention層の順伝播完了 {o.shape=}\")\n",
    "\n",
    "        return o\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a1d5d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.distributed as dist\n",
    "\n",
    "class VocabParallelEmbedding(nn.Module):\n",
    "    \"\"\"\n",
    "    入力の埋め込み層を並列化するクラス（Tensor Parallelism）\n",
    "    語彙を複数のGPUに分割しメモリ使用量を削減し、並列計算を可能にする\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_embeddings: int,\n",
    "        embedding_dim: int,\n",
    "    ):\n",
    "        logger.info(f\"入力の埋め込み層を初期化開始 {num_embeddings=}, {embedding_dim=}\")\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        # 1) 分散環境の情報を取得\n",
    "\n",
    "        # 担当するGPU\n",
    "        self.tp_rank = dist.get_rank()\n",
    "\n",
    "        # 全GPU数\n",
    "        self.tp_size = dist.get_world_size()\n",
    "\n",
    "        logger.debug(f\"分散環境情報取得 {self.tp_rank=}, {self.tp_size=}\")\n",
    "\n",
    "        # 2) 担当する語彙の範囲を計算\n",
    "\n",
    "        assert num_embeddings % self.tp_size == 0\n",
    "\n",
    "        # 全語彙数\n",
    "        self.num_embeddings = num_embeddings\n",
    "\n",
    "        # 担当する語彙数\n",
    "        self.num_embeddings_per_partition = \\\n",
    "            self.num_embeddings // self.tp_size\n",
    "\n",
    "        # 担当する語彙の開始インデックス\n",
    "        self.vocab_start_idx = \\\n",
    "            self.num_embeddings_per_partition * self.tp_rank\n",
    "\n",
    "        # 担当する語彙の終了インデックス\n",
    "        self.vocab_end_idx = \\\n",
    "            self.vocab_start_idx + self.num_embeddings_per_partition\n",
    "\n",
    "        logger.debug(f\"担当する語彙の範囲計算 {self.vocab_start_idx=}, {self.vocab_end_idx=}\")\n",
    "\n",
    "        # 3) 担当する語彙の重み行列を初期化\n",
    "\n",
    "        self.weight = nn.Parameter(\n",
    "            torch.empty(self.num_embeddings_per_partition, embedding_dim))\n",
    "\n",
    "        self.weight.weight_loader = self.weight_loader\n",
    "\n",
    "        logger.debug(f\"担当する語彙の重み行列初期化 {self.weight.shape=}\")\n",
    "\n",
    "        logger.info(f\"入力の埋め込み層を初期化完了\")\n",
    "\n",
    "\n",
    "    def weight_loader(self, param: nn.Parameter, loaded_weight: torch.Tensor):\n",
    "        \"\"\"\n",
    "        モデルのチェックポイントから埋め込み層の重みをスライスしてロードする\n",
    "\n",
    "        Args:\n",
    "            param (nn.Parameter): 埋め込み層のパラメータ\n",
    "            loaded_weight (torch.Tensor): ロードする重み\n",
    "        \"\"\"\n",
    "        logger.info(f\"埋め込み層の重みをロード開始 {param.shape=}, {loaded_weight.shape=}\")\n",
    "\n",
    "        # 重みデータへの参照を取得\n",
    "        param_data = param.data\n",
    "\n",
    "        # 担当する重みのサイズ\n",
    "        shard_size = param_data.size(0)\n",
    "\n",
    "        # 担当する重みの開始インデックスを計算\n",
    "        start_idx = self.tp_rank * shard_size\n",
    "\n",
    "        # 担当する重みをスライスしてコピー\n",
    "        loaded_weight = loaded_weight.narrow(0, start_idx, shard_size)\n",
    "\n",
    "        logger.info(f\"埋め込み層の重みをロード完了 {loaded_weight.shape=}\")\n",
    "\n",
    "        \n",
    "    def forward(self, x: torch.Tensor):\n",
    "        \"\"\"\n",
    "        入力シーケンスを埋め込みベクトルに変換する\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): 入力シーケンスのインデックス\n",
    "        Returns:\n",
    "            torch.Tensor: 埋め込みベクトル\n",
    "        \"\"\"\n",
    "        logger.info(f\"埋め込み層の順伝播開始 {x.shape=}\")\n",
    "\n",
    "        # 1) 担当する語彙の範囲に基づいて入力をマスク\n",
    "\n",
    "        # GPUが複数ある場合\n",
    "        if self.tp_size > 1:\n",
    "\n",
    "            # 担当する語彙のマスクを作成\n",
    "            mask = (x >= self.vocab_start_idx) & (x < self.vocab_end_idx)\n",
    "\n",
    "            # ローカルIDに変換し、担当する語彙以外は0に置き換え\n",
    "            x = mask * (x - self.vocab_start_idx)\n",
    "\n",
    "        # 2) 埋め込みを計算\n",
    "\n",
    "        y = F.embedding(x, self.weight)\n",
    "\n",
    "        # 3) 結果を集約\n",
    "\n",
    "        # GPUが複数ある場合\n",
    "        if self.tp_size > 1:\n",
    "\n",
    "            # 担当する語彙以外の埋め込みを0に置き換え\n",
    "            y = mask.unsqueeze(1) * y\n",
    "\n",
    "            # 全GPUの埋め込みを集約\n",
    "            dist.all_reduce(y)\n",
    "\n",
    "        logger.info(f\"埋め込み層の順伝播完了 {y.shape=}\")\n",
    "        return y\n",
    "\n",
    "\n",
    "class ParallelLMHead(VocabParallelEmbedding):\n",
    "    \"\"\"\n",
    "    隠れ層の状態を語彙数分のロジットに変換する言語モデルヘッド\n",
    "    VocabParallelEmbeddingを継承し、Tensor Parallelismを利用\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_embeddings: int,\n",
    "        embedding_dim: int,\n",
    "        bias: bool = False,\n",
    "    ):\n",
    "        logger.info(f\"出力の言語モデルヘッドを初期化開始 {num_embeddings=}, {embedding_dim=}, {bias=}\")\n",
    "\n",
    "        assert not bias\n",
    "\n",
    "        super().__init__(num_embeddings, embedding_dim)\n",
    "\n",
    "        logger.info(f\"出力の言語モデルヘッドを初期化完了\")\n",
    "\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        \"\"\"\n",
    "        隠れ層の状態を語彙数分のロジットに変換する\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): 隠れ層の状態\n",
    "        \"\"\"\n",
    "\n",
    "        logger.info(f\"出力の言語モデルヘッドの順伝播開始 {x.shape=}\")\n",
    "\n",
    "        # コンテキストを取得\n",
    "        context = get_context()\n",
    "\n",
    "        # Prefillの場合\n",
    "        if context.is_prefill:\n",
    "            # 最後のトークンのインデックスを取得\n",
    "            last_indices = context.cu_seqlens_q[1:] - 1\n",
    "\n",
    "            # 入力から最後のトークンを抽出\n",
    "            x = x[last_indices].contiguous()\n",
    "\n",
    "            logger.debug(f\"最後のトークンを抽出（Prefill） {x.shape=}\")\n",
    "\n",
    "\n",
    "        # 線形射影を適用\n",
    "        logits = F.linear(x, self.weight)\n",
    "\n",
    "        # 複数GPUの場合\n",
    "        if self.tp_size > 1:\n",
    "\n",
    "            # メインプロセスの場合、出力を初期化\n",
    "            all_logits = [\n",
    "                torch.empty_like(logits) for _ in range(self.tp_size)\n",
    "            ] if self.tp_rank == 0 else None\n",
    "\n",
    "            # 全GPUのロジットを集約\n",
    "            dist.gather(logits, all_logits, 0)\n",
    "\n",
    "            # メインプロセスでロジットを連結\n",
    "            logits = torch.cat(all_logits, -1) if self.tp_rank == 0 else None\n",
    "\n",
    "        logger.info(f\"出力の言語モデルヘッドの順伝播完了 {logits.shape if logits is not None else None=}\")\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8480b2c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class RMSNorm(nn.Module):\n",
    "    \"\"\"\n",
    "    Root Mean Square Layer Normalizationの実装\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        hidden_size: int,\n",
    "        eps: float = 1e-6,\n",
    "    ) -> None:\n",
    "        logger.info(f\"RMSNormを初期化 {hidden_size=}, {eps=}\")\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        # 1で初期化\n",
    "        self.weight = nn.Parameter(torch.ones(hidden_size))\n",
    "        logger.info(f\"RMSNormの初期化完了\")\n",
    "\n",
    "    @torch.compile\n",
    "    def rms_forward(\n",
    "        self,\n",
    "        x: torch.Tensor,\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        RMSNormの正規化\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): 入力テンソル\n",
    "        Returns:\n",
    "            torch.Tensor: 正規化された出力テンソル\n",
    "        \"\"\"\n",
    "        logger.info(f\"正規化開始 {x.shape=}\")\n",
    "\n",
    "        orig_dtype = x.dtype\n",
    "\n",
    "        # アップキャスト\n",
    "        x = x.float()\n",
    "\n",
    "        # 分散を計算\n",
    "        var = x.pow(2).mean(dim=-1, keepdim=True)\n",
    "\n",
    "        # 分散の逆平方根を乗じて正規化\n",
    "        x.mul_(torch.rsqrt(var + self.eps))\n",
    "\n",
    "        # 元のデータ型に戻し、ゲインを乗じる\n",
    "        x = x.to(orig_dtype).mul_(self.weight)\n",
    "\n",
    "        logger.info(f\"正規化完了 {x.shape=}\")\n",
    "        return x\n",
    "\n",
    "    @torch.compile\n",
    "    def add_rms_forward(\n",
    "        self,\n",
    "        x: torch.Tensor,\n",
    "        residual: torch.Tensor,\n",
    "    ) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        残差接続を伴うRMSNormの正規化\n",
    "        \"\"\"\n",
    "        logger.info(f\"残差接続を適用\")\n",
    "\n",
    "        orig_dtype = x.dtype\n",
    "\n",
    "        # アップキャストして残差接続を適用\n",
    "        x = x.float().add_(residual.float())\n",
    "\n",
    "        # ダウンキャスト\n",
    "        residual = x.to(orig_dtype)\n",
    "\n",
    "        # 分散を計算\n",
    "        var = x.pow(2).mean(dim=-1, keepdim=True)\n",
    "\n",
    "        # 分散の逆平方根を乗じて正規化\n",
    "        x.mul_(torch.rsqrt(var + self.eps))\n",
    "\n",
    "        # 元のデータ型に戻し、ゲインを乗じる\n",
    "        x = x.to(orig_dtype).mul_(self.weight)\n",
    "\n",
    "        logger.info(f\"残差接続を適用完了\")\n",
    "        return x, residual\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x: torch.Tensor,\n",
    "        residual: torch.Tensor | None = None,\n",
    "    ) -> torch.Tensor | tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        RMSNormの順伝播\n",
    "        \"\"\"\n",
    "        logger.info(f\"RMSNormの順伝播開始 {x.shape=}, {residual.shape if residual is not None else None=}\")\n",
    "\n",
    "        # 残差接続がない場合\n",
    "        if residual is None:\n",
    "            result = self.rms_forward(x)\n",
    "\n",
    "        # 残差接続がある場合\n",
    "        else:\n",
    "            result = self.add_rms_forward(x, residual)\n",
    "\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0c872c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.distributed as dist\n",
    "\n",
    "def divide(numerator, denominator):\n",
    "    assert numerator % denominator == 0\n",
    "    return numerator // denominator\n",
    "\n",
    "\n",
    "class LinearBase(nn.Module):\n",
    "    \"\"\"\n",
    "    テンソル並列化（TP, Tensor Parallelism）に対応した線形層の基底クラス\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_size: int,\n",
    "        output_size: int,\n",
    "        bias: bool = False,\n",
    "        tp_dim: int | None = None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input_size (int): 入力の次元数\n",
    "            output_size (int): 出力の次元数\n",
    "            bias (bool): バイアス項の有無\n",
    "            tp_dim (int | None): 並列化する次元\n",
    "        \"\"\"\n",
    "        logger.info(f\"線形層の基底クラスを初期化開始 {input_size=}, {output_size=}, {bias=}, {tp_dim=}\")\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        # 1) テンソル並列化の設定\n",
    "\n",
    "        self.tp_dim = tp_dim\n",
    "        self.tp_rank = dist.get_rank()\n",
    "        self.tp_size = dist.get_world_size()\n",
    "\n",
    "        # 2) 重みパラメータの初期化\n",
    "\n",
    "        self.weight = nn.Parameter(torch.empty(output_size, input_size))\n",
    "\n",
    "        # 3) 重みの読み込み関数を設定\n",
    "\n",
    "        self.weight.weight_loader = self.weight_loader\n",
    "\n",
    "        # バイアス項がある場合\n",
    "        if bias:\n",
    "\n",
    "            # バイアスパラメータの初期化\n",
    "            self.bias = nn.Parameter(torch.empty(output_size))\n",
    "\n",
    "            # バイアスの読み込み関数を設定\n",
    "            self.bias.weight_loader = self.weight_loader\n",
    "        \n",
    "        # バイアス項がない場合\n",
    "        else:\n",
    "\n",
    "            # バイアスパラメータをNoneに設定\n",
    "            self.register_parameter(\"bias\", None)\n",
    "\n",
    "        logger.info(f\"線形層の基底クラスを初期化完了\")\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "class ReplicatedLinear(LinearBase):\n",
    "    \"\"\"\n",
    "    並列化しない線形層で、全てのGPUで同じ重みを持つ\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_size: int,\n",
    "        output_size: int,\n",
    "        bias: bool = False,\n",
    "    ):\n",
    "        logger.info(f\"並列化なしの線形層を初期化開始 {input_size=}, {output_size=}, {bias=}\")\n",
    "\n",
    "        super().__init__(input_size, output_size, bias)\n",
    "\n",
    "        logger.info(f\"並列化なしの線形層を初期化完了\")\n",
    "\n",
    "    def weight_loader(self, param: nn.Parameter, loaded_weight: torch.Tensor):\n",
    "        \"\"\"\n",
    "        モデルのチェックポイントから重みをロードする\n",
    "        すべてのGPUで同じ重みを使用する\n",
    "\n",
    "        Args:\n",
    "            param (nn.Parameter): ロード先のパラメータ\n",
    "            loaded_weight (torch.Tensor): チェックポイントからロードした重みテンソル\n",
    "        \"\"\"\n",
    "        logger.info(f\"並列化なしの線形層の重みをロード開始 {param.shape=}, {loaded_weight.shape=}\")\n",
    "\n",
    "        param.data.copy_(loaded_weight)\n",
    "\n",
    "        logger.info(f\"並列化なしの線形層の重みをロード完了\")\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        並列化なしの線形層の順伝播\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): 入力テンソル\n",
    "        Returns:\n",
    "            torch.Tensor: 出力テンソル\n",
    "        \"\"\"\n",
    "        logger.info(f\"並列化なしの線形層の順伝播開始 {x.shape=}\")\n",
    "\n",
    "        result = F.linear(x, self.weight, self.bias)\n",
    "\n",
    "        logger.info(f\"並列化なしの線形層の順伝播完了 {result.shape=}\")\n",
    "        return result\n",
    "\n",
    "\n",
    "class ColumnParallelLinear(LinearBase):\n",
    "    \"\"\"\n",
    "    重みの列方向（出力次元）を分割してテンソル並列化する線形層\n",
    "    出力はGPUごとに異なり、全体の一部しか持たない\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_size: int,\n",
    "        output_size: int,\n",
    "        bias: bool = False,\n",
    "    ):\n",
    "        logger.info(f\"出力次元を並列化する線形層を初期化開始 {input_size=}, {output_size=}, {bias=}\")\n",
    "\n",
    "        tp_size = dist.get_world_size()\n",
    "\n",
    "        # output_sizeをGPU数で分割\n",
    "        super().__init__(input_size, divide(output_size, tp_size), bias, 0)\n",
    "\n",
    "        logger.info(f\"出力次元を並列化する線形層を初期化完了 {self.tp_size=}\")\n",
    "\n",
    "    def weight_loader(self, param: nn.Parameter, loaded_weight: torch.Tensor):\n",
    "        \"\"\"\n",
    "        モデルのチェックポイントから出力次元を並列化する線形層の重みをロードする\n",
    "\n",
    "        Args:\n",
    "            param (nn.Parameter): ロード先のパラメータ\n",
    "            loaded_weight (torch.Tensor): チェックポイントからロードした重みテンソル\n",
    "        \"\"\"\n",
    "        logger.info(f\"出力次元を並列化する線形層の重みを読み込み開始 {param.shape=}, {loaded_weight.shape=}\")\n",
    "\n",
    "        param_data = param.data\n",
    "\n",
    "        # 分割方向（dim=0）のサイズを取得\n",
    "        shard_size = param_data.size(self.tp_dim)\n",
    "\n",
    "        # 自分の担当する分割の開始インデックスを計算\n",
    "        start_idx = self.tp_rank * shard_size\n",
    "\n",
    "        # チェックポイントからロードした重みをスライスしてコピー\n",
    "        loaded_weight = loaded_weight.narrow(\n",
    "            self.tp_dim, start_idx, shard_size)\n",
    "\n",
    "        param_data.copy_(loaded_weight)\n",
    "\n",
    "        logger.info(f\"出力次元を並列化する線形層の重みを読み込み完了\")\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        logger.info(f\"出力次元を並列化する線形層の順伝播開始 {x.shape=}\")\n",
    "\n",
    "        result = F.linear(x, self.weight, self.bias)\n",
    "\n",
    "        logger.info(f\"出力次元を並列化する線形層の順伝播完了 {result.shape=}\")\n",
    "        return result\n",
    "\n",
    "\n",
    "class MergedColumnParallelLinear(ColumnParallelLinear):\n",
    "    \"\"\"\n",
    "    複数の線形層を一つにまとめて出力次元をテンソル並列化する線形層\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_size: int,\n",
    "        output_sizes: list[int], # 各線形層の出力次元のリスト\n",
    "        bias: bool = False,\n",
    "    ):\n",
    "        logger.info(f\"MergedColumnParallelLinearを初期化開始 {input_size=}, {output_sizes=}, {bias=}\")\n",
    "\n",
    "        self.output_sizes = output_sizes\n",
    "        super().__init__(input_size, sum(output_sizes), bias)\n",
    "\n",
    "        logger.info(f\"MergedColumnParallelLinearを初期化完了\")\n",
    "\n",
    "    def weight_loader(self, param: nn.Parameter, loaded_weight: torch.Tensor, loaded_shard_id: int):\n",
    "        \"\"\"\n",
    "        モデルのチェックポイントからMergedColumnParallelLinearの重みをロードする\n",
    "        Args:\n",
    "            param (nn.Parameter): ロード先のパラメータ\n",
    "            loaded_weight (torch.Tensor): チェックポイントからロードした重みテンソル\n",
    "            loaded_shard_id (int): ロードする線形層のID\n",
    "        \"\"\"\n",
    "        logger.info(f\"MergedColumnParallelLinearの重みを読み込み開始 {param.shape=}, {loaded_weight.shape=}, {loaded_shard_id=}\")\n",
    "\n",
    "        param_data = param.data\n",
    "\n",
    "        # 書き込み先のオフセットを計算\n",
    "        shard_offset = sum(self.output_sizes[:loaded_shard_id]) // self.tp_size\n",
    "\n",
    "        # 書き込むサイズを計算\n",
    "        shard_size = self.output_sizes[loaded_shard_id] // self.tp_size\n",
    "\n",
    "        # パラメータ内の書き込み位置を特定\n",
    "        param_data = param_data.narrow(self.tp_dim, shard_offset, shard_size)\n",
    "\n",
    "        # ロードした重みを分割\n",
    "        loaded_weight = loaded_weight.chunk(self.tp_size, self.tp_dim)[self.tp_rank]\n",
    "\n",
    "        # 重みをコピー\n",
    "        param_data.copy_(loaded_weight)\n",
    "\n",
    "        logger.info(f\"MergedColumnParallelLinearの重みを読み込み完了\")\n",
    "\n",
    "\n",
    "class QKVParallelLinear(ColumnParallelLinear):\n",
    "    \"\"\"\n",
    "    QKVを一つにまとめて出力次元をテンソル並列化する線形層\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        hidden_size: int,\n",
    "        head_size: int,\n",
    "        total_num_heads: int,\n",
    "        total_num_kv_heads: int | None = None,\n",
    "        bias: bool = False,\n",
    "    ):\n",
    "        logger.info(f\"QKVParallelLinearを初期化開始 {hidden_size=}, {head_size=}, {total_num_heads=}, {total_num_kv_heads=}, {bias=}\")\n",
    "\n",
    "        tp_size = dist.get_world_size()\n",
    "\n",
    "        total_num_kv_heads = total_num_kv_heads or total_num_heads\n",
    "\n",
    "        self.head_size = head_size\n",
    "\n",
    "        # 各GPUが担当するヘッド数を計算\n",
    "        self.num_heads = divide(total_num_heads, tp_size)\n",
    "\n",
    "        # KVヘッド数を計算\n",
    "        self.num_kv_heads = divide(total_num_kv_heads, tp_size)\n",
    "\n",
    "        # 全体の出力サイズ = (Qヘッド数 + Kヘッド数 + Vヘッド数) * ヘッド次元\n",
    "        output_size = (total_num_heads + 2 * total_num_kv_heads) * self.head_size\n",
    "\n",
    "        super().__init__(hidden_size, output_size, bias)\n",
    "\n",
    "        logger.info(f\"QKVParallelLinearを初期化完了 {self.tp_size=}, {self.num_heads=}, {self.num_kv_heads=}\")\n",
    "\n",
    "    def weight_loader(self, param: nn.Parameter, loaded_weight: torch.Tensor, loaded_shard_id: str):\n",
    "        logger.info(f\"QKVParallelLinearの重みを読み込み開始 {param.shape=}, {loaded_weight.shape=}, {loaded_shard_id=}\")\n",
    "\n",
    "        param_data = param.data\n",
    "\n",
    "        assert loaded_shard_id in [\"q\", \"k\", \"v\"]\n",
    "\n",
    "        # Qの場合\n",
    "        if loaded_shard_id == \"q\":\n",
    "            shard_size = self.num_heads * self.head_size\n",
    "            shard_offset = 0\n",
    "\n",
    "        # Kの場合\n",
    "        elif loaded_shard_id == \"k\":\n",
    "            shard_size = self.num_kv_heads * self.head_size\n",
    "            shard_offset = self.num_heads * self.head_size\n",
    "\n",
    "        # Vの場合\n",
    "        else:\n",
    "            shard_size = self.num_kv_heads * self.head_size\n",
    "            shard_offset = self.num_heads * self.head_size + self.num_kv_heads * self.head_size\n",
    "\n",
    "        # パラメータ内の書き込み位置を特定\n",
    "        param_data = param_data.narrow(self.tp_dim, shard_offset, shard_size)\n",
    "\n",
    "        # 重みを分割して担当分を取得\n",
    "        loaded_weight = loaded_weight.chunk(self.tp_size, self.tp_dim)[self.tp_rank]\n",
    "\n",
    "        # 重みをコピー\n",
    "        param_data.copy_(loaded_weight)\n",
    "\n",
    "        logger.info(f\"QKVParallelLinearの重みを読み込み完了\")\n",
    "\n",
    "\n",
    "class RowParallelLinear(LinearBase):\n",
    "    \"\"\"\n",
    "    重みの行方向（入力次元）を分割してテンソル並列化する線形層\n",
    "    すべてのGPUで同じ出力を持つ\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_size: int,\n",
    "        output_size: int,\n",
    "        bias: bool = False,\n",
    "    ):\n",
    "        logger.info(f\"RowParallelLinearを初期化開始 {input_size=}, {output_size=}, {bias=}\")\n",
    "\n",
    "        tp_size = dist.get_world_size()\n",
    "\n",
    "        # input_sizeをGPU数で分割\n",
    "        # 重み行列(out, in)の1次元目（in側）を分割\n",
    "        super().__init__(divide(input_size, tp_size), output_size, bias, 1)\n",
    "\n",
    "        logger.info(f\"RowParallelLinearを初期化完了 {self.tp_size=}\")\n",
    "\n",
    "    def weight_loader(self, param: nn.Parameter, loaded_weight: torch.Tensor):\n",
    "        logger.info(f\"RowParallelLinearの重みを読み込み開始 {param.shape=}, {loaded_weight.shape=}\")\n",
    "\n",
    "        param_data = param.data\n",
    "\n",
    "        # 分割方向（dim=1）のサイズを取得\n",
    "        shard_size = param_data.size(self.tp_dim)\n",
    "\n",
    "        start_idx = self.tp_rank * shard_size\n",
    "\n",
    "        # dim=1（列）をスライスして取り出す\n",
    "        loaded_weight = loaded_weight.narrow(\n",
    "            self.tp_dim, start_idx, shard_size)\n",
    "\n",
    "        param_data.copy_(loaded_weight)\n",
    "\n",
    "        logger.info(f\"RowParallelLinearの重みを読み込み完了\")\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        logger.info(f\"RowParallelLinearの順伝播開始 入力形状: {x.shape}\")\n",
    "\n",
    "        # ローカルでの行列積\n",
    "        # メインプロセスのみバイアスを使用\n",
    "        y = F.linear(x, self.weight, self.bias if self.tp_rank == 0 else None)\n",
    "\n",
    "        # All-Reduceで集約\n",
    "        if self.tp_size > 1:\n",
    "            dist.all_reduce(y)\n",
    "\n",
    "        logger.info(f\"RowParallelLinearの順伝播完了 出力形状: {y.shape}\")\n",
    "        return y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d9fe18d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import lru_cache\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "def apply_rotary_emb(\n",
    "    x: torch.Tensor,\n",
    "    cos: torch.Tensor,\n",
    "    sin: torch.Tensor,\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    ベクトルを回転させる\n",
    "\n",
    "    Args:\n",
    "        x (torch.Tensor): 回転させるテンソル\n",
    "        cos (torch.Tensor): コサイン成分\n",
    "        sin (torch.Tensor): サイン成分\n",
    "    Returns:\n",
    "        torch.Tensor: 回転後のテンソル\n",
    "    \"\"\"\n",
    "    logger.info(f\"RoPEを適用開始 {x.shape=}, {cos.shape=}, {sin.shape=}\")\n",
    "\n",
    "    # 次元分割\n",
    "    x1, x2 = torch.chunk(x.float(), 2, dim=-1)\n",
    "    logger.debug(f\"チャンク化 {x1.shape=}, {x2.shape=}\")\n",
    "\n",
    "    # 回転行列を適用\n",
    "    y1 = x1 * cos - x2 * sin\n",
    "    y2 = x2 * cos + x1 * sin\n",
    "    logger.debug(f\"回転行列適用 {y1.shape=}, {y2.shape=}\")\n",
    "\n",
    "    # 結合\n",
    "    result = torch.cat((y1, y2), dim=-1).to(x.dtype)\n",
    "\n",
    "    logger.info(f\"RoPEを適用完了 {result.shape=}\")\n",
    "    return result\n",
    "\n",
    "\n",
    "class RotaryEmbedding(nn.Module):\n",
    "    \"\"\"\n",
    "    Rotary Positional Embedding (RoPE)の実装\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        head_size: int,\n",
    "        rotary_dim: int,\n",
    "        max_position_embeddings: int,\n",
    "        base: float,\n",
    "    ) -> None:\n",
    "        logger.info(f\"RoPEを初期化 {head_size=}, {rotary_dim=}, {max_position_embeddings=}, {base=}\")\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.head_size = head_size\n",
    "\n",
    "        assert rotary_dim == head_size\n",
    "\n",
    "        # 逆周波数を計算\n",
    "        # theta_i = 1 / (base ** (2i / d))\n",
    "        inv_freq = 1.0 / (base**(torch.arange(0, rotary_dim, 2, dtype=torch.float) / rotary_dim))\n",
    "\n",
    "        # 位置インデックスを生成\n",
    "        t = torch.arange(max_position_embeddings, dtype=torch.float)\n",
    "\n",
    "        # 位置と逆周波数の外積を計算\n",
    "        freqs = torch.einsum(\"i,j -> ij\", t, inv_freq)\n",
    "        logger.debug(f\"周波数計算 {freqs.shape=}\")\n",
    "\n",
    "        # コサインとサインを計算\n",
    "        cos = freqs.cos()\n",
    "        sin = freqs.sin()\n",
    "\n",
    "        # コサインとサインを結合し、バッチ次元を追加\n",
    "        cache = torch.cat((cos, sin), dim=-1).unsqueeze_(1)\n",
    "        logger.debug(f\"コサイン・サインキャッシュ計算 {cache.shape=}\")\n",
    "\n",
    "        # バッファとして登録（state_dictに含めない）\n",
    "        self.register_buffer(\"cos_sin_cache\", cache, persistent=False)\n",
    "\n",
    "        logger.info(f\"RoPEの初期化完了\")\n",
    "\n",
    "    @torch.compile\n",
    "    def forward(\n",
    "        self,\n",
    "        positions: torch.Tensor,\n",
    "        query: torch.Tensor,\n",
    "        key: torch.Tensor,\n",
    "    ) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        順伝播を実行\n",
    "\n",
    "        Args:\n",
    "            positions (torch.Tensor): 位置インデックスのテンソル\n",
    "            query (torch.Tensor): クエリのテンソル\n",
    "            key (torch.Tensor): キーのテンソル\n",
    "        \"\"\"\n",
    "\n",
    "        logger.info(f\"RoPEの順伝播開始 {positions.shape=}, {query.shape=}, {key.shape=}\")\n",
    "\n",
    "        # 位置に対応するコサイン・サインを取得\n",
    "        cos_sin = self.cos_sin_cache[positions]\n",
    "\n",
    "        # コサインとサインに分割\n",
    "        cos, sin = cos_sin.chunk(2, dim=-1)\n",
    "\n",
    "        # クエリとキーにRoPEを適用\n",
    "        query = apply_rotary_emb(query, cos, sin)\n",
    "        key = apply_rotary_emb(key, cos, sin)\n",
    "\n",
    "        logger.info(f\"RoPEの順伝播完了 {query.shape=}, {key.shape=}\")\n",
    "        return query, key\n",
    "\n",
    "\n",
    "@lru_cache(1) # キャッシュを有効にする\n",
    "def get_rope(\n",
    "    head_size: int,\n",
    "    rotary_dim: int,\n",
    "    max_position: int,\n",
    "    base: float,\n",
    "    rope_scaling: dict | None = None,\n",
    "):\n",
    "    \"\"\"\n",
    "    RoPEインスタンスを取得するユーティリティ関数\n",
    "    \"\"\"\n",
    "\n",
    "    assert rope_scaling is None\n",
    "\n",
    "    # RoPEインスタンスを生成\n",
    "    rotary_emb = RotaryEmbedding(head_size, rotary_dim, max_position, base)\n",
    "\n",
    "    return rotary_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a384e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class Sampler(nn.Module):\n",
    "    \"\"\"\n",
    "    Gumbel-Max Trickを用いたサンプリング\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        logger.info(f\"Samplerを初期化\")\n",
    "        super().__init__()\n",
    "        logger.info(f\"Samplerの初期化完了\")\n",
    "\n",
    "    @torch.compile\n",
    "    def forward(self, logits: torch.Tensor, temperatures: torch.Tensor):\n",
    "        logger.info(f\"サンプリング開始 {logits.shape=}, {temperatures.shape=}\")\n",
    "\n",
    "        # ロジットを温度でスケーリング\n",
    "        logits = logits.float().div_(temperatures.unsqueeze(dim=1))\n",
    "\n",
    "        # 確率分布に変換\n",
    "        probs = torch.softmax(logits, dim=-1)\n",
    "\n",
    "        # Gumbel-Max Trickでサンプリング\n",
    "        # 指数分布 Exp(1) に従うノイズを生成し、逆数を乗じて最大値を取る\n",
    "        # 数学的には argmax(log(probs) + Gumbel_noise) と等価\n",
    "        sample_tokens = probs.div_(\n",
    "            torch.empty_like(probs).exponential_(1).clamp_min_(1e-10)\n",
    "            ).argmax(dim=-1)\n",
    "\n",
    "        logger.info(f\"サンプリング完了 {sample_tokens.shape=}\")\n",
    "        return sample_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78574b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.distributed as dist\n",
    "from transformers import Qwen3Config\n",
    "\n",
    "class Qwen3Attention(nn.Module):\n",
    "    \"\"\"\n",
    "    Qwen3のSelf-Attentionを実装したクラス\n",
    "    QK-NormとRoPEをサポート\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        hidden_size: int,\n",
    "        num_heads: int,\n",
    "        num_kv_heads: int,\n",
    "        max_position: int = 4096 * 32,\n",
    "        head_dim: int | None = None,\n",
    "        rms_norm_eps: float = 1e-06,\n",
    "        qkv_bias: bool = False,\n",
    "        rope_theta: float = 10000,\n",
    "        rope_scaling: tuple | None = None,\n",
    "    ) -> None:\n",
    "\n",
    "        logger.info(f\"Qwen3Attentionを初期化開始 hidden_size: {hidden_size}, num_heads: {num_heads}, num_kv_heads: {num_kv_heads}, max_position: {max_position}, head_dim: {head_dim}, rms_norm_eps: {rms_norm_eps}, qkv_bias: {qkv_bias}, rope_theta: {rope_theta}, rope_scaling: {rope_scaling}\")\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        tp_size = dist.get_world_size()\n",
    "\n",
    "        self.total_num_heads = num_heads\n",
    "        assert self.total_num_heads % tp_size == 0\n",
    "        self.num_heads = self.total_num_heads // tp_size\n",
    "        self.total_num_kv_heads = num_kv_heads\n",
    "        assert self.total_num_kv_heads % tp_size == 0\n",
    "        self.num_kv_heads = self.total_num_kv_heads // tp_size\n",
    "        self.head_dim = head_dim or hidden_size // self.total_num_heads\n",
    "        self.q_size = self.num_heads * self.head_dim\n",
    "        self.kv_size = self.num_kv_heads * self.head_dim\n",
    "        self.scaling = self.head_dim ** -0.5\n",
    "        self.qkv_bias = qkv_bias\n",
    "\n",
    "        # 入力をクエリ、キー、バリューに変換する線形層\n",
    "        # QKVは結合され、GPU間で列並列に分割されている\n",
    "        self.qkv_proj = QKVParallelLinear(\n",
    "            hidden_size,\n",
    "            self.head_dim,\n",
    "            self.total_num_heads,\n",
    "            self.total_num_kv_heads,\n",
    "            bias=qkv_bias,\n",
    "        )\n",
    "\n",
    "        # アテンションの出力を元の隠れ層サイズに戻す線形層\n",
    "        # 行並列で、All-Reduce通信を使ってGPU間の出力を集約する\n",
    "        self.o_proj = RowParallelLinear(\n",
    "            self.total_num_heads * self.head_dim,\n",
    "            hidden_size,\n",
    "            bias=False,\n",
    "        )\n",
    "\n",
    "        # RoPEの初期化\n",
    "        self.rotary_emb = get_rope(\n",
    "            self.head_dim,\n",
    "            rotary_dim=self.head_dim,\n",
    "            max_position=max_position,\n",
    "            base=rope_theta,\n",
    "            rope_scaling=rope_scaling,\n",
    "        )\n",
    "\n",
    "        # アテンション機構の初期化\n",
    "        self.attn = Attention(\n",
    "            self.num_heads,\n",
    "            self.head_dim,\n",
    "            self.scaling,\n",
    "            self.num_kv_heads,\n",
    "        )\n",
    "\n",
    "        # QK-Normの初期化\n",
    "        # クエリとキーの内積の前に正規化を行う\n",
    "        if not self.qkv_bias:\n",
    "            self.q_norm = RMSNorm(self.head_dim, eps=rms_norm_eps)\n",
    "            self.k_norm = RMSNorm(self.head_dim, eps=rms_norm_eps)\n",
    "\n",
    "        logger.info(f\"Qwen3Attentionの初期化完了\")\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        positions: torch.Tensor,\n",
    "        hidden_states: torch.Tensor,\n",
    "    ) -> torch.Tensor:\n",
    "        logger.info(f\"Qwen3Attentionの順伝播 {positions.shape=} {hidden_states.shape=}\")\n",
    "\n",
    "        # 入力からQKVに変換\n",
    "        qkv = self.qkv_proj(hidden_states)\n",
    "\n",
    "        # QKVを分割\n",
    "        q, k, v = qkv.split([self.q_size, self.kv_size, self.kv_size], dim=-1)\n",
    "\n",
    "        # QKVの形状を調整\n",
    "        q = q.view(-1, self.num_heads, self.head_dim)\n",
    "        k = k.view(-1, self.num_kv_heads, self.head_dim)\n",
    "        v = v.view(-1, self.num_kv_heads, self.head_dim)\n",
    "\n",
    "        # バイアス項がない場合、QK-Normを適用 \n",
    "        if not self.qkv_bias:\n",
    "            q = self.q_norm(q)\n",
    "            k = self.k_norm(k)\n",
    "\n",
    "        # RoPEを適用\n",
    "        q, k = self.rotary_emb(positions, q, k)\n",
    "\n",
    "        # アテンションを計算\n",
    "        o = self.attn(q, k, v)\n",
    "\n",
    "        # 出力を元の形状に戻し、線形変換を適用\n",
    "        output = self.o_proj(o.flatten(1, -1))\n",
    "\n",
    "        logger.info(f\"Qwen3Attentionの順伝播完了 {output.shape=}\")\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "class Qwen3MLP(nn.Module):\n",
    "    \"\"\"\n",
    "    SwiGLUを採用し、分散並列処理（Tensor Parallelism）に対応したQwen3のMLP層\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        hidden_size: int,\n",
    "        intermediate_size: int,\n",
    "        hidden_act: str,\n",
    "    ) -> None:\n",
    "        logger.info(f\"Qwen3MLPを初期化開始 {hidden_size=} {intermediate_size=} {hidden_act=}\")\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        # Gate層とUp層を単一の行列に結合した線形層\n",
    "        # 列並列化されているため、GPU間で出力が分割される\n",
    "        self.gate_up_proj = MergedColumnParallelLinear(\n",
    "            hidden_size,\n",
    "            [intermediate_size] * 2, # [Gate, Up]\n",
    "            bias=False,\n",
    "        )\n",
    "\n",
    "        # Down層の線形層\n",
    "        # 行並列化されており、GPU間で出力が集約される\n",
    "        self.down_proj = RowParallelLinear(\n",
    "            intermediate_size,\n",
    "            hidden_size,\n",
    "            bias=False,\n",
    "        )\n",
    "        \n",
    "        # SiLU活性化関数\n",
    "        assert hidden_act == \"silu\"\n",
    "        self.act_fn = SiluAndMul()\n",
    "\n",
    "        logger.info(f\"Qwen3MLPの初期化完了\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        logger.info(f\"Qwen3MLPの順伝播 {x.shape=}\")\n",
    "\n",
    "        # GateとUpの線形変換を適用\n",
    "        gate_up = self.gate_up_proj(x)\n",
    "\n",
    "        # SiLU活性化関数を適用\n",
    "        x = self.act_fn(gate_up)\n",
    "\n",
    "        # Downの線形変換を適用\n",
    "        x = self.down_proj(x)\n",
    "\n",
    "        logger.info(f\"Qwen3MLPの順伝播完了 {x.shape=}\")\n",
    "        return x\n",
    "\n",
    "\n",
    "class Qwen3DecoderLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Transformerのデコーダレイヤーを実装したクラス\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        config: Qwen3Config,\n",
    "    ) -> None:\n",
    "        logger.info(f\"Qwen3DecoderLayerを初期化開始 {config.hidden_size=} {config.num_attention_heads=} {config.num_key_value_heads=} {config.intermediate_size=} {config.rms_norm_eps=} {config.hidden_act=}\")\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        # Self-Attentionの初期化\n",
    "        self.self_attn = Qwen3Attention(\n",
    "            hidden_size=config.hidden_size,\n",
    "            num_heads=config.num_attention_heads,\n",
    "            num_kv_heads=config.num_key_value_heads,\n",
    "            max_position=config.max_position_embeddings,\n",
    "            rms_norm_eps=config.rms_norm_eps,\n",
    "            qkv_bias=getattr(config, 'attention_bias', True),\n",
    "            head_dim=getattr(config, 'head_dim', None),\n",
    "            rope_theta=getattr(config, \"rope_theta\", 1000000),\n",
    "            rope_scaling=getattr(config, \"rope_scaling\", None),\n",
    "        )\n",
    "\n",
    "        # MLPの初期化\n",
    "        self.mlp = Qwen3MLP(\n",
    "            hidden_size=config.hidden_size,\n",
    "            intermediate_size=config.intermediate_size,\n",
    "            hidden_act=config.hidden_act,\n",
    "        )\n",
    "\n",
    "        # アテンションの前のレイヤー正規化\n",
    "        self.input_layernorm = RMSNorm(\n",
    "            config.hidden_size, eps=config.rms_norm_eps)\n",
    "\n",
    "        # アテンションの後のレイヤー正規化\n",
    "        self.post_attention_layernorm = RMSNorm(\n",
    "            config.hidden_size, eps=config.rms_norm_eps)\n",
    "\n",
    "        logger.info(f\"Qwen3DecoderLayerの初期化完了\")\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        positions: torch.Tensor,\n",
    "        hidden_states: torch.Tensor,\n",
    "        residual: torch.Tensor | None,\n",
    "    ) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        logger.info(f\"Qwen3DecoderLayerの順伝播 {positions.shape=} {hidden_states.shape=} {residual is None=}\")\n",
    "\n",
    "        # 1) アテンション前のレイヤー正規化と残差接続\n",
    "\n",
    "        # 前の層の残差接続がない場合\n",
    "        if residual is None:\n",
    "\n",
    "            # レイヤー正規化を適用し、残差を設定\n",
    "            hidden_states, residual = self.input_layernorm(hidden_states), hidden_states\n",
    "\n",
    "        # 前の層の残差接続がある場合\n",
    "        else:\n",
    "\n",
    "            # レイヤー正規化を適用し、残差を更新\n",
    "            hidden_states, residual = self.input_layernorm(\n",
    "                hidden_states, residual)\n",
    "\n",
    "        # 2) セルフアテンションの適用\n",
    "\n",
    "        hidden_states = self.self_attn(positions, hidden_states)\n",
    "\n",
    "        # 3) アテンション後のレイヤー正規化と残差接続\n",
    "\n",
    "        hidden_states, residual = \\\n",
    "            self.post_attention_layernorm(hidden_states, residual)\n",
    "\n",
    "        # 4) MLPの適用\n",
    "\n",
    "        hidden_states = self.mlp(hidden_states)\n",
    "\n",
    "        logger.info(f\"Qwen3DecoderLayerの順伝播完了 {hidden_states.shape=}\")\n",
    "\n",
    "        return hidden_states, residual\n",
    "\n",
    "\n",
    "class Qwen3Model(nn.Module):\n",
    "    \"\"\"\n",
    "    埋め込み層、複数のデコーダレイヤー、最終正規化層から構成されるQwen3モデル\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        config: Qwen3Config,\n",
    "    ) -> None:\n",
    "        logger.info(f\"Qwen3Modelを初期化開始 {config.vocab_size=} {config.hidden_size=} {config.num_hidden_layers=}\")\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        # 埋め込み層の初期化\n",
    "        self.embed_tokens = VocabParallelEmbedding(\n",
    "            config.vocab_size, config.hidden_size)\n",
    "\n",
    "        # Transformerブロックの積み重ね\n",
    "        self.layers = nn.ModuleList(\n",
    "            [Qwen3DecoderLayer(config) for _ \\\n",
    "            in range(config.num_hidden_layers)])\n",
    "\n",
    "        # 最終正規化層の初期化\n",
    "        self.norm = RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n",
    "\n",
    "        logger.info(f\"Qwen3Modelの初期化完了\")\n",
    "\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: torch.Tensor,\n",
    "        positions: torch.Tensor,\n",
    "    ) -> torch.Tensor:\n",
    "        logger.info(f\"Qwen3Modelの順伝播開始 {input_ids.shape=} {positions.shape=}\")\n",
    "\n",
    "        # 入力の埋め込み層を適用\n",
    "        hidden_states = self.embed_tokens(input_ids)\n",
    "\n",
    "        residual = None\n",
    "\n",
    "        # 各デコーダレイヤーを順に適用\n",
    "        for layer in self.layers:\n",
    "            hidden_states, residual = layer(\n",
    "                positions, hidden_states, residual)\n",
    "\n",
    "        # 最終正規化層を適用\n",
    "        hidden_states, _ = self.norm(hidden_states, residual)\n",
    "\n",
    "        logger.info(f\"Qwen3Modelの順伝播完了 {hidden_states.shape=}\")\n",
    "\n",
    "        return hidden_states\n",
    "\n",
    "\n",
    "class Qwen3ForCausalLM(nn.Module):\n",
    "    \"\"\"\n",
    "    Qwen3モデルに言語モデリングヘッドを追加したクラス\n",
    "    \"\"\"\n",
    "\n",
    "    # 事前学習済みの重みを正しく読み込むためのマッピング\n",
    "    packed_modules_mapping = {\n",
    "        \"q_proj\": (\"qkv_proj\", \"q\"),\n",
    "        \"k_proj\": (\"qkv_proj\", \"k\"),\n",
    "        \"v_proj\": (\"qkv_proj\", \"v\"),\n",
    "        \"gate_proj\": (\"gate_up_proj\", 0),\n",
    "        \"up_proj\": (\"gate_up_proj\", 1),\n",
    "    }\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        config: Qwen3Config\n",
    "    ) -> None:\n",
    "        logger.info(f\"Qwen3ForCausalLMを初期化開始 {config.vocab_size=} {config.hidden_size=} {config.tie_word_embeddings=}\")\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        # Qwen3モデルの初期化\n",
    "        self.model = Qwen3Model(config)\n",
    "\n",
    "        # 言語モデリングヘッドの初期化\n",
    "        self.lm_head = ParallelLMHead(config.vocab_size, config.hidden_size)\n",
    "\n",
    "        # 埋め込み層と出力層の重みを共有する場合（Weight Tying）\n",
    "        if config.tie_word_embeddings:\n",
    "            # 出力の重みを入力の埋め込み層の重みで初期化\n",
    "            self.lm_head.weight.data = self.model.embed_tokens.weight.data\n",
    "\n",
    "        logger.info(f\"Qwen3ForCausalLMの初期化完了\")\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: torch.Tensor,\n",
    "        positions: torch.Tensor,\n",
    "    ) -> torch.Tensor:\n",
    "        logger.info(f\"Qwen3ForCausalLMの順伝播開始 {input_ids.shape=} {positions.shape=}\")\n",
    "\n",
    "        output = self.model(input_ids, positions)\n",
    "\n",
    "        logger.info(f\"Qwen3ForCausalLMの順伝播完了 {output.shape=}\")\n",
    "\n",
    "        return output\n",
    "\n",
    "    def compute_logits(\n",
    "        self,\n",
    "        hidden_states: torch.Tensor,\n",
    "    ) -> torch.Tensor:\n",
    "        logger.info(f\"Qwen3ForCausalLMのロジット計算開始 {hidden_states.shape=}\")\n",
    "\n",
    "         # 言語モデリングヘッドを適用してロジットを計算\n",
    "        logits = self.lm_head(hidden_states)\n",
    "\n",
    "        logger.info(f\"Qwen3ForCausalLMのロジット計算完了 {logits.shape=}\")\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08243eef",
   "metadata": {},
   "source": [
    "## Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca70fb80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "import torch\n",
    "from torch import nn\n",
    "from safetensors import safe_open\n",
    "\n",
    "def default_weight_loader(param: nn.Parameter, loaded_weight: torch.Tensor):\n",
    "    logger.info(f\"重みを読み込み {param.shape=} {loaded_weight.shape=}\")\n",
    "    param.data.copy_(loaded_weight)\n",
    "\n",
    "\n",
    "def load_model(model: nn.Module, path: str):\n",
    "    \"\"\"\n",
    "    HuggingFaceのsafetensorsファイルを適切に読み込む\n",
    "    名前の変換と結合されたレイヤーへの振り分けを行う\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): 重みを読み込むモデル\n",
    "        path (str): safetensorsファイルが保存されているディレクトリのパス\n",
    "    \"\"\"\n",
    "\n",
    "    logger.info(f\"モデルの重みを読み込み開始 {path=}\")\n",
    "\n",
    "    # マッピングの定義を取得\n",
    "    # 例: q_proj -> (qkv_proj, q)\n",
    "    packed_modules_mapping = getattr(model, \"packed_modules_mapping\", {})\n",
    "\n",
    "    # safetensorsファイルでループ\n",
    "    for file in glob(os.path.join(path, \"*.safetensors\")):\n",
    "        logger.debug(f\"重みファイルを処理中 {file=}\")\n",
    "\n",
    "        # safetensorsファイルを開く\n",
    "        with safe_open(file, \"pt\", \"cpu\") as f:\n",
    "\n",
    "            # ファイル内の各重みでループ\n",
    "            for weight_name in f.keys():\n",
    "                logger.debug(f\"重みを読み込み中 {weight_name=}\")\n",
    "\n",
    "                # マッピングのキーでループ\n",
    "                for k in packed_modules_mapping:\n",
    "                    # マッピングキーが重み名に含まれている場合\n",
    "                    # 例: q_projが含まれている場合\n",
    "                    if k in weight_name:\n",
    "                        logger.debug(f\"マッピングによる重みを読み込み {weight_name=} {k=}\")\n",
    "\n",
    "                        # マッピングから情報を抽出\n",
    "                        v, shard_id = packed_modules_mapping[k]\n",
    "\n",
    "                        # パラメータ名を変更\n",
    "                        # 例: ...q_proj... -> ...qkv_proj...\n",
    "                        param_name = weight_name.replace(k, v)\n",
    "\n",
    "                        # パラメータを取得\n",
    "                        param = model.get_parameter(param_name)\n",
    "\n",
    "                        # カスタムローダーを呼び出し\n",
    "                        weight_loader = getattr(param, \"weight_loader\")\n",
    "\n",
    "                        # 重みを読み込み\n",
    "                        # シャードIDを渡すことでオフセットを指定できるようにする\n",
    "                        weight_loader(\n",
    "                            param,\n",
    "                            f.get_tensor(weight_name),\n",
    "                            shard_id)\n",
    "\n",
    "                        break\n",
    "\n",
    "                # 通常の重み読み込み\n",
    "                else:\n",
    "                    logger.debug(f\"通常の重み読み込み {weight_name=}\")\n",
    "\n",
    "                    # パラメータを取得\n",
    "                    param = model.get_parameter(weight_name)\n",
    "\n",
    "                    # カスタムローダーがあれば使用しなければデフォルトを使用\n",
    "                    weight_loader = getattr(\n",
    "                        param, \"weight_loader\", default_weight_loader)\n",
    "\n",
    "                    # 重みを読み込み\n",
    "                    weight_loader(param, f.get_tensor(weight_name))\n",
    "\n",
    "    logger.info(\"モデルの重みを読み込み完了\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77163e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Config:\n",
    "    \"\"\"\n",
    "    エンジンの起動設定パラメータの管理クラス\n",
    "    \"\"\"\n",
    "\n",
    "    # モデルのディレクトリパス\n",
    "    model: str\n",
    "\n",
    "    # 1バッチ内で同時に処理できる最大トークン数\n",
    "    max_num_batched_tokens: int = 16384\n",
    "\n",
    "    # 1バッチ内で同時に処理できる最大シーケンス数\n",
    "    max_num_seqs: int = 512\n",
    "\n",
    "    # モデルが扱える最大コンテキスト長\n",
    "    max_model_len: int = 4096\n",
    "\n",
    "    # GPUメモリ使用率の上限\n",
    "    gpu_memory_utilization: float = 0.9\n",
    "\n",
    "    # 使用するGPUの数（Tensor Parallelismのサイズ）\n",
    "    tensor_parallel_size: int = 1\n",
    "\n",
    "    # CUDA Graphによる高速化を有効化\n",
    "    enforce_eager: bool = False\n",
    "\n",
    "    # HuggingFaceのモデル設定\n",
    "    hf_config: AutoConfig | None = None\n",
    "\n",
    "    # 特殊トークンID\n",
    "    eos: int = -1\n",
    "\n",
    "    # PagedAttentionの1ブロックあたりのトークン数\n",
    "    kvcache_block_size: int = 256\n",
    "\n",
    "    # KV Cacheの物理ブロック数\n",
    "    num_kvcache_blocks: int = -1\n",
    "\n",
    "    def __post_init__(self):\n",
    "        logger.info(f\"Config初期化の後処理 {self=}\")\n",
    "        assert os.path.isdir(self.model)\n",
    "        assert self.kvcache_block_size % 256 == 0\n",
    "        assert 1 <= self.tensor_parallel_size <= 8\n",
    "        self.hf_config = AutoConfig.from_pretrained(self.model)\n",
    "\n",
    "        # モデルの最大コンテキスト長をHuggingFaceの設定に合わせる\n",
    "        self.max_model_len = min(self.max_model_len, self.hf_config.max_position_embeddings)\n",
    "\n",
    "        assert self.max_num_batched_tokens >= self.max_model_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77acc736",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class SamplingParams:\n",
    "    \"\"\"\n",
    "    テキスト生成の設定\n",
    "    \"\"\"\n",
    "\n",
    "    # 温度\n",
    "    temperature: float = 1.0\n",
    "\n",
    "    # 生成する最大トークン数\n",
    "    max_tokens: int = 64\n",
    "\n",
    "    # EOSトークンを無視するか\n",
    "    ignore_eos: bool = False\n",
    "\n",
    "    def __post_init__(self):\n",
    "        logger.info(f\"SamplingParams初期化の後処理 {self=}\")\n",
    "        assert self.temperature > 1e-10, \"greedy sampling is not permitted\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e0dd61",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@dataclass\n",
    "class Context:\n",
    "    \"\"\"\n",
    "    推論実行時のメタデータを格納するコンテキストクラス\n",
    "    \"\"\"\n",
    "\n",
    "    # PrefillかDecodeかを示すフラグ\n",
    "    is_prefill: bool = False\n",
    "\n",
    "    # クエリーの累積シーケンス長\n",
    "    cu_seqlens_q: torch.Tensor | None = None\n",
    "\n",
    "    # キーの累積シーケンス長\n",
    "    cu_seqlens_k: torch.Tensor | None = None\n",
    "\n",
    "    # クエリーの最大シーケンス長\n",
    "    max_seqlen_q: int = 0\n",
    "\n",
    "    # キーの最大シーケンス長\n",
    "    max_seqlen_k: int = 0\n",
    "\n",
    "    # トークンごとの物理メモリスロットID\n",
    "    # スロットは、トークン1個のキーとバリューを保存するメモリユニット\n",
    "    slot_mapping: torch.Tensor | None = None\n",
    "\n",
    "    # 各シーケンスの現在のコンテキスト長\n",
    "    context_lens: torch.Tensor | None = None\n",
    "\n",
    "    # 論理ブロックIDと物理ブロックIDのマッピングテーブル\n",
    "    # ブロックは複数のスロットをまとめたメモリユニット（256スロットなど）\n",
    "    block_tables: torch.Tensor | None = None\n",
    "\n",
    "_CONTEXT = Context()\n",
    "logger.info(f\"グローバルコンテキスト初期化 {_CONTEXT=}\")\n",
    "\n",
    "def get_context():\n",
    "    logger.info(f\"現在のコンテキスト取得 {_CONTEXT=}\")\n",
    "    return _CONTEXT\n",
    "\n",
    "def set_context(is_prefill, cu_seqlens_q=None, cu_seqlens_k=None, max_seqlen_q=0, max_seqlen_k=0, slot_mapping=None, context_lens=None, block_tables=None):\n",
    "    logger.info(f\"コンテキストを設定 {is_prefill=} {cu_seqlens_q=} {cu_seqlens_k=} {max_seqlen_q=} {max_seqlen_k=} {slot_mapping=} {context_lens=} {block_tables=}\")\n",
    "    global _CONTEXT\n",
    "    _CONTEXT = Context(is_prefill, cu_seqlens_q, cu_seqlens_k, max_seqlen_q, max_seqlen_k, slot_mapping, context_lens, block_tables)\n",
    "\n",
    "def reset_context():\n",
    "    logger.info(\"コンテキストをリセット\")\n",
    "    global _CONTEXT\n",
    "    _CONTEXT = Context()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e59722a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import copy\n",
    "from enum import Enum, auto\n",
    "from itertools import count\n",
    "\n",
    "class SequenceStatus(Enum):\n",
    "    # リクエスト実行の待機状態 = GPUメモリ未割り当て\n",
    "    WAITING = auto()\n",
    "\n",
    "    # PrefillまたはDecodeの実行状態 = GPU上にKVキャッシュが存在\n",
    "    RUNNING = auto()\n",
    "\n",
    "    # リクエスト完了状態 = メモリ解放待ち・メモリ解放済み\n",
    "    FINISHED = auto()\n",
    "\n",
    "\n",
    "class Sequence:\n",
    "    \"\"\"\n",
    "    トークンのシーケンスや論理ブロックと物理ブロックのマッピングを保持するクラス\n",
    "    \"\"\"\n",
    "\n",
    "    # ブロック内のトークン数\n",
    "    block_size = 256\n",
    "\n",
    "    # シーケンスIDのカウンタ\n",
    "    counter = count()\n",
    "\n",
    "    def __init__(self, token_ids: list[int], sampling_params = SamplingParams()):\n",
    "        logger.info(f\"シーケンスを初期化 {len(token_ids)=}, {sampling_params=}\")\n",
    "\n",
    "        self.seq_id = next(Sequence.counter)\n",
    "        self.status = SequenceStatus.WAITING\n",
    "        self.token_ids = copy(token_ids)\n",
    "        self.last_token = token_ids[-1]\n",
    "        self.num_tokens = len(self.token_ids)\n",
    "        self.num_prompt_tokens = len(token_ids)\n",
    "\n",
    "        # KVキャッシュに保存されているトークン数\n",
    "        self.num_cached_tokens = 0\n",
    "\n",
    "        # このシーケンスが使用している物理ブロックIDのリスト\n",
    "        self.block_table = []\n",
    "\n",
    "        self.temperature = sampling_params.temperature\n",
    "        self.max_tokens = sampling_params.max_tokens\n",
    "        self.ignore_eos = sampling_params.ignore_eos\n",
    "        logger.info(f\"シーケンスを初期化完了 {self.seq_id=}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_tokens\n",
    "\n",
    "    def __getitem__(self, key):\n",
    "        return self.token_ids[key]\n",
    "\n",
    "    @property\n",
    "    def is_finished(self):\n",
    "        return self.status == SequenceStatus.FINISHED\n",
    "\n",
    "    @property\n",
    "    def num_completion_tokens(self):\n",
    "        return self.num_tokens - self.num_prompt_tokens\n",
    "\n",
    "    @property\n",
    "    def prompt_token_ids(self):\n",
    "        return self.token_ids[:self.num_prompt_tokens]\n",
    "\n",
    "    @property\n",
    "    def completion_token_ids(self):\n",
    "        return self.token_ids[self.num_prompt_tokens:]\n",
    "\n",
    "    @property\n",
    "    def num_cached_blocks(self):\n",
    "        return self.num_cached_tokens // self.block_size\n",
    "\n",
    "    @property\n",
    "    def num_blocks(self):\n",
    "        return (self.num_tokens + self.block_size - 1) // self.block_size\n",
    "\n",
    "    @property\n",
    "    def last_block_num_tokens(self):\n",
    "        return self.num_tokens - (self.num_blocks - 1) * self.block_size\n",
    "\n",
    "    def block(self, i):\n",
    "        \"\"\"\n",
    "        i番目の論理ブロックに含まれるトークンIDのリストを返す\n",
    "\n",
    "        Args:\n",
    "            i (int): 論理ブロックのインデックス\n",
    "        Returns:\n",
    "            list[int]: 論理ブロックに含まれるトークンIDのリスト\n",
    "        \"\"\"\n",
    "        logger.info(f\"論理ブロックを取得 {i=}\")\n",
    "\n",
    "        assert 0 <= i < self.num_blocks\n",
    "\n",
    "        # トークンIDのスライスを取得\n",
    "        result = self.token_ids[i*self.block_size: (i+1)*self.block_size]\n",
    "\n",
    "        logger.info(f\"論理ブロックを取得完了 {i=} {result=}\")\n",
    "        return result\n",
    "\n",
    "    def append_token(self, token_id: int):\n",
    "        \"\"\"\n",
    "        新しく生成されたトークンをシーケンスに追加し、内部状態を更新する\n",
    "        Scheduler.postprocessから呼び出される\n",
    "\n",
    "        Args:\n",
    "            token_id (int): 追加するトークンID\n",
    "        \"\"\"\n",
    "\n",
    "        logger.info(f\"トークンを追加 {token_id=}\")\n",
    "\n",
    "        # トークンIDを追加\n",
    "        self.token_ids.append(token_id)\n",
    "\n",
    "        # 最新のトークンを更新（次のステップの入力）\n",
    "        self.last_token = token_id\n",
    "\n",
    "        # トークン総数を更新\n",
    "        self.num_tokens += 1\n",
    "\n",
    "        logger.info(f\"トークンを追加完了 {token_id=} {self.num_tokens=}\")\n",
    "\n",
    "    def __getstate__(self):\n",
    "        \"\"\"\n",
    "        シーケンスの状態をシリアライズするためのメソッド\n",
    "        Pickle化する際に呼び出される\n",
    "\n",
    "        Prefill中は全てのトークンIDを送る\n",
    "        Decode中は最後のトークンIDのみ送る\n",
    "        \"\"\"\n",
    "        return (\n",
    "            self.num_tokens,\n",
    "            self.num_prompt_tokens,\n",
    "            self.num_cached_tokens,\n",
    "            self.block_table,\n",
    "            self.token_ids if self.num_completion_tokens == 0 \\\n",
    "                else self.last_token\n",
    "            )\n",
    "\n",
    "    def __setstate__(self, state):\n",
    "        \"\"\"\n",
    "        シーケンスの状態をデシリアライズするためのメソッド\n",
    "        Pickleから復元する際に呼び出される\n",
    "        \"\"\"\n",
    "        self.num_tokens, self.num_prompt_tokens, self.num_cached_tokens, self.block_table = state[:-1]\n",
    "\n",
    "        # Prefill中\n",
    "        if self.num_completion_tokens == 0:\n",
    "            # 全てのトークンIDを復元\n",
    "            self.token_ids = state[-1]\n",
    "\n",
    "        # Decode中\n",
    "        else:\n",
    "            # last_tokenだけ復元\n",
    "            self.last_token = state[-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5321397",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "import xxhash\n",
    "import numpy as np\n",
    "\n",
    "class Block:\n",
    "    \"\"\"\n",
    "    GPU上の巨大なKVキャッシュ領域の一部の状態を追跡するクラス\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, block_id):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            block_id (int): 物理的なブロックID\n",
    "        \"\"\"\n",
    "        logger.info(f\"ブロックを初期化開始 {block_id=}\")\n",
    "\n",
    "        # 物理的なブロックID\n",
    "        # kv_cacheテンソルの何番目のスロットに対応するかを示す\n",
    "        # 0, 1, 2, ...\n",
    "        self.block_id = block_id\n",
    "\n",
    "        # 参照カウント\n",
    "        # このブロックを参照しているシーケンスの数\n",
    "        # 0: 未使用で再利用可能\n",
    "        # 1: 1つのシーケンスが独占して使用\n",
    "        # 2以上: 複数のシーケンスが共有して使用（Prefix CachingやBeam Search時）\n",
    "        self.ref_count = 0\n",
    "\n",
    "        # このブロックに格納されているトークン列のハッシュ値\n",
    "        # 新しいシーケンスが来たとき、同じトークン列を持つブロックが存在するかを検索するため\n",
    "        self.hash = -1\n",
    "\n",
    "        # このブロックに含まれる実際のトークンIDのリスト\n",
    "        self.token_ids = []\n",
    "\n",
    "        logger.info(f\"ブロックを初期化完了 {self.block_id=}\")\n",
    "\n",
    "    def update(self, hash: int, token_ids: list[int]):\n",
    "        \"\"\"\n",
    "        ハッシュ値とトークンの内容を記録し、あとで検索可能にする\n",
    "        ブロックがデータで満たされた（あるいはPrefix Cacheとして登録される）時に\n",
    "        呼び出される\n",
    "        \"\"\"\n",
    "        logger.info(f\"ブロックを更新 {self.block_id} {hash=} {token_ids=}\")\n",
    "        self.hash = hash\n",
    "        self.token_ids = token_ids\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        ブロックを新しく割り当てる時に呼び出される\n",
    "        \"\"\"\n",
    "        logger.info(f\"ブロックをリセット {self.block_id}\")\n",
    "\n",
    "        # ブロックを使用中である状態に変更\n",
    "        self.ref_count = 1\n",
    "\n",
    "        self.hash = -1\n",
    "        self.token_ids = []\n",
    "\n",
    "\n",
    "class BlockManager:\n",
    "    \"\"\"\n",
    "    PagedAttentionで物理メモリブロックを管理するクラス\n",
    "    Blockインスタンスのリストを保持して、以下のフローを制御:\n",
    "\n",
    "    1. 新しいシーケンスが来る\n",
    "    2. そのトークン列のハッシュを計算する\n",
    "    3. 既存のBlockの中から同じハッシュを持つものを探す（Prefix Caching）\n",
    "    3-1. 見つかった場合、そのBlockのref_countを増やして共有する\n",
    "    3-2. 見つからなかった場合、ref_countが0のBlockを探し、reset()して割り当てる\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_blocks: int, block_size: int):\n",
    "        logger.info(f\"ブロックマネージャーを初期化開始 {num_blocks=} {block_size=}\")\n",
    "\n",
    "        # ブロックに格納するトークン数\n",
    "        # 256\n",
    "        self.block_size = block_size\n",
    "\n",
    "        # 全ての物理ブロックを初期化\n",
    "        # 911\n",
    "        self.blocks: list[Block] = [Block(i) for i in range(num_blocks)]\n",
    "\n",
    "        # {ハッシュ値: ブロックID} の辞書を初期化\n",
    "        # Prefix Cachingの検索用\n",
    "        self.hash_to_block_id: dict[int, int] = dict()\n",
    "\n",
    "        # 空きブロックのキューを初期化\n",
    "        # 911\n",
    "        self.free_block_ids: deque[int] = deque(range(num_blocks))\n",
    "\n",
    "        # 使用中のブロックIDのセットを初期化\n",
    "        self.used_block_ids: set[int] = set()\n",
    "\n",
    "        logger.info(f\"ブロックマネージャーを初期化完了\")\n",
    "\n",
    "    @classmethod\n",
    "    def compute_hash(cls, token_ids: list[int], prefix: int = -1):\n",
    "        \"\"\"\n",
    "        高速なハッシュ関数xxhashを使い、トークン列から一意なIDを生成する\n",
    "        キャッシュを検索する際に使用\n",
    "        \"\"\"\n",
    "        logger.info(f\"一意のハッシュを計算 {token_ids=} {prefix=}\")\n",
    "\n",
    "\n",
    "        h = xxhash.xxh64()\n",
    "\n",
    "        if prefix != -1:\n",
    "            h.update(prefix.to_bytes(8, \"little\"))\n",
    "\n",
    "        h.update(np.array(token_ids).tobytes())\n",
    "\n",
    "        # 例: 7975407488731654516\n",
    "        res = h.intdigest()\n",
    "\n",
    "        logger.info(f\"一意のハッシュを計算完了 {res=}\")\n",
    "        return res\n",
    "\n",
    "    def _allocate_block(self, block_id: int) -> Block:\n",
    "        \"\"\"\n",
    "        空きブロックを1つ割り当てて返す\n",
    "        \"\"\"\n",
    "        logger.info(f\"空きブロックを割り当て開始 {block_id=}\")\n",
    "\n",
    "\n",
    "        block = self.blocks[block_id]\n",
    "\n",
    "        assert block.ref_count == 0\n",
    "\n",
    "        # ref_countを1に設定して使用中にする\n",
    "        block.reset()\n",
    "\n",
    "        self.free_block_ids.remove(block_id)\n",
    "\n",
    "        self.used_block_ids.add(block_id)\n",
    "\n",
    "        res = self.blocks[block_id]\n",
    "\n",
    "        logger.info(f\"空きブロックを割り当て完了 {block_id=}\")\n",
    "        return res\n",
    "\n",
    "    def _deallocate_block(self, block_id: int) -> Block:\n",
    "        \"\"\"\n",
    "        ブロックを1つ解放する\n",
    "        \"\"\"\n",
    "        logger.info(f\"ブロックを解放開始 {block_id=}\")\n",
    "\n",
    "        # 参照カウントが0であることを確認\n",
    "        assert self.blocks[block_id].ref_count == 0\n",
    "\n",
    "        self.used_block_ids.remove(block_id)\n",
    "\n",
    "        self.free_block_ids.append(block_id)\n",
    "\n",
    "        logger.info(f\"ブロックを解放完了 {block_id=}\")\n",
    "\n",
    "    def can_allocate(self, seq: Sequence) -> bool:\n",
    "        result =  len(self.free_block_ids) >= seq.num_blocks\n",
    "        logger.info(f\"ブロックを割り当て可能か確認 {seq.seq_id} {seq.num_blocks=} {result=}\")\n",
    "        return result\n",
    "\n",
    "    def allocate(self, seq: Sequence):\n",
    "        \"\"\"\n",
    "        新しいシーケンスが来たとき、過去のキャッシュを利用可否を確認しブロックを割り当てる\n",
    "        \"\"\"\n",
    "\n",
    "        logger.info(f\"シーケンスにブロックを割り当て開始 {seq.seq_id}\")\n",
    "\n",
    "        assert not seq.block_table\n",
    "        h = -1\n",
    "        cache_miss = False\n",
    "\n",
    "        # シーケンスが必要とするブロック数分ループ\n",
    "        for i in range(seq.num_blocks):\n",
    "            token_ids = seq.block(i)\n",
    "\n",
    "            # シーケンス長がブロックサイズと同じ場合、ハッシュを計算\n",
    "            h = self.compute_hash(token_ids, h) if len(token_ids) == self.block_size else -1\n",
    "\n",
    "            # 既存のキャッシュを検索\n",
    "            block_id = self.hash_to_block_id.get(h, -1)\n",
    "\n",
    "            # 存在しない場合\n",
    "            if block_id == -1 or self.blocks[block_id].token_ids != token_ids:\n",
    "                # キャッシュミス\n",
    "                cache_miss = True\n",
    "\n",
    "            # キャッシュミスの場合\n",
    "            if cache_miss:\n",
    "                # 空きのブロックIDを取得\n",
    "                block_id = self.free_block_ids[0]\n",
    "\n",
    "                # 新しいブロックを割り当てる\n",
    "                block = self._allocate_block(block_id)\n",
    "\n",
    "            # キャッシュヒットの場合\n",
    "            else:\n",
    "                # キャッシュ済みのトークン数を更新\n",
    "                seq.num_cached_tokens += self.block_size\n",
    "\n",
    "                # 既に使用中の場合\n",
    "                if block_id in self.used_block_ids:\n",
    "                    # ブロックを共有（Prefix Caching）\n",
    "                    block = self.blocks[block_id]\n",
    "\n",
    "                    # 参照カウントを増やす\n",
    "                    block.ref_count += 1\n",
    "\n",
    "                # 未使用の場合（理論上ここには来ないはず）\n",
    "                else:\n",
    "                    # ブロックを割り当てる\n",
    "                    block = self._allocate_block(block_id)\n",
    "\n",
    "            # ハッシュ値が有効な場合\n",
    "            if h != -1:\n",
    "                # ブロックの内容を更新\n",
    "                block.update(h, token_ids)\n",
    "\n",
    "                # ハッシュテーブルに登録\n",
    "                self.hash_to_block_id[h] = block_id\n",
    "\n",
    "            # シーケンスのブロックテーブルに追加\n",
    "            seq.block_table.append(block_id)\n",
    "\n",
    "        logger.info(f\"シーケンスにブロックを割り当て完了 {seq.seq_id} {seq.block_table=}\")\n",
    "\n",
    "\n",
    "    def deallocate(self, seq: Sequence):\n",
    "        \"\"\"\n",
    "        割り当てたブロックを解放する\n",
    "\n",
    "        Args:\n",
    "            seq (Sequence): 使い終わったシーケンス\n",
    "        \"\"\"\n",
    "        logger.info(f\"シーケンスのブロックを解放開始 {seq.seq_id}\")\n",
    "\n",
    "        # シーケンスが使用しているブロックIDのリストを逆順で処理\n",
    "        for block_id in reversed(seq.block_table):\n",
    "\n",
    "            # ブロックを取得\n",
    "            block = self.blocks[block_id]\n",
    "\n",
    "            # 参照カウントを減らす\n",
    "            block.ref_count -= 1\n",
    "\n",
    "            # 参照カウントが0の場合\n",
    "            if block.ref_count == 0:\n",
    "\n",
    "                # 物理的に解放\n",
    "                self._deallocate_block(block_id)\n",
    "\n",
    "        # シーケンスの状態をリセット\n",
    "        seq.num_cached_tokens = 0\n",
    "        seq.block_table.clear()\n",
    "\n",
    "        logger.info(f\"シーケンスのブロックを解放完了 {seq.seq_id}\")\n",
    "\n",
    "    def can_append(self, seq: Sequence) -> bool:\n",
    "        \"\"\"\n",
    "        新しいブロックが必要にある場合、空きを確認する\n",
    "        \"\"\"\n",
    "        result = len(self.free_block_ids) >= (len(seq) % self.block_size == 1)\n",
    "        logger.info(f\"シーケンスにブロックを追加可能か確認 {seq.seq_id} {len(seq)=} {result=}\")\n",
    "        return result\n",
    "\n",
    "    def may_append(self, seq: Sequence):\n",
    "        \"\"\"\n",
    "        必要に応じてシーケンスに物理ブロックを追加する\n",
    "        1. 直前のブロックが満杯の場合、新しいブロックを割り当てる\n",
    "        2. 直前のブロックがちょうど満杯の場合、ハッシュを計算して登録する\n",
    "        3. 直前のブロックが満杯でない場合、何もしない\n",
    "        \"\"\"\n",
    "        # シーケンスのブロックテーブルを取得\n",
    "        block_table = seq.block_table\n",
    "\n",
    "        # 直前のブロックを取得\n",
    "        last_block = self.blocks[block_table[-1]]\n",
    "\n",
    "        # 直前のブロックが満杯の場合\n",
    "        if len(seq) % self.block_size == 1:\n",
    "\n",
    "            # 新しいブロックを割り当てる\n",
    "            assert last_block.hash != -1\n",
    "            block_id = self.free_block_ids[0]\n",
    "            self._allocate_block(block_id)\n",
    "            block_table.append(block_id)\n",
    "            logger.info(f\"直前のブロックが満杯のため、新しいブロックを追加 {seq.seq_id=} {block_id=}\")\n",
    "\n",
    "        # 直前のブロックがちょうど満杯の場合\n",
    "        elif len(seq) % self.block_size == 0:\n",
    "\n",
    "            # ブロックを更新する\n",
    "            assert last_block.hash == -1\n",
    "            token_ids = seq.block(seq.num_blocks-1)\n",
    "\n",
    "            # 直前のブロックのハッシュをプレフィックスとして取得\n",
    "            prefix = self.blocks[block_table[-2]].hash \\\n",
    "                if len(block_table) > 1 else -1\n",
    "\n",
    "            # 現在のブロックのハッシュを計算\n",
    "            h = self.compute_hash(token_ids, prefix)\n",
    "\n",
    "            # ブロックにハッシュを設定\n",
    "            last_block.update(h, token_ids)\n",
    "\n",
    "            # ハッシュテーブルに登録\n",
    "            logger.info(f\"直前のブロックがちょうど満杯になったため、ハッシュを計算し、hash_to_block_idに登録 {seq.seq_id=} {h=}\")\n",
    "            self.hash_to_block_id[h] = last_block.block_id\n",
    "\n",
    "        # 直前のブロックが満杯でない場合\n",
    "        else:\n",
    "            assert last_block.hash == -1\n",
    "            logger.info(f\"直前のブロックは満杯ではないため、スキップ {seq.seq_id=}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "003497e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "\n",
    "class Scheduler:\n",
    "    \"\"\"\n",
    "    推論リクエストのスケジュールとKVキャッシュの管理を行うクラス\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config: Config):\n",
    "        logger.info(f\"スケジューラを初期化 {config.max_num_seqs=}, {config.max_num_batched_tokens=} {config.eos=}, {config.num_kvcache_blocks=}, {config.kvcache_block_size=}\")\n",
    "\n",
    "        # 1) 制約条件の読み込み\n",
    "\n",
    "        # 同時に処理できるシーケンスの最大数\n",
    "        self.max_num_seqs = config.max_num_seqs\n",
    "\n",
    "        # 1バッチに含められる最大トークン数\n",
    "        self.max_num_batched_tokens = config.max_num_batched_tokens\n",
    "\n",
    "        # 終了トークンID\n",
    "        self.eos = config.eos\n",
    "\n",
    "        # 2) ブロックマネージャの初期化\n",
    "\n",
    "        # BlockManagerインスタンスを作成\n",
    "        self.block_manager = BlockManager(\n",
    "            config.num_kvcache_blocks, # 確保された物理ブロックの総数\n",
    "            config.kvcache_block_size) # 1ブロックあたりのトークン数\n",
    "\n",
    "        # 3) キューの初期化\n",
    "\n",
    "        # GPUメモリが割り当てられていないリクエスト\n",
    "        self.waiting: deque[Sequence] = deque()\n",
    "\n",
    "        # GPUメモリが割り当てられて実行中のリクエスト\n",
    "        self.running: deque[Sequence] = deque()\n",
    "\n",
    "        logger.info(f\"スケジューラを初期化完了\")\n",
    "\n",
    "    def is_finished(self):\n",
    "        \"\"\"\n",
    "        スケジューラが管理しているすべてのリクエストが完了しているかどうかを返す\n",
    "        \"\"\"\n",
    "        result = not self.waiting and not self.running\n",
    "        logger.info(f\"スケジューラの完了状態を確認 {result=}\")\n",
    "        return result\n",
    "\n",
    "    def add(self, seq: Sequence):\n",
    "        \"\"\"\n",
    "        スケジューラに新しいリクエストを追加する\n",
    "\n",
    "        Args:\n",
    "            seq (Sequence): 追加するシーケンス\n",
    "        \"\"\"\n",
    "        self.waiting.append(seq)\n",
    "        logger.info(f\"スケジューラに新しいリクエストを追加 {seq.seq_id=}\")\n",
    "\n",
    "    def schedule(self) -> tuple[list[Sequence], bool]:\n",
    "        \"\"\"\n",
    "        次にどのリクエストをGPUで実行するかを決める\n",
    "        Prefillのリクエストを優先\n",
    "\n",
    "        Returns:\n",
    "            tuple[list[Sequence], bool]:\n",
    "                スケジュールされたシーケンスのリストと、prefillかdecodeかのフラグ\n",
    "        \"\"\"\n",
    "        logger.info(f\"スケジューリングを開始\")\n",
    "\n",
    "        # 1) Prefillリクエストのスケジューリング\n",
    "\n",
    "        scheduled_seqs = []\n",
    "        num_seqs = 0\n",
    "        num_batched_tokens = 0\n",
    "\n",
    "        # waitingキューをループ\n",
    "        while self.waiting and num_seqs < self.max_num_seqs:\n",
    "\n",
    "            # 先頭のシーケンスを取得\n",
    "            seq = self.waiting[0]\n",
    "\n",
    "            if num_batched_tokens + len(seq) > self.max_num_batched_tokens \\\n",
    "                or not self.block_manager.can_allocate(seq):\n",
    "                break\n",
    "\n",
    "            num_seqs += 1\n",
    "\n",
    "            self.block_manager.allocate(seq)\n",
    "\n",
    "            num_batched_tokens += len(seq) - seq.num_cached_tokens\n",
    "            seq.status = SequenceStatus.RUNNING\n",
    "            self.waiting.popleft()\n",
    "            self.running.append(seq)\n",
    "            scheduled_seqs.append(seq)\n",
    "\n",
    "        # Prefillリクエストがあった場合\n",
    "        if scheduled_seqs:\n",
    "\n",
    "            # ここで返す\n",
    "            logger.info(f\"スケジューリング完了（Prefill） {len(scheduled_seqs)=}\")\n",
    "            return scheduled_seqs, True\n",
    "\n",
    "        # 2) Decodeリクエストのスケジューリング\n",
    "\n",
    "        # runningキューをループ\n",
    "        while self.running and num_seqs < self.max_num_seqs:\n",
    "\n",
    "            # 先頭のシーケンスを取得\n",
    "            seq = self.running.popleft()\n",
    "\n",
    "            # 次のトークンをメモリ不足で追加できない場合\n",
    "            while not self.block_manager.can_append(seq):\n",
    "\n",
    "                # 現在実行中のシーケンスがある場合\n",
    "                if self.running:\n",
    "                    # 末尾の低優先度のシーケンスをwaitingキューに戻す\n",
    "                    self.preempt(self.running.pop())\n",
    "                else:\n",
    "                    # 自分自身をwaitingキューに戻す\n",
    "                    self.preempt(seq)\n",
    "                    break\n",
    "\n",
    "            # 次のトークンを追加可能な場合、スケジュールに加える\n",
    "            else:\n",
    "                num_seqs += 1\n",
    "                self.block_manager.may_append(seq)\n",
    "                scheduled_seqs.append(seq)\n",
    "\n",
    "        assert scheduled_seqs\n",
    "        \n",
    "        # スケジュールされたシーケンスをrunningキューの先頭に戻す\n",
    "        self.running.extendleft(reversed(scheduled_seqs))\n",
    "\n",
    "        logger.info(f\"スケジューリング完了（Decode） {len(scheduled_seqs)=}\")\n",
    "        return scheduled_seqs, False\n",
    "\n",
    "    def preempt(self, seq: Sequence):\n",
    "        \"\"\"\n",
    "        実行中のリクエストを中断しwaitingキューに戻し、メモリを解放する\n",
    "\n",
    "        Args:\n",
    "            seq (Sequence): 中断するシーケンス\n",
    "        \"\"\"\n",
    "        logger.info(f\"実行中リクエストを中断開始 {seq.seq_id=}\")\n",
    "\n",
    "        # シーケンスの状態を実行中から待機中に変更\n",
    "        seq.status = SequenceStatus.WAITING\n",
    "\n",
    "        # ブロックマネージャからメモリを解放\n",
    "        self.block_manager.deallocate(seq)\n",
    "\n",
    "        # waitingキューの先頭にシーケンスを戻す\n",
    "        self.waiting.appendleft(seq)\n",
    "\n",
    "        logger.info(f\"実行中リクエストを中断完了 {seq.seq_id=}\")\n",
    "\n",
    "    def postprocess(self, seqs: list[Sequence], token_ids: list[int]) -> list[bool]:\n",
    "        \"\"\"\n",
    "        生成したトークンを各シーケンスに追加し、終了判定を行う\n",
    "\n",
    "        Args:\n",
    "            seqs (list[Sequence]): トークンを追加するシーケンスのリスト\n",
    "            token_ids (list[int]): 各シーケンスに追加するトークンIDのリスト\n",
    "        Returns:\n",
    "            list[bool]: 各シーケンスが終了したかどうかのリスト\n",
    "        \"\"\"\n",
    "        logger.info(f\"生成トークンの後処理を開始 {len(seqs)=} {len(token_ids)=}\")\n",
    "\n",
    "        # シーケンスと生成したトークンIDをループ\n",
    "        for seq, token_id in zip(seqs, token_ids):\n",
    "\n",
    "            # シーケンスにトークンを追加\n",
    "            seq.append_token(token_id)\n",
    "\n",
    "            # 終了判定\n",
    "            if (not seq.ignore_eos and token_id == self.eos) or \\\n",
    "                seq.num_completion_tokens == seq.max_tokens:\n",
    "\n",
    "                # シーケンスの状態を終了に変更\n",
    "                seq.status = SequenceStatus.FINISHED\n",
    "\n",
    "                # ブロックマネージャからメモリを解放\n",
    "                self.block_manager.deallocate(seq)\n",
    "\n",
    "                # runningキューからシーケンスを削除\n",
    "                self.running.remove(seq)\n",
    "\n",
    "        logger.info(f\"生成トークンの後処理完了 {len(seqs)=}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dcc6aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "from multiprocessing.synchronize import Event\n",
    "from multiprocessing.shared_memory import SharedMemory\n",
    "\n",
    "class ModelRunner:\n",
    "    \"\"\"\n",
    "    デバイス上でモデルの推論を実行するための管理クラス\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config: Config, rank: int, event: Event | list[Event]):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            config (Config): モデル設定\n",
    "            rank (int): ランクID\n",
    "            event (Event | list[Event]):\n",
    "                ランク0以外のプロセスでは同期用イベント、\n",
    "                ランク0ではランク1以降のイベントリスト\n",
    "        \"\"\"\n",
    "\n",
    "        logger.info(f\"Initializing ModelRunner on rank {rank}\")\n",
    "\n",
    "        # 1) 設定の保存\n",
    "\n",
    "        self.config = config\n",
    "\n",
    "        hf_config = config.hf_config\n",
    "\n",
    "        self.block_size = config.kvcache_block_size\n",
    "        self.enforce_eager = config.enforce_eager\n",
    "        self.world_size = config.tensor_parallel_size\n",
    "\n",
    "        # 自身のランクID\n",
    "        self.rank = rank\n",
    "\n",
    "        self.event = event\n",
    "\n",
    "        # 2) 分散プロセスの初期化\n",
    "\n",
    "        dist.init_process_group(\n",
    "            \"nccl\", # 分散処理バックエンドをNCCLに設定\n",
    "            \"tcp://localhost:2333\", # 2333ポートを使用して他のプロセスと通信\n",
    "            world_size=self.world_size,\n",
    "            rank=rank\n",
    "        )\n",
    "\n",
    "        # 3) デバイス設定\n",
    "\n",
    "        # このデバイスが使用するデバイスを設定\n",
    "        torch.cuda.set_device(rank)\n",
    "\n",
    "        default_dtype = torch.get_default_dtype()\n",
    "\n",
    "        torch.set_default_dtype(hf_config.torch_dtype)\n",
    "\n",
    "        torch.set_default_device(\"cuda\")\n",
    "\n",
    "        # 4) モデルを読み込み\n",
    "\n",
    "        # モデルをランダムに初期化\n",
    "        self.model = Qwen3ForCausalLM(hf_config)\n",
    "\n",
    "        # モデルにパラメータを読み込み\n",
    "        load_model(self.model, config.model)\n",
    "\n",
    "        # 5) サンプラーの初期化\n",
    "\n",
    "        self.sampler = Sampler()\n",
    "\n",
    "        # 6) 最適化\n",
    "\n",
    "        # 遅延初期化のコンポーネントやメモリ割り当てを実行\n",
    "        self.warmup_model()\n",
    "\n",
    "        # 可能な限り巨大なKVキャッシュ領域を確保\n",
    "        self.allocate_kv_cache()\n",
    "\n",
    "        if not self.enforce_eager:\n",
    "\n",
    "            # 計算グラフをキャプチャ\n",
    "            self.capture_cudagraph()\n",
    "\n",
    "        torch.set_default_device(\"cpu\")\n",
    "\n",
    "        torch.set_default_dtype(default_dtype)\n",
    "\n",
    "        # 7) 分散処理の分岐\n",
    "\n",
    "        if self.world_size > 1:\n",
    "\n",
    "            # メインプロセスの場合\n",
    "            if rank == 0:\n",
    "\n",
    "                # 共有メモリを作成\n",
    "                # メインプロセスからワーカーへ指令を送るための領域\n",
    "                self.shm = SharedMemory(\n",
    "                    name=\"nanovllm\", create=True, size=2**20\n",
    "                )\n",
    "                dist.barrier()\n",
    "\n",
    "            # ワーカープロセスの場合\n",
    "            else:\n",
    "                dist.barrier()\n",
    "\n",
    "                # 共有メモリを開く\n",
    "                self.shm = SharedMemory(name=\"nanovllm\")\n",
    "\n",
    "                # 無限ループに入り、メインプロセスからの指令を待機\n",
    "                self.loop()\n",
    "\n",
    "        logger.info(f\"Initialized ModelRunner on rank {rank}\")\n",
    "\n",
    "    def exit(self):\n",
    "        \"\"\"\n",
    "        ワーカー停止時にリソースを開放する\n",
    "        \"\"\"\n",
    "        logger.info(f\"Exiting ModelRunner on rank {self.rank}\")\n",
    "\n",
    "        # マルチGPU環境の場合\n",
    "        if self.world_size > 1:\n",
    "\n",
    "            # 共有メモリへのアクセスを閉じる\n",
    "            self.shm.close()\n",
    "\n",
    "            # 全てのCPUプロセスを待機\n",
    "            dist.barrier()\n",
    "\n",
    "            # メインプロセスの場合\n",
    "            if self.rank == 0:\n",
    "\n",
    "                # 共有メモリを削除\n",
    "                self.shm.unlink()\n",
    "\n",
    "        # CUDA Graphを使用している場合\n",
    "        if not self.enforce_eager:\n",
    "            # CUDA Graphのインスタンスとメモリプールを削除\n",
    "            del self.graphs, self.graph_pool\n",
    "\n",
    "        # プロセス内のCPUとGPUの同期\n",
    "        torch.cuda.synchronize()\n",
    "\n",
    "        # 分散処理用の通信グループを破棄\n",
    "        dist.destroy_process_group()\n",
    "\n",
    "        logger.info(f\"Exited ModelRunner on rank {self.rank}\")\n",
    "\n",
    "    def loop(self):\n",
    "        \"\"\"\n",
    "        ワーカープロセスの無限ループ\n",
    "        \"\"\"\n",
    "        logger.info(f\"Starting loop on rank {self.rank}\")\n",
    "\n",
    "        while True:\n",
    "            # 指令を待機\n",
    "            method_name, args = self.read_shm()\n",
    "\n",
    "            # 指令を実行\n",
    "            self.call(method_name, *args)\n",
    "\n",
    "            # 終了指令の場合\n",
    "            if method_name == \"exit\":\n",
    "                # ループを抜ける\n",
    "                break\n",
    "\n",
    "    def read_shm(self):\n",
    "        \"\"\"\n",
    "        ワーカープロセスがメインプロセスからの指令を待機して取得する\n",
    "\n",
    "        Returns:\n",
    "            method_name (str): 実行するメソッド名\n",
    "            args (list): メソッドの引数リスト\n",
    "        \"\"\"\n",
    "\n",
    "        # メインプロセス以外であることを確認\n",
    "        assert self.world_size > 1 and self.rank > 0\n",
    "\n",
    "        # 指令が来るまで待機\n",
    "        self.event.wait()\n",
    "\n",
    "        # 共有メモリから先頭4バイトをリトルエンディアンで読み込む\n",
    "        # nは読み取るべきデータの長さ\n",
    "        n = int.from_bytes(self.shm.buf[0:4], \"little\")\n",
    "\n",
    "        # 共有メモリからnバイト分のデータを読み込み、pickleでデシリアライズ\n",
    "        # [メソッド名, 引数1, 引数2, ...] の形式で取得\n",
    "        method_name, *args = pickle.loads(self.shm.buf[4:n+4])\n",
    "\n",
    "        # 次の指令待機のためにイベントをクリア\n",
    "        self.event.clear()\n",
    "\n",
    "        return method_name, args\n",
    "\n",
    "    def write_shm(self, method_name, *args):\n",
    "        \"\"\"\n",
    "        メインプロセスが待機中のワーカープロセスへ指令を送る\n",
    "\n",
    "        Args:\n",
    "            method_name (str): 実行するメソッド名\n",
    "            args (list): メソッドの引数リスト\n",
    "        \"\"\"\n",
    "\n",
    "        # メインプロセスであることを確認\n",
    "        assert self.world_size > 1 and self.rank == 0\n",
    "\n",
    "        # 指令データをpickleでバイト列にシリアライズ\n",
    "        data = pickle.dumps([method_name, *args])\n",
    "\n",
    "        # バイト数を取得\n",
    "        n = len(data)\n",
    "\n",
    "        # 共有メモリへデータをリトルエンディアンで書き込む\n",
    "        self.shm.buf[0:4] = n.to_bytes(4, \"little\")\n",
    "\n",
    "        # データ本体を書き込む\n",
    "        self.shm.buf[4:n+4] = data\n",
    "\n",
    "        # 全てのワーカープロセスに対応するイベントオブジェクトのリストでループ\n",
    "        for event in self.event:\n",
    "\n",
    "            # イベントフラグを有効化\n",
    "            event.set()\n",
    "\n",
    "    def call(self, method_name, *args):\n",
    "        \"\"\"\n",
    "        メインプロセスとワーカープロセスの両方に同じメソッドを一斉に実行する\n",
    "\n",
    "        Args:\n",
    "            method_name (str): 実行するメソッド名\n",
    "            args (list): メソッドの引数リスト \n",
    "        \"\"\"\n",
    "\n",
    "        # マルチGPU環境でかつメインプロセスの場合\n",
    "        if self.world_size > 1 and self.rank == 0:\n",
    "\n",
    "            # 全てのワーカープロセスへ指令を送る\n",
    "            self.write_shm(method_name, *args)\n",
    "\n",
    "        # 自身のインスタンスから指定されたメソッドを取得\n",
    "        method = getattr(self, method_name, None)\n",
    "\n",
    "        # メソッドを実行して結果を返す\n",
    "        return method(*args)\n",
    "\n",
    "    def warmup_model(self):\n",
    "        \"\"\"\n",
    "        推論を開始する前にダミーデータを使ってモデルを実行し、\n",
    "        最大負荷時のメモリ消費量を計測・確定する\n",
    "        これによりKVキャッシュの物理メモリブロックの割り当てを最適化し\n",
    "        Out of Memoryを防止する\n",
    "        \"\"\"\n",
    "        logger.info(f\"Warming up model on rank {self.rank}\")\n",
    "\n",
    "        # 1) メモリ統計のリセット\n",
    "\n",
    "        # CUDAのメモリキャッシュを解放\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        # メモリ使用量の計測カウンターをゼロにリセット\n",
    "        torch.cuda.reset_peak_memory_stats()\n",
    "\n",
    "        # 2) ダミーデータの作成\n",
    "\n",
    "        # 最大バッチサイズを取得\n",
    "        max_num_batched_tokens = self.config.max_num_batched_tokens\n",
    "\n",
    "        # 最大シーケンス長を取得\n",
    "        max_model_len = self.config.max_model_len\n",
    "\n",
    "        # バッチ内のシーケンス数を計算\n",
    "        # OOMを防ぐため両方の制約を満たす最大値を使用する\n",
    "        num_seqs = min(\n",
    "            max_num_batched_tokens // max_model_len,\n",
    "            self.config.max_num_seqs\n",
    "        )\n",
    "\n",
    "        # 最大負荷のダミーシーケンスを作成\n",
    "        seqs = [Sequence([0] * max_model_len) for _ in range(num_seqs)]\n",
    "\n",
    "        # 3) ダミー実行\n",
    "\n",
    "        # is_prefill=Trueでモデルを実行\n",
    "        # この実行によりKVキャッシュの最大メモリ使用量が確定する\n",
    "        self.run(seqs, True)\n",
    "\n",
    "        # 4) 後処理\n",
    "\n",
    "        # ウォームアップで使用したメモリを解放\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        logger.info(f\"Warmed up model on rank {self.rank}\")\n",
    "\n",
    "    def allocate_kv_cache(self):\n",
    "        \"\"\"\n",
    "        GPUの空き容量を計算し、最大のKVキャッシュ領域を確保する\n",
    "        OOMを防ぎ、効率的なメモリ管理（PagedAttention）を可能にする\n",
    "        \"\"\"\n",
    "        logger.info(f\"Allocating KV cache on rank {self.rank}\")\n",
    "\n",
    "        # 1) 現在のGPUメモリ状況を取得\n",
    "\n",
    "        config = self.config\n",
    "        hf_config = config.hf_config\n",
    "\n",
    "        # OSから見たGPUの空き容量と合計容量を取得\n",
    "        free, total = torch.cuda.mem_get_info()\n",
    "\n",
    "        # OSから見た現在のメモリ使用量を計算\n",
    "        used = total - free\n",
    "\n",
    "        # warmup_modelで計測した推論中の最大メモリ使用量を取得\n",
    "        peak = torch.cuda.memory_stats()[\"allocated_bytes.all.peak\"]\n",
    "\n",
    "        # PyTorchが明示的に確保したメモリ使用量を取得\n",
    "        current = torch.cuda.memory_stats()[\"allocated_bytes.all.current\"]\n",
    "\n",
    "        # 2) 1ブロックあたりのバイト数を計算\n",
    "\n",
    "        # KVキャッシュのヘッド数を計算\n",
    "        num_kv_heads = hf_config.num_key_value_heads // self.world_size\n",
    "\n",
    "        # ヘッドの次元数を取得\n",
    "        head_dim = getattr(\n",
    "            hf_config,\n",
    "            \"head_dim\",\n",
    "            hf_config.hidden_size // hf_config.num_attention_heads\n",
    "        )\n",
    "\n",
    "        # 1つの物理ブロック（全層分）のメモリサイズを計算\n",
    "        # キーとバリューの2つ * レイヤー数 * ブロックサイズ * KVヘッド数 * ヘッド次元数 * データ型のバイト数\n",
    "        block_bytes = 2 * hf_config.num_hidden_layers * self.block_size * num_kv_heads * head_dim * hf_config.torch_dtype.itemsize\n",
    "\n",
    "        # 3) 確保可能なKVキャッシュブロック数を計算\n",
    "\n",
    "        # KVキャッシュに使用可能なメモリ容量を計算\n",
    "        # GPU全体の90% - PyTorch以外の使用量 - 推論中のピーク使用量\n",
    "        config.num_kvcache_blocks = int(total * config.gpu_memory_utilization - used - peak + current) // block_bytes\n",
    "\n",
    "        assert config.num_kvcache_blocks > 0\n",
    "\n",
    "        # 4) 物理メモリの一括確保\n",
    "\n",
    "        # 巨大なKVキャッシュテンソルを作成 \n",
    "        self.kv_cache = torch.empty(\n",
    "            2,\n",
    "            hf_config.num_hidden_layers,\n",
    "            config.num_kvcache_blocks,\n",
    "            self.block_size,\n",
    "            num_kv_heads,\n",
    "            head_dim\n",
    "        )\n",
    "\n",
    "        # 5) 各層に参照渡し\n",
    "\n",
    "        layer_id = 0\n",
    "        for module in self.model.modules():\n",
    "            if hasattr(module, \"k_cache\") and hasattr(module, \"v_cache\"):\n",
    "                module.k_cache = self.kv_cache[0, layer_id]\n",
    "                module.v_cache = self.kv_cache[1, layer_id]\n",
    "                layer_id += 1\n",
    "\n",
    "        logger.info(f\"Allocated KV cache with {config.num_kvcache_blocks} blocks on rank {self.rank}\")\n",
    "\n",
    "    def prepare_block_tables(self, seqs: list[Sequence]):\n",
    "        \"\"\"\n",
    "        各シーケンスのブロックテーブルを2次元のテンソルに変換しGPUに転送する\n",
    "\n",
    "        Args:\n",
    "            seqs (list[Sequence]): シーケンスのリスト\n",
    "        Returns:\n",
    "            block_tables (torch.Tensor): ブロックテーブルの2次元テンソル\n",
    "        \"\"\"\n",
    "        logger.info(f\"Preparing block tables for {len(seqs)} sequences on rank {self.rank}\")\n",
    "\n",
    "        # 1) バッチ内で最長のブロックテーブルの長さを取得\n",
    "\n",
    "        max_len = max(len(seq.block_table) for seq in seqs)\n",
    "\n",
    "        # 2) 短いシーケンス末尾を-1でパディングして長さを揃える\n",
    "\n",
    "        block_tables = [seq.block_table + [-1] * (max_len - len(seq.block_table)) for seq in seqs]\n",
    "\n",
    "        # 3) 2次元テンソルに変換してGPUに転送\n",
    "\n",
    "        block_tables = torch.tensor(\n",
    "            block_tables,\n",
    "            dtype=torch.int32,\n",
    "            pin_memory=True, # CPU側のメモリを固定しGPU転送を高速化\n",
    "        ).cuda(non_blocking=True) # データ転送を非同期化\n",
    "\n",
    "        logger.info(f\"Prepared block tables with shape {block_tables.shape} on rank {self.rank}\")\n",
    "\n",
    "        return block_tables\n",
    "\n",
    "    def prepare_prefill(self, seqs: list[Sequence]):\n",
    "        \"\"\"\n",
    "        Prefill（プロンプト処理）で可変長の入力シーケンスのリストを\n",
    "        平坦化し、Attention計算に必要なメタデータを準備する\n",
    "        FlashAttention（VarLen版）の入力は1次元テンソルであるため\n",
    "\n",
    "        Args:\n",
    "            seqs (list[Sequence]): シーケンスのリスト\n",
    "        Returns:\n",
    "            input_ids (torch.Tensor): 平坦化された入力トークンIDのテンソル\n",
    "            positions (torch.Tensor): 平坦化された位置IDのテンソル\n",
    "        \"\"\"\n",
    "\n",
    "        # 1) 各シーケンスのメタデータを準備\n",
    "\n",
    "        input_ids = []\n",
    "        positions = []\n",
    "        cu_seqlens_q = [0]\n",
    "        cu_seqlens_k = [0]\n",
    "        max_seqlen_q = 0\n",
    "        max_seqlen_k = 0\n",
    "        slot_mapping = []\n",
    "        block_tables = None\n",
    "\n",
    "        # 2) 入力データの処理\n",
    "\n",
    "        for seq in seqs:\n",
    "\n",
    "            # 2-1) 入力データの平坦化\n",
    "\n",
    "            seqlen = len(seq)\n",
    "\n",
    "            # キャッシュ済みトークンをスキップしたシーケンスを入力に追加\n",
    "            input_ids.extend(seq[seq.num_cached_tokens:])\n",
    "\n",
    "            # ポジショナルエンコーディング用の位置インデックスも追加\n",
    "            positions.extend(list(range(seq.num_cached_tokens, seqlen)))\n",
    "\n",
    "            # 2-2) FlashAttention用の累積長を計算\n",
    "\n",
    "            # クエリのシーケンス長を計算\n",
    "            seqlen_q = seqlen - seq.num_cached_tokens\n",
    "\n",
    "            # キーのシーケンス長を計算\n",
    "            seqlen_k = seqlen\n",
    "\n",
    "            # クエリの累積シーケンス長を更新\n",
    "            # 例: [0, 5, 12] -> シーケンス1の長さ5、シーケンス2の長さ7\n",
    "            cu_seqlens_q.append(cu_seqlens_q[-1] + seqlen_q)\n",
    "\n",
    "            # キーの累積シーケンス長を更新\n",
    "            cu_seqlens_k.append(cu_seqlens_k[-1] + seqlen_k)\n",
    "\n",
    "            # クエリの最大シーケンス長を更新\n",
    "            max_seqlen_q = max(seqlen_q, max_seqlen_q)\n",
    "\n",
    "            # キーの最大シーケンス長を更新\n",
    "            max_seqlen_k = max(seqlen_k, max_seqlen_k)\n",
    "\n",
    "            if not seq.block_table:    # warmup\n",
    "                continue\n",
    "\n",
    "            # 2-3) PagedAttention用のスロットマップを作成\n",
    "\n",
    "            # キャッシュ済みブロックをスキップしてループ\n",
    "            for i in range(seq.num_cached_blocks, seq.num_blocks):\n",
    "\n",
    "                # i番目のブロックが格納されている物理メモリの開始インデックスを計算\n",
    "                start = seq.block_table[i] * self.block_size\n",
    "\n",
    "                # 最後のブロックではない場合\n",
    "                if i != seq.num_blocks - 1:\n",
    "                    # ブロックサイズ分のインデックスを追加\n",
    "                    end = start + self.block_size\n",
    "\n",
    "                # 最後のブロックの場合\n",
    "                else:\n",
    "                    # 最後のブロックの実際のトークン数分のインデックスを追加\n",
    "                    end = start + seq.last_block_num_tokens \n",
    "\n",
    "                # startからendまでのインデックスをスロットマップに追加\n",
    "                slot_mapping.extend(list(range(start, end)))\n",
    "\n",
    "        # 3) Prefix Cacheの判定\n",
    "\n",
    "        # キーの累積シーケンス長がクエリより大きい場合\n",
    "        if cu_seqlens_k[-1] > cu_seqlens_q[-1]:\n",
    "\n",
    "            # Prefix Cacheを使用する\n",
    "            # Prefix Cacheは、計算しないが参照する過去のトークン\n",
    "            block_tables = self.prepare_block_tables(seqs)\n",
    "\n",
    "        # 4) ブロックテーブルを準備\n",
    "\n",
    "        input_ids = torch.tensor(\n",
    "            input_ids,\n",
    "            dtype=torch.int64,\n",
    "            pin_memory=True\n",
    "        ).cuda(non_blocking=True)\n",
    "\n",
    "        positions = torch.tensor(\n",
    "            positions,\n",
    "            dtype=torch.int64,\n",
    "            pin_memory=True\n",
    "        ).cuda(non_blocking=True)\n",
    "\n",
    "        cu_seqlens_q = torch.tensor(\n",
    "            cu_seqlens_q,\n",
    "            dtype=torch.int32,\n",
    "            pin_memory=True\n",
    "        ).cuda(non_blocking=True)\n",
    "\n",
    "        cu_seqlens_k = torch.tensor(\n",
    "            cu_seqlens_k,\n",
    "            dtype=torch.int32,\n",
    "            pin_memory=True\n",
    "        ).cuda(non_blocking=True)\n",
    "\n",
    "        slot_mapping = torch.tensor(\n",
    "            slot_mapping,\n",
    "            dtype=torch.int32,\n",
    "            pin_memory=True\n",
    "        ).cuda(non_blocking=True)\n",
    "\n",
    "        # 5) コンテキストへの登録\n",
    "\n",
    "        # メタデータをグローバルなコンテキストに登録する\n",
    "        set_context(\n",
    "            True,\n",
    "            cu_seqlens_q,\n",
    "            cu_seqlens_k,\n",
    "            max_seqlen_q,\n",
    "            max_seqlen_k,\n",
    "            slot_mapping,\n",
    "            None,\n",
    "            block_tables)\n",
    "\n",
    "        return input_ids, positions\n",
    "\n",
    "    def prepare_decode(self, seqs: list[Sequence]):\n",
    "        \"\"\"\n",
    "        トークン生成フェーズ（Decode）において次の1トークンを生成するための\n",
    "        入力データとPagedAttentionの計算に必要なメタデータを準備する\n",
    "\n",
    "        Args:\n",
    "            seqs (list[Sequence]): シーケンスのリスト\n",
    "        Returns:\n",
    "            input_ids (torch.Tensor): 入力トークンIDのテンソル\n",
    "            positions (torch.Tensor): 位置IDのテンソル\n",
    "        \"\"\"\n",
    "        logger.info(f\"トークン生成フェーズのメタデータを準備開始 {len(seqs)=}\")\n",
    "\n",
    "        # 1) メタデータを初期化\n",
    "\n",
    "        input_ids = []\n",
    "        positions = []\n",
    "        slot_mapping = []\n",
    "        context_lens = []\n",
    "\n",
    "        # 2) 各シーケンスからメタデータを抽出\n",
    "\n",
    "        # シーケンスのリストでループ\n",
    "        for seq in seqs:\n",
    "\n",
    "            # シーケンスの最後のトークンIDを入力に追加\n",
    "            # Decodeでは直前に生成された1トークンのみを入力とするため\n",
    "            input_ids.append(seq.last_token)\n",
    "\n",
    "            # 最後のトークンの位置インデックスを追加\n",
    "            positions.append(len(seq) - 1)\n",
    "\n",
    "            # シーケンス全体の長さ\n",
    "            # Attention計算時に必要\n",
    "            context_lens.append(len(seq))\n",
    "\n",
    "            # スロットマッピングの計算\n",
    "            # 今回の入力に対するKVキャッシュを格納する物理アドレス（スロットID)\n",
    "            slot_mapping.append(\n",
    "                seq.block_table[-1] * self.block_size + \\\n",
    "                seq.last_block_num_tokens  - 1)\n",
    "\n",
    "        # 3) テンソルに変換してGPUに転送\n",
    "\n",
    "        input_ids = torch.tensor(\n",
    "            input_ids,\n",
    "            dtype=torch.int64,\n",
    "            pin_memory=True).cuda(non_blocking=True)\n",
    "\n",
    "        positions = torch.tensor(\n",
    "            positions,\n",
    "            dtype=torch.int64,\n",
    "            pin_memory=True).cuda(non_blocking=True)\n",
    "\n",
    "        slot_mapping = torch.tensor(\n",
    "            slot_mapping,\n",
    "            dtype=torch.int32,\n",
    "            pin_memory=True).cuda(non_blocking=True)\n",
    "\n",
    "        context_lens = torch.tensor(\n",
    "            context_lens,\n",
    "            dtype=torch.int32,\n",
    "            pin_memory=True).cuda(non_blocking=True)\n",
    "\n",
    "        # 4) ブロックテーブルの準備\n",
    "\n",
    "        # 各シーケンスのブロックIDのリストを2次元テンソルに変換\n",
    "        block_tables = self.prepare_block_tables(seqs)\n",
    "\n",
    "        # 5) コンテキストへの登録\n",
    "\n",
    "        # PagedAttention用のカーネルに必要なメタデータをコンテキストに登録\n",
    "        set_context(\n",
    "            False, # デコードフェーズ\n",
    "            slot_mapping=slot_mapping,\n",
    "            context_lens=context_lens,\n",
    "            block_tables=block_tables)\n",
    "\n",
    "        logger.info(f\"トークン生成フェーズのメタデータを準備完了 {len(input_ids)=} {len(positions)=}\")\n",
    "\n",
    "        return input_ids, positions\n",
    "\n",
    "    def prepare_sample(self, seqs: list[Sequence]):\n",
    "        \"\"\"\n",
    "        サンプリングに必要なパラメータを抽出してGPUテンソルにまとめる\n",
    "        推論サイクルの最後のサンプリング処理で使用する\n",
    "\n",
    "        Args:\n",
    "            seqs (list[Sequence]): シーケンスのリスト\n",
    "        Returns:\n",
    "            temperatures (torch.Tensor): 各シーケンスの温度パラメータのテンソル\n",
    "        \"\"\"\n",
    "        logger.info(f\"サンプリングパラメータを準備開始 {len(seqs)=}\")\n",
    "\n",
    "        temperatures = []\n",
    "\n",
    "        for seq in seqs:\n",
    "            temperatures.append(seq.temperature)\n",
    "\n",
    "        temperatures = torch.tensor(\n",
    "            temperatures,\n",
    "            dtype=torch.float32,\n",
    "            pin_memory=True).cuda(non_blocking=True)\n",
    "\n",
    "        logger.info(f\"サンプリングパラメータを準備完了 {temperatures=}\")\n",
    "\n",
    "        return temperatures\n",
    "\n",
    "    @torch.inference_mode()\n",
    "    def run_model(self, input_ids: torch.Tensor, positions: torch.Tensor, is_prefill: bool):\n",
    "        \"\"\"\n",
    "        準備した入力データでモデルを順伝搬し、次のトークン予測のロジットを計算する\n",
    "\n",
    "        プロンプト処理（Prefill)とトークン生成（Decode)の両方に対応\n",
    "\n",
    "        Args:\n",
    "            input_ids (torch.Tensor): 入力トークンIDのテンソル\n",
    "            positions (torch.Tensor): 位置IDのテンソル\n",
    "            is_prefill (bool): PrefillフェーズかDecodeフェーズかのフラグ\n",
    "        Returns:\n",
    "            logits (torch.Tensor): 次のトークン予測のロジットテンソル\n",
    "        \"\"\"\n",
    "        logger.info(f\"モデルの順伝搬を実行開始 {input_ids.shape=} {positions.shape=} {is_prefill=}\")\n",
    "\n",
    "        # A) 通常実行の場合（PrefillまたはCuda Graphが無効化）\n",
    "        if is_prefill or self.enforce_eager or input_ids.size(0) > 512:\n",
    "\n",
    "            # モデルの順伝搬を実行し、ロジットを計算\n",
    "            result = self.model.compute_logits(\n",
    "                self.model(input_ids, positions)\n",
    "            )\n",
    "\n",
    "            logger.info(f\"モデルの順伝搬（prefill）を実行完了 {result.shape=}\")\n",
    "            return result\n",
    "\n",
    "        # B) 高速実行（Decode + CUDA Graphs）の場合\n",
    "        else:\n",
    "\n",
    "            # B-1) 初期化\n",
    "\n",
    "            # バッチサイズを取得\n",
    "            bs = input_ids.size(0)\n",
    "\n",
    "            # コンテキストを取得 \n",
    "            context = get_context()\n",
    "\n",
    "            # B-2) グラフの選択\n",
    "\n",
    "            # バッチサイズをカバーできる最小の録画済みグラフを選択\n",
    "            graph = self.graphs[next(x for x in self.graph_bs if x >= bs)]\n",
    "\n",
    "            # B-3) データのセット\n",
    "\n",
    "            # グラフ専用のメモリ領域の参照を取得\n",
    "            graph_vars = self.graph_vars\n",
    "\n",
    "            # グラフ専用のメモリ領域にデータをコピー\n",
    "            graph_vars[\"input_ids\"][:bs] = input_ids\n",
    "            graph_vars[\"positions\"][:bs] = positions\n",
    "            graph_vars[\"slot_mapping\"].fill_(-1)\n",
    "            graph_vars[\"slot_mapping\"][:bs] = context.slot_mapping\n",
    "            graph_vars[\"context_lens\"].zero_()\n",
    "            graph_vars[\"context_lens\"][:bs] = context.context_lens\n",
    "            graph_vars[\"block_tables\"][:bs, :context.block_tables.size(1)] = context.block_tables\n",
    "\n",
    "            # B-4) グラフの再生\n",
    "\n",
    "            # CPU介入なしに一気にGPUカーネルを実行\n",
    "            graph.replay()\n",
    "\n",
    "            # B-5) 結果の取得\n",
    "\n",
    "            # 結果を取り出してロジットを計算\n",
    "            result = self.model.compute_logits(graph_vars[\"outputs\"][:bs])\n",
    "\n",
    "            logger.info(f\"モデルの順伝播（decode）を実行完了 {result.shape=}\")\n",
    "            return result\n",
    "\n",
    "    def run(self, seqs: list[Sequence], is_prefill: bool) -> list[int]:\n",
    "        \"\"\"\n",
    "        ModelRunnerのメインメソッド\n",
    "        入力データを準備し、モデルを実行し、次のトークンを決定する\n",
    "\n",
    "        Args:\n",
    "            seqs (list[Sequence]): シーケンスのリスト\n",
    "            is_prefill (bool): PrefillフェーズかDecodeフェーズかのフラグ\n",
    "        Returns:\n",
    "            token_ids (list[int]): 生成されたトークンIDのリスト\n",
    "        \"\"\"\n",
    "        logger.info(f\"推論の1サイクルを実行開始 {len(seqs)=} {is_prefill=}\")\n",
    "\n",
    "        # 1) 入力データの準備\n",
    "\n",
    "        input_ids, positions = self.prepare_prefill(seqs) if is_prefill \\\n",
    "            else self.prepare_decode(seqs)\n",
    "\n",
    "        # 2) サンプリングパラメータの準備\n",
    "\n",
    "        temperatures = self.prepare_sample(seqs) if self.rank == 0 \\\n",
    "            else None\n",
    "\n",
    "        # 3) モデルの順伝搬を実行し、ロジットを計算\n",
    "\n",
    "        logits = self.run_model(input_ids, positions, is_prefill)\n",
    "\n",
    "        # 4) サンプリングを実行し、次のトークンを決定\n",
    "\n",
    "        token_ids = self.sampler(logits, temperatures).tolist() \\\n",
    "            if self.rank == 0 else None\n",
    "\n",
    "        # 5) コンテキストのリセット\n",
    "\n",
    "        reset_context()\n",
    "\n",
    "        logger.info(f\"推論の1サイクルを実行完了 {(len(token_ids) if token_ids else None)=}\")\n",
    "\n",
    "        return token_ids\n",
    "\n",
    "    @torch.inference_mode()\n",
    "    def capture_cudagraph(self):\n",
    "        \"\"\"\n",
    "        様々なバッチサイズの計算グラフを事前にキャプチャする\n",
    "        Decodeフェーズの計算を高速化するため\n",
    "        \"\"\"\n",
    "\n",
    "        logger.info(f\"CUDA Graphをキャプチャ開始 {self.rank=}\")\n",
    "\n",
    "        # 1) キャプチャ用の固定メモリを初期化\n",
    "\n",
    "        config = self.config\n",
    "\n",
    "        hf_config = config.hf_config\n",
    "\n",
    "        # 最大バッチサイズ\n",
    "        max_bs = min(self.config.max_num_seqs, 512)\n",
    "\n",
    "        # 最大ブロック数\n",
    "        max_num_blocks = (config.max_model_len + self.block_size - 1) \\\n",
    "            // self.block_size\n",
    "\n",
    "        # 入力\n",
    "        input_ids = torch.zeros(max_bs, dtype=torch.int64)\n",
    "\n",
    "        # 位置\n",
    "        positions = torch.zeros(max_bs, dtype=torch.int64)\n",
    "\n",
    "        # スロットマッピング\n",
    "        slot_mapping = torch.zeros(max_bs, dtype=torch.int32)\n",
    "\n",
    "        # コンテキスト長\n",
    "        context_lens = torch.zeros(max_bs, dtype=torch.int32)\n",
    "\n",
    "        # ブロックテーブル\n",
    "        block_tables = torch.zeros(max_bs, max_num_blocks, dtype=torch.int32)\n",
    "\n",
    "        # 出力\n",
    "        outputs = torch.zeros(max_bs, hf_config.hidden_size)\n",
    "\n",
    "        # 2) キャプチャするバッチサイズのパターンを決める\n",
    "\n",
    "        # バッチサイズのパターン\n",
    "        # 1, 2, 4, 8, 16, 32, ..., max_bs\n",
    "        self.graph_bs = [1, 2, 4, 8] + list(range(16, max_bs + 1, 16))\n",
    "\n",
    "        self.graphs = {}\n",
    "\n",
    "        self.graph_pool = None\n",
    "\n",
    "        logger.debug(f\"{self.rank=} キャプチャパターン: {self.graph_bs}\")\n",
    "\n",
    "        # 3) パターンごとに計算グラフをキャプチャする\n",
    "\n",
    "        # バッチサイズのパターンで逆順にループ\n",
    "        for bs in reversed(self.graph_bs):\n",
    "\n",
    "            # 3-1) ウォームアップ\n",
    "\n",
    "            # CUDA Graphのインスタンスを作成\n",
    "            graph = torch.cuda.CUDAGraph()\n",
    "\n",
    "            # PagedAttention用のメタデータをコンテキストに登録\n",
    "            set_context(\n",
    "                False,\n",
    "                slot_mapping=slot_mapping[:bs],\n",
    "                context_lens=context_lens[:bs],\n",
    "                block_tables=block_tables[:bs])\n",
    "\n",
    "            # ウォームアップ実行\n",
    "            # カーネルのコンパイル・メモリアロケーターの初期化\n",
    "            outputs[:bs] = self.model(input_ids[:bs], positions[:bs])\n",
    "\n",
    "            # 3-2) キャプチャ開始\n",
    "\n",
    "            # メモリプールを共有し、グラフごとに作業領域を使い回す\n",
    "            with torch.cuda.graph(graph, self.graph_pool):\n",
    "\n",
    "                # モデルの順伝搬をキャプチャ\n",
    "                outputs[:bs] = self.model(input_ids[:bs], positions[:bs])\n",
    "\n",
    "            # メモリプールがない場合\n",
    "            if self.graph_pool is None:\n",
    "                # 最初にキャプチャしたグラフからメモリループを取得\n",
    "                # メモリループは使用したメモリ領域の管理情報\n",
    "                self.graph_pool = graph.pool()\n",
    "\n",
    "            # 3-3) グラフを保存\n",
    "\n",
    "            self.graphs[bs] = graph\n",
    "\n",
    "            # GPU上でキャプチャが終わるまで待機\n",
    "            torch.cuda.synchronize()\n",
    "\n",
    "            # コンテキストをリセット\n",
    "            reset_context()\n",
    "\n",
    "        # 4) キャプチャに使用した固定メモリへの参照を保存\n",
    "\n",
    "        # キャプチャに使用したテンソルへの参照を辞書にまとめる\n",
    "        # 実行時はここにデータをコピーしてからグラフを再生する\n",
    "        self.graph_vars = dict(\n",
    "            input_ids=input_ids,\n",
    "            positions=positions,\n",
    "            slot_mapping=slot_mapping,\n",
    "            context_lens=context_lens,\n",
    "            block_tables=block_tables,\n",
    "            outputs=outputs,\n",
    "        )\n",
    "\n",
    "        logger.info(f\"CUDA Graphをキャプチャ完了 {self.rank=}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa2bbf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import atexit\n",
    "from dataclasses import fields\n",
    "from time import perf_counter\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import AutoTokenizer\n",
    "import torch.multiprocessing as mp\n",
    "\n",
    "class LLMEngine:\n",
    "\n",
    "    def __init__(self, model, **kwargs):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            model (str): モデル名またはパス\n",
    "        \"\"\"\n",
    "        logger.info(f\"エンジンを初期化開始 {model=}\")\n",
    "\n",
    "\n",
    "        # 1) 設定の初期化\n",
    "\n",
    "        config_fields = {field.name for field in fields(Config)}\n",
    "        config_kwargs = {k: v for k, v in kwargs.items() if k in config_fields}\n",
    "        config = Config(model, **config_kwargs)\n",
    "\n",
    "        # 2) ワーカープロセスの起動\n",
    "\n",
    "        self.ps = []\n",
    "        self.events = []\n",
    "\n",
    "        # プロセス生成方式をspawnに設定\n",
    "        ctx = mp.get_context(\"spawn\")\n",
    "\n",
    "        # ランク1移行のサブプロセスを生成\n",
    "        for i in range(1, config.tensor_parallel_size):\n",
    "\n",
    "            event = ctx.Event()\n",
    "\n",
    "            # サブプロセスでModelRunnerを実行\n",
    "            process = ctx.Process(target=ModelRunner, args=(config, i, event))\n",
    "            process.start()\n",
    "\n",
    "            self.ps.append(process)\n",
    "            self.events.append(event)\n",
    "\n",
    "        # 3) メインプロセスの初期化\n",
    "\n",
    "        # ランク0（メインプロセス）のModelRunnerを作成\n",
    "        self.model_runner = ModelRunner(config, 0, self.events)\n",
    "\n",
    "        # 4) トークナイザーの初期化\n",
    "\n",
    "        # トークナイザーを読み込み\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\n",
    "            config.model, use_fast=True\n",
    "        )\n",
    "\n",
    "        # 終了判定に使用するEOSトークンIDを取得\n",
    "        config.eos = self.tokenizer.eos_token_id\n",
    "\n",
    "        # 5) スケジューラーの初期化\n",
    "\n",
    "        # リクエストの順番待ちや、KVキャッシュのメモリブロック管理を行うスケジューラ\n",
    "        self.scheduler = Scheduler(config)\n",
    "\n",
    "        # 6) 終了処理の登録\n",
    "\n",
    "        # サブプロセスが確実に終了するようにatexitで登録\n",
    "        atexit.register(self.exit)\n",
    "\n",
    "        logger.info(f\"エンジンの初期化完了\")\n",
    "\n",
    "    def exit(self):\n",
    "        \"\"\"\n",
    "        エンジン停止時にワーカープロセスや共有メモリを解放する\n",
    "        \"\"\"\n",
    "        logger.info(f\"エンジンの終了処理開始\")\n",
    "\n",
    "        # メインプロセスのModelRunnerのexitメソッドを実行\n",
    "        self.model_runner.call(\"exit\")\n",
    "\n",
    "        # オブジェクトを削除\n",
    "        del self.model_runner\n",
    "\n",
    "        # サブプロセスでループ\n",
    "        for p in self.ps:\n",
    "            # プロセス終了を待機\n",
    "            p.join()\n",
    "\n",
    "        logger.info(f\"エンジンの終了処理完了\")\n",
    "\n",
    "    def add_request(self, prompt: str | list[int], sampling_params: SamplingParams):\n",
    "        \"\"\"\n",
    "        新しいプロンプト（生成リクエスト）を受け取り、キューに登録する\n",
    "\n",
    "        Args:\n",
    "            prompt (str | list[int]): プロンプト文字列またはトークンIDのリスト\n",
    "            sampling_params (SamplingParams): サンプリングパラメータ\n",
    "        \"\"\"\n",
    "        logger.info(f\"リクエストをキューに登録開始 {prompt=} {sampling_params=}\")\n",
    "\n",
    "        # 1) Sequenceオブジェクトを作成\n",
    "\n",
    "        # プロンプトが文字列の場合\n",
    "        if isinstance(prompt, str):\n",
    "            # トークンIDのリストに変換\n",
    "            prompt = self.tokenizer.encode(prompt)\n",
    "\n",
    "        # Sequenceオブジェクトを初期化\n",
    "        seq = Sequence(prompt, sampling_params)\n",
    "\n",
    "        # 2) キューに登録\n",
    "\n",
    "        # スケジューラに登録\n",
    "        self.scheduler.add(seq)\n",
    "\n",
    "        logger.info(f\"リクエストをキューに登録完了 {seq.seq_id=}\")\n",
    "\n",
    "    def step(self):\n",
    "        \"\"\"\n",
    "        推論エンジンの1サイクルを実行し、完了したシーケンスの出力を返す\n",
    "        \"\"\"\n",
    "\n",
    "        logger.info(f\"推論エンジンの1サイクルを実行開始\")\n",
    "\n",
    "        # 1) スケジューリング\n",
    "\n",
    "        # 処理するシーケンスをスケジューラから取得\n",
    "        # is_prefillにより、prefill（プロンプト処理）かdecode（生成処理）かを判定\n",
    "        seqs, is_prefill = self.scheduler.schedule()\n",
    "\n",
    "        # 2) モデル実行\n",
    "\n",
    "        # GPUワーカーにシーケンスを渡してrunメソッドを実行し、トークンを生成\n",
    "        token_ids = self.model_runner.call(\"run\", seqs, is_prefill)\n",
    "\n",
    "        # 3) 後処理\n",
    "\n",
    "        # 新しいトークンをシーケンスに追加し、スケジューラで終了判定する\n",
    "        self.scheduler.postprocess(seqs, token_ids)\n",
    "\n",
    "        # 4) 出力の収集\n",
    "\n",
    "        # 完了したシーケンスの結果を抽出\n",
    "        outputs = [(seq.seq_id, seq.completion_token_ids) \\\n",
    "            for seq in seqs if seq.is_finished]\n",
    "\n",
    "        # スループット計算のためのトークン数を計算\n",
    "        # prefill時: 処理したプロンプトの全トークン数\n",
    "        # decode時: 生成したトークン数（負の値にして区別）\n",
    "        num_tokens = sum(len(seq) for seq in seqs) \\\n",
    "            if is_prefill else -len(seqs)\n",
    "\n",
    "        logger.info(f\"推論エンジンの1サイクルを実行完了 {outputs=} {num_tokens=}\")\n",
    "\n",
    "        return outputs, num_tokens\n",
    "\n",
    "    def is_finished(self):\n",
    "        \"\"\"\n",
    "        推論エンジンに処理するべきタスクが残っているかを確認する\n",
    "        \"\"\"\n",
    "        res = self.scheduler.is_finished()\n",
    "        logger.info(f\"スケジューラに処理すべきタスクが残っているか確認 {res=}\")\n",
    "        return res\n",
    "\n",
    "    def generate(\n",
    "        self,\n",
    "        prompts: list[str] | list[list[int]],\n",
    "        sampling_params: SamplingParams | list[SamplingParams],\n",
    "        use_tqdm: bool = True,\n",
    "    ) -> list[str]:\n",
    "        \"\"\"\n",
    "        プロンプトに対してテキスト生成を行い、生成結果を返す\n",
    "\n",
    "        Args:\n",
    "            prompts (list[str] | list[list[int]]):\n",
    "                プロンプト文字列またはトークンIDのリストのリスト\n",
    "            sampling_params (SamplingParams | list[SamplingParams]):\n",
    "                サンプリングパラメータまたはそのリスト\n",
    "            use_tqdm (bool): 進捗表示にtqdmを使用するかどうか\n",
    "\n",
    "        Returns:\n",
    "            list[str]: 生成結果の文字列のリスト\n",
    "        \"\"\"\n",
    "        logger.info(f\"テキスト生成を開始 {len(prompts)=} {sampling_params=}\")\n",
    "\n",
    "        # 1) 初期化\n",
    "\n",
    "        # 進捗表示する場合\n",
    "        if use_tqdm:\n",
    "\n",
    "            # プログレスバーを初期化\n",
    "            pbar = tqdm(\n",
    "                total=len(prompts), desc=\"Generating\", dynamic_ncols=True\n",
    "            )\n",
    "\n",
    "        # SamplingParamsがリストの場合\n",
    "        if not isinstance(sampling_params, list):\n",
    "\n",
    "            # 全てのプロンプトに対して同じSamplingParamsを使用するように拡張\n",
    "            sampling_params = [sampling_params] * len(prompts)\n",
    "\n",
    "        # 2) リクエストの登録\n",
    "\n",
    "        # 全てのプロンプトをキューに登録\n",
    "        for prompt, sp in zip(prompts, sampling_params):\n",
    "            self.add_request(prompt, sp)\n",
    "\n",
    "        # 3) 生成ループ\n",
    "\n",
    "        outputs = {}\n",
    "\n",
    "        # prefillとdecodeのスループットを初期化\n",
    "        # スループットは毎秒のトークン処理数\n",
    "        prefill_throughput = decode_throughput = 0.\n",
    "\n",
    "        while not self.is_finished():\n",
    "\n",
    "            # 3-1) トークン生成ステップの実行\n",
    "\n",
    "            t = perf_counter()\n",
    "\n",
    "            # 1サイクル実行し、finishedになったシーケンスの出力を取得\n",
    "            output, num_tokens = self.step()\n",
    "\n",
    "            # 3-2) スループットの計算\n",
    "\n",
    "            # 進捗を表示する場合\n",
    "            if use_tqdm:\n",
    "\n",
    "                # num_tokensが正の場合（prefillの場合）\n",
    "                if num_tokens > 0:\n",
    "\n",
    "                    # prefillスループットを計算\n",
    "                    prefill_throughput = num_tokens / (perf_counter() - t)\n",
    "\n",
    "                # num_tokensが負の場合（decodeの場合）\n",
    "                else:\n",
    "\n",
    "                    # decodeスループットを計算\n",
    "                    decode_throughput = -num_tokens / (perf_counter() - t)\n",
    "\n",
    "                # 進捗バーにスループットを表示\n",
    "                pbar.set_postfix({\n",
    "                    \"Prefill\": f\"{int(prefill_throughput)}tok/s\",\n",
    "                    \"Decode\": f\"{int(decode_throughput)}tok/s\",\n",
    "                })\n",
    "\n",
    "            # 3-3) 完了した結果の収集\n",
    "\n",
    "            # finishedになったシーケンスでループ\n",
    "            for seq_id, token_ids in output:\n",
    "\n",
    "                # 出力を保存\n",
    "                outputs[seq_id] = token_ids\n",
    "\n",
    "                # 進捗表示する場合\n",
    "                if use_tqdm:\n",
    "\n",
    "                    # プログレスバーを更新\n",
    "                    pbar.update(1)\n",
    "\n",
    "        # 4) 結果のデコード\n",
    "\n",
    "        # シーケンスIDの順にソートし、入力順序に整列\n",
    "        outputs = [outputs[seq_id] for seq_id in sorted(outputs.keys())]\n",
    "\n",
    "        # トークンIDのリストを文字列にデコード\n",
    "        outputs = [{\n",
    "            \"text\": self.tokenizer.decode(token_ids),\n",
    "            \"token_ids\": token_ids\n",
    "        } for token_ids in outputs]\n",
    "\n",
    "        # 進捗表示する場合\n",
    "        if use_tqdm:\n",
    "\n",
    "            # プログレスバーを閉じる\n",
    "            pbar.close()\n",
    "\n",
    "        logger.info(f\"テキスト生成を完了 {len(outputs)=}\")\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1052ff8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LLM(LLMEngine):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a66bd32",
   "metadata": {},
   "source": [
    "## 推論"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e9e4737",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if os.path.exists(\"debug.log\"):\n",
    "    os.remove(\"debug.log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f333de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "path =  \"/root/huggingface/Qwen3-0.6B\"\n",
    "\n",
    "if not os.path.exists(path):\n",
    "    !huggingface-cli download --resume-download Qwen/Qwen3-0.6B \\\n",
    "    --local-dir ~/huggingface/Qwen3-0.6B/ \\\n",
    "    --local-dir-use-symlinks False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c214c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"llm\" not in globals():\n",
    "    llm = LLM(path, enforce_eager=True, tensor_parallel_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70604833",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_params = SamplingParams(temperature=0.6, max_tokens=1)\n",
    "prompts = [\"Hello\"]\n",
    "outputs = llm.generate(prompts, sampling_params)\n",
    "outputs[0][\"text\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
