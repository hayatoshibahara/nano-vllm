{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff311100",
   "metadata": {},
   "source": [
    "# Nano-vLLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b9419c3",
   "metadata": {},
   "source": [
    "[Nano-vLLM][1]\n",
    "\n",
    "[1]: https://github.com/GeeeekExplorer/nano-vllm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a002c4c",
   "metadata": {},
   "source": [
    "## æ¦‚è¦\n",
    "\n",
    "vLLMã¯ã€PagedAttentionã«ã‚ˆã‚Šå¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ã‚’é«˜é€Ÿã‹ã¤åŠ¹ç‡çš„ã«æ¨è«–ã•ã›ã‚‹ãŸã‚ã®ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯\n",
    "\n",
    "Nano-vLLMã¯ã€vLLMã‚’å˜ç´”åŒ–ã—ãŸãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯\n",
    "\n",
    "PagedAttentionã¯ã€OSã®ãƒšãƒ¼ã‚¸ãƒ³ã‚°æ–¹å¼ã‹ã‚‰ç€æƒ³ã‚’å¾—ãŸKVã‚­ãƒ£ãƒƒã‚·ãƒ¥ã®ç®¡ç†æ–¹å¼\n",
    "\n",
    "OSã®ãƒšãƒ¼ã‚¸ãƒ³ã‚°æ–¹å¼ã¯ã€ç‰©ç†ãƒ¡ãƒ¢ãƒªç©ºé–“ã‚’ãƒšãƒ¼ã‚¸ã”ã¨ã«åˆ†å‰²ã—ã€æ–­ç‰‡åŒ–ã‚’é˜²ãã€ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’å‰Šæ¸›ã™ã‚‹æ‰‹æ³•:\n",
    "\n",
    "![](image/fig1.png)\n",
    "\n",
    "- å·¦: è«–ç†ãƒ¡ãƒ¢ãƒªç©ºé–“\n",
    "- ä¸­: ãƒšãƒ¼ã‚¸\n",
    "- å³: ç‰©ç†ãƒ¡ãƒ¢ãƒªç©ºé–“\n",
    "- ãƒšãƒ¼ã‚¸ãƒ†ãƒ¼ãƒ–ãƒ«ã¯ã€è«–ç†ãƒ¡ãƒ¢ãƒªç©ºé–“ã®ã‚¢ãƒ‰ãƒ¬ã‚¹ã¨ç‰©ç†ãƒ¡ãƒ¢ãƒªç©ºé–“ã®ãƒšãƒ¼ã‚¸ã®å¯¾å¿œè¡¨\n",
    "\n",
    "Nano-vLLMã§ã¯ã€ãƒšãƒ¼ã‚¸ã‚’ãƒ–ãƒ­ãƒƒã‚¯ã¨å‘¼ã³ã€KVã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ãƒšãƒ¼ã‚¸ãƒ³ã‚°æ–¹å¼ã§ç®¡ç†ã—ã¦ã„ã‚‹\n",
    "\n",
    "1ã¤ã®ãƒ–ãƒ­ãƒƒã‚¯ã«ã¯256å€‹ã®ã‚­ãƒ¼ã¨ãƒãƒªãƒ¥ãƒ¼ã®ãƒšã‚¢ãŒå«ã¾ã‚Œã‚‹"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65614287",
   "metadata": {},
   "source": [
    "## æ³¨æ„"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1a15c63",
   "metadata": {},
   "source": [
    "Nano-vLLMã¯ã€è¤‡æ•°ã®GPUã§ä¸¦åˆ—ã—ã¦ãƒˆãƒ¼ã‚¯ãƒ³ã‚’å‡¦ç†ã—ã€KVã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’å‹•çš„ã«ç®¡ç†ã™ã‚‹ãŸã‚è¤‡é›‘\n",
    "\n",
    "ã™ã¹ã¦ã®ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã®å½¹å‰²ã‚„å‹•ãã‚’è¦šãˆã‚‹å¿…è¦ãŒã‚ã‚Šã€å…¨ä½“ã®ç†è§£ã«ã¯æ™‚é–“ãŒçµæ§‹ã‹ã‹ã‚‹"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e70cdb7f",
   "metadata": {},
   "source": [
    "## æ§‹æˆ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f6a20e4",
   "metadata": {},
   "source": [
    "### ã‚µãƒ–ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆ\n",
    "\n",
    "- Sequence: æ¨è«–ã«å¿…è¦ãªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¨å…¥åŠ›ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã‚’ä¿æŒã™ã‚‹ã‚¯ãƒ©ã‚¹\n",
    "- Block: 256å€‹ã®ã‚¹ãƒ­ãƒƒãƒˆï¼ˆãƒˆãƒ¼ã‚¯ãƒ³1å€‹ã«å¯¾å¿œã™ã‚‹ã‚­ãƒ¼ã¨ãƒãƒªãƒ¥ãƒ¼ã‚­ãƒ£ãƒƒã‚·ãƒ¥ï¼‰ã®å¡Š\n",
    "- Context: æ¨è«–ã«å¿…è¦ãªãƒ•ãƒ©ã‚°ã‚„ä¸€æ™‚è¨ˆç®—çµæœã‚’ç®¡ç†ã™ã‚‹ã‚°ãƒ­ãƒ¼ãƒãƒ«ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b01582f",
   "metadata": {},
   "source": [
    "### ãƒ¡ã‚¤ãƒ³ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆ\n",
    "\n",
    "- LLMEngine: å…¨ä½“ã‚’çµ±æ‹¬ã™ã‚‹ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ã‚¤ã‚¹\n",
    "    - ãƒ¡ã‚¤ãƒ³ãƒ—ãƒ­ã‚»ã‚¹ãƒ»è¤‡æ•°ã®ãƒ¯ãƒ¼ã‚«ãƒ¼ãƒ—ãƒ­ã‚»ã‚¹ã‚’èµ·å‹•ã—ã€å®‰å…¨ã«çµ‚äº†ã•ã›ã‚‹\n",
    "    - ãƒ¡ã‚¤ãƒ³ãƒ—ãƒ­ã‚»ã‚¹ã«Schedulerã‚’åˆæœŸåŒ–ã™ã‚‹\n",
    "    - ãƒ—ãƒ­ã‚»ã‚¹ã”ã¨ã«ModelRunnerã‚’èµ·å‹•ã™ã‚‹\n",
    "    - ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’å—ã‘ä»˜ã‘ã€Sequenceã«ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ã—ã€Schedulerã«ç™»éŒ²ã™ã‚‹\n",
    "    - Schedulerã‹ã‚‰å–å¾—ã—ãŸSequenceã‚’ãƒ—ãƒ­ã‚»ã‚¹ã¨å…±æœ‰ã—ã€ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆã‚’ç®¡ç†ã™ã‚‹\n",
    "    - ç”Ÿæˆçµæœã‚’å¹³æ–‡ã«ãƒ‡ã‚³ãƒ¼ãƒ‰ã™ã‚‹\n",
    "- Scheduler: Sequenceã®ã‚­ãƒ¥ãƒ¼ã®ç®¡ç†ã¨KVã‚­ãƒ£ãƒƒã‚·ãƒ¥ã®ç®¡ç†ã‚’è¡Œã†\n",
    "    - å¾…æ©Ÿä¸­ã®ã‚­ãƒ¥ãƒ¼ã¨æ¨è«–ä¸­ã®ã‚­ãƒ¥ãƒ¼ã‚’åˆæœŸåŒ–ã™ã‚‹\n",
    "    - KVã‚­ãƒ£ãƒƒã‚·ãƒ¥ã®æ“ä½œã‚’æŠ½è±¡åŒ–ã—ãŸBlockManagerã‚’åˆæœŸåŒ–ã™ã‚‹\n",
    "    - ãƒ¡ãƒ¢ãƒªä¸è¶³ã«ãªã‚‹ç›´å‰ã«ä½å„ªå…ˆåº¦ã®æ¨è«–ã‚¿ã‚¹ã‚¯ã‚’ä¸­æ–­ã™ã‚‹\n",
    "- ModelRunner: ãƒ—ãƒ­ã‚»ã‚¹é–“ã§é€£æºã—ãªãŒã‚‰ãƒ¢ãƒ‡ãƒ«ã®æ¨è«–ã‚’è¡Œã†\n",
    "    - ãƒ¢ãƒ‡ãƒ«ã‚’åˆæœŸåŒ–ã—ã€ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‚’èª­ã¿è¾¼ã‚€\n",
    "    - åˆæœŸåŒ–æ™‚ã«KVã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ¡ãƒ¢ãƒªã‚’ç¢ºä¿ã—ã€è¨ˆç®—ã‚°ãƒ©ãƒ•ã‚’ã‚­ãƒ£ãƒ—ãƒãƒ£ã™ã‚‹\n",
    "    - LLMEngineã®æŒ‡ä»¤ã‚’ãƒ¡ã‚¤ãƒ³ãƒ—ãƒ­ã‚»ã‚¹ãŒå—ã‘å–ã‚Šã€ã‚µãƒ–ãƒ—ãƒ­ã‚»ã‚¹ã¨é€£æºã—ã¦æ¨è«–ã™ã‚‹\n",
    "- BlockManager: KVã‚­ãƒ£ãƒƒã‚·ãƒ¥ã®Blockã®å‰²ã‚Šå½“ã¦ã€è§£æ”¾ã€ãƒãƒƒã‚·ãƒ¥å€¤ã§ã®æ¤œç´¢ã‚’è¡Œã†\n",
    "    - Blockã®ãƒãƒƒã‚·ãƒ¥å€¤ã®è¨ˆç®—\n",
    "    - Blockã®ãƒãƒƒã‚·ãƒ¥å€¤ã¨Block IDã®å¯¾å¿œè¡¨ã‚’ç®¡ç†\n",
    "    - éå»ã®KVã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’æ¤œç´¢ã—ã€Sequenceã®Blockãƒ†ãƒ¼ãƒ–ãƒ«ã«ç‰©ç†ãƒ–ãƒ­ãƒƒã‚¯IDã‚’æ›¸ãè¾¼ã‚€\n",
    "    - å‹•çš„ã«ãƒ¡ãƒ¢ãƒªã‚’å‰²ã‚Šå½“ã¦ã€Sequenceã®Blockãƒ†ãƒ¼ãƒ–ãƒ«ã«ç‰©ç†ãƒ–ãƒ­ãƒƒã‚¯IDã‚’æ›¸ãè¾¼ã‚€\n",
    "    - Blockã®å‚ç…§ã‚«ã‚¦ãƒ³ã‚¿ã‚’ç®¡ç†ã—ã€å‹•çš„ã«ãƒ¡ãƒ¢ãƒªã‚’è§£æ”¾ã™ã‚‹\n",
    "- Qwen3: è¤‡æ•°ã®ãƒ—ãƒ­ã‚»ã‚¹ã§é€£æºã—ã¦é †ä¼æ’­ãŒå¯èƒ½ã§ã€FlashAttentionã‚’çµ„ã¿è¾¼ã‚“ã ãƒ¢ãƒ‡ãƒ«"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6821398",
   "metadata": {},
   "source": [
    "## å‡¦ç†ã®æµã‚Œ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "318655b6",
   "metadata": {},
   "source": [
    "## ç’°å¢ƒæ§‹ç¯‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d7997b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FlashAttentionã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ï¼ˆGoogle Colabã®å ´åˆã¯T4ã§ã¯ãªãã€A100ã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„ï¼‰\n",
    "\n",
    "%pip install -qU torch==2.8.0 transformers=4.57.1\n",
    "%pip install -q flash-attn --no-build-isolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b367e634",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "from copy import copy\n",
    "from dataclasses import dataclass\n",
    "from dataclasses import fields\n",
    "from enum import Enum, auto\n",
    "from functools import lru_cache\n",
    "from glob import glob\n",
    "from itertools import count\n",
    "from safetensors import safe_open\n",
    "from time import perf_counter\n",
    "from torch import nn\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import AutoConfig\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import Qwen3Config\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "import torch.multiprocessing as mp\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# ãƒ—ãƒ­ã‚°ãƒ©ãƒ çµ‚äº†æ™‚ã®ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«\n",
    "import atexit\n",
    "\n",
    "# ä¸¦åˆ—å‡¦ç†ç”¨ã®ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«\n",
    "from multiprocessing.shared_memory import SharedMemory\n",
    "from multiprocessing.synchronize import Event\n",
    "\n",
    "# ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚’é«˜é€Ÿã«è¨ˆç®—ã™ã‚‹FlashAttention\n",
    "from flash_attn import flash_attn_varlen_func, flash_attn_with_kvcache\n",
    "\n",
    "# GPUã‚«ãƒ¼ãƒãƒ«ã‚’å®Ÿè£…ã™ã‚‹ãŸã‚ã®DSLï¼ˆãƒ‰ãƒ¡ã‚¤ãƒ³å›ºæœ‰è¨€èªï¼‰\n",
    "import triton\n",
    "import triton.language as tl\n",
    "\n",
    "# é«˜é€Ÿãªãƒãƒƒã‚·ãƒ¥é–¢æ•°\n",
    "import xxhash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39eb56a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ãƒ­ã‚°åˆæœŸåŒ–\n",
    "\n",
    "import logging\n",
    "\n",
    "if os.path.exists(\"debug.log\"):\n",
    "    os.remove(\"debug.log\")\n",
    "\n",
    "def custom_format(record):\n",
    "    match record.levelno:\n",
    "        case logging.DEBUG:\n",
    "            level = \"ğŸŸ¦\"\n",
    "        case logging.INFO:\n",
    "            level = \"ğŸŸ©\"\n",
    "        case logging.WARNING:\n",
    "            level = \"ğŸŸ¨\"\n",
    "        case logging.ERROR:\n",
    "            level = \"ğŸŸ¥\"\n",
    "        case logging.CRITICAL:\n",
    "            level = \"ğŸ›‘\"\n",
    "    return f\"{level} {record.getMessage()}\"\n",
    "\n",
    "logger = logging.getLogger()\n",
    "\n",
    "for handler in logger.handlers:\n",
    "    logger.removeHandler(handler)\n",
    "\n",
    "formatter = logging.Formatter()\n",
    "formatter.format = custom_format\n",
    "\n",
    "file_handler = logging.FileHandler(\"debug.log\")\n",
    "file_handler.setFormatter(formatter)\n",
    "logger.addHandler(file_handler)\n",
    "\n",
    "stream_handler = logging.StreamHandler()\n",
    "stream_handler.setFormatter(formatter)\n",
    "logger.addHandler(stream_handler)\n",
    "\n",
    "logger.setLevel(logging.DEBUG)\n",
    "logger.debug(\"ãƒ­ã‚°ã‚’åˆæœŸåŒ–\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b05320f9",
   "metadata": {},
   "source": [
    "## Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89c894b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Context:\n",
    "    \"\"\"\n",
    "    æ¨è«–å®Ÿè¡Œæ™‚ã®ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’ä¸€æ™‚çš„ã«ä¿å­˜ã™ã‚‹ã‚°ãƒ­ãƒ¼ãƒãƒ«ãªçŠ¶æ…‹ç®¡ç†ã‚¯ãƒ©ã‚¹\n",
    "    \"\"\"\n",
    "\n",
    "    # Prefillã‹Decodeã‹ã‚’ç¤ºã™ãƒ•ãƒ©ã‚°\n",
    "    is_prefill: bool = False\n",
    "\n",
    "    # ã‚¯ã‚¨ãƒªãƒ¼ã®ç´¯ç©ã‚·ãƒ¼ã‚±ãƒ³ã‚¹é•·\n",
    "    cu_seqlens_q: torch.Tensor | None = None\n",
    "\n",
    "    # ã‚­ãƒ¼ã®ç´¯ç©ã‚·ãƒ¼ã‚±ãƒ³ã‚¹é•·\n",
    "    cu_seqlens_k: torch.Tensor | None = None\n",
    "\n",
    "    # ã‚¯ã‚¨ãƒªãƒ¼ã®æœ€å¤§ã‚·ãƒ¼ã‚±ãƒ³ã‚¹é•·\n",
    "    max_seqlen_q: int = 0\n",
    "\n",
    "    # ã‚­ãƒ¼ã®æœ€å¤§ã‚·ãƒ¼ã‚±ãƒ³ã‚¹é•·\n",
    "    max_seqlen_k: int = 0\n",
    "\n",
    "    # ãƒˆãƒ¼ã‚¯ãƒ³ã”ã¨ã®ç‰©ç†ãƒ¡ãƒ¢ãƒªã‚¹ãƒ­ãƒƒãƒˆID\n",
    "    # ã‚¹ãƒ­ãƒƒãƒˆã¯ã€ãƒˆãƒ¼ã‚¯ãƒ³1å€‹ã®ã‚­ãƒ¼ã¨ãƒãƒªãƒ¥ãƒ¼ã‚’ä¿å­˜ã™ã‚‹ãƒ¡ãƒ¢ãƒªãƒ¦ãƒ‹ãƒƒãƒˆ\n",
    "    slot_mapping: torch.Tensor | None = None\n",
    "\n",
    "    # å„ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã®ç¾åœ¨ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆé•·\n",
    "    context_lens: torch.Tensor | None = None\n",
    "\n",
    "    # è«–ç†ãƒ–ãƒ­ãƒƒã‚¯IDã¨ç‰©ç†ãƒ–ãƒ­ãƒƒã‚¯IDã®ãƒãƒƒãƒ”ãƒ³ã‚°ãƒ†ãƒ¼ãƒ–ãƒ«\n",
    "    # ãƒ–ãƒ­ãƒƒã‚¯ã¯è¤‡æ•°ã®ã‚¹ãƒ­ãƒƒãƒˆã‚’ã¾ã¨ã‚ãŸãƒ¡ãƒ¢ãƒªãƒ¦ãƒ‹ãƒƒãƒˆï¼ˆ256ã‚¹ãƒ­ãƒƒãƒˆãªã©ï¼‰\n",
    "    block_tables: torch.Tensor | None = None\n",
    "\n",
    "_CONTEXT = Context()\n",
    "logger.info(f\"ã‚°ãƒ­ãƒ¼ãƒãƒ«ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆåˆæœŸåŒ– {_CONTEXT=}\")\n",
    "\n",
    "def get_context():\n",
    "    logger.info(f\"ç¾åœ¨ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆå–å¾— {_CONTEXT=}\")\n",
    "    return _CONTEXT\n",
    "\n",
    "def set_context(is_prefill, cu_seqlens_q=None, cu_seqlens_k=None, max_seqlen_q=0, max_seqlen_k=0, slot_mapping=None, context_lens=None, block_tables=None):\n",
    "    logger.info(f\"ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’è¨­å®š {is_prefill=} {cu_seqlens_q=} {cu_seqlens_k=} {max_seqlen_q=} {max_seqlen_k=} {slot_mapping=} {context_lens=} {block_tables=}\")\n",
    "    global _CONTEXT\n",
    "    _CONTEXT = Context(is_prefill, cu_seqlens_q, cu_seqlens_k, max_seqlen_q, max_seqlen_k, slot_mapping, context_lens, block_tables)\n",
    "\n",
    "def reset_context():\n",
    "    logger.info(\"ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒªã‚»ãƒƒãƒˆ\")\n",
    "    global _CONTEXT\n",
    "    _CONTEXT = Context()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c415eed4",
   "metadata": {},
   "source": [
    "## Qwen3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70aefe34",
   "metadata": {},
   "source": [
    "### Linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52d5bced",
   "metadata": {},
   "outputs": [],
   "source": [
    "def divide(numerator, denominator):\n",
    "    assert numerator % denominator == 0\n",
    "    return numerator // denominator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dde2ad6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearBase(nn.Module):\n",
    "    \"\"\"\n",
    "    ãƒ†ãƒ³ã‚½ãƒ«ä¸¦åˆ—åŒ–ï¼ˆTP, Tensor Parallelismï¼‰ã«å¯¾å¿œã—ãŸç·šå½¢å±¤ã®åŸºåº•ã‚¯ãƒ©ã‚¹\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_size: int,\n",
    "        output_size: int,\n",
    "        bias: bool = False,\n",
    "        tp_dim: int | None = None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input_size (int): å…¥åŠ›ã®æ¬¡å…ƒæ•°\n",
    "            output_size (int): å‡ºåŠ›ã®æ¬¡å…ƒæ•°\n",
    "            bias (bool): ãƒã‚¤ã‚¢ã‚¹é …ã®æœ‰ç„¡\n",
    "            tp_dim (int | None): ä¸¦åˆ—åŒ–ã™ã‚‹æ¬¡å…ƒ\n",
    "        \"\"\"\n",
    "        logger.info(f\"ç·šå½¢å±¤ã®åŸºåº•ã‚¯ãƒ©ã‚¹ã‚’åˆæœŸåŒ–é–‹å§‹ {input_size=}, {output_size=}, {bias=}, {tp_dim=}\")\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        # 1) ãƒ†ãƒ³ã‚½ãƒ«ä¸¦åˆ—åŒ–ã®è¨­å®š\n",
    "\n",
    "        self.tp_dim = tp_dim\n",
    "        self.tp_rank = dist.get_rank()\n",
    "        self.tp_size = dist.get_world_size()\n",
    "\n",
    "        # 2) é‡ã¿ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®åˆæœŸåŒ–\n",
    "\n",
    "        self.weight = nn.Parameter(torch.empty(output_size, input_size))\n",
    "\n",
    "        # 3) é‡ã¿ã®èª­ã¿è¾¼ã¿é–¢æ•°ã‚’è¨­å®š\n",
    "\n",
    "        self.weight.weight_loader = self.weight_loader\n",
    "\n",
    "        # ãƒã‚¤ã‚¢ã‚¹é …ãŒã‚ã‚‹å ´åˆ\n",
    "        if bias:\n",
    "\n",
    "            # ãƒã‚¤ã‚¢ã‚¹ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®åˆæœŸåŒ–\n",
    "            self.bias = nn.Parameter(torch.empty(output_size))\n",
    "\n",
    "            # ãƒã‚¤ã‚¢ã‚¹ã®èª­ã¿è¾¼ã¿é–¢æ•°ã‚’è¨­å®š\n",
    "            self.bias.weight_loader = self.weight_loader\n",
    "        \n",
    "        # ãƒã‚¤ã‚¢ã‚¹é …ãŒãªã„å ´åˆ\n",
    "        else:\n",
    "\n",
    "            # ãƒã‚¤ã‚¢ã‚¹ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’Noneã«è¨­å®š\n",
    "            self.register_parameter(\"bias\", None)\n",
    "\n",
    "        logger.info(f\"ç·šå½¢å±¤ã®åŸºåº•ã‚¯ãƒ©ã‚¹ã‚’åˆæœŸåŒ–å®Œäº†\")\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d9def2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplicatedLinear(LinearBase):\n",
    "    \"\"\"\n",
    "    ä¸¦åˆ—åŒ–ã—ãªã„ç·šå½¢å±¤ã§ã€å…¨ã¦ã®GPUã§åŒã˜é‡ã¿ã‚’æŒã¤\n",
    "    ä»Šå›ã¯ä½¿ã‚ãªã„\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_size: int,\n",
    "        output_size: int,\n",
    "        bias: bool = False,\n",
    "    ):\n",
    "        logger.info(f\"ä¸¦åˆ—åŒ–ãªã—ã®ç·šå½¢å±¤ã‚’åˆæœŸåŒ–é–‹å§‹ {input_size=}, {output_size=}, {bias=}\")\n",
    "\n",
    "        super().__init__(input_size, output_size, bias)\n",
    "\n",
    "        logger.info(f\"ä¸¦åˆ—åŒ–ãªã—ã®ç·šå½¢å±¤ã‚’åˆæœŸåŒ–å®Œäº†\")\n",
    "\n",
    "    def weight_loader(self, param: nn.Parameter, loaded_weight: torch.Tensor):\n",
    "        \"\"\"\n",
    "        ãƒ¢ãƒ‡ãƒ«ã®ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‹ã‚‰é‡ã¿ã‚’ãƒ­ãƒ¼ãƒ‰ã™ã‚‹\n",
    "        ã™ã¹ã¦ã®GPUã§åŒã˜é‡ã¿ã‚’ä½¿ç”¨ã™ã‚‹\n",
    "\n",
    "        Args:\n",
    "            param (nn.Parameter): ãƒ­ãƒ¼ãƒ‰å…ˆã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿\n",
    "            loaded_weight (torch.Tensor): ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‹ã‚‰ãƒ­ãƒ¼ãƒ‰ã—ãŸé‡ã¿ãƒ†ãƒ³ã‚½ãƒ«\n",
    "        \"\"\"\n",
    "        logger.info(f\"ä¸¦åˆ—åŒ–ãªã—ã®ç·šå½¢å±¤ã®é‡ã¿ã‚’ãƒ­ãƒ¼ãƒ‰é–‹å§‹ {param.shape=}, {loaded_weight.shape=}\")\n",
    "\n",
    "        param.data.copy_(loaded_weight)\n",
    "\n",
    "        logger.info(f\"ä¸¦åˆ—åŒ–ãªã—ã®ç·šå½¢å±¤ã®é‡ã¿ã‚’ãƒ­ãƒ¼ãƒ‰å®Œäº†\")\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        ä¸¦åˆ—åŒ–ãªã—ã®ç·šå½¢å±¤ã®é †ä¼æ’­\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): å…¥åŠ›ãƒ†ãƒ³ã‚½ãƒ«\n",
    "        Returns:\n",
    "            torch.Tensor: å‡ºåŠ›ãƒ†ãƒ³ã‚½ãƒ«\n",
    "        \"\"\"\n",
    "        logger.info(f\"ä¸¦åˆ—åŒ–ãªã—ã®ç·šå½¢å±¤ã®é †ä¼æ’­é–‹å§‹ {x.shape=}\")\n",
    "\n",
    "        result = F.linear(x, self.weight, self.bias)\n",
    "\n",
    "        logger.info(f\"ä¸¦åˆ—åŒ–ãªã—ã®ç·šå½¢å±¤ã®é †ä¼æ’­å®Œäº† {result.shape=}\")\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d61b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ColumnParallelLinear(LinearBase):\n",
    "    \"\"\"\n",
    "    é‡ã¿ã®åˆ—æ–¹å‘ï¼ˆå‡ºåŠ›æ¬¡å…ƒï¼‰ã‚’åˆ†å‰²ã—ã¦ãƒ†ãƒ³ã‚½ãƒ«ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤\n",
    "    å‡ºåŠ›ã¯GPUã”ã¨ã«ç•°ãªã‚Šã€å…¨ä½“ã®ä¸€éƒ¨ã—ã‹æŒãŸãªã„\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_size: int,\n",
    "        output_size: int,\n",
    "        bias: bool = False,\n",
    "    ):\n",
    "        logger.info(f\"å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã‚’åˆæœŸåŒ–é–‹å§‹ {input_size=}, {output_size=}, {bias=}\")\n",
    "\n",
    "        tp_size = dist.get_world_size()\n",
    "\n",
    "        # output_sizeã‚’GPUæ•°ã§åˆ†å‰²\n",
    "        super().__init__(input_size, divide(output_size, tp_size), bias, 0)\n",
    "\n",
    "        logger.info(f\"å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã‚’åˆæœŸåŒ–å®Œäº† {self.tp_size=}\")\n",
    "\n",
    "    def weight_loader(self, param: nn.Parameter, loaded_weight: torch.Tensor):\n",
    "        \"\"\"\n",
    "        ãƒ¢ãƒ‡ãƒ«ã®ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‹ã‚‰å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é‡ã¿ã‚’ãƒ­ãƒ¼ãƒ‰ã™ã‚‹\n",
    "\n",
    "        Args:\n",
    "            param (nn.Parameter): ãƒ­ãƒ¼ãƒ‰å…ˆã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿\n",
    "            loaded_weight (torch.Tensor): ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‹ã‚‰ãƒ­ãƒ¼ãƒ‰ã—ãŸé‡ã¿ãƒ†ãƒ³ã‚½ãƒ«\n",
    "        \"\"\"\n",
    "        logger.info(f\"å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿é–‹å§‹ {param.shape=}, {loaded_weight.shape=}\")\n",
    "\n",
    "        param_data = param.data\n",
    "\n",
    "        # åˆ†å‰²æ–¹å‘ï¼ˆdim=0ï¼‰ã®ã‚µã‚¤ã‚ºã‚’å–å¾—\n",
    "        shard_size = param_data.size(self.tp_dim)\n",
    "\n",
    "        # è‡ªåˆ†ã®æ‹…å½“ã™ã‚‹åˆ†å‰²ã®é–‹å§‹ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’è¨ˆç®—\n",
    "        start_idx = self.tp_rank * shard_size\n",
    "\n",
    "        # ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‹ã‚‰ãƒ­ãƒ¼ãƒ‰ã—ãŸé‡ã¿ã‚’ã‚¹ãƒ©ã‚¤ã‚¹ã—ã¦ã‚³ãƒ”ãƒ¼\n",
    "        loaded_weight = loaded_weight.narrow(\n",
    "            self.tp_dim, start_idx, shard_size)\n",
    "\n",
    "        param_data.copy_(loaded_weight)\n",
    "\n",
    "        logger.info(f\"å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿å®Œäº†\")\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        logger.info(f\"å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­é–‹å§‹ {x.shape=}\")\n",
    "\n",
    "        result = F.linear(x, self.weight, self.bias)\n",
    "\n",
    "        logger.info(f\"å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­å®Œäº† {result.shape=}\")\n",
    "        return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f5ea237",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MergedColumnParallelLinear(ColumnParallelLinear):\n",
    "    \"\"\"\n",
    "    è¤‡æ•°ã®ç·šå½¢å±¤ã‚’ä¸€ã¤ã«ã¾ã¨ã‚ã¦å‡ºåŠ›æ¬¡å…ƒã‚’ãƒ†ãƒ³ã‚½ãƒ«ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_size: int,\n",
    "        output_sizes: list[int], # å„ç·šå½¢å±¤ã®å‡ºåŠ›æ¬¡å…ƒã®ãƒªã‚¹ãƒˆ\n",
    "        bias: bool = False,\n",
    "    ):\n",
    "        logger.info(f\"MergedColumnParallelLinearã‚’åˆæœŸåŒ–é–‹å§‹ {input_size=}, {output_sizes=}, {bias=}\")\n",
    "\n",
    "        self.output_sizes = output_sizes\n",
    "        super().__init__(input_size, sum(output_sizes), bias)\n",
    "\n",
    "        logger.info(f\"MergedColumnParallelLinearã‚’åˆæœŸåŒ–å®Œäº†\")\n",
    "\n",
    "    def weight_loader(self, param: nn.Parameter, loaded_weight: torch.Tensor, loaded_shard_id: int):\n",
    "        \"\"\"\n",
    "        ãƒ¢ãƒ‡ãƒ«ã®ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‹ã‚‰MergedColumnParallelLinearã®é‡ã¿ã‚’ãƒ­ãƒ¼ãƒ‰ã™ã‚‹\n",
    "        Args:\n",
    "            param (nn.Parameter): ãƒ­ãƒ¼ãƒ‰å…ˆã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿\n",
    "            loaded_weight (torch.Tensor): ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‹ã‚‰ãƒ­ãƒ¼ãƒ‰ã—ãŸé‡ã¿ãƒ†ãƒ³ã‚½ãƒ«\n",
    "            loaded_shard_id (int): ãƒ­ãƒ¼ãƒ‰ã™ã‚‹ç·šå½¢å±¤ã®ID\n",
    "        \"\"\"\n",
    "        logger.info(f\"MergedColumnParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿é–‹å§‹ {param.shape=}, {loaded_weight.shape=}, {loaded_shard_id=}\")\n",
    "\n",
    "        param_data = param.data\n",
    "\n",
    "        # æ›¸ãè¾¼ã¿å…ˆã®ã‚ªãƒ•ã‚»ãƒƒãƒˆã‚’è¨ˆç®—\n",
    "        shard_offset = sum(self.output_sizes[:loaded_shard_id]) // self.tp_size\n",
    "\n",
    "        # æ›¸ãè¾¼ã‚€ã‚µã‚¤ã‚ºã‚’è¨ˆç®—\n",
    "        shard_size = self.output_sizes[loaded_shard_id] // self.tp_size\n",
    "\n",
    "        # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿å†…ã®æ›¸ãè¾¼ã¿ä½ç½®ã‚’ç‰¹å®š\n",
    "        param_data = param_data.narrow(self.tp_dim, shard_offset, shard_size)\n",
    "\n",
    "        # ãƒ­ãƒ¼ãƒ‰ã—ãŸé‡ã¿ã‚’åˆ†å‰²\n",
    "        loaded_weight = loaded_weight.chunk(self.tp_size, self.tp_dim)[self.tp_rank]\n",
    "\n",
    "        # é‡ã¿ã‚’ã‚³ãƒ”ãƒ¼\n",
    "        param_data.copy_(loaded_weight)\n",
    "\n",
    "        logger.info(f\"MergedColumnParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿å®Œäº†\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ffe5265",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QKVParallelLinear(ColumnParallelLinear):\n",
    "    \"\"\"\n",
    "    QKVã‚’ä¸€ã¤ã«ã¾ã¨ã‚ã¦å‡ºåŠ›æ¬¡å…ƒã‚’ãƒ†ãƒ³ã‚½ãƒ«ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        hidden_size: int,\n",
    "        head_size: int,\n",
    "        total_num_heads: int,\n",
    "        total_num_kv_heads: int | None = None,\n",
    "        bias: bool = False,\n",
    "    ):\n",
    "        logger.info(f\"QKVParallelLinearã‚’åˆæœŸåŒ–é–‹å§‹ {hidden_size=}, {head_size=}, {total_num_heads=}, {total_num_kv_heads=}, {bias=}\")\n",
    "\n",
    "        tp_size = dist.get_world_size()\n",
    "\n",
    "        total_num_kv_heads = total_num_kv_heads or total_num_heads\n",
    "\n",
    "        self.head_size = head_size\n",
    "\n",
    "        # å„GPUãŒæ‹…å½“ã™ã‚‹ãƒ˜ãƒƒãƒ‰æ•°ã‚’è¨ˆç®—\n",
    "        self.num_heads = divide(total_num_heads, tp_size)\n",
    "\n",
    "        # KVãƒ˜ãƒƒãƒ‰æ•°ã‚’è¨ˆç®—\n",
    "        self.num_kv_heads = divide(total_num_kv_heads, tp_size)\n",
    "\n",
    "        # å…¨ä½“ã®å‡ºåŠ›ã‚µã‚¤ã‚º = (Qãƒ˜ãƒƒãƒ‰æ•° + Kãƒ˜ãƒƒãƒ‰æ•° + Vãƒ˜ãƒƒãƒ‰æ•°) * ãƒ˜ãƒƒãƒ‰æ¬¡å…ƒ\n",
    "        output_size = (total_num_heads + 2 * total_num_kv_heads) * self.head_size\n",
    "\n",
    "        super().__init__(hidden_size, output_size, bias)\n",
    "\n",
    "        logger.info(f\"QKVParallelLinearã‚’åˆæœŸåŒ–å®Œäº† {self.tp_size=}, {self.num_heads=}, {self.num_kv_heads=}\")\n",
    "\n",
    "    def weight_loader(self, param: nn.Parameter, loaded_weight: torch.Tensor, loaded_shard_id: str):\n",
    "        logger.info(f\"QKVParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿é–‹å§‹ {param.shape=}, {loaded_weight.shape=}, {loaded_shard_id=}\")\n",
    "\n",
    "        param_data = param.data\n",
    "\n",
    "        assert loaded_shard_id in [\"q\", \"k\", \"v\"]\n",
    "\n",
    "        # Qã®å ´åˆ\n",
    "        if loaded_shard_id == \"q\":\n",
    "            shard_size = self.num_heads * self.head_size\n",
    "            shard_offset = 0\n",
    "\n",
    "        # Kã®å ´åˆ\n",
    "        elif loaded_shard_id == \"k\":\n",
    "            shard_size = self.num_kv_heads * self.head_size\n",
    "            shard_offset = self.num_heads * self.head_size\n",
    "\n",
    "        # Vã®å ´åˆ\n",
    "        else:\n",
    "            shard_size = self.num_kv_heads * self.head_size\n",
    "            shard_offset = self.num_heads * self.head_size + self.num_kv_heads * self.head_size\n",
    "\n",
    "        # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿å†…ã®æ›¸ãè¾¼ã¿ä½ç½®ã‚’ç‰¹å®š\n",
    "        param_data = param_data.narrow(self.tp_dim, shard_offset, shard_size)\n",
    "\n",
    "        # é‡ã¿ã‚’åˆ†å‰²ã—ã¦æ‹…å½“åˆ†ã‚’å–å¾—\n",
    "        loaded_weight = loaded_weight.chunk(self.tp_size, self.tp_dim)[self.tp_rank]\n",
    "\n",
    "        # é‡ã¿ã‚’ã‚³ãƒ”ãƒ¼\n",
    "        param_data.copy_(loaded_weight)\n",
    "\n",
    "        logger.info(f\"QKVParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿å®Œäº†\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3238b250",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RowParallelLinear(LinearBase):\n",
    "    \"\"\"\n",
    "    é‡ã¿ã®è¡Œæ–¹å‘ï¼ˆå…¥åŠ›æ¬¡å…ƒï¼‰ã‚’åˆ†å‰²ã—ã¦ãƒ†ãƒ³ã‚½ãƒ«ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤\n",
    "    ã™ã¹ã¦ã®GPUã§åŒã˜å‡ºåŠ›ã‚’æŒã¤\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_size: int,\n",
    "        output_size: int,\n",
    "        bias: bool = False,\n",
    "    ):\n",
    "        logger.info(f\"RowParallelLinearã‚’åˆæœŸåŒ–é–‹å§‹ {input_size=}, {output_size=}, {bias=}\")\n",
    "\n",
    "        tp_size = dist.get_world_size()\n",
    "\n",
    "        # input_sizeã‚’GPUæ•°ã§åˆ†å‰²\n",
    "        # é‡ã¿è¡Œåˆ—(out, in)ã®1æ¬¡å…ƒç›®ï¼ˆinå´ï¼‰ã‚’åˆ†å‰²\n",
    "        super().__init__(divide(input_size, tp_size), output_size, bias, 1)\n",
    "\n",
    "        logger.info(f\"RowParallelLinearã‚’åˆæœŸåŒ–å®Œäº† {self.tp_size=}\")\n",
    "\n",
    "    def weight_loader(self, param: nn.Parameter, loaded_weight: torch.Tensor):\n",
    "        logger.info(f\"RowParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿é–‹å§‹ {param.shape=}, {loaded_weight.shape=}\")\n",
    "\n",
    "        param_data = param.data\n",
    "\n",
    "        # åˆ†å‰²æ–¹å‘ï¼ˆdim=1ï¼‰ã®ã‚µã‚¤ã‚ºã‚’å–å¾—\n",
    "        shard_size = param_data.size(self.tp_dim)\n",
    "\n",
    "        start_idx = self.tp_rank * shard_size\n",
    "\n",
    "        # dim=1ï¼ˆåˆ—ï¼‰ã‚’ã‚¹ãƒ©ã‚¤ã‚¹ã—ã¦å–ã‚Šå‡ºã™\n",
    "        loaded_weight = loaded_weight.narrow(\n",
    "            self.tp_dim, start_idx, shard_size)\n",
    "\n",
    "        param_data.copy_(loaded_weight)\n",
    "\n",
    "        logger.info(f\"RowParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿å®Œäº†\")\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        logger.info(f\"RowParallelLinearã®é †ä¼æ’­é–‹å§‹ å…¥åŠ›å½¢çŠ¶: {x.shape}\")\n",
    "\n",
    "        # ãƒ­ãƒ¼ã‚«ãƒ«ã§ã®è¡Œåˆ—ç©\n",
    "        # ãƒ¡ã‚¤ãƒ³ãƒ—ãƒ­ã‚»ã‚¹ã®ã¿ãƒã‚¤ã‚¢ã‚¹ã‚’ä½¿ç”¨\n",
    "        y = F.linear(x, self.weight, self.bias if self.tp_rank == 0 else None)\n",
    "\n",
    "        # All-Reduceã§é›†ç´„\n",
    "        if self.tp_size > 1:\n",
    "            dist.all_reduce(y)\n",
    "\n",
    "        logger.info(f\"RowParallelLinearã®é †ä¼æ’­å®Œäº† å‡ºåŠ›å½¢çŠ¶: {y.shape}\")\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2366d582",
   "metadata": {},
   "source": [
    "### RMSNorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5694130",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSNorm(nn.Module):\n",
    "    \"\"\"\n",
    "    Root Mean Square Layer Normalizationã®å®Ÿè£…\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        hidden_size: int,\n",
    "        eps: float = 1e-6,\n",
    "    ) -> None:\n",
    "        logger.info(f\"RMSNormã‚’åˆæœŸåŒ– {hidden_size=}, {eps=}\")\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        # 1ã§åˆæœŸåŒ–\n",
    "        self.weight = nn.Parameter(torch.ones(hidden_size))\n",
    "        logger.info(f\"RMSNormã®åˆæœŸåŒ–å®Œäº†\")\n",
    "\n",
    "    @torch.compile\n",
    "    def rms_forward(\n",
    "        self,\n",
    "        x: torch.Tensor,\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        RMSNormã®æ­£è¦åŒ–\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): å…¥åŠ›ãƒ†ãƒ³ã‚½ãƒ«\n",
    "        Returns:\n",
    "            torch.Tensor: æ­£è¦åŒ–ã•ã‚ŒãŸå‡ºåŠ›ãƒ†ãƒ³ã‚½ãƒ«\n",
    "        \"\"\"\n",
    "        logger.info(f\"æ­£è¦åŒ–é–‹å§‹ {x.shape=}\")\n",
    "\n",
    "        orig_dtype = x.dtype\n",
    "\n",
    "        # ã‚¢ãƒƒãƒ—ã‚­ãƒ£ã‚¹ãƒˆ\n",
    "        x = x.float()\n",
    "\n",
    "        # åˆ†æ•£ã‚’è¨ˆç®—\n",
    "        var = x.pow(2).mean(dim=-1, keepdim=True)\n",
    "\n",
    "        # åˆ†æ•£ã®é€†å¹³æ–¹æ ¹ã‚’ä¹—ã˜ã¦æ­£è¦åŒ–\n",
    "        x.mul_(torch.rsqrt(var + self.eps))\n",
    "\n",
    "        # å…ƒã®ãƒ‡ãƒ¼ã‚¿å‹ã«æˆ»ã—ã€ã‚²ã‚¤ãƒ³ã‚’ä¹—ã˜ã‚‹\n",
    "        x = x.to(orig_dtype).mul_(self.weight)\n",
    "\n",
    "        logger.info(f\"æ­£è¦åŒ–å®Œäº† {x.shape=}\")\n",
    "        return x\n",
    "\n",
    "    @torch.compile\n",
    "    def add_rms_forward(\n",
    "        self,\n",
    "        x: torch.Tensor,\n",
    "        residual: torch.Tensor,\n",
    "    ) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        æ®‹å·®æ¥ç¶šã‚’ä¼´ã†RMSNormã®æ­£è¦åŒ–\n",
    "        \"\"\"\n",
    "        logger.info(f\"æ®‹å·®æ¥ç¶šã‚’é©ç”¨\")\n",
    "\n",
    "        orig_dtype = x.dtype\n",
    "\n",
    "        # ã‚¢ãƒƒãƒ—ã‚­ãƒ£ã‚¹ãƒˆã—ã¦æ®‹å·®æ¥ç¶šã‚’é©ç”¨\n",
    "        x = x.float().add_(residual.float())\n",
    "\n",
    "        # ãƒ€ã‚¦ãƒ³ã‚­ãƒ£ã‚¹ãƒˆ\n",
    "        residual = x.to(orig_dtype)\n",
    "\n",
    "        # åˆ†æ•£ã‚’è¨ˆç®—\n",
    "        var = x.pow(2).mean(dim=-1, keepdim=True)\n",
    "\n",
    "        # åˆ†æ•£ã®é€†å¹³æ–¹æ ¹ã‚’ä¹—ã˜ã¦æ­£è¦åŒ–\n",
    "        x.mul_(torch.rsqrt(var + self.eps))\n",
    "\n",
    "        # å…ƒã®ãƒ‡ãƒ¼ã‚¿å‹ã«æˆ»ã—ã€ã‚²ã‚¤ãƒ³ã‚’ä¹—ã˜ã‚‹\n",
    "        x = x.to(orig_dtype).mul_(self.weight)\n",
    "\n",
    "        logger.info(f\"æ®‹å·®æ¥ç¶šã‚’é©ç”¨å®Œäº†\")\n",
    "        return x, residual\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x: torch.Tensor,\n",
    "        residual: torch.Tensor | None = None,\n",
    "    ) -> torch.Tensor | tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        RMSNormã®é †ä¼æ’­\n",
    "        \"\"\"\n",
    "        logger.info(f\"RMSNormã®é †ä¼æ’­é–‹å§‹ {x.shape=}, {residual.shape if residual is not None else None=}\")\n",
    "\n",
    "        # æ®‹å·®æ¥ç¶šãŒãªã„å ´åˆ\n",
    "        if residual is None:\n",
    "            result = self.rms_forward(x)\n",
    "\n",
    "        # æ®‹å·®æ¥ç¶šãŒã‚ã‚‹å ´åˆ\n",
    "        else:\n",
    "            result = self.add_rms_forward(x, residual)\n",
    "\n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4b074c4",
   "metadata": {},
   "source": [
    "### Self-Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e66d175",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_rotary_emb(\n",
    "    x: torch.Tensor,\n",
    "    cos: torch.Tensor,\n",
    "    sin: torch.Tensor,\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    ãƒ™ã‚¯ãƒˆãƒ«ã‚’å›è»¢ã•ã›ã‚‹\n",
    "\n",
    "    Args:\n",
    "        x (torch.Tensor): å›è»¢ã•ã›ã‚‹ãƒ†ãƒ³ã‚½ãƒ«\n",
    "        cos (torch.Tensor): ã‚³ã‚µã‚¤ãƒ³æˆåˆ†\n",
    "        sin (torch.Tensor): ã‚µã‚¤ãƒ³æˆåˆ†\n",
    "    Returns:\n",
    "        torch.Tensor: å›è»¢å¾Œã®ãƒ†ãƒ³ã‚½ãƒ«\n",
    "    \"\"\"\n",
    "    logger.info(f\"RoPEã‚’é©ç”¨é–‹å§‹ {x.shape=}, {cos.shape=}, {sin.shape=}\")\n",
    "\n",
    "    # æ¬¡å…ƒåˆ†å‰²\n",
    "    x1, x2 = torch.chunk(x.float(), 2, dim=-1)\n",
    "    logger.debug(f\"ãƒãƒ£ãƒ³ã‚¯åŒ– {x1.shape=}, {x2.shape=}\")\n",
    "\n",
    "    # å›è»¢è¡Œåˆ—ã‚’é©ç”¨\n",
    "    y1 = x1 * cos - x2 * sin\n",
    "    y2 = x2 * cos + x1 * sin\n",
    "    logger.debug(f\"å›è»¢è¡Œåˆ—é©ç”¨ {y1.shape=}, {y2.shape=}\")\n",
    "\n",
    "    # çµåˆ\n",
    "    result = torch.cat((y1, y2), dim=-1).to(x.dtype)\n",
    "\n",
    "    logger.info(f\"RoPEã‚’é©ç”¨å®Œäº† {result.shape=}\")\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac30c6b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RotaryEmbedding(nn.Module):\n",
    "    \"\"\"\n",
    "    Rotary Positional Embedding (RoPE)ã®å®Ÿè£…\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        head_size: int,\n",
    "        rotary_dim: int,\n",
    "        max_position_embeddings: int,\n",
    "        base: float,\n",
    "    ) -> None:\n",
    "        logger.info(f\"RoPEã‚’åˆæœŸåŒ– {head_size=}, {rotary_dim=}, {max_position_embeddings=}, {base=}\")\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.head_size = head_size\n",
    "\n",
    "        assert rotary_dim == head_size\n",
    "\n",
    "        # é€†å‘¨æ³¢æ•°ã‚’è¨ˆç®—\n",
    "        # theta_i = 1 / (base ** (2i / d))\n",
    "        inv_freq = 1.0 / (base**(torch.arange(0, rotary_dim, 2, dtype=torch.float) / rotary_dim))\n",
    "\n",
    "        # ä½ç½®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ç”Ÿæˆ\n",
    "        t = torch.arange(max_position_embeddings, dtype=torch.float)\n",
    "\n",
    "        # ä½ç½®ã¨é€†å‘¨æ³¢æ•°ã®å¤–ç©ã‚’è¨ˆç®—\n",
    "        freqs = torch.einsum(\"i,j -> ij\", t, inv_freq)\n",
    "        logger.debug(f\"å‘¨æ³¢æ•°è¨ˆç®— {freqs.shape=}\")\n",
    "\n",
    "        # ã‚³ã‚µã‚¤ãƒ³ã¨ã‚µã‚¤ãƒ³ã‚’è¨ˆç®—\n",
    "        cos = freqs.cos()\n",
    "        sin = freqs.sin()\n",
    "\n",
    "        # ã‚³ã‚µã‚¤ãƒ³ã¨ã‚µã‚¤ãƒ³ã‚’çµåˆã—ã€ãƒãƒƒãƒæ¬¡å…ƒã‚’è¿½åŠ \n",
    "        cache = torch.cat((cos, sin), dim=-1).unsqueeze_(1)\n",
    "        logger.debug(f\"ã‚³ã‚µã‚¤ãƒ³ãƒ»ã‚µã‚¤ãƒ³ã‚­ãƒ£ãƒƒã‚·ãƒ¥è¨ˆç®— {cache.shape=}\")\n",
    "\n",
    "        # ãƒãƒƒãƒ•ã‚¡ã¨ã—ã¦ç™»éŒ²ï¼ˆstate_dictã«å«ã‚ãªã„ï¼‰\n",
    "        self.register_buffer(\"cos_sin_cache\", cache, persistent=False)\n",
    "\n",
    "        logger.info(f\"RoPEã®åˆæœŸåŒ–å®Œäº†\")\n",
    "\n",
    "    @torch.compile\n",
    "    def forward(\n",
    "        self,\n",
    "        positions: torch.Tensor,\n",
    "        query: torch.Tensor,\n",
    "        key: torch.Tensor,\n",
    "    ) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        é †ä¼æ’­ã‚’å®Ÿè¡Œ\n",
    "\n",
    "        Args:\n",
    "            positions (torch.Tensor): ä½ç½®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®ãƒ†ãƒ³ã‚½ãƒ«\n",
    "            query (torch.Tensor): ã‚¯ã‚¨ãƒªã®ãƒ†ãƒ³ã‚½ãƒ«\n",
    "            key (torch.Tensor): ã‚­ãƒ¼ã®ãƒ†ãƒ³ã‚½ãƒ«\n",
    "        \"\"\"\n",
    "\n",
    "        logger.info(f\"RoPEã®é †ä¼æ’­é–‹å§‹ {positions.shape=}, {query.shape=}, {key.shape=}\")\n",
    "\n",
    "        # ä½ç½®ã«å¯¾å¿œã™ã‚‹ã‚³ã‚µã‚¤ãƒ³ãƒ»ã‚µã‚¤ãƒ³ã‚’å–å¾—\n",
    "        cos_sin = self.cos_sin_cache[positions]\n",
    "\n",
    "        # ã‚³ã‚µã‚¤ãƒ³ã¨ã‚µã‚¤ãƒ³ã«åˆ†å‰²\n",
    "        cos, sin = cos_sin.chunk(2, dim=-1)\n",
    "\n",
    "        # ã‚¯ã‚¨ãƒªã¨ã‚­ãƒ¼ã«RoPEã‚’é©ç”¨\n",
    "        query = apply_rotary_emb(query, cos, sin)\n",
    "        key = apply_rotary_emb(key, cos, sin)\n",
    "\n",
    "        logger.info(f\"RoPEã®é †ä¼æ’­å®Œäº† {query.shape=}, {key.shape=}\")\n",
    "        return query, key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20cb8143",
   "metadata": {},
   "outputs": [],
   "source": [
    "@lru_cache(1) # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’æœ‰åŠ¹ã«ã™ã‚‹\n",
    "def get_rope(\n",
    "    head_size: int,\n",
    "    rotary_dim: int,\n",
    "    max_position: int,\n",
    "    base: float,\n",
    "    rope_scaling: dict | None = None,\n",
    "):\n",
    "    \"\"\"\n",
    "    RoPEã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’å–å¾—ã™ã‚‹ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£é–¢æ•°\n",
    "    \"\"\"\n",
    "\n",
    "    assert rope_scaling is None\n",
    "\n",
    "    # RoPEã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’ç”Ÿæˆ\n",
    "    rotary_emb = RotaryEmbedding(head_size, rotary_dim, max_position, base)\n",
    "\n",
    "    return rotary_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a05e29fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "@triton.jit\n",
    "def store_kvcache_kernel(\n",
    "    key_ptr,\n",
    "    key_stride,\n",
    "    value_ptr,\n",
    "    value_stride,\n",
    "    k_cache_ptr,\n",
    "    v_cache_ptr,\n",
    "    slot_mapping_ptr,\n",
    "    D: tl.constexpr,\n",
    "):\n",
    "    \"\"\"\n",
    "    è¨ˆç®—ã—ãŸã‚­ãƒ¼ã¨ãƒãƒªãƒ¥ãƒ¼ã‚’KVã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜ã™ã‚‹Tritonã‚«ãƒ¼ãƒãƒ«\n",
    "    ã‚¹ãƒ­ãƒƒãƒˆãƒãƒƒãƒ”ãƒ³ã‚°ã‚’å‚ç…§ã—ã¦é©åˆ‡ãªç‰©ç†ãƒ¡ãƒ¢ãƒªã«ã‚­ãƒ¼ã¨ãƒãƒªãƒ¥ãƒ¼ã‚’ä¿å­˜ã™ã‚‹\n",
    "\n",
    "    Args:\n",
    "        key_ptr: ã‚­ãƒ¼ã®ãƒã‚¤ãƒ³ã‚¿\n",
    "        key_stride: ã‚­ãƒ¼ã®ã‚¹ãƒˆãƒ©ã‚¤ãƒ‰\n",
    "        value_ptr: ãƒãƒªãƒ¥ãƒ¼ã®ãƒã‚¤ãƒ³ã‚¿\n",
    "        value_stride: ãƒãƒªãƒ¥ãƒ¼ã®ã‚¹ãƒˆãƒ©ã‚¤ãƒ‰\n",
    "        k_cache_ptr: ã‚­ãƒ¼ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã®å…ˆé ­ãƒã‚¤ãƒ³ã‚¿\n",
    "        v_cache_ptr: ãƒãƒªãƒ¥ãƒ¼ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã®å…ˆé ­ãƒã‚¤ãƒ³ã‚¿\n",
    "        slot_mapping_ptr: ã‚¹ãƒ­ãƒƒãƒˆãƒãƒƒãƒ”ãƒ³ã‚°ã®ãƒã‚¤ãƒ³ã‚¿\n",
    "        D: ã‚­ãƒ¼ã¨ãƒãƒªãƒ¥ãƒ¼ã®æ¬¡å…ƒæ•°\n",
    "    \"\"\"\n",
    "\n",
    "    # 1) æ‹…å½“ã™ã‚‹ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’å–å¾—\n",
    "\n",
    "    # æ‹…å½“ã™ã‚‹ãƒˆãƒ¼ã‚¯ãƒ³ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’å–å¾—\n",
    "    idx = tl.program_id(0)\n",
    "\n",
    "    # 2) ä¿å­˜å…ˆã‚¹ãƒ­ãƒƒãƒˆã‚’ç‰¹å®š\n",
    "\n",
    "    # ã‚¹ãƒ­ãƒƒãƒˆãƒãƒƒãƒ”ãƒ³ã‚°ã‹ã‚‰ãƒˆãƒ¼ã‚¯ãƒ³ã«å¯¾å¿œã™ã‚‹ç‰©ç†ã‚¹ãƒ­ãƒƒãƒˆã‚’å–å¾—\n",
    "    slot = tl.load(slot_mapping_ptr + idx)\n",
    "\n",
    "    # å‰²ã‚Šå½“ã¦ãŒãªã„å ´åˆã¯ä½•ã‚‚ã—ãªã„\n",
    "    if slot == -1: return\n",
    "\n",
    "    # 3) å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿\n",
    "\n",
    "    # ã‚­ãƒ¼ã®èª­ã¿è¾¼ã¿ã‚ªãƒ•ã‚»ãƒƒãƒˆã‚’ä½œæˆï¼ˆDæ¬¡å…ƒåˆ†ï¼‰\n",
    "    key_offsets = idx * key_stride + tl.arange(0, D)\n",
    "\n",
    "    # ãƒãƒªãƒ¥ãƒ¼ã®èª­ã¿è¾¼ã¿ã‚ªãƒ•ã‚»ãƒƒãƒˆã‚’ä½œæˆï¼ˆDæ¬¡å…ƒåˆ†ï¼‰\n",
    "    value_offsets = idx * value_stride + tl.arange(0, D)\n",
    "\n",
    "    # å˜ä¸€ã®ã‚­ãƒ¼ã‚’èª­ã¿è¾¼ã¿\n",
    "    key = tl.load(key_ptr + key_offsets)\n",
    "\n",
    "    # å˜ä¸€ã®ãƒãƒªãƒ¥ãƒ¼ã‚’èª­ã¿è¾¼ã¿\n",
    "    value = tl.load(value_ptr + value_offsets)\n",
    "\n",
    "    # 4) KVã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«æ›¸ãè¾¼ã¿\n",
    "\n",
    "    # KVã‚­ãƒ£ãƒƒã‚·ãƒ¥ã®æ›¸ãè¾¼ã¿ã‚ªãƒ•ã‚»ãƒƒãƒˆã‚’ä½œæˆï¼ˆDæ¬¡å…ƒåˆ†ï¼‰\n",
    "    cache_offsets = slot * D + tl.arange(0, D)\n",
    "\n",
    "    # å˜ä¸€ã®ã‚­ãƒ¼ã‚’KVã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«æ›¸ãè¾¼ã¿\n",
    "    tl.store(k_cache_ptr + cache_offsets, key)\n",
    "\n",
    "    # å˜ä¸€ã®ãƒãƒªãƒ¥ãƒ¼ã‚’KVã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«æ›¸ãè¾¼ã¿\n",
    "    tl.store(v_cache_ptr + cache_offsets, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2cb0ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_kvcache(key: torch.Tensor, value: torch.Tensor, k_cache: torch.Tensor, v_cache: torch.Tensor, slot_mapping: torch.Tensor):\n",
    "    \"\"\"\n",
    "    è¨ˆç®—ã—ãŸã‚­ãƒ¼ã¨ãƒãƒªãƒ¥ãƒ¼ã‚’KVã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜ã™ã‚‹\n",
    "    store_kvcache_kernelã®ãƒ©ãƒƒãƒ‘ãƒ¼\n",
    "\n",
    "    Args:\n",
    "        key (torch.Tensor): è¨ˆç®—ã—ãŸã‚­ãƒ¼ã®ãƒ†ãƒ³ã‚½ãƒ«\n",
    "        value (torch.Tensor): è¨ˆç®—ã—ãŸãƒãƒªãƒ¥ãƒ¼ã®ãƒ†ãƒ³ã‚½ãƒ«\n",
    "        k_cache (torch.Tensor): ã‚­ãƒ¼ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã®ãƒ†ãƒ³ã‚½ãƒ«\n",
    "        v_cache (torch.Tensor): ãƒãƒªãƒ¥ãƒ¼ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã®ãƒ†ãƒ³ã‚½ãƒ«\n",
    "        slot_mapping (torch.Tensor): ã‚¹ãƒ­ãƒƒãƒˆãƒãƒƒãƒ”ãƒ³ã‚°ã®ãƒ†ãƒ³ã‚½ãƒ«\n",
    "    \"\"\"\n",
    "    logger.info(f\"KVã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ã‚­ãƒ¼ã¨ãƒãƒªãƒ¥ãƒ¼ã‚’ä¿å­˜é–‹å§‹ {key.shape=}, {value.shape=}, {k_cache.shape=}, {v_cache.shape=}, {slot_mapping.shape=}\")\n",
    "\n",
    "    # 1) å…¥åŠ›ã‚’æ¤œè¨¼\n",
    "\n",
    "    # ãƒˆãƒ¼ã‚¯ãƒ³æ•°Nã€ãƒ˜ãƒƒãƒ‰æ•°num_headsã€ãƒ˜ãƒƒãƒ‰æ¬¡å…ƒhead_dim\n",
    "    N, num_heads, head_dim = key.shape\n",
    "\n",
    "    # ã‚­ãƒ¼ã¨ãƒãƒªãƒ¥ãƒ¼ã®æ¬¡å…ƒæ•°D\n",
    "    D = num_heads * head_dim\n",
    "\n",
    "    assert key.stride(-1) == 1 and value.stride(-1) == 1\n",
    "    assert key.stride(1) == head_dim and value.stride(1) == head_dim\n",
    "    assert k_cache.stride(1) == D and v_cache.stride(1) == D\n",
    "    assert slot_mapping.numel() == N\n",
    "\n",
    "    # 2) Tritonã‚«ãƒ¼ãƒãƒ«ã‚’èµ·å‹•\n",
    "    store_kvcache_kernel[(N,)](\n",
    "        key,\n",
    "        key.stride(0),\n",
    "        value,\n",
    "        value.stride(0),\n",
    "        k_cache, v_cache,\n",
    "        slot_mapping,\n",
    "        D)\n",
    "\n",
    "    logger.info(f\"KVã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ã‚­ãƒ¼ã¨ãƒãƒªãƒ¥ãƒ¼ã‚’ä¿å­˜å®Œäº†\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c772ecdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    \"\"\"\n",
    "    FlashAttentionã®ãƒ©ãƒƒãƒ‘ãƒ¼\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_heads,\n",
    "        head_dim,\n",
    "        scale,\n",
    "        num_kv_heads,\n",
    "    ):\n",
    "        logger.info(f\"Attentionå±¤ã‚’åˆæœŸåŒ– {num_heads=}, {head_dim=}, {scale=}, {num_kv_heads=}\")\n",
    "        super().__init__()\n",
    "\n",
    "        # ãƒ˜ãƒƒãƒ‰æ•°\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        # ãƒ˜ãƒƒãƒ‰ã®æ¬¡å…ƒæ•°\n",
    "        self.head_dim = head_dim\n",
    "\n",
    "        # QKã®ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ä¿‚æ•°\n",
    "        self.scale = scale\n",
    "\n",
    "        # KVãƒ˜ãƒƒãƒ‰æ•°ï¼ˆGQAç”¨ï¼‰\n",
    "        self.num_kv_heads = num_kv_heads\n",
    "\n",
    "        # KVã‚­ãƒ£ãƒƒã‚·ãƒ¥é ˜åŸŸã¸ã®å‚ç…§ã‚’åˆæœŸåŒ–\n",
    "        self.k_cache = self.v_cache = torch.tensor([])\n",
    "\n",
    "        logger.info(f\"Attentionå±¤ã‚’åˆæœŸåŒ–å®Œäº†\")\n",
    "\n",
    "    def forward(self, q: torch.Tensor, k: torch.Tensor, v: torch.Tensor):\n",
    "        \"\"\"\n",
    "        FlashAttentionã®é †ä¼æ’­\n",
    "\n",
    "        Args:\n",
    "            q (torch.Tensor): è¨ˆç®—ã—ãŸã°ã‹ã‚Šã®ã‚¯ã‚¨ãƒª\n",
    "            k (torch.Tensor): è¨ˆç®—ã—ãŸã°ã‹ã‚Šã®ã‚­ãƒ¼\n",
    "            v (torch.Tensor): è¨ˆç®—ã—ãŸã°ã‹ã‚Šã®ãƒãƒªãƒ¥ãƒ¼\n",
    "        Returns:\n",
    "            torch.Tensor: ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³\n",
    "        \"\"\"\n",
    "        logger.info(f\"Attentionå±¤ã®é †ä¼æ’­é–‹å§‹ {q.shape=}, {k.shape=}, {v.shape=}\")\n",
    "\n",
    "        # 1) ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’å–å¾—\n",
    "\n",
    "        context = get_context()\n",
    "\n",
    "        logger.debug(f\"ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæƒ…å ±å–å¾— {context.is_prefill=}, {context.slot_mapping=}, {context.max_seqlen_q=}, {context.cu_seqlens_q=}, {context.max_seqlen_k=}, {context.cu_seqlens_k=}, {context.context_lens=}, {context.block_tables=}\")\n",
    "\n",
    "        # 2) KVã‚­ãƒ£ãƒƒã‚·ãƒ¥ã¸ã®ä¿å­˜\n",
    "\n",
    "        # KVã‚­ãƒ£ãƒƒã‚·ãƒ¥ã¸ã®å‚ç…§ã‚’å–å¾—\n",
    "        k_cache, v_cache = self.k_cache, self.v_cache\n",
    "\n",
    "        # KVã‚­ãƒ£ãƒƒã‚·ãƒ¥ãŒå­˜åœ¨ã™ã‚‹å ´åˆ\n",
    "        if k_cache.numel() and v_cache.numel():\n",
    "\n",
    "            # KVã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ã‚­ãƒ¼ã¨ãƒãƒªãƒ¥ãƒ¼ã‚’ä¿å­˜\n",
    "            store_kvcache(k, v, k_cache, v_cache, context.slot_mapping)\n",
    "\n",
    "        # 3) FlashAttentionã®å®Ÿè¡Œ\n",
    "\n",
    "        # Prefillã®å ´åˆ\n",
    "        if context.is_prefill:\n",
    "\n",
    "            # PrefixCachingã®å ´åˆ\n",
    "            if context.block_tables is not None:\n",
    "\n",
    "                # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰ã‚­ãƒ¼ã¨ãƒãƒªãƒ¥ãƒ¼ã‚’å–å¾—\n",
    "                k, v = k_cache, v_cache\n",
    "\n",
    "            logger.debug(f\"Prefillãƒ¢ãƒ¼ãƒ‰ã§FlashAttentionã‚’å®Ÿè¡Œ {context.max_seqlen_q=}, {context.cu_seqlens_q=}, {context.max_seqlen_k=}, {context.cu_seqlens_k=}\")\n",
    "\n",
    "            # å¯å¤‰é•·ã«å¯¾å¿œã—ãŸFlashAttentionã‚’å®Ÿè¡Œ\n",
    "            o = flash_attn_varlen_func(\n",
    "                q,\n",
    "                k,\n",
    "                v,\n",
    "                max_seqlen_q=context.max_seqlen_q,\n",
    "                cu_seqlens_q=context.cu_seqlens_q,\n",
    "                max_seqlen_k=context.max_seqlen_k,\n",
    "                cu_seqlens_k=context.cu_seqlens_k,\n",
    "                softmax_scale=self.scale,\n",
    "                causal=True,\n",
    "                block_table=context.block_tables)\n",
    "        \n",
    "        # Decodeã®å ´åˆ\n",
    "        else:\n",
    "\n",
    "            logger.debug(f\"Decodeãƒ¢ãƒ¼ãƒ‰ã§FlashAttentionã‚’å®Ÿè¡Œ {context.context_lens=}, {context.block_tables=}\")\n",
    "\n",
    "            # KVã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ç”¨ã„ãŸFlashAttentionã‚’å®Ÿè¡Œ\n",
    "            o = flash_attn_with_kvcache(\n",
    "                q.unsqueeze(1),\n",
    "                k_cache,\n",
    "                v_cache,\n",
    "                cache_seqlens=context.context_lens,\n",
    "                block_table=context.block_tables, \n",
    "                softmax_scale=self.scale,\n",
    "                causal=True)\n",
    "\n",
    "        logger.info(f\"Attentionå±¤ã®é †ä¼æ’­å®Œäº† {o.shape=}\")\n",
    "\n",
    "        return o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cf9e102",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Qwen3Attention(nn.Module):\n",
    "    \"\"\"\n",
    "    Qwen3ã®Self-Attentionã‚’å®Ÿè£…ã—ãŸã‚¯ãƒ©ã‚¹\n",
    "    QK-Normã¨RoPEã‚’ã‚µãƒãƒ¼ãƒˆ\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        hidden_size: int,\n",
    "        num_heads: int,\n",
    "        num_kv_heads: int,\n",
    "        max_position: int = 4096 * 32,\n",
    "        head_dim: int | None = None,\n",
    "        rms_norm_eps: float = 1e-06,\n",
    "        qkv_bias: bool = False,\n",
    "        rope_theta: float = 10000,\n",
    "        rope_scaling: tuple | None = None,\n",
    "    ) -> None:\n",
    "\n",
    "        logger.info(f\"Qwen3Attentionã‚’åˆæœŸåŒ–é–‹å§‹ hidden_size: {hidden_size}, num_heads: {num_heads}, num_kv_heads: {num_kv_heads}, max_position: {max_position}, head_dim: {head_dim}, rms_norm_eps: {rms_norm_eps}, qkv_bias: {qkv_bias}, rope_theta: {rope_theta}, rope_scaling: {rope_scaling}\")\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        tp_size = dist.get_world_size()\n",
    "\n",
    "        self.total_num_heads = num_heads\n",
    "        assert self.total_num_heads % tp_size == 0\n",
    "        self.num_heads = self.total_num_heads // tp_size\n",
    "        self.total_num_kv_heads = num_kv_heads\n",
    "        assert self.total_num_kv_heads % tp_size == 0\n",
    "        self.num_kv_heads = self.total_num_kv_heads // tp_size\n",
    "        self.head_dim = head_dim or hidden_size // self.total_num_heads\n",
    "        self.q_size = self.num_heads * self.head_dim\n",
    "        self.kv_size = self.num_kv_heads * self.head_dim\n",
    "        self.scaling = self.head_dim ** -0.5\n",
    "        self.qkv_bias = qkv_bias\n",
    "\n",
    "        # å…¥åŠ›ã‚’ã‚¯ã‚¨ãƒªã€ã‚­ãƒ¼ã€ãƒãƒªãƒ¥ãƒ¼ã«å¤‰æ›ã™ã‚‹ç·šå½¢å±¤\n",
    "        # QKVã¯çµåˆã•ã‚Œã€GPUé–“ã§åˆ—ä¸¦åˆ—ã«åˆ†å‰²ã•ã‚Œã¦ã„ã‚‹\n",
    "        self.qkv_proj = QKVParallelLinear(\n",
    "            hidden_size,\n",
    "            self.head_dim,\n",
    "            self.total_num_heads,\n",
    "            self.total_num_kv_heads,\n",
    "            bias=qkv_bias,\n",
    "        )\n",
    "\n",
    "        # ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã®å‡ºåŠ›ã‚’å…ƒã®éš ã‚Œå±¤ã‚µã‚¤ã‚ºã«æˆ»ã™ç·šå½¢å±¤\n",
    "        # è¡Œä¸¦åˆ—ã§ã€All-Reduceé€šä¿¡ã‚’ä½¿ã£ã¦GPUé–“ã®å‡ºåŠ›ã‚’é›†ç´„ã™ã‚‹\n",
    "        self.o_proj = RowParallelLinear(\n",
    "            self.total_num_heads * self.head_dim,\n",
    "            hidden_size,\n",
    "            bias=False,\n",
    "        )\n",
    "\n",
    "        # RoPEã®åˆæœŸåŒ–\n",
    "        self.rotary_emb = get_rope(\n",
    "            self.head_dim,\n",
    "            rotary_dim=self.head_dim,\n",
    "            max_position=max_position,\n",
    "            base=rope_theta,\n",
    "            rope_scaling=rope_scaling,\n",
    "        )\n",
    "\n",
    "        # ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³æ©Ÿæ§‹ã®åˆæœŸåŒ–\n",
    "        self.attn = Attention(\n",
    "            self.num_heads,\n",
    "            self.head_dim,\n",
    "            self.scaling,\n",
    "            self.num_kv_heads,\n",
    "        )\n",
    "\n",
    "        # QK-Normã®åˆæœŸåŒ–\n",
    "        # ã‚¯ã‚¨ãƒªã¨ã‚­ãƒ¼ã®å†…ç©ã®å‰ã«æ­£è¦åŒ–ã‚’è¡Œã†\n",
    "        if not self.qkv_bias:\n",
    "            self.q_norm = RMSNorm(self.head_dim, eps=rms_norm_eps)\n",
    "            self.k_norm = RMSNorm(self.head_dim, eps=rms_norm_eps)\n",
    "\n",
    "        logger.info(f\"Qwen3Attentionã®åˆæœŸåŒ–å®Œäº†\")\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        positions: torch.Tensor,\n",
    "        hidden_states: torch.Tensor,\n",
    "    ) -> torch.Tensor:\n",
    "        logger.info(f\"Qwen3Attentionã®é †ä¼æ’­ {positions.shape=} {hidden_states.shape=}\")\n",
    "\n",
    "        # å…¥åŠ›ã‹ã‚‰QKVã«å¤‰æ›\n",
    "        qkv = self.qkv_proj(hidden_states)\n",
    "\n",
    "        # QKVã‚’åˆ†å‰²\n",
    "        q, k, v = qkv.split([self.q_size, self.kv_size, self.kv_size], dim=-1)\n",
    "\n",
    "        # QKVã®å½¢çŠ¶ã‚’èª¿æ•´\n",
    "        q = q.view(-1, self.num_heads, self.head_dim)\n",
    "        k = k.view(-1, self.num_kv_heads, self.head_dim)\n",
    "        v = v.view(-1, self.num_kv_heads, self.head_dim)\n",
    "\n",
    "        # ãƒã‚¤ã‚¢ã‚¹é …ãŒãªã„å ´åˆã€QK-Normã‚’é©ç”¨ \n",
    "        if not self.qkv_bias:\n",
    "            q = self.q_norm(q)\n",
    "            k = self.k_norm(k)\n",
    "\n",
    "        # RoPEã‚’é©ç”¨\n",
    "        q, k = self.rotary_emb(positions, q, k)\n",
    "\n",
    "        # ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®—\n",
    "        o = self.attn(q, k, v)\n",
    "\n",
    "        # å‡ºåŠ›ã‚’å…ƒã®å½¢çŠ¶ã«æˆ»ã—ã€ç·šå½¢å¤‰æ›ã‚’é©ç”¨\n",
    "        output = self.o_proj(o.flatten(1, -1))\n",
    "\n",
    "        logger.info(f\"Qwen3Attentionã®é †ä¼æ’­å®Œäº† {output.shape=}\")\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d3980bd",
   "metadata": {},
   "source": [
    "### Qwen3MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9782f1b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiluAndMul(nn.Module):\n",
    "    \"\"\"\n",
    "    SwiGLUï¼ˆSwish-Gated Linear Unitï¼‰æ´»æ€§åŒ–é–¢æ•°ã®å®Ÿè£…\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        logger.info(f\"SwiGLUã‚’åˆæœŸåŒ–\")\n",
    "        super().__init__()\n",
    "\n",
    "    @torch.compile\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        logger.info(f\"SwiGLUã‚’é †ä¼æ’­é–‹å§‹ {x.shape=}\")\n",
    "\n",
    "        # å…¥åŠ›ã‚’2ã¤ã®ãƒãƒ£ãƒ³ã‚¯ã«åˆ†å‰²\n",
    "        x, y = x.chunk(2, -1)\n",
    "        logger.debug(f\"ãƒãƒ£ãƒ³ã‚¯åŒ– {x.shape=}, {y.shape=}\")\n",
    "\n",
    "        # SwiGLUæ´»æ€§åŒ–é–¢æ•°ã®é©ç”¨\n",
    "        result = F.silu(x) * y\n",
    "\n",
    "        logger.info(f\"SwiGLUã‚’é †ä¼æ’­å®Œäº† {result.shape=}\")\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11aa33f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Qwen3MLP(nn.Module):\n",
    "    \"\"\"\n",
    "    SwiGLUã‚’æ¡ç”¨ã—ã€åˆ†æ•£ä¸¦åˆ—å‡¦ç†ï¼ˆTensor Parallelismï¼‰ã«å¯¾å¿œã—ãŸQwen3ã®MLPå±¤\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        hidden_size: int,\n",
    "        intermediate_size: int,\n",
    "        hidden_act: str,\n",
    "    ) -> None:\n",
    "        logger.info(f\"Qwen3MLPã‚’åˆæœŸåŒ–é–‹å§‹ {hidden_size=} {intermediate_size=} {hidden_act=}\")\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        # Gateå±¤ã¨Upå±¤ã‚’å˜ä¸€ã®è¡Œåˆ—ã«çµåˆã—ãŸç·šå½¢å±¤\n",
    "        # åˆ—ä¸¦åˆ—åŒ–ã•ã‚Œã¦ã„ã‚‹ãŸã‚ã€GPUé–“ã§å‡ºåŠ›ãŒåˆ†å‰²ã•ã‚Œã‚‹\n",
    "        self.gate_up_proj = MergedColumnParallelLinear(\n",
    "            hidden_size,\n",
    "            [intermediate_size] * 2, # [Gate, Up]\n",
    "            bias=False,\n",
    "        )\n",
    "\n",
    "        # Downå±¤ã®ç·šå½¢å±¤\n",
    "        # è¡Œä¸¦åˆ—åŒ–ã•ã‚Œã¦ãŠã‚Šã€GPUé–“ã§å‡ºåŠ›ãŒé›†ç´„ã•ã‚Œã‚‹\n",
    "        self.down_proj = RowParallelLinear(\n",
    "            intermediate_size,\n",
    "            hidden_size,\n",
    "            bias=False,\n",
    "        )\n",
    "        \n",
    "        # SiLUæ´»æ€§åŒ–é–¢æ•°\n",
    "        assert hidden_act == \"silu\"\n",
    "        self.act_fn = SiluAndMul()\n",
    "\n",
    "        logger.info(f\"Qwen3MLPã®åˆæœŸåŒ–å®Œäº†\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        logger.info(f\"Qwen3MLPã®é †ä¼æ’­ {x.shape=}\")\n",
    "\n",
    "        # Gateã¨Upã®ç·šå½¢å¤‰æ›ã‚’é©ç”¨\n",
    "        gate_up = self.gate_up_proj(x)\n",
    "\n",
    "        # SiLUæ´»æ€§åŒ–é–¢æ•°ã‚’é©ç”¨\n",
    "        x = self.act_fn(gate_up)\n",
    "\n",
    "        # Downã®ç·šå½¢å¤‰æ›ã‚’é©ç”¨\n",
    "        x = self.down_proj(x)\n",
    "\n",
    "        logger.info(f\"Qwen3MLPã®é †ä¼æ’­å®Œäº† {x.shape=}\")\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "271ee2ce",
   "metadata": {},
   "source": [
    "### Qwen3DecoderLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d2c5c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Qwen3DecoderLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Transformerã®ãƒ‡ã‚³ãƒ¼ãƒ€ãƒ¬ã‚¤ãƒ¤ãƒ¼ã‚’å®Ÿè£…ã—ãŸã‚¯ãƒ©ã‚¹\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        config: Qwen3Config,\n",
    "    ) -> None:\n",
    "        logger.info(f\"Qwen3DecoderLayerã‚’åˆæœŸåŒ–é–‹å§‹ {config.hidden_size=} {config.num_attention_heads=} {config.num_key_value_heads=} {config.intermediate_size=} {config.rms_norm_eps=} {config.hidden_act=}\")\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        # Self-Attentionã®åˆæœŸåŒ–\n",
    "        self.self_attn = Qwen3Attention(\n",
    "            hidden_size=config.hidden_size,\n",
    "            num_heads=config.num_attention_heads,\n",
    "            num_kv_heads=config.num_key_value_heads,\n",
    "            max_position=config.max_position_embeddings,\n",
    "            rms_norm_eps=config.rms_norm_eps,\n",
    "            qkv_bias=getattr(config, 'attention_bias', True),\n",
    "            head_dim=getattr(config, 'head_dim', None),\n",
    "            rope_theta=getattr(config, \"rope_theta\", 1000000),\n",
    "            rope_scaling=getattr(config, \"rope_scaling\", None),\n",
    "        )\n",
    "\n",
    "        # MLPã®åˆæœŸåŒ–\n",
    "        self.mlp = Qwen3MLP(\n",
    "            hidden_size=config.hidden_size,\n",
    "            intermediate_size=config.intermediate_size,\n",
    "            hidden_act=config.hidden_act,\n",
    "        )\n",
    "\n",
    "        # ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã®å‰ã®ãƒ¬ã‚¤ãƒ¤ãƒ¼æ­£è¦åŒ–\n",
    "        self.input_layernorm = RMSNorm(\n",
    "            config.hidden_size, eps=config.rms_norm_eps)\n",
    "\n",
    "        # ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã®å¾Œã®ãƒ¬ã‚¤ãƒ¤ãƒ¼æ­£è¦åŒ–\n",
    "        self.post_attention_layernorm = RMSNorm(\n",
    "            config.hidden_size, eps=config.rms_norm_eps)\n",
    "\n",
    "        logger.info(f\"Qwen3DecoderLayerã®åˆæœŸåŒ–å®Œäº†\")\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        positions: torch.Tensor,\n",
    "        hidden_states: torch.Tensor,\n",
    "        residual: torch.Tensor | None,\n",
    "    ) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        logger.info(f\"Qwen3DecoderLayerã®é †ä¼æ’­ {positions.shape=} {hidden_states.shape=} {residual is None=}\")\n",
    "\n",
    "        # 1) ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³å‰ã®ãƒ¬ã‚¤ãƒ¤ãƒ¼æ­£è¦åŒ–ã¨æ®‹å·®æ¥ç¶š\n",
    "\n",
    "        # å‰ã®å±¤ã®æ®‹å·®æ¥ç¶šãŒãªã„å ´åˆ\n",
    "        if residual is None:\n",
    "\n",
    "            # ãƒ¬ã‚¤ãƒ¤ãƒ¼æ­£è¦åŒ–ã‚’é©ç”¨ã—ã€æ®‹å·®ã‚’è¨­å®š\n",
    "            hidden_states, residual = self.input_layernorm(hidden_states), hidden_states\n",
    "\n",
    "        # å‰ã®å±¤ã®æ®‹å·®æ¥ç¶šãŒã‚ã‚‹å ´åˆ\n",
    "        else:\n",
    "\n",
    "            # ãƒ¬ã‚¤ãƒ¤ãƒ¼æ­£è¦åŒ–ã‚’é©ç”¨ã—ã€æ®‹å·®ã‚’æ›´æ–°\n",
    "            hidden_states, residual = self.input_layernorm(\n",
    "                hidden_states, residual)\n",
    "\n",
    "        # 2) ã‚»ãƒ«ãƒ•ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã®é©ç”¨\n",
    "\n",
    "        hidden_states = self.self_attn(positions, hidden_states)\n",
    "\n",
    "        # 3) ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³å¾Œã®ãƒ¬ã‚¤ãƒ¤ãƒ¼æ­£è¦åŒ–ã¨æ®‹å·®æ¥ç¶š\n",
    "\n",
    "        hidden_states, residual = \\\n",
    "            self.post_attention_layernorm(hidden_states, residual)\n",
    "\n",
    "        # 4) MLPã®é©ç”¨\n",
    "\n",
    "        hidden_states = self.mlp(hidden_states)\n",
    "\n",
    "        logger.info(f\"Qwen3DecoderLayerã®é †ä¼æ’­å®Œäº† {hidden_states.shape=}\")\n",
    "\n",
    "        return hidden_states, residual"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30e19177",
   "metadata": {},
   "source": [
    "### Qwen3Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca91b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VocabParallelEmbedding(nn.Module):\n",
    "    \"\"\"\n",
    "    å…¥åŠ›ã®åŸ‹ã‚è¾¼ã¿å±¤ã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ã‚¯ãƒ©ã‚¹ï¼ˆTensor Parallelismï¼‰\n",
    "    èªå½™ã‚’è¤‡æ•°ã®GPUã«åˆ†å‰²ã—ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’å‰Šæ¸›ã—ã€ä¸¦åˆ—è¨ˆç®—ã‚’å¯èƒ½ã«ã™ã‚‹\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_embeddings: int,\n",
    "        embedding_dim: int,\n",
    "    ):\n",
    "        logger.info(f\"å…¥åŠ›ã®åŸ‹ã‚è¾¼ã¿å±¤ã‚’åˆæœŸåŒ–é–‹å§‹ {num_embeddings=}, {embedding_dim=}\")\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        # 1) åˆ†æ•£ç’°å¢ƒã®æƒ…å ±ã‚’å–å¾—\n",
    "\n",
    "        # æ‹…å½“ã™ã‚‹GPU\n",
    "        self.tp_rank = dist.get_rank()\n",
    "\n",
    "        # å…¨GPUæ•°\n",
    "        self.tp_size = dist.get_world_size()\n",
    "\n",
    "        logger.debug(f\"åˆ†æ•£ç’°å¢ƒæƒ…å ±å–å¾— {self.tp_rank=}, {self.tp_size=}\")\n",
    "\n",
    "        # 2) æ‹…å½“ã™ã‚‹èªå½™ã®ç¯„å›²ã‚’è¨ˆç®—\n",
    "\n",
    "        assert num_embeddings % self.tp_size == 0\n",
    "\n",
    "        # å…¨èªå½™æ•°\n",
    "        self.num_embeddings = num_embeddings\n",
    "\n",
    "        # æ‹…å½“ã™ã‚‹èªå½™æ•°\n",
    "        self.num_embeddings_per_partition = \\\n",
    "            self.num_embeddings // self.tp_size\n",
    "\n",
    "        # æ‹…å½“ã™ã‚‹èªå½™ã®é–‹å§‹ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹\n",
    "        self.vocab_start_idx = \\\n",
    "            self.num_embeddings_per_partition * self.tp_rank\n",
    "\n",
    "        # æ‹…å½“ã™ã‚‹èªå½™ã®çµ‚äº†ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹\n",
    "        self.vocab_end_idx = \\\n",
    "            self.vocab_start_idx + self.num_embeddings_per_partition\n",
    "\n",
    "        logger.debug(f\"æ‹…å½“ã™ã‚‹èªå½™ã®ç¯„å›²è¨ˆç®— {self.vocab_start_idx=}, {self.vocab_end_idx=}\")\n",
    "\n",
    "        # 3) æ‹…å½“ã™ã‚‹èªå½™ã®é‡ã¿è¡Œåˆ—ã‚’åˆæœŸåŒ–\n",
    "\n",
    "        self.weight = nn.Parameter(\n",
    "            torch.empty(self.num_embeddings_per_partition, embedding_dim))\n",
    "\n",
    "        self.weight.weight_loader = self.weight_loader\n",
    "\n",
    "        logger.debug(f\"æ‹…å½“ã™ã‚‹èªå½™ã®é‡ã¿è¡Œåˆ—åˆæœŸåŒ– {self.weight.shape=}\")\n",
    "\n",
    "        logger.info(f\"å…¥åŠ›ã®åŸ‹ã‚è¾¼ã¿å±¤ã‚’åˆæœŸåŒ–å®Œäº†\")\n",
    "\n",
    "\n",
    "    def weight_loader(self, param: nn.Parameter, loaded_weight: torch.Tensor):\n",
    "        \"\"\"\n",
    "        ãƒ¢ãƒ‡ãƒ«ã®ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‹ã‚‰åŸ‹ã‚è¾¼ã¿å±¤ã®é‡ã¿ã‚’ã‚¹ãƒ©ã‚¤ã‚¹ã—ã¦ãƒ­ãƒ¼ãƒ‰ã™ã‚‹\n",
    "\n",
    "        Args:\n",
    "            param (nn.Parameter): åŸ‹ã‚è¾¼ã¿å±¤ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿\n",
    "            loaded_weight (torch.Tensor): ãƒ­ãƒ¼ãƒ‰ã™ã‚‹é‡ã¿\n",
    "        \"\"\"\n",
    "        logger.info(f\"åŸ‹ã‚è¾¼ã¿å±¤ã®é‡ã¿ã‚’ãƒ­ãƒ¼ãƒ‰é–‹å§‹ {param.shape=}, {loaded_weight.shape=}\")\n",
    "\n",
    "        # é‡ã¿ãƒ‡ãƒ¼ã‚¿ã¸ã®å‚ç…§ã‚’å–å¾—\n",
    "        param_data = param.data\n",
    "\n",
    "        # æ‹…å½“ã™ã‚‹é‡ã¿ã®ã‚µã‚¤ã‚º\n",
    "        shard_size = param_data.size(0)\n",
    "\n",
    "        # æ‹…å½“ã™ã‚‹é‡ã¿ã®é–‹å§‹ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’è¨ˆç®—\n",
    "        start_idx = self.tp_rank * shard_size\n",
    "\n",
    "        # æ‹…å½“ã™ã‚‹é‡ã¿ã‚’ã‚¹ãƒ©ã‚¤ã‚¹ã—ã¦ã‚³ãƒ”ãƒ¼\n",
    "        loaded_weight = loaded_weight.narrow(0, start_idx, shard_size)\n",
    "\n",
    "        logger.info(f\"åŸ‹ã‚è¾¼ã¿å±¤ã®é‡ã¿ã‚’ãƒ­ãƒ¼ãƒ‰å®Œäº† {loaded_weight.shape=}\")\n",
    "\n",
    "        \n",
    "    def forward(self, x: torch.Tensor):\n",
    "        \"\"\"\n",
    "        å…¥åŠ›ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã‚’åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«ã«å¤‰æ›ã™ã‚‹\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): å…¥åŠ›ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹\n",
    "        Returns:\n",
    "            torch.Tensor: åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«\n",
    "        \"\"\"\n",
    "        logger.info(f\"åŸ‹ã‚è¾¼ã¿å±¤ã®é †ä¼æ’­é–‹å§‹ {x.shape=}\")\n",
    "\n",
    "        # 1) æ‹…å½“ã™ã‚‹èªå½™ã®ç¯„å›²ã«åŸºã¥ã„ã¦å…¥åŠ›ã‚’ãƒã‚¹ã‚¯\n",
    "\n",
    "        # GPUãŒè¤‡æ•°ã‚ã‚‹å ´åˆ\n",
    "        if self.tp_size > 1:\n",
    "\n",
    "            # æ‹…å½“ã™ã‚‹èªå½™ã®ãƒã‚¹ã‚¯ã‚’ä½œæˆ\n",
    "            mask = (x >= self.vocab_start_idx) & (x < self.vocab_end_idx)\n",
    "\n",
    "            # ãƒ­ãƒ¼ã‚«ãƒ«IDã«å¤‰æ›ã—ã€æ‹…å½“ã™ã‚‹èªå½™ä»¥å¤–ã¯0ã«ç½®ãæ›ãˆ\n",
    "            x = mask * (x - self.vocab_start_idx)\n",
    "\n",
    "        # 2) åŸ‹ã‚è¾¼ã¿ã‚’è¨ˆç®—\n",
    "\n",
    "        y = F.embedding(x, self.weight)\n",
    "\n",
    "        # 3) çµæœã‚’é›†ç´„\n",
    "\n",
    "        # GPUãŒè¤‡æ•°ã‚ã‚‹å ´åˆ\n",
    "        if self.tp_size > 1:\n",
    "\n",
    "            # æ‹…å½“ã™ã‚‹èªå½™ä»¥å¤–ã®åŸ‹ã‚è¾¼ã¿ã‚’0ã«ç½®ãæ›ãˆ\n",
    "            y = mask.unsqueeze(1) * y\n",
    "\n",
    "            # å…¨GPUã®åŸ‹ã‚è¾¼ã¿ã‚’é›†ç´„\n",
    "            dist.all_reduce(y)\n",
    "\n",
    "        logger.info(f\"åŸ‹ã‚è¾¼ã¿å±¤ã®é †ä¼æ’­å®Œäº† {y.shape=}\")\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14e7a397",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Qwen3Model(nn.Module):\n",
    "    \"\"\"\n",
    "    åŸ‹ã‚è¾¼ã¿å±¤ã€è¤‡æ•°ã®ãƒ‡ã‚³ãƒ¼ãƒ€ãƒ¬ã‚¤ãƒ¤ãƒ¼ã€æœ€çµ‚æ­£è¦åŒ–å±¤ã‹ã‚‰æ§‹æˆã•ã‚Œã‚‹Qwen3ãƒ¢ãƒ‡ãƒ«\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        config: Qwen3Config,\n",
    "    ) -> None:\n",
    "        logger.info(f\"Qwen3Modelã‚’åˆæœŸåŒ–é–‹å§‹ {config.vocab_size=} {config.hidden_size=} {config.num_hidden_layers=}\")\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        # åŸ‹ã‚è¾¼ã¿å±¤ã®åˆæœŸåŒ–\n",
    "        self.embed_tokens = VocabParallelEmbedding(\n",
    "            config.vocab_size, config.hidden_size)\n",
    "\n",
    "        # Transformerãƒ–ãƒ­ãƒƒã‚¯ã®ç©ã¿é‡ã­\n",
    "        self.layers = nn.ModuleList(\n",
    "            [Qwen3DecoderLayer(config) for _ \\\n",
    "            in range(config.num_hidden_layers)])\n",
    "\n",
    "        # æœ€çµ‚æ­£è¦åŒ–å±¤ã®åˆæœŸåŒ–\n",
    "        self.norm = RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n",
    "\n",
    "        logger.info(f\"Qwen3Modelã®åˆæœŸåŒ–å®Œäº†\")\n",
    "\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: torch.Tensor,\n",
    "        positions: torch.Tensor,\n",
    "    ) -> torch.Tensor:\n",
    "        logger.info(f\"Qwen3Modelã®é †ä¼æ’­é–‹å§‹ {input_ids.shape=} {positions.shape=}\")\n",
    "\n",
    "        # å…¥åŠ›ã®åŸ‹ã‚è¾¼ã¿å±¤ã‚’é©ç”¨\n",
    "        hidden_states = self.embed_tokens(input_ids)\n",
    "\n",
    "        residual = None\n",
    "\n",
    "        # å„ãƒ‡ã‚³ãƒ¼ãƒ€ãƒ¬ã‚¤ãƒ¤ãƒ¼ã‚’é †ã«é©ç”¨\n",
    "        for layer in self.layers:\n",
    "            hidden_states, residual = layer(\n",
    "                positions, hidden_states, residual)\n",
    "\n",
    "        # æœ€çµ‚æ­£è¦åŒ–å±¤ã‚’é©ç”¨\n",
    "        hidden_states, _ = self.norm(hidden_states, residual)\n",
    "\n",
    "        logger.info(f\"Qwen3Modelã®é †ä¼æ’­å®Œäº† {hidden_states.shape=}\")\n",
    "\n",
    "        return hidden_states"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c071b1c",
   "metadata": {},
   "source": [
    "### Qwen3ForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de924ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParallelLMHead(VocabParallelEmbedding):\n",
    "    \"\"\"\n",
    "    éš ã‚Œå±¤ã®çŠ¶æ…‹ã‚’èªå½™æ•°åˆ†ã®ãƒ­ã‚¸ãƒƒãƒˆã«å¤‰æ›ã™ã‚‹è¨€èªãƒ¢ãƒ‡ãƒ«ãƒ˜ãƒƒãƒ‰\n",
    "    VocabParallelEmbeddingã‚’ç¶™æ‰¿ã—ã€Tensor Parallelismã‚’åˆ©ç”¨\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_embeddings: int,\n",
    "        embedding_dim: int,\n",
    "        bias: bool = False,\n",
    "    ):\n",
    "        logger.info(f\"å‡ºåŠ›ã®è¨€èªãƒ¢ãƒ‡ãƒ«ãƒ˜ãƒƒãƒ‰ã‚’åˆæœŸåŒ–é–‹å§‹ {num_embeddings=}, {embedding_dim=}, {bias=}\")\n",
    "\n",
    "        assert not bias\n",
    "\n",
    "        super().__init__(num_embeddings, embedding_dim)\n",
    "\n",
    "        logger.info(f\"å‡ºåŠ›ã®è¨€èªãƒ¢ãƒ‡ãƒ«ãƒ˜ãƒƒãƒ‰ã‚’åˆæœŸåŒ–å®Œäº†\")\n",
    "\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        \"\"\"\n",
    "        éš ã‚Œå±¤ã®çŠ¶æ…‹ã‚’èªå½™æ•°åˆ†ã®ãƒ­ã‚¸ãƒƒãƒˆã«å¤‰æ›ã™ã‚‹\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): éš ã‚Œå±¤ã®çŠ¶æ…‹\n",
    "        \"\"\"\n",
    "\n",
    "        logger.info(f\"å‡ºåŠ›ã®è¨€èªãƒ¢ãƒ‡ãƒ«ãƒ˜ãƒƒãƒ‰ã®é †ä¼æ’­é–‹å§‹ {x.shape=}\")\n",
    "\n",
    "        # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’å–å¾—\n",
    "        context = get_context()\n",
    "\n",
    "        # Prefillã®å ´åˆ\n",
    "        if context.is_prefill:\n",
    "            # æœ€å¾Œã®ãƒˆãƒ¼ã‚¯ãƒ³ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’å–å¾—\n",
    "            last_indices = context.cu_seqlens_q[1:] - 1\n",
    "\n",
    "            # å…¥åŠ›ã‹ã‚‰æœ€å¾Œã®ãƒˆãƒ¼ã‚¯ãƒ³ã‚’æŠ½å‡º\n",
    "            x = x[last_indices].contiguous()\n",
    "\n",
    "            logger.debug(f\"æœ€å¾Œã®ãƒˆãƒ¼ã‚¯ãƒ³ã‚’æŠ½å‡ºï¼ˆPrefillï¼‰ {x.shape=}\")\n",
    "\n",
    "\n",
    "        # ç·šå½¢å°„å½±ã‚’é©ç”¨\n",
    "        logits = F.linear(x, self.weight)\n",
    "\n",
    "        # è¤‡æ•°GPUã®å ´åˆ\n",
    "        if self.tp_size > 1:\n",
    "\n",
    "            # ãƒ¡ã‚¤ãƒ³ãƒ—ãƒ­ã‚»ã‚¹ã®å ´åˆã€å‡ºåŠ›ã‚’åˆæœŸåŒ–\n",
    "            all_logits = [\n",
    "                torch.empty_like(logits) for _ in range(self.tp_size)\n",
    "            ] if self.tp_rank == 0 else None\n",
    "\n",
    "            # å…¨GPUã®ãƒ­ã‚¸ãƒƒãƒˆã‚’é›†ç´„\n",
    "            dist.gather(logits, all_logits, 0)\n",
    "\n",
    "            # ãƒ¡ã‚¤ãƒ³ãƒ—ãƒ­ã‚»ã‚¹ã§ãƒ­ã‚¸ãƒƒãƒˆã‚’é€£çµ\n",
    "            logits = torch.cat(all_logits, -1) if self.tp_rank == 0 else None\n",
    "\n",
    "        logger.info(f\"å‡ºåŠ›ã®è¨€èªãƒ¢ãƒ‡ãƒ«ãƒ˜ãƒƒãƒ‰ã®é †ä¼æ’­å®Œäº† {logits.shape if logits is not None else None=}\")\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "892956d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Qwen3ForCausalLM(nn.Module):\n",
    "    \"\"\"\n",
    "    Qwen3ãƒ¢ãƒ‡ãƒ«ã«è¨€èªãƒ¢ãƒ‡ãƒªãƒ³ã‚°ãƒ˜ãƒƒãƒ‰ã‚’è¿½åŠ ã—ãŸã‚¯ãƒ©ã‚¹\n",
    "    \"\"\"\n",
    "\n",
    "    # äº‹å‰å­¦ç¿’æ¸ˆã¿ã®é‡ã¿ã‚’æ­£ã—ãèª­ã¿è¾¼ã‚€ãŸã‚ã®ãƒãƒƒãƒ”ãƒ³ã‚°\n",
    "    packed_modules_mapping = {\n",
    "        \"q_proj\": (\"qkv_proj\", \"q\"),\n",
    "        \"k_proj\": (\"qkv_proj\", \"k\"),\n",
    "        \"v_proj\": (\"qkv_proj\", \"v\"),\n",
    "        \"gate_proj\": (\"gate_up_proj\", 0),\n",
    "        \"up_proj\": (\"gate_up_proj\", 1),\n",
    "    }\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        config: Qwen3Config\n",
    "    ) -> None:\n",
    "        logger.info(f\"Qwen3ForCausalLMã‚’åˆæœŸåŒ–é–‹å§‹ {config.vocab_size=} {config.hidden_size=} {config.tie_word_embeddings=}\")\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        # Qwen3ãƒ¢ãƒ‡ãƒ«ã®åˆæœŸåŒ–\n",
    "        self.model = Qwen3Model(config)\n",
    "\n",
    "        # è¨€èªãƒ¢ãƒ‡ãƒªãƒ³ã‚°ãƒ˜ãƒƒãƒ‰ã®åˆæœŸåŒ–\n",
    "        self.lm_head = ParallelLMHead(config.vocab_size, config.hidden_size)\n",
    "\n",
    "        # åŸ‹ã‚è¾¼ã¿å±¤ã¨å‡ºåŠ›å±¤ã®é‡ã¿ã‚’å…±æœ‰ã™ã‚‹å ´åˆï¼ˆWeight Tyingï¼‰\n",
    "        if config.tie_word_embeddings:\n",
    "            # å‡ºåŠ›ã®é‡ã¿ã‚’å…¥åŠ›ã®åŸ‹ã‚è¾¼ã¿å±¤ã®é‡ã¿ã§åˆæœŸåŒ–\n",
    "            self.lm_head.weight.data = self.model.embed_tokens.weight.data\n",
    "\n",
    "        logger.info(f\"Qwen3ForCausalLMã®åˆæœŸåŒ–å®Œäº†\")\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: torch.Tensor,\n",
    "        positions: torch.Tensor,\n",
    "    ) -> torch.Tensor:\n",
    "        logger.info(f\"Qwen3ForCausalLMã®é †ä¼æ’­é–‹å§‹ {input_ids.shape=} {positions.shape=}\")\n",
    "\n",
    "        output = self.model(input_ids, positions)\n",
    "\n",
    "        logger.info(f\"Qwen3ForCausalLMã®é †ä¼æ’­å®Œäº† {output.shape=}\")\n",
    "\n",
    "        return output\n",
    "\n",
    "    def compute_logits(\n",
    "        self,\n",
    "        hidden_states: torch.Tensor,\n",
    "    ) -> torch.Tensor:\n",
    "        logger.info(f\"Qwen3ForCausalLMã®ãƒ­ã‚¸ãƒƒãƒˆè¨ˆç®—é–‹å§‹ {hidden_states.shape=}\")\n",
    "\n",
    "         # è¨€èªãƒ¢ãƒ‡ãƒªãƒ³ã‚°ãƒ˜ãƒƒãƒ‰ã‚’é©ç”¨ã—ã¦ãƒ­ã‚¸ãƒƒãƒˆã‚’è¨ˆç®—\n",
    "        logits = self.lm_head(hidden_states)\n",
    "\n",
    "        logger.info(f\"Qwen3ForCausalLMã®ãƒ­ã‚¸ãƒƒãƒˆè¨ˆç®—å®Œäº† {logits.shape=}\")\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08243eef",
   "metadata": {},
   "source": [
    "## Engine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c776fc9",
   "metadata": {},
   "source": [
    "### SamplingParams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77acc736",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class SamplingParams:\n",
    "    \"\"\"\n",
    "    ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆã®è¨­å®š\n",
    "    \"\"\"\n",
    "\n",
    "    # æ¸©åº¦\n",
    "    temperature: float = 1.0\n",
    "\n",
    "    # ç”Ÿæˆã™ã‚‹æœ€å¤§ãƒˆãƒ¼ã‚¯ãƒ³æ•°\n",
    "    max_tokens: int = 64\n",
    "\n",
    "    # EOSãƒˆãƒ¼ã‚¯ãƒ³ã‚’ç„¡è¦–ã™ã‚‹ã‹\n",
    "    ignore_eos: bool = False\n",
    "\n",
    "    def __post_init__(self):\n",
    "        logger.info(f\"SamplingParamsåˆæœŸåŒ–ã®å¾Œå‡¦ç† {self=}\")\n",
    "        assert self.temperature > 1e-10, \"greedy sampling is not permitted\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61f6374e",
   "metadata": {},
   "source": [
    "### Sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88812459",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequenceStatus(Enum):\n",
    "    # ãƒªã‚¯ã‚¨ã‚¹ãƒˆå®Ÿè¡Œã®å¾…æ©ŸçŠ¶æ…‹ = GPUãƒ¡ãƒ¢ãƒªæœªå‰²ã‚Šå½“ã¦\n",
    "    WAITING = auto()\n",
    "\n",
    "    # Prefillã¾ãŸã¯Decodeã®å®Ÿè¡ŒçŠ¶æ…‹ = GPUä¸Šã«KVã‚­ãƒ£ãƒƒã‚·ãƒ¥ãŒå­˜åœ¨\n",
    "    RUNNING = auto()\n",
    "\n",
    "    # ãƒªã‚¯ã‚¨ã‚¹ãƒˆå®Œäº†çŠ¶æ…‹ = ãƒ¡ãƒ¢ãƒªè§£æ”¾å¾…ã¡ãƒ»ãƒ¡ãƒ¢ãƒªè§£æ”¾æ¸ˆã¿\n",
    "    FINISHED = auto()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e59722a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sequence:\n",
    "    \"\"\"\n",
    "    ãƒˆãƒ¼ã‚¯ãƒ³ã®ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã‚„è«–ç†ãƒ–ãƒ­ãƒƒã‚¯ã¨ç‰©ç†ãƒ–ãƒ­ãƒƒã‚¯ã®ãƒãƒƒãƒ”ãƒ³ã‚°ã‚’ä¿æŒã™ã‚‹ã‚¯ãƒ©ã‚¹\n",
    "    \"\"\"\n",
    "\n",
    "    # ãƒ–ãƒ­ãƒƒã‚¯å†…ã®ãƒˆãƒ¼ã‚¯ãƒ³æ•°\n",
    "    block_size = 256\n",
    "\n",
    "    # ã‚·ãƒ¼ã‚±ãƒ³ã‚¹IDã®ã‚«ã‚¦ãƒ³ã‚¿\n",
    "    counter = count()\n",
    "\n",
    "    def __init__(self, token_ids: list[int], sampling_params = SamplingParams()):\n",
    "        logger.info(f\"ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã‚’åˆæœŸåŒ– {len(token_ids)=}, {sampling_params=}\")\n",
    "\n",
    "        self.seq_id = next(Sequence.counter)\n",
    "        self.status = SequenceStatus.WAITING\n",
    "        self.token_ids = copy(token_ids)\n",
    "        self.last_token = token_ids[-1]\n",
    "        self.num_tokens = len(self.token_ids)\n",
    "        self.num_prompt_tokens = len(token_ids)\n",
    "\n",
    "        # KVã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜ã•ã‚Œã¦ã„ã‚‹ãƒˆãƒ¼ã‚¯ãƒ³æ•°\n",
    "        self.num_cached_tokens = 0\n",
    "\n",
    "        # ã“ã®ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ãŒä½¿ç”¨ã—ã¦ã„ã‚‹ç‰©ç†ãƒ–ãƒ­ãƒƒã‚¯IDã®ãƒªã‚¹ãƒˆ\n",
    "        self.block_table = []\n",
    "\n",
    "        self.temperature = sampling_params.temperature\n",
    "        self.max_tokens = sampling_params.max_tokens\n",
    "        self.ignore_eos = sampling_params.ignore_eos\n",
    "        logger.info(f\"ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã‚’åˆæœŸåŒ–å®Œäº† {self.seq_id=}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_tokens\n",
    "\n",
    "    def __getitem__(self, key):\n",
    "        return self.token_ids[key]\n",
    "\n",
    "    @property\n",
    "    def is_finished(self):\n",
    "        return self.status == SequenceStatus.FINISHED\n",
    "\n",
    "    @property\n",
    "    def num_completion_tokens(self):\n",
    "        return self.num_tokens - self.num_prompt_tokens\n",
    "\n",
    "    @property\n",
    "    def prompt_token_ids(self):\n",
    "        return self.token_ids[:self.num_prompt_tokens]\n",
    "\n",
    "    @property\n",
    "    def completion_token_ids(self):\n",
    "        return self.token_ids[self.num_prompt_tokens:]\n",
    "\n",
    "    @property\n",
    "    def num_cached_blocks(self):\n",
    "        return self.num_cached_tokens // self.block_size\n",
    "\n",
    "    @property\n",
    "    def num_blocks(self):\n",
    "        return (self.num_tokens + self.block_size - 1) // self.block_size\n",
    "\n",
    "    @property\n",
    "    def last_block_num_tokens(self):\n",
    "        return self.num_tokens - (self.num_blocks - 1) * self.block_size\n",
    "\n",
    "    def block(self, i):\n",
    "        \"\"\"\n",
    "        iç•ªç›®ã®è«–ç†ãƒ–ãƒ­ãƒƒã‚¯ã«å«ã¾ã‚Œã‚‹ãƒˆãƒ¼ã‚¯ãƒ³IDã®ãƒªã‚¹ãƒˆã‚’è¿”ã™\n",
    "\n",
    "        Args:\n",
    "            i (int): è«–ç†ãƒ–ãƒ­ãƒƒã‚¯ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹\n",
    "        Returns:\n",
    "            list[int]: è«–ç†ãƒ–ãƒ­ãƒƒã‚¯ã«å«ã¾ã‚Œã‚‹ãƒˆãƒ¼ã‚¯ãƒ³IDã®ãƒªã‚¹ãƒˆ\n",
    "        \"\"\"\n",
    "        logger.info(f\"è«–ç†ãƒ–ãƒ­ãƒƒã‚¯ã‚’å–å¾— {i=}\")\n",
    "\n",
    "        assert 0 <= i < self.num_blocks\n",
    "\n",
    "        # ãƒˆãƒ¼ã‚¯ãƒ³IDã®ã‚¹ãƒ©ã‚¤ã‚¹ã‚’å–å¾—\n",
    "        result = self.token_ids[i*self.block_size: (i+1)*self.block_size]\n",
    "\n",
    "        logger.info(f\"è«–ç†ãƒ–ãƒ­ãƒƒã‚¯ã‚’å–å¾—å®Œäº† {i=} {result=}\")\n",
    "        return result\n",
    "\n",
    "    def append_token(self, token_id: int):\n",
    "        \"\"\"\n",
    "        æ–°ã—ãç”Ÿæˆã•ã‚ŒãŸãƒˆãƒ¼ã‚¯ãƒ³ã‚’ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã«è¿½åŠ ã—ã€å†…éƒ¨çŠ¶æ…‹ã‚’æ›´æ–°ã™ã‚‹\n",
    "        Scheduler.postprocessã‹ã‚‰å‘¼ã³å‡ºã•ã‚Œã‚‹\n",
    "\n",
    "        Args:\n",
    "            token_id (int): è¿½åŠ ã™ã‚‹ãƒˆãƒ¼ã‚¯ãƒ³ID\n",
    "        \"\"\"\n",
    "\n",
    "        logger.info(f\"ãƒˆãƒ¼ã‚¯ãƒ³ã‚’è¿½åŠ  {token_id=}\")\n",
    "\n",
    "        # ãƒˆãƒ¼ã‚¯ãƒ³IDã‚’è¿½åŠ \n",
    "        self.token_ids.append(token_id)\n",
    "\n",
    "        # æœ€æ–°ã®ãƒˆãƒ¼ã‚¯ãƒ³ã‚’æ›´æ–°ï¼ˆæ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—ã®å…¥åŠ›ï¼‰\n",
    "        self.last_token = token_id\n",
    "\n",
    "        # ãƒˆãƒ¼ã‚¯ãƒ³ç·æ•°ã‚’æ›´æ–°\n",
    "        self.num_tokens += 1\n",
    "\n",
    "        logger.info(f\"ãƒˆãƒ¼ã‚¯ãƒ³ã‚’è¿½åŠ å®Œäº† {token_id=} {self.num_tokens=}\")\n",
    "\n",
    "    def __getstate__(self):\n",
    "        \"\"\"\n",
    "        ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã®çŠ¶æ…‹ã‚’ã‚·ãƒªã‚¢ãƒ©ã‚¤ã‚ºã™ã‚‹ãŸã‚ã®ãƒ¡ã‚½ãƒƒãƒ‰\n",
    "        PickleåŒ–ã™ã‚‹éš›ã«å‘¼ã³å‡ºã•ã‚Œã‚‹\n",
    "\n",
    "        Prefillä¸­ã¯å…¨ã¦ã®ãƒˆãƒ¼ã‚¯ãƒ³IDã‚’é€ã‚‹\n",
    "        Decodeä¸­ã¯æœ€å¾Œã®ãƒˆãƒ¼ã‚¯ãƒ³IDã®ã¿é€ã‚‹\n",
    "        \"\"\"\n",
    "        return (\n",
    "            self.num_tokens,\n",
    "            self.num_prompt_tokens,\n",
    "            self.num_cached_tokens,\n",
    "            self.block_table,\n",
    "            self.token_ids if self.num_completion_tokens == 0 \\\n",
    "                else self.last_token\n",
    "            )\n",
    "\n",
    "    def __setstate__(self, state):\n",
    "        \"\"\"\n",
    "        ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã®çŠ¶æ…‹ã‚’ãƒ‡ã‚·ãƒªã‚¢ãƒ©ã‚¤ã‚ºã™ã‚‹ãŸã‚ã®ãƒ¡ã‚½ãƒƒãƒ‰\n",
    "        Pickleã‹ã‚‰å¾©å…ƒã™ã‚‹éš›ã«å‘¼ã³å‡ºã•ã‚Œã‚‹\n",
    "        \"\"\"\n",
    "        self.num_tokens, self.num_prompt_tokens, self.num_cached_tokens, self.block_table = state[:-1]\n",
    "\n",
    "        # Prefillä¸­\n",
    "        if self.num_completion_tokens == 0:\n",
    "            # å…¨ã¦ã®ãƒˆãƒ¼ã‚¯ãƒ³IDã‚’å¾©å…ƒ\n",
    "            self.token_ids = state[-1]\n",
    "\n",
    "        # Decodeä¸­\n",
    "        else:\n",
    "            # last_tokenã ã‘å¾©å…ƒ\n",
    "            self.last_token = state[-1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aabb9ea",
   "metadata": {},
   "source": [
    "### Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ca78aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block:\n",
    "    \"\"\"\n",
    "    GPUä¸Šã®å·¨å¤§ãªKVã‚­ãƒ£ãƒƒã‚·ãƒ¥é ˜åŸŸã®ä¸€éƒ¨ã®çŠ¶æ…‹ã‚’è¿½è·¡ã™ã‚‹ã‚¯ãƒ©ã‚¹\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, block_id):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            block_id (int): ç‰©ç†çš„ãªãƒ–ãƒ­ãƒƒã‚¯ID\n",
    "        \"\"\"\n",
    "        logger.info(f\"ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ {block_id=}\")\n",
    "\n",
    "        # ç‰©ç†çš„ãªãƒ–ãƒ­ãƒƒã‚¯ID\n",
    "        # kv_cacheãƒ†ãƒ³ã‚½ãƒ«ã®ä½•ç•ªç›®ã®ã‚¹ãƒ­ãƒƒãƒˆã«å¯¾å¿œã™ã‚‹ã‹ã‚’ç¤ºã™\n",
    "        # 0, 1, 2, ...\n",
    "        self.block_id = block_id\n",
    "\n",
    "        # å‚ç…§ã‚«ã‚¦ãƒ³ãƒˆ\n",
    "        # ã“ã®ãƒ–ãƒ­ãƒƒã‚¯ã‚’å‚ç…§ã—ã¦ã„ã‚‹ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã®æ•°\n",
    "        # 0: æœªä½¿ç”¨ã§å†åˆ©ç”¨å¯èƒ½\n",
    "        # 1: 1ã¤ã®ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ãŒç‹¬å ã—ã¦ä½¿ç”¨\n",
    "        # 2ä»¥ä¸Š: è¤‡æ•°ã®ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ãŒå…±æœ‰ã—ã¦ä½¿ç”¨ï¼ˆPrefix Cachingã‚„Beam Searchæ™‚ï¼‰\n",
    "        self.ref_count = 0\n",
    "\n",
    "        # ã“ã®ãƒ–ãƒ­ãƒƒã‚¯ã«æ ¼ç´ã•ã‚Œã¦ã„ã‚‹ãƒˆãƒ¼ã‚¯ãƒ³åˆ—ã®ãƒãƒƒã‚·ãƒ¥å€¤\n",
    "        # æ–°ã—ã„ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ãŒæ¥ãŸã¨ãã€åŒã˜ãƒˆãƒ¼ã‚¯ãƒ³åˆ—ã‚’æŒã¤ãƒ–ãƒ­ãƒƒã‚¯ãŒå­˜åœ¨ã™ã‚‹ã‹ã‚’æ¤œç´¢ã™ã‚‹ãŸã‚\n",
    "        self.hash = -1\n",
    "\n",
    "        # ã“ã®ãƒ–ãƒ­ãƒƒã‚¯ã«å«ã¾ã‚Œã‚‹å®Ÿéš›ã®ãƒˆãƒ¼ã‚¯ãƒ³IDã®ãƒªã‚¹ãƒˆ\n",
    "        self.token_ids = []\n",
    "\n",
    "        logger.info(f\"ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† {self.block_id=}\")\n",
    "\n",
    "    def update(self, hash: int, token_ids: list[int]):\n",
    "        \"\"\"\n",
    "        ãƒãƒƒã‚·ãƒ¥å€¤ã¨ãƒˆãƒ¼ã‚¯ãƒ³ã®å†…å®¹ã‚’è¨˜éŒ²ã—ã€ã‚ã¨ã§æ¤œç´¢å¯èƒ½ã«ã™ã‚‹\n",
    "        ãƒ–ãƒ­ãƒƒã‚¯ãŒãƒ‡ãƒ¼ã‚¿ã§æº€ãŸã•ã‚ŒãŸï¼ˆã‚ã‚‹ã„ã¯Prefix Cacheã¨ã—ã¦ç™»éŒ²ã•ã‚Œã‚‹ï¼‰æ™‚ã«\n",
    "        å‘¼ã³å‡ºã•ã‚Œã‚‹\n",
    "        \"\"\"\n",
    "        logger.info(f\"ãƒ–ãƒ­ãƒƒã‚¯ã‚’æ›´æ–° {self.block_id} {hash=} {token_ids=}\")\n",
    "        self.hash = hash\n",
    "        self.token_ids = token_ids\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        ãƒ–ãƒ­ãƒƒã‚¯ã‚’æ–°ã—ãå‰²ã‚Šå½“ã¦ã‚‹æ™‚ã«å‘¼ã³å‡ºã•ã‚Œã‚‹\n",
    "        \"\"\"\n",
    "        logger.info(f\"ãƒ–ãƒ­ãƒƒã‚¯ã‚’ãƒªã‚»ãƒƒãƒˆ {self.block_id}\")\n",
    "\n",
    "        # ãƒ–ãƒ­ãƒƒã‚¯ã‚’ä½¿ç”¨ä¸­ã§ã‚ã‚‹çŠ¶æ…‹ã«å¤‰æ›´\n",
    "        self.ref_count = 1\n",
    "\n",
    "        self.hash = -1\n",
    "        self.token_ids = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fb0ae4f",
   "metadata": {},
   "source": [
    "### Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f761b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Config:\n",
    "    \"\"\"\n",
    "    ã‚¨ãƒ³ã‚¸ãƒ³ã®èµ·å‹•è¨­å®šãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®ç®¡ç†ã‚¯ãƒ©ã‚¹\n",
    "    \"\"\"\n",
    "\n",
    "    # ãƒ¢ãƒ‡ãƒ«ã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãƒ‘ã‚¹\n",
    "    model: str\n",
    "\n",
    "    # 1ãƒãƒƒãƒå†…ã§åŒæ™‚ã«å‡¦ç†ã§ãã‚‹æœ€å¤§ãƒˆãƒ¼ã‚¯ãƒ³æ•°\n",
    "    max_num_batched_tokens: int = 16384\n",
    "\n",
    "    # 1ãƒãƒƒãƒå†…ã§åŒæ™‚ã«å‡¦ç†ã§ãã‚‹æœ€å¤§ã‚·ãƒ¼ã‚±ãƒ³ã‚¹æ•°\n",
    "    max_num_seqs: int = 512\n",
    "\n",
    "    # ãƒ¢ãƒ‡ãƒ«ãŒæ‰±ãˆã‚‹æœ€å¤§ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆé•·\n",
    "    max_model_len: int = 4096\n",
    "\n",
    "    # GPUãƒ¡ãƒ¢ãƒªä½¿ç”¨ç‡ã®ä¸Šé™\n",
    "    gpu_memory_utilization: float = 0.9\n",
    "\n",
    "    # ä½¿ç”¨ã™ã‚‹GPUã®æ•°ï¼ˆTensor Parallelismã®ã‚µã‚¤ã‚ºï¼‰\n",
    "    tensor_parallel_size: int = 1\n",
    "\n",
    "    # CUDA Graphã«ã‚ˆã‚‹é«˜é€ŸåŒ–ã‚’æœ‰åŠ¹åŒ–\n",
    "    enforce_eager: bool = False\n",
    "\n",
    "    # HuggingFaceã®ãƒ¢ãƒ‡ãƒ«è¨­å®š\n",
    "    hf_config: AutoConfig | None = None\n",
    "\n",
    "    # ç‰¹æ®Šãƒˆãƒ¼ã‚¯ãƒ³ID\n",
    "    eos: int = -1\n",
    "\n",
    "    # PagedAttentionã®1ãƒ–ãƒ­ãƒƒã‚¯ã‚ãŸã‚Šã®ãƒˆãƒ¼ã‚¯ãƒ³æ•°\n",
    "    kvcache_block_size: int = 256\n",
    "\n",
    "    # KV Cacheã®ç‰©ç†ãƒ–ãƒ­ãƒƒã‚¯æ•°\n",
    "    num_kvcache_blocks: int = -1\n",
    "\n",
    "    def __post_init__(self):\n",
    "        logger.info(f\"ConfigåˆæœŸåŒ–ã®å¾Œå‡¦ç† {self=}\")\n",
    "        assert os.path.isdir(self.model)\n",
    "        assert self.kvcache_block_size % 256 == 0\n",
    "        assert 1 <= self.tensor_parallel_size <= 8\n",
    "        self.hf_config = AutoConfig.from_pretrained(self.model)\n",
    "\n",
    "        # ãƒ¢ãƒ‡ãƒ«ã®æœ€å¤§ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆé•·ã‚’HuggingFaceã®è¨­å®šã«åˆã‚ã›ã‚‹\n",
    "        self.max_model_len = min(self.max_model_len, self.hf_config.max_position_embeddings)\n",
    "\n",
    "        assert self.max_num_batched_tokens >= self.max_model_len"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04034ef9",
   "metadata": {},
   "source": [
    "### BlockManager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5321397",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BlockManager:\n",
    "    \"\"\"\n",
    "    PagedAttentionã§ç‰©ç†ãƒ¡ãƒ¢ãƒªãƒ–ãƒ­ãƒƒã‚¯ã‚’ç®¡ç†ã™ã‚‹ã‚¯ãƒ©ã‚¹\n",
    "    Blockã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã®ãƒªã‚¹ãƒˆã‚’ä¿æŒã—ã¦ã€ä»¥ä¸‹ã®ãƒ•ãƒ­ãƒ¼ã‚’åˆ¶å¾¡:\n",
    "\n",
    "    1. æ–°ã—ã„ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ãŒæ¥ã‚‹\n",
    "    2. ãã®ãƒˆãƒ¼ã‚¯ãƒ³åˆ—ã®ãƒãƒƒã‚·ãƒ¥ã‚’è¨ˆç®—ã™ã‚‹\n",
    "    3. æ—¢å­˜ã®Blockã®ä¸­ã‹ã‚‰åŒã˜ãƒãƒƒã‚·ãƒ¥ã‚’æŒã¤ã‚‚ã®ã‚’æ¢ã™ï¼ˆPrefix Cachingï¼‰\n",
    "    3-1. è¦‹ã¤ã‹ã£ãŸå ´åˆã€ãã®Blockã®ref_countã‚’å¢—ã‚„ã—ã¦å…±æœ‰ã™ã‚‹\n",
    "    3-2. è¦‹ã¤ã‹ã‚‰ãªã‹ã£ãŸå ´åˆã€ref_countãŒ0ã®Blockã‚’æ¢ã—ã€reset()ã—ã¦å‰²ã‚Šå½“ã¦ã‚‹\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_blocks: int, block_size: int):\n",
    "        logger.info(f\"ãƒ–ãƒ­ãƒƒã‚¯ãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼ã‚’åˆæœŸåŒ–é–‹å§‹ {num_blocks=} {block_size=}\")\n",
    "\n",
    "        # ãƒ–ãƒ­ãƒƒã‚¯ã«æ ¼ç´ã™ã‚‹ãƒˆãƒ¼ã‚¯ãƒ³æ•°\n",
    "        # 256\n",
    "        self.block_size = block_size\n",
    "\n",
    "        # å…¨ã¦ã®ç‰©ç†ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–\n",
    "        # 911\n",
    "        self.blocks: list[Block] = [Block(i) for i in range(num_blocks)]\n",
    "\n",
    "        # {ãƒãƒƒã‚·ãƒ¥å€¤: ãƒ–ãƒ­ãƒƒã‚¯ID} ã®è¾æ›¸ã‚’åˆæœŸåŒ–\n",
    "        # Prefix Cachingã®æ¤œç´¢ç”¨\n",
    "        self.hash_to_block_id: dict[int, int] = dict()\n",
    "\n",
    "        # ç©ºããƒ–ãƒ­ãƒƒã‚¯ã®ã‚­ãƒ¥ãƒ¼ã‚’åˆæœŸåŒ–\n",
    "        # 911\n",
    "        self.free_block_ids: deque[int] = deque(range(num_blocks))\n",
    "\n",
    "        # ä½¿ç”¨ä¸­ã®ãƒ–ãƒ­ãƒƒã‚¯IDã®ã‚»ãƒƒãƒˆã‚’åˆæœŸåŒ–\n",
    "        self.used_block_ids: set[int] = set()\n",
    "\n",
    "        logger.info(f\"ãƒ–ãƒ­ãƒƒã‚¯ãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼ã‚’åˆæœŸåŒ–å®Œäº†\")\n",
    "\n",
    "    @classmethod\n",
    "    def compute_hash(cls, token_ids: list[int], prefix: int = -1):\n",
    "        \"\"\"\n",
    "        é«˜é€Ÿãªãƒãƒƒã‚·ãƒ¥é–¢æ•°xxhashã‚’ä½¿ã„ã€ãƒˆãƒ¼ã‚¯ãƒ³åˆ—ã‹ã‚‰ä¸€æ„ãªIDã‚’ç”Ÿæˆã™ã‚‹\n",
    "        ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’æ¤œç´¢ã™ã‚‹éš›ã«ä½¿ç”¨\n",
    "        \"\"\"\n",
    "        logger.info(f\"ä¸€æ„ã®ãƒãƒƒã‚·ãƒ¥ã‚’è¨ˆç®— {token_ids=} {prefix=}\")\n",
    "\n",
    "        h = xxhash.xxh64()\n",
    "\n",
    "        if prefix != -1:\n",
    "            h.update(prefix.to_bytes(8, \"little\"))\n",
    "\n",
    "        h.update(np.array(token_ids).tobytes())\n",
    "\n",
    "        # ä¾‹: 7975407488731654516\n",
    "        res = h.intdigest()\n",
    "\n",
    "        logger.info(f\"ä¸€æ„ã®ãƒãƒƒã‚·ãƒ¥ã‚’è¨ˆç®—å®Œäº† {res=}\")\n",
    "        return res\n",
    "\n",
    "    def _allocate_block(self, block_id: int) -> Block:\n",
    "        \"\"\"\n",
    "        ç©ºããƒ–ãƒ­ãƒƒã‚¯ã‚’1ã¤å‰²ã‚Šå½“ã¦ã¦è¿”ã™\n",
    "        \"\"\"\n",
    "        logger.info(f\"ç©ºããƒ–ãƒ­ãƒƒã‚¯ã‚’å‰²ã‚Šå½“ã¦é–‹å§‹ {block_id=}\")\n",
    "\n",
    "        block = self.blocks[block_id]\n",
    "\n",
    "        assert block.ref_count == 0\n",
    "\n",
    "        # ref_countã‚’1ã«è¨­å®šã—ã¦ä½¿ç”¨ä¸­ã«ã™ã‚‹\n",
    "        block.reset()\n",
    "\n",
    "        self.free_block_ids.remove(block_id)\n",
    "\n",
    "        self.used_block_ids.add(block_id)\n",
    "\n",
    "        res = self.blocks[block_id]\n",
    "\n",
    "        logger.info(f\"ç©ºããƒ–ãƒ­ãƒƒã‚¯ã‚’å‰²ã‚Šå½“ã¦å®Œäº† {block_id=}\")\n",
    "        return res\n",
    "\n",
    "    def _deallocate_block(self, block_id: int) -> Block:\n",
    "        \"\"\"\n",
    "        ãƒ–ãƒ­ãƒƒã‚¯ã‚’1ã¤è§£æ”¾ã™ã‚‹\n",
    "        \"\"\"\n",
    "        logger.info(f\"ãƒ–ãƒ­ãƒƒã‚¯ã‚’è§£æ”¾é–‹å§‹ {block_id=}\")\n",
    "\n",
    "        # å‚ç…§ã‚«ã‚¦ãƒ³ãƒˆãŒ0ã§ã‚ã‚‹ã“ã¨ã‚’ç¢ºèª\n",
    "        assert self.blocks[block_id].ref_count == 0\n",
    "\n",
    "        self.used_block_ids.remove(block_id)\n",
    "\n",
    "        self.free_block_ids.append(block_id)\n",
    "\n",
    "        logger.info(f\"ãƒ–ãƒ­ãƒƒã‚¯ã‚’è§£æ”¾å®Œäº† {block_id=}\")\n",
    "\n",
    "    def can_allocate(self, seq: Sequence) -> bool:\n",
    "        result =  len(self.free_block_ids) >= seq.num_blocks\n",
    "        logger.info(f\"ãƒ–ãƒ­ãƒƒã‚¯ã‚’å‰²ã‚Šå½“ã¦å¯èƒ½ã‹ç¢ºèª {seq.seq_id} {seq.num_blocks=} {result=}\")\n",
    "        return result\n",
    "\n",
    "    def allocate(self, seq: Sequence):\n",
    "        \"\"\"\n",
    "        éå»ã®ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã®åˆ©ç”¨å¯å¦ã‚’ç¢ºèªã—ã€ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã«ç‰©ç†ãƒ–ãƒ­ãƒƒã‚¯ã‚’å‰²ã‚Šå½“ã¦ã‚‹\n",
    "\n",
    "        Args:\n",
    "            seq (Sequence): æ–°ã—ã„ã‚·ãƒ¼ã‚±ãƒ³ã‚¹\n",
    "        \"\"\"\n",
    "\n",
    "        logger.info(f\"ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã«ãƒ–ãƒ­ãƒƒã‚¯ã‚’å‰²ã‚Šå½“ã¦é–‹å§‹ {seq.seq_id}\")\n",
    "\n",
    "        assert not seq.block_table\n",
    "        h = -1\n",
    "        cache_miss = False\n",
    "\n",
    "        # ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ãŒå¿…è¦ã¨ã™ã‚‹ãƒ–ãƒ­ãƒƒã‚¯æ•°åˆ†ãƒ«ãƒ¼ãƒ—\n",
    "        for i in range(seq.num_blocks):\n",
    "            token_ids = seq.block(i)\n",
    "\n",
    "            # ã‚·ãƒ¼ã‚±ãƒ³ã‚¹é•·ãŒãƒ–ãƒ­ãƒƒã‚¯ã‚µã‚¤ã‚ºã¨åŒã˜å ´åˆã€ãƒãƒƒã‚·ãƒ¥ã‚’è¨ˆç®—\n",
    "            h = self.compute_hash(token_ids, h) if len(token_ids) == self.block_size else -1\n",
    "\n",
    "            # æ—¢å­˜ã®ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’æ¤œç´¢\n",
    "            block_id = self.hash_to_block_id.get(h, -1)\n",
    "\n",
    "            # å­˜åœ¨ã—ãªã„å ´åˆ\n",
    "            if block_id == -1 or self.blocks[block_id].token_ids != token_ids:\n",
    "                # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒŸã‚¹\n",
    "                cache_miss = True\n",
    "\n",
    "            # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒŸã‚¹ã®å ´åˆ\n",
    "            if cache_miss:\n",
    "                # ç©ºãã®ãƒ–ãƒ­ãƒƒã‚¯IDã‚’å–å¾—\n",
    "                block_id = self.free_block_ids[0]\n",
    "\n",
    "                # æ–°ã—ã„ãƒ–ãƒ­ãƒƒã‚¯ã‚’å‰²ã‚Šå½“ã¦ã‚‹\n",
    "                block = self._allocate_block(block_id)\n",
    "\n",
    "            # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ’ãƒƒãƒˆã®å ´åˆ\n",
    "            else:\n",
    "                # ã‚­ãƒ£ãƒƒã‚·ãƒ¥æ¸ˆã¿ã®ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã‚’æ›´æ–°\n",
    "                seq.num_cached_tokens += self.block_size\n",
    "\n",
    "                # æ—¢ã«ä½¿ç”¨ä¸­ã®å ´åˆ\n",
    "                if block_id in self.used_block_ids:\n",
    "                    # ãƒ–ãƒ­ãƒƒã‚¯ã‚’å…±æœ‰ï¼ˆPrefix Cachingï¼‰\n",
    "                    block = self.blocks[block_id]\n",
    "\n",
    "                    # å‚ç…§ã‚«ã‚¦ãƒ³ãƒˆã‚’å¢—ã‚„ã™\n",
    "                    block.ref_count += 1\n",
    "\n",
    "                # æœªä½¿ç”¨ã®å ´åˆï¼ˆç†è«–ä¸Šã“ã“ã«ã¯æ¥ãªã„ã¯ãšï¼‰\n",
    "                else:\n",
    "                    # ãƒ–ãƒ­ãƒƒã‚¯ã‚’å‰²ã‚Šå½“ã¦ã‚‹\n",
    "                    block = self._allocate_block(block_id)\n",
    "\n",
    "            # ãƒãƒƒã‚·ãƒ¥å€¤ãŒæœ‰åŠ¹ãªå ´åˆ\n",
    "            if h != -1:\n",
    "                # ãƒ–ãƒ­ãƒƒã‚¯ã®å†…å®¹ã‚’æ›´æ–°\n",
    "                block.update(h, token_ids)\n",
    "\n",
    "                # ãƒãƒƒã‚·ãƒ¥ãƒ†ãƒ¼ãƒ–ãƒ«ã«ç™»éŒ²\n",
    "                self.hash_to_block_id[h] = block_id\n",
    "\n",
    "            # ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã®ãƒ–ãƒ­ãƒƒã‚¯ãƒ†ãƒ¼ãƒ–ãƒ«ã«è¿½åŠ \n",
    "            seq.block_table.append(block_id)\n",
    "\n",
    "        logger.info(f\"ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã«ãƒ–ãƒ­ãƒƒã‚¯ã‚’å‰²ã‚Šå½“ã¦å®Œäº† {seq.seq_id} {seq.block_table=}\")\n",
    "\n",
    "\n",
    "    def deallocate(self, seq: Sequence):\n",
    "        \"\"\"\n",
    "        å‰²ã‚Šå½“ã¦ãŸãƒ–ãƒ­ãƒƒã‚¯ã‚’è§£æ”¾ã™ã‚‹\n",
    "\n",
    "        Args:\n",
    "            seq (Sequence): ä½¿ã„çµ‚ã‚ã£ãŸã‚·ãƒ¼ã‚±ãƒ³ã‚¹\n",
    "        \"\"\"\n",
    "        logger.info(f\"ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã®ãƒ–ãƒ­ãƒƒã‚¯ã‚’è§£æ”¾é–‹å§‹ {seq.seq_id}\")\n",
    "\n",
    "        # ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ãŒä½¿ç”¨ã—ã¦ã„ã‚‹ãƒ–ãƒ­ãƒƒã‚¯IDã®ãƒªã‚¹ãƒˆã‚’é€†é †ã§å‡¦ç†\n",
    "        for block_id in reversed(seq.block_table):\n",
    "\n",
    "            # ãƒ–ãƒ­ãƒƒã‚¯ã‚’å–å¾—\n",
    "            block = self.blocks[block_id]\n",
    "\n",
    "            # å‚ç…§ã‚«ã‚¦ãƒ³ãƒˆã‚’æ¸›ã‚‰ã™\n",
    "            block.ref_count -= 1\n",
    "\n",
    "            # å‚ç…§ã‚«ã‚¦ãƒ³ãƒˆãŒ0ã®å ´åˆ\n",
    "            if block.ref_count == 0:\n",
    "\n",
    "                # ç‰©ç†çš„ã«è§£æ”¾\n",
    "                self._deallocate_block(block_id)\n",
    "\n",
    "        # ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã®çŠ¶æ…‹ã‚’ãƒªã‚»ãƒƒãƒˆ\n",
    "        seq.num_cached_tokens = 0\n",
    "        seq.block_table.clear()\n",
    "\n",
    "        logger.info(f\"ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã®ãƒ–ãƒ­ãƒƒã‚¯ã‚’è§£æ”¾å®Œäº† {seq.seq_id}\")\n",
    "\n",
    "    def can_append(self, seq: Sequence) -> bool:\n",
    "        \"\"\"\n",
    "        æ–°ã—ã„ãƒ–ãƒ­ãƒƒã‚¯ãŒå¿…è¦ã«ã‚ã‚‹å ´åˆã€ç©ºãã‚’ç¢ºèªã™ã‚‹\n",
    "        \"\"\"\n",
    "        result = len(self.free_block_ids) >= (len(seq) % self.block_size == 1)\n",
    "        logger.info(f\"ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã«ãƒ–ãƒ­ãƒƒã‚¯ã‚’è¿½åŠ å¯èƒ½ã‹ç¢ºèª {seq.seq_id} {len(seq)=} {result=}\")\n",
    "        return result\n",
    "\n",
    "    def may_append(self, seq: Sequence):\n",
    "        \"\"\"\n",
    "        å¿…è¦ã«å¿œã˜ã¦ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã«ç‰©ç†ãƒ–ãƒ­ãƒƒã‚¯ã‚’è¿½åŠ ã™ã‚‹\n",
    "        1. ç›´å‰ã®ãƒ–ãƒ­ãƒƒã‚¯ãŒæº€æ¯ã®å ´åˆã€æ–°ã—ã„ãƒ–ãƒ­ãƒƒã‚¯ã‚’å‰²ã‚Šå½“ã¦ã‚‹\n",
    "        2. ç›´å‰ã®ãƒ–ãƒ­ãƒƒã‚¯ãŒã¡ã‚‡ã†ã©æº€æ¯ã®å ´åˆã€ãƒãƒƒã‚·ãƒ¥ã‚’è¨ˆç®—ã—ã¦ç™»éŒ²ã™ã‚‹\n",
    "        3. ç›´å‰ã®ãƒ–ãƒ­ãƒƒã‚¯ãŒæº€æ¯ã§ãªã„å ´åˆã€ä½•ã‚‚ã—ãªã„\n",
    "        \"\"\"\n",
    "        # ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã®ãƒ–ãƒ­ãƒƒã‚¯ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’å–å¾—\n",
    "        block_table = seq.block_table\n",
    "\n",
    "        # ç›´å‰ã®ãƒ–ãƒ­ãƒƒã‚¯ã‚’å–å¾—\n",
    "        last_block = self.blocks[block_table[-1]]\n",
    "\n",
    "        # ç›´å‰ã®ãƒ–ãƒ­ãƒƒã‚¯ãŒæº€æ¯ã®å ´åˆ\n",
    "        if len(seq) % self.block_size == 1:\n",
    "\n",
    "            # æ–°ã—ã„ãƒ–ãƒ­ãƒƒã‚¯ã‚’å‰²ã‚Šå½“ã¦ã‚‹\n",
    "            assert last_block.hash != -1\n",
    "            block_id = self.free_block_ids[0]\n",
    "            self._allocate_block(block_id)\n",
    "            block_table.append(block_id)\n",
    "            logger.info(f\"ç›´å‰ã®ãƒ–ãƒ­ãƒƒã‚¯ãŒæº€æ¯ã®ãŸã‚ã€æ–°ã—ã„ãƒ–ãƒ­ãƒƒã‚¯ã‚’è¿½åŠ  {seq.seq_id=} {block_id=}\")\n",
    "\n",
    "        # ç›´å‰ã®ãƒ–ãƒ­ãƒƒã‚¯ãŒã¡ã‚‡ã†ã©æº€æ¯ã®å ´åˆ\n",
    "        elif len(seq) % self.block_size == 0:\n",
    "\n",
    "            # ãƒ–ãƒ­ãƒƒã‚¯ã‚’æ›´æ–°ã™ã‚‹\n",
    "            assert last_block.hash == -1\n",
    "            token_ids = seq.block(seq.num_blocks-1)\n",
    "\n",
    "            # ç›´å‰ã®ãƒ–ãƒ­ãƒƒã‚¯ã®ãƒãƒƒã‚·ãƒ¥ã‚’ãƒ—ãƒ¬ãƒ•ã‚£ãƒƒã‚¯ã‚¹ã¨ã—ã¦å–å¾—\n",
    "            prefix = self.blocks[block_table[-2]].hash \\\n",
    "                if len(block_table) > 1 else -1\n",
    "\n",
    "            # ç¾åœ¨ã®ãƒ–ãƒ­ãƒƒã‚¯ã®ãƒãƒƒã‚·ãƒ¥ã‚’è¨ˆç®—\n",
    "            h = self.compute_hash(token_ids, prefix)\n",
    "\n",
    "            # ãƒ–ãƒ­ãƒƒã‚¯ã«ãƒãƒƒã‚·ãƒ¥ã‚’è¨­å®š\n",
    "            last_block.update(h, token_ids)\n",
    "\n",
    "            # ãƒãƒƒã‚·ãƒ¥ãƒ†ãƒ¼ãƒ–ãƒ«ã«ç™»éŒ²\n",
    "            logger.info(f\"ç›´å‰ã®ãƒ–ãƒ­ãƒƒã‚¯ãŒã¡ã‚‡ã†ã©æº€æ¯ã«ãªã£ãŸãŸã‚ã€ãƒãƒƒã‚·ãƒ¥ã‚’è¨ˆç®—ã—ã€hash_to_block_idã«ç™»éŒ² {seq.seq_id=} {h=}\")\n",
    "            self.hash_to_block_id[h] = last_block.block_id\n",
    "\n",
    "        # ç›´å‰ã®ãƒ–ãƒ­ãƒƒã‚¯ãŒæº€æ¯ã§ãªã„å ´åˆ\n",
    "        else:\n",
    "            assert last_block.hash == -1\n",
    "            logger.info(f\"ç›´å‰ã®ãƒ–ãƒ­ãƒƒã‚¯ã¯æº€æ¯ã§ã¯ãªã„ãŸã‚ã€ã‚¹ã‚­ãƒƒãƒ— {seq.seq_id=}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f4db693",
   "metadata": {},
   "source": [
    "### ModelRunner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11796e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def default_weight_loader(param: nn.Parameter, loaded_weight: torch.Tensor):\n",
    "    logger.info(f\"é‡ã¿ã‚’èª­ã¿è¾¼ã¿ {param.shape=} {loaded_weight.shape=}\")\n",
    "    param.data.copy_(loaded_weight)\n",
    "\n",
    "\n",
    "def load_model(model: nn.Module, path: str):\n",
    "    \"\"\"\n",
    "    HuggingFaceã®safetensorsãƒ•ã‚¡ã‚¤ãƒ«ã‚’é©åˆ‡ã«èª­ã¿è¾¼ã‚€\n",
    "    åå‰ã®å¤‰æ›ã¨çµåˆã•ã‚ŒãŸãƒ¬ã‚¤ãƒ¤ãƒ¼ã¸ã®æŒ¯ã‚Šåˆ†ã‘ã‚’è¡Œã†\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): é‡ã¿ã‚’èª­ã¿è¾¼ã‚€ãƒ¢ãƒ‡ãƒ«\n",
    "        path (str): safetensorsãƒ•ã‚¡ã‚¤ãƒ«ãŒä¿å­˜ã•ã‚Œã¦ã„ã‚‹ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ãƒ‘ã‚¹\n",
    "    \"\"\"\n",
    "\n",
    "    logger.info(f\"ãƒ¢ãƒ‡ãƒ«ã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿é–‹å§‹ {path=}\")\n",
    "\n",
    "    # ãƒãƒƒãƒ”ãƒ³ã‚°ã®å®šç¾©ã‚’å–å¾—\n",
    "    # ä¾‹: q_proj -> (qkv_proj, q)\n",
    "    packed_modules_mapping = getattr(model, \"packed_modules_mapping\", {})\n",
    "\n",
    "    # safetensorsãƒ•ã‚¡ã‚¤ãƒ«ã§ãƒ«ãƒ¼ãƒ—\n",
    "    for file in glob(os.path.join(path, \"*.safetensors\")):\n",
    "        logger.debug(f\"é‡ã¿ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‡¦ç†ä¸­ {file=}\")\n",
    "\n",
    "        # safetensorsãƒ•ã‚¡ã‚¤ãƒ«ã‚’é–‹ã\n",
    "        with safe_open(file, \"pt\", \"cpu\") as f:\n",
    "\n",
    "            # ãƒ•ã‚¡ã‚¤ãƒ«å†…ã®å„é‡ã¿ã§ãƒ«ãƒ¼ãƒ—\n",
    "            for weight_name in f.keys():\n",
    "                logger.debug(f\"é‡ã¿ã‚’èª­ã¿è¾¼ã¿ä¸­ {weight_name=}\")\n",
    "\n",
    "                # ãƒãƒƒãƒ”ãƒ³ã‚°ã®ã‚­ãƒ¼ã§ãƒ«ãƒ¼ãƒ—\n",
    "                for k in packed_modules_mapping:\n",
    "                    # ãƒãƒƒãƒ”ãƒ³ã‚°ã‚­ãƒ¼ãŒé‡ã¿åã«å«ã¾ã‚Œã¦ã„ã‚‹å ´åˆ\n",
    "                    # ä¾‹: q_projãŒå«ã¾ã‚Œã¦ã„ã‚‹å ´åˆ\n",
    "                    if k in weight_name:\n",
    "                        logger.debug(f\"ãƒãƒƒãƒ”ãƒ³ã‚°ã«ã‚ˆã‚‹é‡ã¿ã‚’èª­ã¿è¾¼ã¿ {weight_name=} {k=}\")\n",
    "\n",
    "                        # ãƒãƒƒãƒ”ãƒ³ã‚°ã‹ã‚‰æƒ…å ±ã‚’æŠ½å‡º\n",
    "                        v, shard_id = packed_modules_mapping[k]\n",
    "\n",
    "                        # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿åã‚’å¤‰æ›´\n",
    "                        # ä¾‹: ...q_proj... -> ...qkv_proj...\n",
    "                        param_name = weight_name.replace(k, v)\n",
    "\n",
    "                        # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’å–å¾—\n",
    "                        param = model.get_parameter(param_name)\n",
    "\n",
    "                        # ã‚«ã‚¹ã‚¿ãƒ ãƒ­ãƒ¼ãƒ€ãƒ¼ã‚’å‘¼ã³å‡ºã—\n",
    "                        weight_loader = getattr(param, \"weight_loader\")\n",
    "\n",
    "                        # é‡ã¿ã‚’èª­ã¿è¾¼ã¿\n",
    "                        # ã‚·ãƒ£ãƒ¼ãƒ‰IDã‚’æ¸¡ã™ã“ã¨ã§ã‚ªãƒ•ã‚»ãƒƒãƒˆã‚’æŒ‡å®šã§ãã‚‹ã‚ˆã†ã«ã™ã‚‹\n",
    "                        weight_loader(\n",
    "                            param,\n",
    "                            f.get_tensor(weight_name),\n",
    "                            shard_id)\n",
    "\n",
    "                        break\n",
    "\n",
    "                # é€šå¸¸ã®é‡ã¿èª­ã¿è¾¼ã¿\n",
    "                else:\n",
    "                    logger.debug(f\"é€šå¸¸ã®é‡ã¿èª­ã¿è¾¼ã¿ {weight_name=}\")\n",
    "\n",
    "                    # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’å–å¾—\n",
    "                    param = model.get_parameter(weight_name)\n",
    "\n",
    "                    # ã‚«ã‚¹ã‚¿ãƒ ãƒ­ãƒ¼ãƒ€ãƒ¼ãŒã‚ã‚Œã°ä½¿ç”¨ã—ãªã‘ã‚Œã°ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã‚’ä½¿ç”¨\n",
    "                    weight_loader = getattr(\n",
    "                        param, \"weight_loader\", default_weight_loader)\n",
    "\n",
    "                    # é‡ã¿ã‚’èª­ã¿è¾¼ã¿\n",
    "                    weight_loader(param, f.get_tensor(weight_name))\n",
    "\n",
    "    logger.info(\"ãƒ¢ãƒ‡ãƒ«ã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿å®Œäº†\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bedc0efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sampler(nn.Module):\n",
    "    \"\"\"\n",
    "    Gumbel-Max Trickã‚’ç”¨ã„ãŸã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        logger.info(f\"Samplerã‚’åˆæœŸåŒ–\")\n",
    "        super().__init__()\n",
    "        logger.info(f\"Samplerã®åˆæœŸåŒ–å®Œäº†\")\n",
    "\n",
    "    @torch.compile\n",
    "    def forward(self, logits: torch.Tensor, temperatures: torch.Tensor):\n",
    "        logger.info(f\"ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°é–‹å§‹ {logits.shape=}, {temperatures.shape=}\")\n",
    "\n",
    "        # ãƒ­ã‚¸ãƒƒãƒˆã‚’æ¸©åº¦ã§ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°\n",
    "        logits = logits.float().div_(temperatures.unsqueeze(dim=1))\n",
    "\n",
    "        # ç¢ºç‡åˆ†å¸ƒã«å¤‰æ›\n",
    "        probs = torch.softmax(logits, dim=-1)\n",
    "\n",
    "        # Gumbel-Max Trickã§ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°\n",
    "        # æŒ‡æ•°åˆ†å¸ƒ Exp(1) ã«å¾“ã†ãƒã‚¤ã‚ºã‚’ç”Ÿæˆã—ã€é€†æ•°ã‚’ä¹—ã˜ã¦æœ€å¤§å€¤ã‚’å–ã‚‹\n",
    "        # æ•°å­¦çš„ã«ã¯ argmax(log(probs) + Gumbel_noise) ã¨ç­‰ä¾¡\n",
    "        sample_tokens = probs.div_(\n",
    "            torch.empty_like(probs).exponential_(1).clamp_min_(1e-10)\n",
    "            ).argmax(dim=-1)\n",
    "\n",
    "        logger.info(f\"ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°å®Œäº† {sample_tokens.shape=}\")\n",
    "        return sample_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dcc6aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelRunner:\n",
    "    \"\"\"\n",
    "    ãƒ‡ãƒã‚¤ã‚¹ä¸Šã§ãƒ¢ãƒ‡ãƒ«ã®æ¨è«–ã‚’å®Ÿè¡Œã™ã‚‹ãŸã‚ã®ç®¡ç†ã‚¯ãƒ©ã‚¹\n",
    "    ãƒ—ãƒ­ã‚»ã‚¹ã”ã¨ã«ï¼ˆGPUã”ã¨ã«ï¼‰1ã¤ã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ãŒç”Ÿæˆã•ã‚Œã‚‹\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config: Config, rank: int, event: Event | list[Event]):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            config (Config): ãƒ¢ãƒ‡ãƒ«è¨­å®š\n",
    "            rank (int): ãƒ©ãƒ³ã‚¯ID\n",
    "            event (Event | list[Event]):\n",
    "                ãƒ©ãƒ³ã‚¯0ä»¥å¤–ã®ãƒ—ãƒ­ã‚»ã‚¹ã§ã¯åŒæœŸç”¨ã‚¤ãƒ™ãƒ³ãƒˆã€\n",
    "                ãƒ©ãƒ³ã‚¯0ã§ã¯ãƒ©ãƒ³ã‚¯1ä»¥é™ã®ã‚¤ãƒ™ãƒ³ãƒˆãƒªã‚¹ãƒˆ\n",
    "        \"\"\"\n",
    "\n",
    "        logger.info(f\"Initializing ModelRunner on rank {rank}\")\n",
    "\n",
    "        # 1) è¨­å®šã®ä¿å­˜\n",
    "\n",
    "        self.config = config\n",
    "\n",
    "        hf_config = config.hf_config\n",
    "\n",
    "        self.block_size = config.kvcache_block_size\n",
    "        self.enforce_eager = config.enforce_eager\n",
    "        self.world_size = config.tensor_parallel_size\n",
    "\n",
    "        # è‡ªèº«ã®ãƒ©ãƒ³ã‚¯ID\n",
    "        self.rank = rank\n",
    "\n",
    "        # åŒæœŸç”¨ã‚¤ãƒ™ãƒ³ãƒˆ\n",
    "        self.event = event\n",
    "\n",
    "        # 2) åˆ†æ•£ãƒ—ãƒ­ã‚»ã‚¹ã®åˆæœŸåŒ–\n",
    "\n",
    "        dist.init_process_group(\n",
    "            \"nccl\", # åˆ†æ•£å‡¦ç†ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ã‚’NCCLã«è¨­å®š\n",
    "            \"tcp://localhost:2333\", # 2333ãƒãƒ¼ãƒˆã‚’ä½¿ç”¨ã—ã¦ä»–ã®ãƒ—ãƒ­ã‚»ã‚¹ã¨åŒæœŸ\n",
    "            world_size=self.world_size,\n",
    "            rank=rank\n",
    "        )\n",
    "\n",
    "        # 3) ãƒ‡ãƒã‚¤ã‚¹è¨­å®š\n",
    "\n",
    "        # ã“ã®ãƒ‡ãƒã‚¤ã‚¹ãŒä½¿ç”¨ã™ã‚‹ãƒ‡ãƒã‚¤ã‚¹ã‚’è¨­å®š\n",
    "        torch.cuda.set_device(rank)\n",
    "\n",
    "        default_dtype = torch.get_default_dtype()\n",
    "\n",
    "        torch.set_default_dtype(hf_config.torch_dtype)\n",
    "\n",
    "        torch.set_default_device(\"cuda\")\n",
    "\n",
    "        # 4) ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿\n",
    "\n",
    "        # ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ©ãƒ³ãƒ€ãƒ ã«åˆæœŸåŒ–\n",
    "        self.model = Qwen3ForCausalLM(hf_config)\n",
    "\n",
    "        # ãƒ¢ãƒ‡ãƒ«ã«ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿\n",
    "        load_model(self.model, config.model)\n",
    "\n",
    "        # 5) ã‚µãƒ³ãƒ—ãƒ©ãƒ¼ã®åˆæœŸåŒ–\n",
    "\n",
    "        self.sampler = Sampler()\n",
    "\n",
    "        # 6) æœ€é©åŒ–\n",
    "\n",
    "        # é…å»¶åˆæœŸåŒ–ã®ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã‚„ãƒ¡ãƒ¢ãƒªå‰²ã‚Šå½“ã¦ã‚’å®Ÿè¡Œ\n",
    "        self.warmup_model()\n",
    "\n",
    "        # å¯èƒ½ãªé™ã‚Šå·¨å¤§ãªKVã‚­ãƒ£ãƒƒã‚·ãƒ¥é ˜åŸŸã‚’ç¢ºä¿\n",
    "        self.allocate_kv_cache()\n",
    "\n",
    "        if not self.enforce_eager:\n",
    "\n",
    "            # è¨ˆç®—ã‚°ãƒ©ãƒ•ã‚’ã‚­ãƒ£ãƒ—ãƒãƒ£\n",
    "            self.capture_cudagraph()\n",
    "\n",
    "        torch.set_default_device(\"cpu\")\n",
    "\n",
    "        torch.set_default_dtype(default_dtype)\n",
    "\n",
    "        # 7) åˆ†æ•£å‡¦ç†ã®åˆ†å²\n",
    "\n",
    "        if self.world_size > 1:\n",
    "\n",
    "            # ãƒ¡ã‚¤ãƒ³ãƒ—ãƒ­ã‚»ã‚¹ã®å ´åˆ\n",
    "            if rank == 0:\n",
    "\n",
    "                # å…±æœ‰ãƒ¡ãƒ¢ãƒªã‚’ä½œæˆ\n",
    "                # ãƒ¡ã‚¤ãƒ³ãƒ—ãƒ­ã‚»ã‚¹ã‹ã‚‰ãƒ¯ãƒ¼ã‚«ãƒ¼ã¸æŒ‡ä»¤ã‚’é€ã‚‹ãŸã‚ã®é ˜åŸŸ\n",
    "                # ãƒ¡ã‚½ãƒƒãƒ‰åãƒ»å¼•æ•°ã‚’æ ¼ç´ã™ã‚‹\n",
    "                self.shm = SharedMemory(\n",
    "                    name=\"nanovllm\",\n",
    "                    create=True,\n",
    "                    size=2**20 # 2^20ãƒã‚¤ãƒˆ = 1MB\n",
    "                )\n",
    "\n",
    "                dist.barrier()\n",
    "\n",
    "            # ãƒ¯ãƒ¼ã‚«ãƒ¼ãƒ—ãƒ­ã‚»ã‚¹ã®å ´åˆ\n",
    "            else:\n",
    "                dist.barrier()\n",
    "\n",
    "                # å…±æœ‰ãƒ¡ãƒ¢ãƒªã‚’é–‹ã\n",
    "                self.shm = SharedMemory(name=\"nanovllm\")\n",
    "\n",
    "                # ç„¡é™ãƒ«ãƒ¼ãƒ—ã«å…¥ã‚Šã€ãƒ¡ã‚¤ãƒ³ãƒ—ãƒ­ã‚»ã‚¹ã‹ã‚‰ã®æŒ‡ä»¤ã‚’å¾…æ©Ÿ\n",
    "                self.loop()\n",
    "\n",
    "        logger.info(f\"Initialized ModelRunner on rank {rank}\")\n",
    "\n",
    "    def exit(self):\n",
    "        \"\"\"\n",
    "        ãƒ¯ãƒ¼ã‚«ãƒ¼åœæ­¢æ™‚ã«ãƒªã‚½ãƒ¼ã‚¹ã‚’é–‹æ”¾ã™ã‚‹\n",
    "        \"\"\"\n",
    "        logger.info(f\"Exiting ModelRunner on rank {self.rank}\")\n",
    "\n",
    "        # ãƒãƒ«ãƒGPUç’°å¢ƒã®å ´åˆ\n",
    "        if self.world_size > 1:\n",
    "\n",
    "            # å…±æœ‰ãƒ¡ãƒ¢ãƒªã¸ã®ã‚¢ã‚¯ã‚»ã‚¹ã‚’é–‰ã˜ã‚‹\n",
    "            self.shm.close()\n",
    "\n",
    "            # å…¨ã¦ã®CPUãƒ—ãƒ­ã‚»ã‚¹ã‚’å¾…æ©Ÿ\n",
    "            dist.barrier()\n",
    "\n",
    "            # ãƒ¡ã‚¤ãƒ³ãƒ—ãƒ­ã‚»ã‚¹ã®å ´åˆ\n",
    "            if self.rank == 0:\n",
    "\n",
    "                # å…±æœ‰ãƒ¡ãƒ¢ãƒªã‚’å‰Šé™¤\n",
    "                self.shm.unlink()\n",
    "\n",
    "        # CUDA Graphã‚’ä½¿ç”¨ã—ã¦ã„ã‚‹å ´åˆ\n",
    "        if not self.enforce_eager:\n",
    "            # CUDA Graphã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã¨ãƒ¡ãƒ¢ãƒªãƒ—ãƒ¼ãƒ«ã‚’å‰Šé™¤\n",
    "            del self.graphs, self.graph_pool\n",
    "\n",
    "        # ãƒ—ãƒ­ã‚»ã‚¹å†…ã®CPUã¨GPUã®åŒæœŸ\n",
    "        torch.cuda.synchronize()\n",
    "\n",
    "        # åˆ†æ•£å‡¦ç†ç”¨ã®é€šä¿¡ã‚°ãƒ«ãƒ¼ãƒ—ã‚’ç ´æ£„\n",
    "        dist.destroy_process_group()\n",
    "\n",
    "        logger.info(f\"Exited ModelRunner on rank {self.rank}\")\n",
    "\n",
    "    def loop(self):\n",
    "        \"\"\"\n",
    "        ãƒ¯ãƒ¼ã‚«ãƒ¼ãƒ—ãƒ­ã‚»ã‚¹ã®ç„¡é™ãƒ«ãƒ¼ãƒ—\n",
    "        \"\"\"\n",
    "        logger.info(f\"ãƒ¯ãƒ¼ã‚«ãƒ¼ãƒ—ãƒ­ã‚»ã‚¹ã®ãƒ«ãƒ¼ãƒ—ã‚’é–‹å§‹ {self.rank}\")\n",
    "\n",
    "        while True:\n",
    "            # æŒ‡ä»¤ã‚’å¾…æ©Ÿ\n",
    "            method_name, args = self.read_shm()\n",
    "\n",
    "            # æŒ‡ä»¤ã‚’å®Ÿè¡Œ\n",
    "            self.call(method_name, *args)\n",
    "\n",
    "            # çµ‚äº†æŒ‡ä»¤ã®å ´åˆ\n",
    "            if method_name == \"exit\":\n",
    "                # ãƒ«ãƒ¼ãƒ—ã‚’æŠœã‘ã‚‹\n",
    "                break\n",
    "\n",
    "    def read_shm(self):\n",
    "        \"\"\"\n",
    "        read shared memory\n",
    "        ãƒ¯ãƒ¼ã‚«ãƒ¼ãƒ—ãƒ­ã‚»ã‚¹ãŒãƒ¡ã‚¤ãƒ³ãƒ—ãƒ­ã‚»ã‚¹ã‹ã‚‰ã®æŒ‡ä»¤ã‚’å¾…æ©Ÿã—ã¦å–å¾—ã™ã‚‹\n",
    "\n",
    "        Returns:\n",
    "            method_name (str): å®Ÿè¡Œã™ã‚‹ãƒ¡ã‚½ãƒƒãƒ‰å\n",
    "            args (list): ãƒ¡ã‚½ãƒƒãƒ‰ã®å¼•æ•°ãƒªã‚¹ãƒˆ\n",
    "        \"\"\"\n",
    "\n",
    "        # ãƒ¡ã‚¤ãƒ³ãƒ—ãƒ­ã‚»ã‚¹ä»¥å¤–ã§ã‚ã‚‹ã“ã¨ã‚’ç¢ºèª\n",
    "        assert self.world_size > 1 and self.rank > 0\n",
    "\n",
    "        # æŒ‡ä»¤ãŒæ¥ã‚‹ã¾ã§å¾…æ©Ÿ\n",
    "        self.event.wait()\n",
    "\n",
    "        # å…±æœ‰ãƒ¡ãƒ¢ãƒªã‹ã‚‰å…ˆé ­4ãƒã‚¤ãƒˆã‚’ãƒªãƒˆãƒ«ã‚¨ãƒ³ãƒ‡ã‚£ã‚¢ãƒ³ã§èª­ã¿è¾¼ã‚€\n",
    "        # nã¯èª­ã¿å–ã‚‹ã¹ããƒ‡ãƒ¼ã‚¿ã®é•·ã•\n",
    "        n = int.from_bytes(self.shm.buf[0:4], \"little\")\n",
    "\n",
    "        # å…±æœ‰ãƒ¡ãƒ¢ãƒªã‹ã‚‰nãƒã‚¤ãƒˆåˆ†ã®ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ã€pickleã§ãƒ‡ã‚·ãƒªã‚¢ãƒ©ã‚¤ã‚º\n",
    "        # [ãƒ¡ã‚½ãƒƒãƒ‰å, å¼•æ•°1, å¼•æ•°2, ...] ã®å½¢å¼ã§å–å¾—\n",
    "        method_name, *args = pickle.loads(self.shm.buf[4:n+4])\n",
    "\n",
    "        # æ¬¡ã®æŒ‡ä»¤å¾…æ©Ÿã®ãŸã‚ã«ã‚¤ãƒ™ãƒ³ãƒˆã‚’ã‚¯ãƒªã‚¢\n",
    "        self.event.clear()\n",
    "\n",
    "        return method_name, args\n",
    "\n",
    "    def write_shm(self, method_name, *args):\n",
    "        \"\"\"\n",
    "        write shared memory\n",
    "        ãƒ¡ã‚¤ãƒ³ãƒ—ãƒ­ã‚»ã‚¹ãŒæŒ‡ç¤ºãƒ‡ãƒ¼ã‚¿ã‚’å…±æœ‰ãƒ¡ãƒ¢ãƒªã«æ›¸ãè¾¼ã¿ã€\n",
    "        ã™ã¹ã¦ã®ãƒ¯ãƒ¼ã‚«ãƒ¼ãƒ—ãƒ­ã‚»ã‚¹ã®å¾…æ©ŸçŠ¶æ…‹ã‚’è§£é™¤ã™ã‚‹\n",
    "\n",
    "        Args:\n",
    "            method_name (str): å®Ÿè¡Œã™ã‚‹ãƒ¡ã‚½ãƒƒãƒ‰å\n",
    "            args (list): ãƒ¡ã‚½ãƒƒãƒ‰ã®å¼•æ•°ãƒªã‚¹ãƒˆ\n",
    "        \"\"\"\n",
    "\n",
    "        # 1) æŒ‡ç¤ºãƒ‡ãƒ¼ã‚¿ã‚’å…±æœ‰ãƒ¡ãƒ¢ãƒªã«æ›¸ãè¾¼ã‚€\n",
    "\n",
    "        # ãƒ¡ã‚¤ãƒ³ãƒ—ãƒ­ã‚»ã‚¹ã§ã‚ã‚‹ã“ã¨ã‚’ç¢ºèª\n",
    "        assert self.world_size > 1 and self.rank == 0\n",
    "\n",
    "        # æŒ‡ä»¤ãƒ‡ãƒ¼ã‚¿ã‚’pickleã§ãƒã‚¤ãƒˆåˆ—ã«ã‚·ãƒªã‚¢ãƒ©ã‚¤ã‚º\n",
    "        data = pickle.dumps([method_name, *args])\n",
    "\n",
    "        # ãƒã‚¤ãƒˆæ•°ã‚’å–å¾—\n",
    "        n = len(data)\n",
    "\n",
    "        # å…±æœ‰ãƒ¡ãƒ¢ãƒªã¸ãƒ‡ãƒ¼ã‚¿ã‚’ãƒªãƒˆãƒ«ã‚¨ãƒ³ãƒ‡ã‚£ã‚¢ãƒ³ã§æ›¸ãè¾¼ã‚€\n",
    "        self.shm.buf[0:4] = n.to_bytes(4, \"little\")\n",
    "\n",
    "        # ãƒ‡ãƒ¼ã‚¿æœ¬ä½“ã‚’æ›¸ãè¾¼ã‚€\n",
    "        self.shm.buf[4:n+4] = data\n",
    "\n",
    "        # å…¨ã¦ã®ãƒ¯ãƒ¼ã‚«ãƒ¼ãƒ—ãƒ­ã‚»ã‚¹ã«å¯¾å¿œã™ã‚‹ã‚¤ãƒ™ãƒ³ãƒˆã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã®ãƒªã‚¹ãƒˆã§ãƒ«ãƒ¼ãƒ—\n",
    "        for event in self.event:\n",
    "\n",
    "            # ã‚¤ãƒ™ãƒ³ãƒˆãƒ•ãƒ©ã‚°ã‚’æœ‰åŠ¹åŒ–\n",
    "            # ãƒ¯ãƒ¼ã‚«ãƒ¼ã®read_shmã®self.event.wait()ãŒè§£é™¤ã•ã‚Œã‚‹\n",
    "            event.set()\n",
    "\n",
    "    def call(self, method_name, *args):\n",
    "        \"\"\"\n",
    "        ãƒ¡ã‚¤ãƒ³ãƒ—ãƒ­ã‚»ã‚¹ã¨ãƒ¯ãƒ¼ã‚«ãƒ¼ãƒ—ãƒ­ã‚»ã‚¹ã®ä¸¡æ–¹ã«åŒã˜ãƒ¡ã‚½ãƒƒãƒ‰ã‚’ä¸€æ–‰ã«å®Ÿè¡Œã™ã‚‹\n",
    "        LLMEngine.step()ã€LLMEngine.exit()ã€ModelRunner.loop()ã‹ã‚‰å‘¼ã³å‡ºã•ã‚Œã‚‹\n",
    "\n",
    "        Args:\n",
    "            method_name (str):\n",
    "                å®Ÿè¡Œã™ã‚‹ãƒ¡ã‚½ãƒƒãƒ‰å\n",
    "                    \"run\": æ¨è«–å®Ÿè¡Œ\n",
    "                    \"exit\": ãƒ¯ãƒ¼ã‚«ãƒ¼ãƒ—ãƒ­ã‚»ã‚¹ã®çµ‚äº†\n",
    "            args (list): ãƒ¡ã‚½ãƒƒãƒ‰ã®å¼•æ•°ãƒªã‚¹ãƒˆ \n",
    "        \"\"\"\n",
    "\n",
    "        # ãƒãƒ«ãƒGPUç’°å¢ƒã§ã‹ã¤ãƒ¡ã‚¤ãƒ³ãƒ—ãƒ­ã‚»ã‚¹ã®å ´åˆ\n",
    "        if self.world_size > 1 and self.rank == 0:\n",
    "\n",
    "            # å…¨ã¦ã®ãƒ¯ãƒ¼ã‚«ãƒ¼ãƒ—ãƒ­ã‚»ã‚¹ã¸å¾…æ©Ÿè§£é™¤ã®æŒ‡ä»¤ã‚’é€ã‚‹\n",
    "            self.write_shm(method_name, *args)\n",
    "\n",
    "        # getattrã§æ–‡å­—åˆ—ã®ãƒ¡ã‚½ãƒƒãƒ‰ã‚’å®Ÿéš›ã®ãƒ¡ã‚½ãƒƒãƒ‰ã«å¤‰æ›\n",
    "        method = getattr(self, method_name, None)\n",
    "\n",
    "        # ãƒ¡ã‚½ãƒƒãƒ‰ã‚’å®Ÿè¡Œã—ã¦çµæœã‚’è¿”ã™\n",
    "        return method(*args)\n",
    "\n",
    "    def warmup_model(self):\n",
    "        \"\"\"\n",
    "        ãƒ€ãƒŸãƒ¼ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ã£ã¦é †ä¼æ¬ã—ã€ãƒ¡ãƒ¢ãƒªæœ€å¤§ä½¿ç”¨é‡ã‚’è¨ˆæ¸¬ã™ã‚‹\n",
    "        ModelRunner.__init__ã‹ã‚‰å‘¼ã³å‡ºã•ã‚Œã‚‹\n",
    "        æ¬¡ã®self.allocate_kv_cacheã§è¨ˆæ¸¬çµæœã‚’ä½¿ç”¨ã™ã‚‹\n",
    "        \"\"\"\n",
    "        logger.info(f\"ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—é–‹å§‹ {self.rank=}\")\n",
    "\n",
    "        # 1) ãƒ¡ãƒ¢ãƒªçµ±è¨ˆã®ãƒªã‚»ãƒƒãƒˆ\n",
    "\n",
    "        # CUDAã®ãƒ¡ãƒ¢ãƒªã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’è§£æ”¾\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        # ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã®è¨ˆæ¸¬ã‚«ã‚¦ãƒ³ã‚¿ãƒ¼ã‚’ã‚¼ãƒ­ã«ãƒªã‚»ãƒƒãƒˆ\n",
    "        torch.cuda.reset_peak_memory_stats()\n",
    "\n",
    "        # 2) ãƒ€ãƒŸãƒ¼ãƒ‡ãƒ¼ã‚¿ã®ä½œæˆ\n",
    "\n",
    "        # æœ€å¤§ãƒãƒƒãƒã‚µã‚¤ã‚ºã‚’å–å¾—\n",
    "        max_num_batched_tokens = self.config.max_num_batched_tokens\n",
    "\n",
    "        # æœ€å¤§ã‚·ãƒ¼ã‚±ãƒ³ã‚¹é•·ã‚’å–å¾—\n",
    "        max_model_len = self.config.max_model_len\n",
    "\n",
    "        # ãƒãƒƒãƒå†…ã®ã‚·ãƒ¼ã‚±ãƒ³ã‚¹æ•°ã‚’è¨ˆç®—\n",
    "        # OOMã‚’é˜²ããŸã‚ä¸¡æ–¹ã®åˆ¶ç´„ã‚’æº€ãŸã™æœ€å¤§å€¤ã‚’ä½¿ç”¨ã™ã‚‹\n",
    "        num_seqs = min(\n",
    "            max_num_batched_tokens // max_model_len,\n",
    "            self.config.max_num_seqs\n",
    "        )\n",
    "\n",
    "        # æœ€å¤§è² è·ã®ãƒ€ãƒŸãƒ¼ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã‚’ä½œæˆ\n",
    "        seqs = [Sequence([0] * max_model_len) for _ in range(num_seqs)]\n",
    "\n",
    "        # 3) ãƒ€ãƒŸãƒ¼å®Ÿè¡Œ\n",
    "\n",
    "        # is_prefill=Trueã§ãƒ¢ãƒ‡ãƒ«ã‚’å®Ÿè¡Œ\n",
    "        # ã“ã®å®Ÿè¡Œã«ã‚ˆã‚ŠKVã‚­ãƒ£ãƒƒã‚·ãƒ¥ã®æœ€å¤§ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ãŒç¢ºå®šã™ã‚‹\n",
    "        self.run(seqs, True)\n",
    "\n",
    "        # 4) å¾Œå‡¦ç†\n",
    "\n",
    "        # ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—ã§ä½¿ç”¨ã—ãŸãƒ¡ãƒ¢ãƒªã‚’è§£æ”¾\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        logger.info(f\"ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—å®Œäº† {self.rank=}\")\n",
    "\n",
    "    def allocate_kv_cache(self):\n",
    "        \"\"\"\n",
    "        GPUã®ç©ºãå®¹é‡ã‚’è¨ˆç®—ã—ã€æœ€å¤§ã®KVã‚­ãƒ£ãƒƒã‚·ãƒ¥é ˜åŸŸã‚’ç¢ºä¿ã™ã‚‹\n",
    "        \"\"\"\n",
    "        logger.info(f\"Allocating KV cache on rank {self.rank}\")\n",
    "\n",
    "        # 1) ç¾åœ¨ã®GPUãƒ¡ãƒ¢ãƒªçŠ¶æ³ã‚’å–å¾—\n",
    "\n",
    "        config = self.config\n",
    "        hf_config = config.hf_config\n",
    "\n",
    "        # OSã‹ã‚‰è¦‹ãŸGPUã®ç©ºãå®¹é‡ã¨åˆè¨ˆå®¹é‡ã‚’å–å¾—\n",
    "        free, total = torch.cuda.mem_get_info()\n",
    "\n",
    "        # OSã‹ã‚‰è¦‹ãŸç¾åœ¨ã®ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’è¨ˆç®—\n",
    "        used = total - free\n",
    "\n",
    "        # warmup_modelã§è¨ˆæ¸¬ã—ãŸæ¨è«–ä¸­ã®æœ€å¤§ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’å–å¾—\n",
    "        peak = torch.cuda.memory_stats()[\"allocated_bytes.all.peak\"]\n",
    "\n",
    "        # PyTorchãŒæ˜ç¤ºçš„ã«ç¢ºä¿ã—ãŸãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’å–å¾—\n",
    "        current = torch.cuda.memory_stats()[\"allocated_bytes.all.current\"]\n",
    "\n",
    "        # 2) 1ãƒ–ãƒ­ãƒƒã‚¯ã‚ãŸã‚Šã®ãƒã‚¤ãƒˆæ•°ã‚’è¨ˆç®—\n",
    "\n",
    "        # KVã‚­ãƒ£ãƒƒã‚·ãƒ¥ã®ãƒ˜ãƒƒãƒ‰æ•°ã‚’è¨ˆç®—\n",
    "        num_kv_heads = hf_config.num_key_value_heads // self.world_size\n",
    "\n",
    "        # ãƒ˜ãƒƒãƒ‰ã®æ¬¡å…ƒæ•°ã‚’å–å¾—\n",
    "        head_dim = getattr(\n",
    "            hf_config,\n",
    "            \"head_dim\",\n",
    "            hf_config.hidden_size // hf_config.num_attention_heads\n",
    "        )\n",
    "\n",
    "        # 1ã¤ã®ç‰©ç†ãƒ–ãƒ­ãƒƒã‚¯ï¼ˆå…¨å±¤åˆ†ï¼‰ã®ãƒ¡ãƒ¢ãƒªã‚µã‚¤ã‚ºã‚’è¨ˆç®—\n",
    "        # ã‚­ãƒ¼ã¨ãƒãƒªãƒ¥ãƒ¼ã®2ã¤ * ãƒ¬ã‚¤ãƒ¤ãƒ¼æ•° * ãƒ–ãƒ­ãƒƒã‚¯ã‚µã‚¤ã‚º * KVãƒ˜ãƒƒãƒ‰æ•° * ãƒ˜ãƒƒãƒ‰æ¬¡å…ƒæ•° * ãƒ‡ãƒ¼ã‚¿å‹ã®ãƒã‚¤ãƒˆæ•°\n",
    "        block_bytes = 2 * hf_config.num_hidden_layers * self.block_size * num_kv_heads * head_dim * hf_config.torch_dtype.itemsize\n",
    "\n",
    "        # 3) ç¢ºä¿å¯èƒ½ãªKVã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ–ãƒ­ãƒƒã‚¯æ•°ã‚’è¨ˆç®—\n",
    "\n",
    "        # KVã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä½¿ç”¨å¯èƒ½ãªãƒ¡ãƒ¢ãƒªå®¹é‡ã‚’è¨ˆç®—\n",
    "        # GPUå…¨ä½“ã®90% - PyTorchä»¥å¤–ã®ä½¿ç”¨é‡ - æ¨è«–ä¸­ã®ãƒ”ãƒ¼ã‚¯ä½¿ç”¨é‡\n",
    "        config.num_kvcache_blocks = int(total * config.gpu_memory_utilization - used - peak + current) // block_bytes\n",
    "\n",
    "        assert config.num_kvcache_blocks > 0\n",
    "\n",
    "        # 4) ç‰©ç†ãƒ¡ãƒ¢ãƒªã®ä¸€æ‹¬ç¢ºä¿\n",
    "\n",
    "        # å·¨å¤§ãªKVã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ†ãƒ³ã‚½ãƒ«ã‚’ä½œæˆ \n",
    "        self.kv_cache = torch.empty(\n",
    "            2,\n",
    "            hf_config.num_hidden_layers,\n",
    "            config.num_kvcache_blocks,\n",
    "            self.block_size,\n",
    "            num_kv_heads,\n",
    "            head_dim\n",
    "        )\n",
    "\n",
    "        # 5) å„å±¤ã«å‚ç…§æ¸¡ã—\n",
    "\n",
    "        layer_id = 0\n",
    "        for module in self.model.modules():\n",
    "            if hasattr(module, \"k_cache\") and hasattr(module, \"v_cache\"):\n",
    "                module.k_cache = self.kv_cache[0, layer_id]\n",
    "                module.v_cache = self.kv_cache[1, layer_id]\n",
    "                layer_id += 1\n",
    "\n",
    "        logger.info(f\"Allocated KV cache with {config.num_kvcache_blocks} blocks on rank {self.rank}\")\n",
    "\n",
    "    def prepare_block_tables(self, seqs: list[Sequence]):\n",
    "        \"\"\"\n",
    "        å„ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã®ãƒ–ãƒ­ãƒƒã‚¯ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’2æ¬¡å…ƒã®ãƒ†ãƒ³ã‚½ãƒ«ã«å¤‰æ›ã—GPUã«è»¢é€ã™ã‚‹\n",
    "\n",
    "        Args:\n",
    "            seqs (list[Sequence]): ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã®ãƒªã‚¹ãƒˆ\n",
    "        Returns:\n",
    "            block_tables (torch.Tensor): ãƒ–ãƒ­ãƒƒã‚¯ãƒ†ãƒ¼ãƒ–ãƒ«ã®2æ¬¡å…ƒãƒ†ãƒ³ã‚½ãƒ«\n",
    "        \"\"\"\n",
    "        logger.info(f\"Preparing block tables for {len(seqs)} sequences on rank {self.rank}\")\n",
    "\n",
    "        # 1) ãƒãƒƒãƒå†…ã§æœ€é•·ã®ãƒ–ãƒ­ãƒƒã‚¯ãƒ†ãƒ¼ãƒ–ãƒ«ã®é•·ã•ã‚’å–å¾—\n",
    "\n",
    "        max_len = max(len(seq.block_table) for seq in seqs)\n",
    "\n",
    "        # 2) çŸ­ã„ã‚·ãƒ¼ã‚±ãƒ³ã‚¹æœ«å°¾ã‚’-1ã§ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ã—ã¦é•·ã•ã‚’æƒãˆã‚‹\n",
    "\n",
    "        block_tables = [seq.block_table + [-1] * (max_len - len(seq.block_table)) for seq in seqs]\n",
    "\n",
    "        # 3) 2æ¬¡å…ƒãƒ†ãƒ³ã‚½ãƒ«ã«å¤‰æ›ã—ã¦GPUã«è»¢é€\n",
    "\n",
    "        block_tables = torch.tensor(\n",
    "            block_tables,\n",
    "            dtype=torch.int32,\n",
    "            pin_memory=True, # CPUå´ã®ãƒ¡ãƒ¢ãƒªã‚’å›ºå®šã—GPUè»¢é€ã‚’é«˜é€ŸåŒ–\n",
    "        ).cuda(non_blocking=True) # ãƒ‡ãƒ¼ã‚¿è»¢é€ã‚’éåŒæœŸåŒ–\n",
    "\n",
    "        logger.info(f\"Prepared block tables with shape {block_tables.shape} on rank {self.rank}\")\n",
    "\n",
    "        return block_tables\n",
    "\n",
    "    def prepare_prefill(self, seqs: list[Sequence]):\n",
    "        \"\"\"\n",
    "        Prefillï¼ˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆå‡¦ç†ï¼‰ã§å¯å¤‰é•·ã®å…¥åŠ›ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã®ãƒªã‚¹ãƒˆã‚’\n",
    "        å¹³å¦åŒ–ã—ã€Attentionè¨ˆç®—ã«å¿…è¦ãªãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’æº–å‚™ã™ã‚‹\n",
    "        FlashAttentionï¼ˆVarLenç‰ˆï¼‰ã®å…¥åŠ›ã¯1æ¬¡å…ƒãƒ†ãƒ³ã‚½ãƒ«ã§ã‚ã‚‹ãŸã‚\n",
    "\n",
    "        Args:\n",
    "            seqs (list[Sequence]): ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã®ãƒªã‚¹ãƒˆ\n",
    "        Returns:\n",
    "            input_ids (torch.Tensor): å¹³å¦åŒ–ã•ã‚ŒãŸå…¥åŠ›ãƒˆãƒ¼ã‚¯ãƒ³IDã®ãƒ†ãƒ³ã‚½ãƒ«\n",
    "            positions (torch.Tensor): å¹³å¦åŒ–ã•ã‚ŒãŸä½ç½®IDã®ãƒ†ãƒ³ã‚½ãƒ«\n",
    "        \"\"\"\n",
    "\n",
    "        # 1) å„ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã®ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’æº–å‚™\n",
    "\n",
    "        input_ids = []\n",
    "        positions = []\n",
    "        cu_seqlens_q = [0]\n",
    "        cu_seqlens_k = [0]\n",
    "        max_seqlen_q = 0\n",
    "        max_seqlen_k = 0\n",
    "        slot_mapping = []\n",
    "        block_tables = None\n",
    "\n",
    "        # 2) å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ã®å‡¦ç†\n",
    "\n",
    "        for seq in seqs:\n",
    "\n",
    "            # 2-1) å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ã®å¹³å¦åŒ–\n",
    "\n",
    "            seqlen = len(seq)\n",
    "\n",
    "            # ã‚­ãƒ£ãƒƒã‚·ãƒ¥æ¸ˆã¿ãƒˆãƒ¼ã‚¯ãƒ³ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ãŸã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã‚’å…¥åŠ›ã«è¿½åŠ \n",
    "            input_ids.extend(seq[seq.num_cached_tokens:])\n",
    "\n",
    "            # ãƒã‚¸ã‚·ãƒ§ãƒŠãƒ«ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ç”¨ã®ä½ç½®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚‚è¿½åŠ \n",
    "            positions.extend(list(range(seq.num_cached_tokens, seqlen)))\n",
    "\n",
    "            # 2-2) FlashAttentionç”¨ã®ç´¯ç©é•·ã‚’è¨ˆç®—\n",
    "\n",
    "            # ã‚¯ã‚¨ãƒªã®ã‚·ãƒ¼ã‚±ãƒ³ã‚¹é•·ã‚’è¨ˆç®—\n",
    "            seqlen_q = seqlen - seq.num_cached_tokens\n",
    "\n",
    "            # ã‚­ãƒ¼ã®ã‚·ãƒ¼ã‚±ãƒ³ã‚¹é•·ã‚’è¨ˆç®—\n",
    "            seqlen_k = seqlen\n",
    "\n",
    "            # ã‚¯ã‚¨ãƒªã®ç´¯ç©ã‚·ãƒ¼ã‚±ãƒ³ã‚¹é•·ã‚’æ›´æ–°\n",
    "            # ä¾‹: [0, 5, 12] -> ã‚·ãƒ¼ã‚±ãƒ³ã‚¹1ã®é•·ã•5ã€ã‚·ãƒ¼ã‚±ãƒ³ã‚¹2ã®é•·ã•7\n",
    "            cu_seqlens_q.append(cu_seqlens_q[-1] + seqlen_q)\n",
    "\n",
    "            # ã‚­ãƒ¼ã®ç´¯ç©ã‚·ãƒ¼ã‚±ãƒ³ã‚¹é•·ã‚’æ›´æ–°\n",
    "            cu_seqlens_k.append(cu_seqlens_k[-1] + seqlen_k)\n",
    "\n",
    "            # ã‚¯ã‚¨ãƒªã®æœ€å¤§ã‚·ãƒ¼ã‚±ãƒ³ã‚¹é•·ã‚’æ›´æ–°\n",
    "            max_seqlen_q = max(seqlen_q, max_seqlen_q)\n",
    "\n",
    "            # ã‚­ãƒ¼ã®æœ€å¤§ã‚·ãƒ¼ã‚±ãƒ³ã‚¹é•·ã‚’æ›´æ–°\n",
    "            max_seqlen_k = max(seqlen_k, max_seqlen_k)\n",
    "\n",
    "            if not seq.block_table:    # warmup\n",
    "                continue\n",
    "\n",
    "            # 2-3) PagedAttentionç”¨ã®ã‚¹ãƒ­ãƒƒãƒˆãƒãƒƒãƒ—ã‚’ä½œæˆ\n",
    "\n",
    "            # ã‚­ãƒ£ãƒƒã‚·ãƒ¥æ¸ˆã¿ãƒ–ãƒ­ãƒƒã‚¯ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¦ãƒ«ãƒ¼ãƒ—\n",
    "            for i in range(seq.num_cached_blocks, seq.num_blocks):\n",
    "\n",
    "                # iç•ªç›®ã®ãƒ–ãƒ­ãƒƒã‚¯ãŒæ ¼ç´ã•ã‚Œã¦ã„ã‚‹ç‰©ç†ãƒ¡ãƒ¢ãƒªã®é–‹å§‹ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’è¨ˆç®—\n",
    "                start = seq.block_table[i] * self.block_size\n",
    "\n",
    "                # æœ€å¾Œã®ãƒ–ãƒ­ãƒƒã‚¯ã§ã¯ãªã„å ´åˆ\n",
    "                if i != seq.num_blocks - 1:\n",
    "                    # ãƒ–ãƒ­ãƒƒã‚¯ã‚µã‚¤ã‚ºåˆ†ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’è¿½åŠ \n",
    "                    end = start + self.block_size\n",
    "\n",
    "                # æœ€å¾Œã®ãƒ–ãƒ­ãƒƒã‚¯ã®å ´åˆ\n",
    "                else:\n",
    "                    # æœ€å¾Œã®ãƒ–ãƒ­ãƒƒã‚¯ã®å®Ÿéš›ã®ãƒˆãƒ¼ã‚¯ãƒ³æ•°åˆ†ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’è¿½åŠ \n",
    "                    end = start + seq.last_block_num_tokens \n",
    "\n",
    "                # startã‹ã‚‰endã¾ã§ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ã‚¹ãƒ­ãƒƒãƒˆãƒãƒƒãƒ—ã«è¿½åŠ \n",
    "                slot_mapping.extend(list(range(start, end)))\n",
    "\n",
    "        # 3) Prefix Cacheã®åˆ¤å®š\n",
    "\n",
    "        # ã‚­ãƒ¼ã®ç´¯ç©ã‚·ãƒ¼ã‚±ãƒ³ã‚¹é•·ãŒã‚¯ã‚¨ãƒªã‚ˆã‚Šå¤§ãã„å ´åˆ\n",
    "        if cu_seqlens_k[-1] > cu_seqlens_q[-1]:\n",
    "\n",
    "            # Prefix Cacheã‚’ä½¿ç”¨ã™ã‚‹\n",
    "            # Prefix Cacheã¯ã€è¨ˆç®—ã—ãªã„ãŒå‚ç…§ã™ã‚‹éå»ã®ãƒˆãƒ¼ã‚¯ãƒ³\n",
    "            block_tables = self.prepare_block_tables(seqs)\n",
    "\n",
    "        # 4) ãƒ–ãƒ­ãƒƒã‚¯ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’æº–å‚™\n",
    "\n",
    "        input_ids = torch.tensor(\n",
    "            input_ids,\n",
    "            dtype=torch.int64,\n",
    "            pin_memory=True\n",
    "        ).cuda(non_blocking=True)\n",
    "\n",
    "        positions = torch.tensor(\n",
    "            positions,\n",
    "            dtype=torch.int64,\n",
    "            pin_memory=True\n",
    "        ).cuda(non_blocking=True)\n",
    "\n",
    "        cu_seqlens_q = torch.tensor(\n",
    "            cu_seqlens_q,\n",
    "            dtype=torch.int32,\n",
    "            pin_memory=True\n",
    "        ).cuda(non_blocking=True)\n",
    "\n",
    "        cu_seqlens_k = torch.tensor(\n",
    "            cu_seqlens_k,\n",
    "            dtype=torch.int32,\n",
    "            pin_memory=True\n",
    "        ).cuda(non_blocking=True)\n",
    "\n",
    "        slot_mapping = torch.tensor(\n",
    "            slot_mapping,\n",
    "            dtype=torch.int32,\n",
    "            pin_memory=True\n",
    "        ).cuda(non_blocking=True)\n",
    "\n",
    "        # 5) ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã¸ã®ç™»éŒ²\n",
    "\n",
    "        # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’ã‚°ãƒ­ãƒ¼ãƒãƒ«ãªã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã«ç™»éŒ²ã™ã‚‹\n",
    "        set_context(\n",
    "            True,\n",
    "            cu_seqlens_q,\n",
    "            cu_seqlens_k,\n",
    "            max_seqlen_q,\n",
    "            max_seqlen_k,\n",
    "            slot_mapping,\n",
    "            None,\n",
    "            block_tables)\n",
    "\n",
    "        return input_ids, positions\n",
    "\n",
    "    def prepare_decode(self, seqs: list[Sequence]):\n",
    "        \"\"\"\n",
    "        ãƒˆãƒ¼ã‚¯ãƒ³ç”Ÿæˆãƒ•ã‚§ãƒ¼ã‚ºï¼ˆDecodeï¼‰ã«ãŠã„ã¦æ¬¡ã®1ãƒˆãƒ¼ã‚¯ãƒ³ã‚’ç”Ÿæˆã™ã‚‹ãŸã‚ã®\n",
    "        å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ã¨PagedAttentionã®è¨ˆç®—ã«å¿…è¦ãªãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’æº–å‚™ã™ã‚‹\n",
    "\n",
    "        Args:\n",
    "            seqs (list[Sequence]): ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã®ãƒªã‚¹ãƒˆ\n",
    "        Returns:\n",
    "            input_ids (torch.Tensor): å…¥åŠ›ãƒˆãƒ¼ã‚¯ãƒ³IDã®ãƒ†ãƒ³ã‚½ãƒ«\n",
    "            positions (torch.Tensor): ä½ç½®IDã®ãƒ†ãƒ³ã‚½ãƒ«\n",
    "        \"\"\"\n",
    "        logger.info(f\"ãƒˆãƒ¼ã‚¯ãƒ³ç”Ÿæˆãƒ•ã‚§ãƒ¼ã‚ºã®ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’æº–å‚™é–‹å§‹ {len(seqs)=}\")\n",
    "\n",
    "        # 1) ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’åˆæœŸåŒ–\n",
    "\n",
    "        input_ids = []\n",
    "        positions = []\n",
    "        slot_mapping = []\n",
    "        context_lens = []\n",
    "\n",
    "        # 2) å„ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã‹ã‚‰ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º\n",
    "\n",
    "        # ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã®ãƒªã‚¹ãƒˆã§ãƒ«ãƒ¼ãƒ—\n",
    "        for seq in seqs:\n",
    "\n",
    "            # ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã®æœ€å¾Œã®ãƒˆãƒ¼ã‚¯ãƒ³IDã‚’å…¥åŠ›ã«è¿½åŠ \n",
    "            # Decodeã§ã¯ç›´å‰ã«ç”Ÿæˆã•ã‚ŒãŸ1ãƒˆãƒ¼ã‚¯ãƒ³ã®ã¿ã‚’å…¥åŠ›ã¨ã™ã‚‹ãŸã‚\n",
    "            input_ids.append(seq.last_token)\n",
    "\n",
    "            # æœ€å¾Œã®ãƒˆãƒ¼ã‚¯ãƒ³ã®ä½ç½®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’è¿½åŠ \n",
    "            positions.append(len(seq) - 1)\n",
    "\n",
    "            # ã‚·ãƒ¼ã‚±ãƒ³ã‚¹å…¨ä½“ã®é•·ã•\n",
    "            # Attentionè¨ˆç®—æ™‚ã«å¿…è¦\n",
    "            context_lens.append(len(seq))\n",
    "\n",
    "            # ã‚¹ãƒ­ãƒƒãƒˆãƒãƒƒãƒ”ãƒ³ã‚°ã®è¨ˆç®—\n",
    "            # ä»Šå›ã®å…¥åŠ›ã«å¯¾ã™ã‚‹KVã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’æ ¼ç´ã™ã‚‹ç‰©ç†ã‚¢ãƒ‰ãƒ¬ã‚¹ï¼ˆã‚¹ãƒ­ãƒƒãƒˆID)\n",
    "            slot_mapping.append(\n",
    "                seq.block_table[-1] * self.block_size + \\\n",
    "                seq.last_block_num_tokens  - 1)\n",
    "\n",
    "        # 3) ãƒ†ãƒ³ã‚½ãƒ«ã«å¤‰æ›ã—ã¦GPUã«è»¢é€\n",
    "\n",
    "        input_ids = torch.tensor(\n",
    "            input_ids,\n",
    "            dtype=torch.int64,\n",
    "            pin_memory=True).cuda(non_blocking=True)\n",
    "\n",
    "        positions = torch.tensor(\n",
    "            positions,\n",
    "            dtype=torch.int64,\n",
    "            pin_memory=True).cuda(non_blocking=True)\n",
    "\n",
    "        slot_mapping = torch.tensor(\n",
    "            slot_mapping,\n",
    "            dtype=torch.int32,\n",
    "            pin_memory=True).cuda(non_blocking=True)\n",
    "\n",
    "        context_lens = torch.tensor(\n",
    "            context_lens,\n",
    "            dtype=torch.int32,\n",
    "            pin_memory=True).cuda(non_blocking=True)\n",
    "\n",
    "        # 4) ãƒ–ãƒ­ãƒƒã‚¯ãƒ†ãƒ¼ãƒ–ãƒ«ã®æº–å‚™\n",
    "\n",
    "        # å„ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã®ãƒ–ãƒ­ãƒƒã‚¯IDã®ãƒªã‚¹ãƒˆã‚’2æ¬¡å…ƒãƒ†ãƒ³ã‚½ãƒ«ã«å¤‰æ›\n",
    "        block_tables = self.prepare_block_tables(seqs)\n",
    "\n",
    "        # 5) ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã¸ã®ç™»éŒ²\n",
    "\n",
    "        # PagedAttentionç”¨ã®ã‚«ãƒ¼ãƒãƒ«ã«å¿…è¦ãªãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã«ç™»éŒ²\n",
    "        set_context(\n",
    "            False, # ãƒ‡ã‚³ãƒ¼ãƒ‰ãƒ•ã‚§ãƒ¼ã‚º\n",
    "            slot_mapping=slot_mapping,\n",
    "            context_lens=context_lens,\n",
    "            block_tables=block_tables)\n",
    "\n",
    "        logger.info(f\"ãƒˆãƒ¼ã‚¯ãƒ³ç”Ÿæˆãƒ•ã‚§ãƒ¼ã‚ºã®ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’æº–å‚™å®Œäº† {len(input_ids)=} {len(positions)=}\")\n",
    "\n",
    "        return input_ids, positions\n",
    "\n",
    "    def prepare_sample(self, seqs: list[Sequence]):\n",
    "        \"\"\"\n",
    "        ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã«å¿…è¦ãªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æŠ½å‡ºã—ã¦GPUãƒ†ãƒ³ã‚½ãƒ«ã«ã¾ã¨ã‚ã‚‹\n",
    "        æ¨è«–ã‚µã‚¤ã‚¯ãƒ«ã®æœ€å¾Œã®ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°å‡¦ç†ã§ä½¿ç”¨ã™ã‚‹\n",
    "\n",
    "        Args:\n",
    "            seqs (list[Sequence]): ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã®ãƒªã‚¹ãƒˆ\n",
    "        Returns:\n",
    "            temperatures (torch.Tensor): å„ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã®æ¸©åº¦ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®ãƒ†ãƒ³ã‚½ãƒ«\n",
    "        \"\"\"\n",
    "        logger.info(f\"ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æº–å‚™é–‹å§‹ {len(seqs)=}\")\n",
    "\n",
    "        temperatures = []\n",
    "\n",
    "        for seq in seqs:\n",
    "            temperatures.append(seq.temperature)\n",
    "\n",
    "        temperatures = torch.tensor(\n",
    "            temperatures,\n",
    "            dtype=torch.float32,\n",
    "            pin_memory=True).cuda(non_blocking=True)\n",
    "\n",
    "        logger.info(f\"ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æº–å‚™å®Œäº† {temperatures=}\")\n",
    "\n",
    "        return temperatures\n",
    "\n",
    "    @torch.inference_mode()\n",
    "    def run_model(self, input_ids: torch.Tensor, positions: torch.Tensor, is_prefill: bool):\n",
    "        \"\"\"\n",
    "        æº–å‚™ã—ãŸå…¥åŠ›ãƒ‡ãƒ¼ã‚¿ã§ãƒ¢ãƒ‡ãƒ«ã‚’é †ä¼æ¬ã—ã€æ¬¡ã®ãƒˆãƒ¼ã‚¯ãƒ³äºˆæ¸¬ã®ãƒ­ã‚¸ãƒƒãƒˆã‚’è¨ˆç®—ã™ã‚‹\n",
    "        ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆå‡¦ç†ï¼ˆPrefill)ã¨ãƒˆãƒ¼ã‚¯ãƒ³ç”Ÿæˆï¼ˆDecode)ã®ä¸¡æ–¹ã«å¯¾å¿œ\n",
    "        ModelRunner.run()ã‹ã‚‰å‘¼ã³å‡ºã•ã‚Œã‚‹\n",
    "\n",
    "        Args:\n",
    "            input_ids (torch.Tensor): å…¥åŠ›ãƒˆãƒ¼ã‚¯ãƒ³IDã®ãƒ†ãƒ³ã‚½ãƒ«\n",
    "            positions (torch.Tensor): ä½ç½®IDã®ãƒ†ãƒ³ã‚½ãƒ«\n",
    "            is_prefill (bool): Prefillãƒ•ã‚§ãƒ¼ã‚ºã‹Decodeãƒ•ã‚§ãƒ¼ã‚ºã‹ã®ãƒ•ãƒ©ã‚°\n",
    "        Returns:\n",
    "            logits (torch.Tensor): æ¬¡ã®ãƒˆãƒ¼ã‚¯ãƒ³äºˆæ¸¬ã®ãƒ­ã‚¸ãƒƒãƒˆãƒ†ãƒ³ã‚½ãƒ«\n",
    "        \"\"\"\n",
    "        logger.info(f\"ãƒ¢ãƒ‡ãƒ«ã®é †ä¼æ¬ã‚’å®Ÿè¡Œé–‹å§‹ {input_ids.shape=} {positions.shape=} {is_prefill=}\")\n",
    "\n",
    "        # A) é€šå¸¸å®Ÿè¡Œã®å ´åˆï¼ˆPrefillã¾ãŸã¯Cuda GraphãŒç„¡åŠ¹åŒ–ï¼‰\n",
    "        # Prefillã®å ´åˆã¯å¯å¤‰é•·ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã«å¯¾å¿œã™ã‚‹å¿…è¦ãŒã‚ã‚‹ãŸã‚Graphã¯ä½¿ç”¨ã—ãªã„\n",
    "        if is_prefill or self.enforce_eager or input_ids.size(0) > 512:\n",
    "\n",
    "            # ãƒ¢ãƒ‡ãƒ«ã®é †ä¼æ¬ã‚’å®Ÿè¡Œã—ã€ãƒ­ã‚¸ãƒƒãƒˆã‚’è¨ˆç®—\n",
    "            result = self.model.compute_logits(\n",
    "                self.model(input_ids, positions)\n",
    "            )\n",
    "\n",
    "            logger.info(f\"ãƒ¢ãƒ‡ãƒ«ã®é †ä¼æ¬ï¼ˆprefillï¼‰ã‚’å®Ÿè¡Œå®Œäº† {result.shape=}\")\n",
    "            return result\n",
    "\n",
    "        # B) é«˜é€Ÿå®Ÿè¡Œï¼ˆDecode + CUDA Graphsï¼‰ã®å ´åˆ\n",
    "        else:\n",
    "\n",
    "            # B-1) åˆæœŸåŒ–\n",
    "\n",
    "            # ãƒãƒƒãƒã‚µã‚¤ã‚ºã‚’å–å¾—\n",
    "            bs = input_ids.size(0)\n",
    "\n",
    "            # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’å–å¾— \n",
    "            context = get_context()\n",
    "\n",
    "            # B-2) ã‚°ãƒ©ãƒ•ã®é¸æŠ\n",
    "\n",
    "            # ãƒãƒƒãƒã‚µã‚¤ã‚ºã‚’ã‚«ãƒãƒ¼ã§ãã‚‹æœ€å°ã®éŒ²ç”»æ¸ˆã¿ã‚°ãƒ©ãƒ•ã‚’é¸æŠ\n",
    "            graph = self.graphs[next(x for x in self.graph_bs if x >= bs)]\n",
    "\n",
    "            # B-3) ãƒ‡ãƒ¼ã‚¿ã®ã‚»ãƒƒãƒˆ\n",
    "\n",
    "            # ã‚°ãƒ©ãƒ•å°‚ç”¨ã®ãƒ¡ãƒ¢ãƒªé ˜åŸŸã®å‚ç…§ã‚’å–å¾—\n",
    "            graph_vars = self.graph_vars\n",
    "\n",
    "            # ã‚°ãƒ©ãƒ•å°‚ç”¨ã®ãƒ¡ãƒ¢ãƒªé ˜åŸŸã«ãƒ‡ãƒ¼ã‚¿ã‚’ã‚³ãƒ”ãƒ¼\n",
    "            graph_vars[\"input_ids\"][:bs] = input_ids\n",
    "            graph_vars[\"positions\"][:bs] = positions\n",
    "            graph_vars[\"slot_mapping\"].fill_(-1)\n",
    "            graph_vars[\"slot_mapping\"][:bs] = context.slot_mapping\n",
    "            graph_vars[\"context_lens\"].zero_()\n",
    "            graph_vars[\"context_lens\"][:bs] = context.context_lens\n",
    "            graph_vars[\"block_tables\"][:bs, :context.block_tables.size(1)] = context.block_tables\n",
    "\n",
    "            # B-4) ã‚°ãƒ©ãƒ•ã®å†ç”Ÿ\n",
    "\n",
    "            # CPUä»‹å…¥ãªã—ã«ä¸€æ°—ã«GPUã‚«ãƒ¼ãƒãƒ«ã‚’å®Ÿè¡Œ\n",
    "            graph.replay()\n",
    "\n",
    "            # B-5) çµæœã®å–å¾—\n",
    "\n",
    "            # çµæœã‚’å–ã‚Šå‡ºã—ã¦ãƒ­ã‚¸ãƒƒãƒˆã‚’è¨ˆç®—\n",
    "            result = self.model.compute_logits(graph_vars[\"outputs\"][:bs])\n",
    "\n",
    "            logger.info(f\"ãƒ¢ãƒ‡ãƒ«ã®é †ä¼æ’­ï¼ˆdecodeï¼‰ã‚’å®Ÿè¡Œå®Œäº† {result.shape=}\")\n",
    "            return result\n",
    "\n",
    "    def run(self, seqs: list[Sequence], is_prefill: bool) -> list[int]:\n",
    "        \"\"\"\n",
    "        ModelRunnerã®ãƒ¡ã‚¤ãƒ³ãƒ¡ã‚½ãƒƒãƒ‰\n",
    "        å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ã‚’æº–å‚™ã—ã€ãƒ¢ãƒ‡ãƒ«ã‚’å®Ÿè¡Œã—ã€æ¬¡ã®ãƒˆãƒ¼ã‚¯ãƒ³ã‚’æ±ºå®šã™ã‚‹\n",
    "\n",
    "        Args:\n",
    "            seqs (list[Sequence]): ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã®ãƒªã‚¹ãƒˆ\n",
    "            is_prefill (bool): Prefillãƒ•ã‚§ãƒ¼ã‚ºã‹Decodeãƒ•ã‚§ãƒ¼ã‚ºã‹ã®ãƒ•ãƒ©ã‚°\n",
    "        Returns:\n",
    "            token_ids (list[int]): ç”Ÿæˆã•ã‚ŒãŸãƒˆãƒ¼ã‚¯ãƒ³IDã®ãƒªã‚¹ãƒˆ\n",
    "        \"\"\"\n",
    "        logger.info(f\"æ¨è«–ã®1ã‚µã‚¤ã‚¯ãƒ«ã‚’å®Ÿè¡Œé–‹å§‹ {len(seqs)=} {is_prefill=}\")\n",
    "\n",
    "        # 1) å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ã®æº–å‚™\n",
    "\n",
    "        input_ids, positions = self.prepare_prefill(seqs) if is_prefill \\\n",
    "            else self.prepare_decode(seqs)\n",
    "\n",
    "        # 2) ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®æº–å‚™\n",
    "\n",
    "        temperatures = self.prepare_sample(seqs) if self.rank == 0 \\\n",
    "            else None\n",
    "\n",
    "        # 3) ãƒ¢ãƒ‡ãƒ«ã®é †ä¼æ¬ã‚’å®Ÿè¡Œã—ã€ãƒ­ã‚¸ãƒƒãƒˆã‚’è¨ˆç®—\n",
    "\n",
    "        logits = self.run_model(input_ids, positions, is_prefill)\n",
    "\n",
    "        # 4) ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã‚’å®Ÿè¡Œã—ã€æ¬¡ã®ãƒˆãƒ¼ã‚¯ãƒ³ã‚’æ±ºå®š\n",
    "\n",
    "        token_ids = self.sampler(logits, temperatures).tolist() \\\n",
    "            if self.rank == 0 else None\n",
    "\n",
    "        # 5) ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã®ãƒªã‚»ãƒƒãƒˆ\n",
    "\n",
    "        reset_context()\n",
    "\n",
    "        logger.info(f\"æ¨è«–ã®1ã‚µã‚¤ã‚¯ãƒ«ã‚’å®Ÿè¡Œå®Œäº† {(len(token_ids) if token_ids else None)=}\")\n",
    "\n",
    "        return token_ids\n",
    "\n",
    "    @torch.inference_mode()\n",
    "    def capture_cudagraph(self):\n",
    "        \"\"\"\n",
    "        æ§˜ã€…ãªãƒãƒƒãƒã‚µã‚¤ã‚ºã®è¨ˆç®—ã‚°ãƒ©ãƒ•ã‚’äº‹å‰ã«ã‚­ãƒ£ãƒ—ãƒãƒ£ã™ã‚‹\n",
    "        Decodeãƒ•ã‚§ãƒ¼ã‚ºã®è¨ˆç®—ã‚’é«˜é€ŸåŒ–ã™ã‚‹ãŸã‚\n",
    "        ModelRunner.__init__()ã‹ã‚‰å‘¼ã³å‡ºã•ã‚Œã‚‹\n",
    "        \"\"\"\n",
    "\n",
    "        logger.info(f\"CUDA Graphã‚’ã‚­ãƒ£ãƒ—ãƒãƒ£é–‹å§‹ {self.rank=}\")\n",
    "\n",
    "        # 1) ã‚­ãƒ£ãƒ—ãƒãƒ£ç”¨ã®å›ºå®šãƒ¡ãƒ¢ãƒªã‚’åˆæœŸåŒ–\n",
    "\n",
    "        config = self.config\n",
    "\n",
    "        hf_config = config.hf_config\n",
    "\n",
    "        # æœ€å¤§ãƒãƒƒãƒã‚µã‚¤ã‚º\n",
    "        max_bs = min(self.config.max_num_seqs, 512)\n",
    "\n",
    "        # æœ€å¤§ãƒ–ãƒ­ãƒƒã‚¯æ•°\n",
    "        max_num_blocks = (config.max_model_len + self.block_size - 1) \\\n",
    "            // self.block_size\n",
    "\n",
    "        # å…¥åŠ›\n",
    "        input_ids = torch.zeros(max_bs, dtype=torch.int64)\n",
    "\n",
    "        # ä½ç½®\n",
    "        positions = torch.zeros(max_bs, dtype=torch.int64)\n",
    "\n",
    "        # ã‚¹ãƒ­ãƒƒãƒˆãƒãƒƒãƒ”ãƒ³ã‚°\n",
    "        slot_mapping = torch.zeros(max_bs, dtype=torch.int32)\n",
    "\n",
    "        # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆé•·\n",
    "        context_lens = torch.zeros(max_bs, dtype=torch.int32)\n",
    "\n",
    "        # ãƒ–ãƒ­ãƒƒã‚¯ãƒ†ãƒ¼ãƒ–ãƒ«\n",
    "        block_tables = torch.zeros(max_bs, max_num_blocks, dtype=torch.int32)\n",
    "\n",
    "        # å‡ºåŠ›\n",
    "        outputs = torch.zeros(max_bs, hf_config.hidden_size)\n",
    "\n",
    "        # 2) ã‚­ãƒ£ãƒ—ãƒãƒ£ã™ã‚‹ãƒãƒƒãƒã‚µã‚¤ã‚ºã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æ±ºã‚ã‚‹\n",
    "\n",
    "        # ãƒãƒƒãƒã‚µã‚¤ã‚ºã®ãƒ‘ã‚¿ãƒ¼ãƒ³\n",
    "        # 1, 2, 4, 8, 16, 32, ..., max_bs\n",
    "        self.graph_bs = [1, 2, 4, 8] + list(range(16, max_bs + 1, 16))\n",
    "\n",
    "        self.graphs = {}\n",
    "\n",
    "        self.graph_pool = None\n",
    "\n",
    "        logger.debug(f\"{self.rank=} ã‚­ãƒ£ãƒ—ãƒãƒ£ãƒ‘ã‚¿ãƒ¼ãƒ³: {self.graph_bs}\")\n",
    "\n",
    "        # 3) ãƒ‘ã‚¿ãƒ¼ãƒ³ã”ã¨ã«è¨ˆç®—ã‚°ãƒ©ãƒ•ã‚’ã‚­ãƒ£ãƒ—ãƒãƒ£ã™ã‚‹\n",
    "\n",
    "        # ãƒãƒƒãƒã‚µã‚¤ã‚ºã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã§é€†é †ã«ãƒ«ãƒ¼ãƒ—\n",
    "        for bs in reversed(self.graph_bs):\n",
    "\n",
    "            # 3-1) ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—\n",
    "\n",
    "            # CUDA Graphã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’ä½œæˆ\n",
    "            graph = torch.cuda.CUDAGraph()\n",
    "\n",
    "            # PagedAttentionç”¨ã®ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã«ç™»éŒ²\n",
    "            set_context(\n",
    "                False,\n",
    "                slot_mapping=slot_mapping[:bs],\n",
    "                context_lens=context_lens[:bs],\n",
    "                block_tables=block_tables[:bs])\n",
    "\n",
    "            # ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—å®Ÿè¡Œ\n",
    "            # ã‚«ãƒ¼ãƒãƒ«ã®ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«ãƒ»ãƒ¡ãƒ¢ãƒªã‚¢ãƒ­ã‚±ãƒ¼ã‚¿ãƒ¼ã®åˆæœŸåŒ–\n",
    "            outputs[:bs] = self.model(input_ids[:bs], positions[:bs])\n",
    "\n",
    "            # 3-2) ã‚­ãƒ£ãƒ—ãƒãƒ£é–‹å§‹\n",
    "\n",
    "            # ãƒ¡ãƒ¢ãƒªãƒ—ãƒ¼ãƒ«ã‚’å…±æœ‰ã—ã€ã‚°ãƒ©ãƒ•ã”ã¨ã«ä½œæ¥­é ˜åŸŸã‚’ä½¿ã„å›ã™\n",
    "            with torch.cuda.graph(graph, self.graph_pool):\n",
    "\n",
    "                # ãƒ¢ãƒ‡ãƒ«ã®é †ä¼æ¬ã‚’ã‚­ãƒ£ãƒ—ãƒãƒ£\n",
    "                outputs[:bs] = self.model(input_ids[:bs], positions[:bs])\n",
    "\n",
    "            # ãƒ¡ãƒ¢ãƒªãƒ—ãƒ¼ãƒ«ãŒãªã„å ´åˆ\n",
    "            if self.graph_pool is None:\n",
    "                # æœ€åˆã«ã‚­ãƒ£ãƒ—ãƒãƒ£ã—ãŸã‚°ãƒ©ãƒ•ã‹ã‚‰ãƒ¡ãƒ¢ãƒªãƒ«ãƒ¼ãƒ—ã‚’å–å¾—\n",
    "                # ãƒ¡ãƒ¢ãƒªãƒ«ãƒ¼ãƒ—ã¯ä½¿ç”¨ã—ãŸãƒ¡ãƒ¢ãƒªé ˜åŸŸã®ç®¡ç†æƒ…å ±\n",
    "                self.graph_pool = graph.pool()\n",
    "\n",
    "            # 3-3) ã‚°ãƒ©ãƒ•ã‚’ä¿å­˜\n",
    "\n",
    "            self.graphs[bs] = graph\n",
    "\n",
    "            # GPUä¸Šã§ã‚­ãƒ£ãƒ—ãƒãƒ£ãŒçµ‚ã‚ã‚‹ã¾ã§å¾…æ©Ÿ\n",
    "            torch.cuda.synchronize()\n",
    "\n",
    "            # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒªã‚»ãƒƒãƒˆ\n",
    "            reset_context()\n",
    "\n",
    "        # 4) ã‚­ãƒ£ãƒ—ãƒãƒ£ã«ä½¿ç”¨ã—ãŸå›ºå®šãƒ¡ãƒ¢ãƒªã¸ã®å‚ç…§ã‚’ä¿å­˜\n",
    "\n",
    "        # ã‚­ãƒ£ãƒ—ãƒãƒ£ã«ä½¿ç”¨ã—ãŸãƒ†ãƒ³ã‚½ãƒ«ã¸ã®å‚ç…§ã‚’è¾æ›¸ã«ã¾ã¨ã‚ã‚‹\n",
    "        # å®Ÿè¡Œæ™‚ã¯ã“ã“ã«ãƒ‡ãƒ¼ã‚¿ã‚’ã‚³ãƒ”ãƒ¼ã—ã¦ã‹ã‚‰ã‚°ãƒ©ãƒ•ã‚’å†ç”Ÿã™ã‚‹\n",
    "        self.graph_vars = dict(\n",
    "            input_ids=input_ids,\n",
    "            positions=positions,\n",
    "            slot_mapping=slot_mapping,\n",
    "            context_lens=context_lens,\n",
    "            block_tables=block_tables,\n",
    "            outputs=outputs,\n",
    "        )\n",
    "\n",
    "        logger.info(f\"CUDA Graphã‚’ã‚­ãƒ£ãƒ—ãƒãƒ£å®Œäº† {self.rank=}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "939f7d76",
   "metadata": {},
   "source": [
    "### Scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae5dd058",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Scheduler:\n",
    "    \"\"\"\n",
    "    æ¨è«–ãƒªã‚¯ã‚¨ã‚¹ãƒˆã®ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ã¨KVã‚­ãƒ£ãƒƒã‚·ãƒ¥ã®ç®¡ç†ã‚’è¡Œã†ã‚¯ãƒ©ã‚¹\n",
    "    ãƒ¡ã‚¤ãƒ³ãƒ—ãƒ­ã‚»ã‚¹ï¼ˆãƒ©ãƒ³ã‚¯0ï¼‰ã®CPUä¸Šã«ã®ã¿å­˜åœ¨ã™ã‚‹\n",
    "    LLMEngine.__init__ã§åˆæœŸåŒ–ã•ã‚Œã‚‹\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config: Config):\n",
    "        logger.info(f\"ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ã‚’åˆæœŸåŒ– {config.max_num_seqs=}, {config.max_num_batched_tokens=} {config.eos=}, {config.num_kvcache_blocks=}, {config.kvcache_block_size=}\")\n",
    "\n",
    "        # 1) åˆ¶ç´„æ¡ä»¶ã®èª­ã¿è¾¼ã¿\n",
    "\n",
    "        # åŒæ™‚ã«å‡¦ç†ã§ãã‚‹ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã®æœ€å¤§æ•°\n",
    "        self.max_num_seqs = config.max_num_seqs\n",
    "\n",
    "        # 1ãƒãƒƒãƒã«å«ã‚ã‚‰ã‚Œã‚‹æœ€å¤§ãƒˆãƒ¼ã‚¯ãƒ³æ•°\n",
    "        self.max_num_batched_tokens = config.max_num_batched_tokens\n",
    "\n",
    "        # çµ‚äº†ãƒˆãƒ¼ã‚¯ãƒ³ID\n",
    "        self.eos = config.eos\n",
    "\n",
    "        # 2) ãƒ–ãƒ­ãƒƒã‚¯ãƒãƒãƒ¼ã‚¸ãƒ£ã®åˆæœŸåŒ–\n",
    "\n",
    "        # BlockManagerã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’ä½œæˆ\n",
    "        self.block_manager = BlockManager(\n",
    "            config.num_kvcache_blocks, # ç¢ºä¿ã•ã‚ŒãŸç‰©ç†ãƒ–ãƒ­ãƒƒã‚¯ã®ç·æ•°\n",
    "            config.kvcache_block_size) # 1ãƒ–ãƒ­ãƒƒã‚¯ã‚ãŸã‚Šã®ãƒˆãƒ¼ã‚¯ãƒ³æ•°\n",
    "\n",
    "        # 3) ã‚­ãƒ¥ãƒ¼ã®åˆæœŸåŒ–\n",
    "\n",
    "        # GPUãƒ¡ãƒ¢ãƒªãŒå‰²ã‚Šå½“ã¦ã‚‰ã‚Œã¦ã„ãªã„å¾…æ©Ÿä¸­ãƒªã‚¯ã‚¨ã‚¹ãƒˆ\n",
    "        self.waiting: deque[Sequence] = deque()\n",
    "\n",
    "        # GPUãƒ¡ãƒ¢ãƒªãŒå‰²ã‚Šå½“ã¦ã‚‰ã‚Œã¦ã„ã‚‹æ¨è«–ä¸­ã®ãƒªã‚¯ã‚¨ã‚¹ãƒˆ\n",
    "        self.running: deque[Sequence] = deque()\n",
    "\n",
    "        logger.info(f\"ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ã‚’åˆæœŸåŒ–å®Œäº†\")\n",
    "\n",
    "    def is_finished(self):\n",
    "        \"\"\"\n",
    "        ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãŒç®¡ç†ã—ã¦ã„ã‚‹ã™ã¹ã¦ã®ãƒªã‚¯ã‚¨ã‚¹ãƒˆãŒå®Œäº†ã—ã¦ã„ã‚‹ã‹ã©ã†ã‹ã‚’è¿”ã™\n",
    "        \"\"\"\n",
    "        result = not self.waiting and not self.running\n",
    "        logger.info(f\"ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ã®å®Œäº†çŠ¶æ…‹ã‚’ç¢ºèª {result=}\")\n",
    "        return result\n",
    "\n",
    "    def add(self, seq: Sequence):\n",
    "        \"\"\"\n",
    "        ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ã®waitingã‚­ãƒ¥ãƒ¼ã«æ–°ã—ã„ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’è¿½åŠ ã™ã‚‹\n",
    "        LLMEngine.add_requestã‹ã‚‰å‘¼ã³å‡ºã•ã‚Œã‚‹\n",
    "\n",
    "        Args:\n",
    "            seq (Sequence): è¿½åŠ ã™ã‚‹ã‚·ãƒ¼ã‚±ãƒ³ã‚¹\n",
    "        \"\"\"\n",
    "        self.waiting.append(seq)\n",
    "        logger.info(f\"ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ã«æ–°ã—ã„ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’è¿½åŠ  {seq.seq_id=}\")\n",
    "\n",
    "    def schedule(self) -> tuple[list[Sequence], bool]:\n",
    "        \"\"\"\n",
    "        æ¬¡ã«å‡¦ç†ã™ã‚‹Sequenceã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã®ãƒªã‚¹ãƒˆã‚’è¿”ã™\n",
    "        Decodeã‚ˆã‚ŠPrefillã®ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’å„ªå…ˆ\n",
    "        LLMEngineã®æ¨è«–ã‚µã‚¤ã‚¯ãƒ«ã®æœ€åˆã«å‘¼ã³å‡ºã•ã‚Œã‚‹\n",
    "\n",
    "        Returns:\n",
    "            tuple[list[Sequence], bool]:\n",
    "                ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ã•ã‚ŒãŸã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã®ãƒªã‚¹ãƒˆ\n",
    "                prefillã‹decodeã‹ã®ãƒ•ãƒ©ã‚°\n",
    "        \"\"\"\n",
    "        logger.info(f\"ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒªãƒ³ã‚°ã‚’é–‹å§‹\")\n",
    "\n",
    "        # 1) Prefillãƒªã‚¯ã‚¨ã‚¹ãƒˆã®ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒªãƒ³ã‚°\n",
    "\n",
    "        scheduled_seqs = []\n",
    "        num_seqs = 0\n",
    "        num_batched_tokens = 0\n",
    "\n",
    "        # waitingã‚­ãƒ¥ãƒ¼ã‚’ãƒ«ãƒ¼ãƒ—\n",
    "        while self.waiting and num_seqs < self.max_num_seqs:\n",
    "\n",
    "            # å…ˆé ­ã®ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã‚’å–å¾—\n",
    "            seq = self.waiting[0]\n",
    "\n",
    "            # ãƒ¡ãƒ¢ãƒªä¸è¶³ã«ãªã‚‹å ´åˆã€ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒªãƒ³ã‚°ã‚’ä¸­æ–­\n",
    "            if num_batched_tokens + len(seq) > self.max_num_batched_tokens \\\n",
    "                or not self.block_manager.can_allocate(seq):\n",
    "                break\n",
    "\n",
    "\n",
    "            num_seqs += 1\n",
    "\n",
    "            # ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã«ãƒ–ãƒ­ãƒƒã‚¯ã‚’å‰²ã‚Šå½“ã¦\n",
    "            self.block_manager.allocate(seq)\n",
    "\n",
    "            num_batched_tokens += len(seq) - seq.num_cached_tokens\n",
    "\n",
    "            # ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã®çŠ¶æ…‹ã‚’å®Ÿè¡Œä¸­ã«å¤‰æ›´\n",
    "            seq.status = SequenceStatus.RUNNING\n",
    "\n",
    "            self.waiting.popleft()\n",
    "\n",
    "            self.running.append(seq)\n",
    "\n",
    "            scheduled_seqs.append(seq)\n",
    "\n",
    "        # Prefillãƒªã‚¯ã‚¨ã‚¹ãƒˆãŒã‚ã£ãŸå ´åˆã€ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’çµ‚äº†\n",
    "        if scheduled_seqs:\n",
    "            logger.info(f\"ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒªãƒ³ã‚°å®Œäº†ï¼ˆPrefillï¼‰ {len(scheduled_seqs)=}\")\n",
    "            return scheduled_seqs, True\n",
    "\n",
    "        # 2) Decodeãƒªã‚¯ã‚¨ã‚¹ãƒˆã®ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒªãƒ³ã‚°\n",
    "\n",
    "        # runningã‚­ãƒ¥ãƒ¼ã‚’ãƒ«ãƒ¼ãƒ—\n",
    "        while self.running and num_seqs < self.max_num_seqs:\n",
    "\n",
    "            # å…ˆé ­ã®ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã‚’å–å¾—\n",
    "            seq = self.running.popleft()\n",
    "\n",
    "            # æ¬¡ã®ãƒˆãƒ¼ã‚¯ãƒ³ã‚’ãƒ¡ãƒ¢ãƒªä¸è¶³ã§è¿½åŠ ã§ããªã„å ´åˆ\n",
    "            while not self.block_manager.can_append(seq):\n",
    "\n",
    "                # ç¾åœ¨å®Ÿè¡Œä¸­ã®ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ãŒã‚ã‚‹å ´åˆ\n",
    "                if self.running:\n",
    "                    # æœ«å°¾ã®ä½å„ªå…ˆåº¦ã®ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã‚’waitingã‚­ãƒ¥ãƒ¼ã«æˆ»ã™\n",
    "                    self.preempt(self.running.pop())\n",
    "                else:\n",
    "                    # è‡ªåˆ†è‡ªèº«ã‚’waitingã‚­ãƒ¥ãƒ¼ã«æˆ»ã™\n",
    "                    self.preempt(seq)\n",
    "                    break\n",
    "\n",
    "            # æ¬¡ã®ãƒˆãƒ¼ã‚¯ãƒ³ã‚’è¿½åŠ å¯èƒ½ãªå ´åˆã€ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ã«åŠ ãˆã‚‹\n",
    "            else:\n",
    "                num_seqs += 1\n",
    "                self.block_manager.may_append(seq)\n",
    "                scheduled_seqs.append(seq)\n",
    "\n",
    "        assert scheduled_seqs\n",
    "        \n",
    "        # ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ã•ã‚ŒãŸã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã‚’runningã‚­ãƒ¥ãƒ¼ã®å…ˆé ­ã«æˆ»ã™\n",
    "        self.running.extendleft(reversed(scheduled_seqs))\n",
    "\n",
    "        logger.info(f\"ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒªãƒ³ã‚°å®Œäº†ï¼ˆDecodeï¼‰ {len(scheduled_seqs)=}\")\n",
    "        return scheduled_seqs, False\n",
    "\n",
    "    def preempt(self, seq: Sequence):\n",
    "        \"\"\"\n",
    "        å®Ÿè¡Œä¸­ã®ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’ä¸­æ–­ã—waitingã‚­ãƒ¥ãƒ¼ã«æˆ»ã—ã€ãƒ¡ãƒ¢ãƒªã‚’è§£æ”¾ã™ã‚‹\n",
    "\n",
    "        Args:\n",
    "            seq (Sequence): ä¸­æ–­ã™ã‚‹ã‚·ãƒ¼ã‚±ãƒ³ã‚¹\n",
    "        \"\"\"\n",
    "        logger.info(f\"å®Ÿè¡Œä¸­ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’ä¸­æ–­é–‹å§‹ {seq.seq_id=}\")\n",
    "\n",
    "        # ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã®çŠ¶æ…‹ã‚’å®Ÿè¡Œä¸­ã‹ã‚‰å¾…æ©Ÿä¸­ã«å¤‰æ›´\n",
    "        seq.status = SequenceStatus.WAITING\n",
    "\n",
    "        # ãƒ–ãƒ­ãƒƒã‚¯ãƒãƒãƒ¼ã‚¸ãƒ£ã‹ã‚‰ãƒ¡ãƒ¢ãƒªã‚’è§£æ”¾\n",
    "        self.block_manager.deallocate(seq)\n",
    "\n",
    "        # waitingã‚­ãƒ¥ãƒ¼ã®å…ˆé ­ã«ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã‚’æˆ»ã™\n",
    "        self.waiting.appendleft(seq)\n",
    "\n",
    "        logger.info(f\"å®Ÿè¡Œä¸­ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’ä¸­æ–­å®Œäº† {seq.seq_id=}\")\n",
    "\n",
    "    def postprocess(self, seqs: list[Sequence], token_ids: list[int]) -> list[bool]:\n",
    "        \"\"\"\n",
    "        ç”Ÿæˆã—ãŸãƒˆãƒ¼ã‚¯ãƒ³ã‚’å„ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã«è¿½åŠ ã—ã€çµ‚äº†åˆ¤å®šã‚’è¡Œã†\n",
    "\n",
    "        Args:\n",
    "            seqs (list[Sequence]): ãƒˆãƒ¼ã‚¯ãƒ³ã‚’è¿½åŠ ã™ã‚‹ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã®ãƒªã‚¹ãƒˆ\n",
    "            token_ids (list[int]): å„ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã«è¿½åŠ ã™ã‚‹ãƒˆãƒ¼ã‚¯ãƒ³IDã®ãƒªã‚¹ãƒˆ\n",
    "        Returns:\n",
    "            list[bool]: å„ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ãŒçµ‚äº†ã—ãŸã‹ã©ã†ã‹ã®ãƒªã‚¹ãƒˆ\n",
    "        \"\"\"\n",
    "        logger.info(f\"ç”Ÿæˆãƒˆãƒ¼ã‚¯ãƒ³ã®å¾Œå‡¦ç†ã‚’é–‹å§‹ {len(seqs)=} {len(token_ids)=}\")\n",
    "\n",
    "        # ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã¨ç”Ÿæˆã—ãŸãƒˆãƒ¼ã‚¯ãƒ³IDã‚’ãƒ«ãƒ¼ãƒ—\n",
    "        for seq, token_id in zip(seqs, token_ids):\n",
    "\n",
    "            # ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã«ãƒˆãƒ¼ã‚¯ãƒ³ã‚’è¿½åŠ \n",
    "            seq.append_token(token_id)\n",
    "\n",
    "            # çµ‚äº†åˆ¤å®š\n",
    "            if (not seq.ignore_eos and token_id == self.eos) or \\\n",
    "                seq.num_completion_tokens == seq.max_tokens:\n",
    "\n",
    "                # ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã®çŠ¶æ…‹ã‚’çµ‚äº†ã«å¤‰æ›´\n",
    "                seq.status = SequenceStatus.FINISHED\n",
    "\n",
    "                # ãƒ–ãƒ­ãƒƒã‚¯ãƒãƒãƒ¼ã‚¸ãƒ£ã‹ã‚‰ãƒ¡ãƒ¢ãƒªã‚’è§£æ”¾\n",
    "                self.block_manager.deallocate(seq)\n",
    "\n",
    "                # runningã‚­ãƒ¥ãƒ¼ã‹ã‚‰ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã‚’å‰Šé™¤\n",
    "                self.running.remove(seq)\n",
    "\n",
    "        logger.info(f\"ç”Ÿæˆãƒˆãƒ¼ã‚¯ãƒ³ã®å¾Œå‡¦ç†å®Œäº† {len(seqs)=}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b628408b",
   "metadata": {},
   "source": [
    "### LLMEngine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa2bbf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LLMEngine:\n",
    "    \"\"\"\n",
    "    ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒè§¦ã‚Œã‚‹ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ã‚¤ã‚¹ã‚¯ãƒ©ã‚¹\n",
    "    ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ã¨ãƒ¢ãƒ‡ãƒ«ãƒ©ãƒ³ãƒŠãƒ¼ã‚’ã¤ãªãã€æ¨è«–ã‚µã‚¤ã‚¯ãƒ«ã‚’å®Ÿè¡Œã™ã‚‹\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model, **kwargs):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            model (str): ãƒ¢ãƒ‡ãƒ«åã¾ãŸã¯ãƒ‘ã‚¹\n",
    "        \"\"\"\n",
    "        logger.info(f\"ã‚¨ãƒ³ã‚¸ãƒ³ã‚’åˆæœŸåŒ–é–‹å§‹ {model=}\")\n",
    "\n",
    "\n",
    "        # 1) è¨­å®šã®åˆæœŸåŒ–\n",
    "\n",
    "        config_fields = {field.name for field in fields(Config)}\n",
    "        config_kwargs = {k: v for k, v in kwargs.items() if k in config_fields}\n",
    "        config = Config(model, **config_kwargs)\n",
    "\n",
    "        # 2) ãƒ¯ãƒ¼ã‚«ãƒ¼ãƒ—ãƒ­ã‚»ã‚¹ã®èµ·å‹•\n",
    "\n",
    "        self.ps = []\n",
    "        self.events = []\n",
    "\n",
    "        # ãƒ—ãƒ­ã‚»ã‚¹ç”Ÿæˆæ–¹å¼ãŒspawnã§ã‚ã‚‹ãƒ•ã‚¡ã‚¯ãƒˆãƒªctxã‚’ä½œæˆ\n",
    "        # spawnã¯ã€æ–°ã—ã„ç©ºã®Pythonã‚¤ãƒ³ã‚¿ãƒ—ãƒªã‚¿ã‚’èµ·å‹•ã—ã¦ã‚¿ãƒ¼ã‚²ãƒƒãƒˆé–¢æ•°ã‚’å®Ÿè¡Œã™ã‚‹\n",
    "        ctx = mp.get_context(\"spawn\")\n",
    "\n",
    "        # ãƒ©ãƒ³ã‚¯1ä»¥é™ã®ãƒ¯ãƒ¼ã‚«ãƒ¼ãƒ—ãƒ­ã‚»ã‚¹ã‚’ç”Ÿæˆ\n",
    "        for i in range(1, config.tensor_parallel_size):\n",
    "\n",
    "            # ã‚¤ãƒ™ãƒ³ãƒˆã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’ä½œæˆ\n",
    "            event = ctx.Event()\n",
    "\n",
    "            # ãƒ¯ãƒ¼ã‚«ãƒ¼ãƒ—ãƒ­ã‚»ã‚¹ã§ModelRunnerã‚’å®Ÿè¡Œ\n",
    "            process = ctx.Process(target=ModelRunner, args=(config, i, event))\n",
    "            process.start()\n",
    "\n",
    "            self.ps.append(process)\n",
    "            self.events.append(event)\n",
    "\n",
    "        # 3) ãƒ¡ã‚¤ãƒ³ãƒ—ãƒ­ã‚»ã‚¹ã®åˆæœŸåŒ–\n",
    "\n",
    "        # ãƒ©ãƒ³ã‚¯0ï¼ˆãƒ¡ã‚¤ãƒ³ãƒ—ãƒ­ã‚»ã‚¹ï¼‰ã®ModelRunnerã‚’ä½œæˆ\n",
    "        self.model_runner = ModelRunner(config, 0, self.events)\n",
    "\n",
    "        # 4) ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®åˆæœŸåŒ–\n",
    "\n",
    "        # ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã‚’èª­ã¿è¾¼ã¿\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\n",
    "            config.model, use_fast=True\n",
    "        )\n",
    "\n",
    "        # çµ‚äº†åˆ¤å®šã«ä½¿ç”¨ã™ã‚‹EOSãƒˆãƒ¼ã‚¯ãƒ³IDã‚’å–å¾—\n",
    "        config.eos = self.tokenizer.eos_token_id\n",
    "\n",
    "        # 5) ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼ã®åˆæœŸåŒ–\n",
    "\n",
    "        # ãƒªã‚¯ã‚¨ã‚¹ãƒˆã®é †ç•ªå¾…ã¡ã‚„ã€KVã‚­ãƒ£ãƒƒã‚·ãƒ¥ã®ãƒ¡ãƒ¢ãƒªãƒ–ãƒ­ãƒƒã‚¯ç®¡ç†ã‚’è¡Œã†ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©\n",
    "        self.scheduler = Scheduler(config)\n",
    "\n",
    "        # 6) çµ‚äº†å‡¦ç†ã®ç™»éŒ²\n",
    "\n",
    "        # ãƒ¯ãƒ¼ã‚«ãƒ¼ãƒ—ãƒ­ã‚»ã‚¹ãŒç¢ºå®Ÿã«çµ‚äº†ã™ã‚‹ã‚ˆã†ã«atexitã§ç™»éŒ²\n",
    "        atexit.register(self.exit)\n",
    "\n",
    "        logger.info(f\"ã‚¨ãƒ³ã‚¸ãƒ³ã®åˆæœŸåŒ–å®Œäº†\")\n",
    "\n",
    "    def exit(self):\n",
    "        \"\"\"\n",
    "        ã‚¨ãƒ³ã‚¸ãƒ³åœæ­¢æ™‚ã«ãƒ¯ãƒ¼ã‚«ãƒ¼ãƒ—ãƒ­ã‚»ã‚¹ã‚„å…±æœ‰ãƒ¡ãƒ¢ãƒªã‚’è§£æ”¾ã™ã‚‹\n",
    "        \"\"\"\n",
    "        logger.info(f\"ã‚¨ãƒ³ã‚¸ãƒ³ã®çµ‚äº†å‡¦ç†é–‹å§‹\")\n",
    "\n",
    "        # ãƒ¡ã‚¤ãƒ³ãƒ—ãƒ­ã‚»ã‚¹ã®ModelRunnerã®exitãƒ¡ã‚½ãƒƒãƒ‰ã‚’å®Ÿè¡Œ\n",
    "        self.model_runner.call(\"exit\")\n",
    "\n",
    "        # ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’å‰Šé™¤\n",
    "        del self.model_runner\n",
    "\n",
    "        # ãƒ¯ãƒ¼ã‚«ãƒ¼ãƒ—ãƒ­ã‚»ã‚¹ã§ãƒ«ãƒ¼ãƒ—\n",
    "        for p in self.ps:\n",
    "            # ãƒ—ãƒ­ã‚»ã‚¹çµ‚äº†ã‚’å¾…æ©Ÿ\n",
    "            p.join()\n",
    "\n",
    "        logger.info(f\"ã‚¨ãƒ³ã‚¸ãƒ³ã®çµ‚äº†å‡¦ç†å®Œäº†\")\n",
    "\n",
    "    def add_request(self, prompt: str | list[int], sampling_params: SamplingParams):\n",
    "        \"\"\"\n",
    "        ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’Sequenceã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã«å¤‰æ›ã—ã€ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ã«ç™»éŒ²ã™ã‚‹\n",
    "        LLMEngine.generate()ã‹ã‚‰å‘¼ã³å‡ºã•ã‚Œã‚‹\n",
    "\n",
    "        Args:\n",
    "            prompt (str | list[int]): ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæ–‡å­—åˆ—ã¾ãŸã¯ãƒˆãƒ¼ã‚¯ãƒ³IDã®ãƒªã‚¹ãƒˆ\n",
    "            sampling_params (SamplingParams): ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿\n",
    "        \"\"\"\n",
    "        logger.info(f\"ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ã«ç™»éŒ²é–‹å§‹ {prompt=} {sampling_params=}\")\n",
    "\n",
    "        # 1) Sequenceã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã«å¤‰æ›\n",
    "\n",
    "        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãŒæ–‡å­—åˆ—ã®å ´åˆ\n",
    "        if isinstance(prompt, str):\n",
    "            # ãƒˆãƒ¼ã‚¯ãƒ³IDã®ãƒªã‚¹ãƒˆã«å¤‰æ›\n",
    "            prompt = self.tokenizer.encode(prompt)\n",
    "\n",
    "        # Sequenceã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’ä½œæˆ\n",
    "        seq = Sequence(prompt, sampling_params)\n",
    "\n",
    "        # 2) ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ã«ç™»éŒ²\n",
    "\n",
    "        # ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ã‚’ã‚¦ã‚§ã‚¤ãƒ†ã‚£ãƒ³ã‚°ã‚­ãƒ¥ãƒ¼ã«ç™»éŒ²\n",
    "        self.scheduler.add(seq)\n",
    "\n",
    "        logger.info(f\"ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ã«ç™»éŒ²å®Œäº† {seq.seq_id=}\")\n",
    "\n",
    "    def step(self):\n",
    "        \"\"\"\n",
    "        æ¨è«–ã‚¨ãƒ³ã‚¸ãƒ³ã®1ã‚µã‚¤ã‚¯ãƒ«ã‚’å®Ÿè¡Œã—ã€å®Œäº†ã—ãŸã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã®å‡ºåŠ›ã‚’è¿”ã™\n",
    "        self.generate()ã‹ã‚‰å‘¼ã³å‡ºã•ã‚Œã‚‹\n",
    "        \"\"\"\n",
    "\n",
    "        logger.info(f\"æ¨è«–ã‚¨ãƒ³ã‚¸ãƒ³ã®1ã‚µã‚¤ã‚¯ãƒ«ã‚’å®Ÿè¡Œé–‹å§‹\")\n",
    "\n",
    "        # 1) ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒªãƒ³ã‚°\n",
    "\n",
    "        # å‡¦ç†ã™ã‚‹Sequenceã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ã‹ã‚‰å–å¾—\n",
    "        # is_prefillã«ã‚ˆã‚Šã€prefillï¼ˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆå‡¦ç†ï¼‰ã‹decodeï¼ˆç”Ÿæˆå‡¦ç†ï¼‰ã‹ã‚’åˆ¤å®š\n",
    "        seqs, is_prefill = self.scheduler.schedule()\n",
    "\n",
    "        # 2) ãƒ¢ãƒ‡ãƒ«å®Ÿè¡Œ\n",
    "\n",
    "        # GPUãƒ¯ãƒ¼ã‚«ãƒ¼ã«ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã‚’æ¸¡ã—ã¦runãƒ¡ã‚½ãƒƒãƒ‰ã‚’å®Ÿè¡Œã—ã€ãƒˆãƒ¼ã‚¯ãƒ³ã‚’ç”Ÿæˆ\n",
    "        token_ids = self.model_runner.call(\"run\", seqs, is_prefill)\n",
    "\n",
    "        # 3) å¾Œå‡¦ç†\n",
    "\n",
    "        # æ–°ã—ã„ãƒˆãƒ¼ã‚¯ãƒ³ã‚’ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã«è¿½åŠ ã—ã€ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ã§çµ‚äº†åˆ¤å®šã™ã‚‹\n",
    "        self.scheduler.postprocess(seqs, token_ids)\n",
    "\n",
    "        # 4) å‡ºåŠ›ã®åé›†\n",
    "\n",
    "        # å®Œäº†ã—ãŸã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã®çµæœã‚’æŠ½å‡º\n",
    "        outputs = [(seq.seq_id, seq.completion_token_ids) \\\n",
    "            for seq in seqs if seq.is_finished]\n",
    "\n",
    "        # ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆè¨ˆç®—ã®ãŸã‚ã®ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã‚’è¨ˆç®—\n",
    "        # prefillæ™‚: å‡¦ç†ã—ãŸãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®å…¨ãƒˆãƒ¼ã‚¯ãƒ³æ•°\n",
    "        # decodeæ™‚: ç”Ÿæˆã—ãŸãƒˆãƒ¼ã‚¯ãƒ³æ•°ï¼ˆè² ã®å€¤ã«ã—ã¦åŒºåˆ¥ï¼‰\n",
    "        num_tokens = sum(len(seq) for seq in seqs) \\\n",
    "            if is_prefill else -len(seqs)\n",
    "\n",
    "        logger.info(f\"æ¨è«–ã‚¨ãƒ³ã‚¸ãƒ³ã®1ã‚µã‚¤ã‚¯ãƒ«ã‚’å®Ÿè¡Œå®Œäº† {outputs=} {num_tokens=}\")\n",
    "\n",
    "        return outputs, num_tokens\n",
    "\n",
    "    def is_finished(self):\n",
    "        \"\"\"\n",
    "        æ¨è«–ã‚¨ãƒ³ã‚¸ãƒ³ã«å‡¦ç†ã™ã‚‹ã¹ãã‚¿ã‚¹ã‚¯ãŒæ®‹ã£ã¦ã„ã‚‹ã‹ã‚’ç¢ºèªã™ã‚‹\n",
    "        LLMEngine.generate()ã®ãƒ«ãƒ¼ãƒ—ã®çµ‚äº†åˆ¤å®šã«ä½¿ç”¨ã™ã‚‹\n",
    "        \"\"\"\n",
    "        res = self.scheduler.is_finished()\n",
    "        logger.info(f\"ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ã«å‡¦ç†ã™ã¹ãã‚¿ã‚¹ã‚¯ãŒæ®‹ã£ã¦ã„ã‚‹ã‹ç¢ºèª {res=}\")\n",
    "        return res\n",
    "\n",
    "    def generate(\n",
    "        self,\n",
    "        prompts: list[str] | list[list[int]],\n",
    "        sampling_params: SamplingParams | list[SamplingParams],\n",
    "        use_tqdm: bool = True,\n",
    "    ) -> list[str]:\n",
    "        \"\"\"\n",
    "        ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã«å¯¾ã—ã¦ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆã‚’è¡Œã„ã€ç”Ÿæˆçµæœã‚’è¿”ã™\n",
    "\n",
    "        Args:\n",
    "            prompts (list[str] | list[list[int]]):\n",
    "                ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæ–‡å­—åˆ—ã¾ãŸã¯ãƒˆãƒ¼ã‚¯ãƒ³IDã®ãƒªã‚¹ãƒˆã®ãƒªã‚¹ãƒˆ\n",
    "            sampling_params (SamplingParams | list[SamplingParams]):\n",
    "                ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¾ãŸã¯ãã®ãƒªã‚¹ãƒˆ\n",
    "            use_tqdm (bool): é€²æ—è¡¨ç¤ºã«tqdmã‚’ä½¿ç”¨ã™ã‚‹ã‹ã©ã†ã‹\n",
    "\n",
    "        Returns:\n",
    "            list[str]: ç”Ÿæˆçµæœã®æ–‡å­—åˆ—ã®ãƒªã‚¹ãƒˆ\n",
    "        \"\"\"\n",
    "        logger.info(f\"ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆã‚’é–‹å§‹ {len(prompts)=} {sampling_params=}\")\n",
    "\n",
    "        # 1) åˆæœŸåŒ–\n",
    "\n",
    "        # é€²æ—è¡¨ç¤ºã™ã‚‹å ´åˆ\n",
    "        if use_tqdm:\n",
    "\n",
    "            # ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ãƒãƒ¼ã‚’åˆæœŸåŒ–\n",
    "            pbar = tqdm(\n",
    "                total=len(prompts), desc=\"Generating\", dynamic_ncols=True\n",
    "            )\n",
    "\n",
    "        # SamplingParamsãŒãƒªã‚¹ãƒˆã®å ´åˆ\n",
    "        if not isinstance(sampling_params, list):\n",
    "\n",
    "            # å…¨ã¦ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã«å¯¾ã—ã¦åŒã˜SamplingParamsã‚’ä½¿ç”¨ã™ã‚‹ã‚ˆã†ã«æ‹¡å¼µ\n",
    "            sampling_params = [sampling_params] * len(prompts)\n",
    "\n",
    "        # 2) ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ã«ç™»éŒ²\n",
    "\n",
    "        # å…¨ã¦ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’Sequenceã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã«å¤‰æ›ã—ã€ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ã«ç™»éŒ²\n",
    "        for prompt, sp in zip(prompts, sampling_params):\n",
    "            self.add_request(prompt, sp)\n",
    "\n",
    "        # 3) ç”Ÿæˆãƒ«ãƒ¼ãƒ—\n",
    "\n",
    "        outputs = {}\n",
    "\n",
    "        # prefillã¨decodeã®ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆã‚’åˆæœŸåŒ–\n",
    "        # ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆã¯æ¯ç§’ã®ãƒˆãƒ¼ã‚¯ãƒ³å‡¦ç†æ•°\n",
    "        # ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ãƒãƒ¼ã«è¡¨ç¤ºã™ã‚‹ç”¨\n",
    "        prefill_throughput = decode_throughput = 0.\n",
    "\n",
    "        while not self.is_finished():\n",
    "\n",
    "            # 3-1) ãƒˆãƒ¼ã‚¯ãƒ³ç”Ÿæˆã‚¹ãƒ†ãƒƒãƒ—ã®å®Ÿè¡Œ\n",
    "\n",
    "            t = perf_counter()\n",
    "\n",
    "            # 1ã‚µã‚¤ã‚¯ãƒ«å®Ÿè¡Œã—ã€finishedã«ãªã£ãŸã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã®å‡ºåŠ›ã‚’å–å¾—\n",
    "            output, num_tokens = self.step()\n",
    "\n",
    "            # 3-2) ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆã®è¨ˆç®—\n",
    "\n",
    "            # é€²æ—ã‚’è¡¨ç¤ºã™ã‚‹å ´åˆ\n",
    "            if use_tqdm:\n",
    "\n",
    "                # num_tokensãŒæ­£ã®å ´åˆï¼ˆprefillã®å ´åˆï¼‰\n",
    "                if num_tokens > 0:\n",
    "\n",
    "                    # prefillã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆã‚’è¨ˆç®—\n",
    "                    prefill_throughput = num_tokens / (perf_counter() - t)\n",
    "\n",
    "                # num_tokensãŒè² ã®å ´åˆï¼ˆdecodeã®å ´åˆï¼‰\n",
    "                else:\n",
    "\n",
    "                    # decodeã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆã‚’è¨ˆç®—\n",
    "                    # prefillã¨åŒºåˆ¥ã™ã‚‹ãŸã‚ã«è² ã®å€¤ã«ã—ã¦ã„ã‚‹\n",
    "                    decode_throughput = -num_tokens / (perf_counter() - t)\n",
    "\n",
    "                # é€²æ—ãƒãƒ¼ã«ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆã‚’è¡¨ç¤º\n",
    "                pbar.set_postfix({\n",
    "                    \"Prefill\": f\"{int(prefill_throughput)}tok/s\",\n",
    "                    \"Decode\": f\"{int(decode_throughput)}tok/s\",\n",
    "                })\n",
    "\n",
    "            # 3-3) å®Œäº†ã—ãŸçµæœã®åé›†\n",
    "\n",
    "            # finishedã«ãªã£ãŸã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã§ãƒ«ãƒ¼ãƒ—\n",
    "            for seq_id, token_ids in output:\n",
    "\n",
    "                # å‡ºåŠ›ã‚’ä¿å­˜\n",
    "                outputs[seq_id] = token_ids\n",
    "\n",
    "                # é€²æ—è¡¨ç¤ºã™ã‚‹å ´åˆ\n",
    "                if use_tqdm:\n",
    "\n",
    "                    # ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ãƒãƒ¼ã‚’æ›´æ–°\n",
    "                    pbar.update(1)\n",
    "\n",
    "        # 4) çµæœã®ãƒ‡ã‚³ãƒ¼ãƒ‰\n",
    "\n",
    "        # ã‚·ãƒ¼ã‚±ãƒ³ã‚¹IDã®é †ã«ã‚½ãƒ¼ãƒˆã—ã€å…¥åŠ›é †åºã«æ•´åˆ—\n",
    "        outputs = [outputs[seq_id] for seq_id in sorted(outputs.keys())]\n",
    "\n",
    "        # ãƒˆãƒ¼ã‚¯ãƒ³IDã®ãƒªã‚¹ãƒˆã‚’æ–‡å­—åˆ—ã«ãƒ‡ã‚³ãƒ¼ãƒ‰\n",
    "        outputs = [{\n",
    "            \"text\": self.tokenizer.decode(token_ids),\n",
    "            \"token_ids\": token_ids\n",
    "        } for token_ids in outputs]\n",
    "\n",
    "        # é€²æ—è¡¨ç¤ºã™ã‚‹å ´åˆ\n",
    "        if use_tqdm:\n",
    "\n",
    "            # ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ãƒãƒ¼ã‚’é–‰ã˜ã‚‹\n",
    "            pbar.close()\n",
    "\n",
    "        logger.info(f\"ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆã‚’å®Œäº† {len(outputs)=}\")\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1052ff8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LLM(LLMEngine):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a66bd32",
   "metadata": {},
   "source": [
    "## æ¨è«–"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f333de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "path =  \"/root/huggingface/Qwen3-0.6B\"\n",
    "\n",
    "if not os.path.exists(path):\n",
    "    !huggingface-cli download --resume-download Qwen/Qwen3-0.6B \\\n",
    "    --local-dir ~/huggingface/Qwen3-0.6B/ \\\n",
    "    --local-dir-use-symlinks False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c214c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"llm\" not in globals():\n",
    "    llm = LLM(path, enforce_eager=True, tensor_parallel_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70604833",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_params = SamplingParams(temperature=0.6, max_tokens=1)\n",
    "prompts = [\"Hello\"]\n",
    "outputs = llm.generate(prompts, sampling_params)\n",
    "outputs[0][\"text\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
