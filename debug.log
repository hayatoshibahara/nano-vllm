ğŸŸ¦ ãƒ­ã‚°ã‚’åˆæœŸåŒ–
ğŸŸ© SamplingParamsåˆæœŸåŒ–ã®å¾Œå‡¦ç† self=SamplingParams(temperature=1.0, max_tokens=64, ignore_eos=False)
ğŸŸ© ã‚°ãƒ­ãƒ¼ãƒãƒ«ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆåˆæœŸåŒ– _CONTEXT=Context(is_prefill=False, cu_seqlens_q=None, cu_seqlens_k=None, max_seqlen_q=0, max_seqlen_k=0, slot_mapping=None, context_lens=None, block_tables=None)
ğŸŸ© ã‚¨ãƒ³ã‚¸ãƒ³ã‚’åˆæœŸåŒ–é–‹å§‹ model='/root/huggingface/Qwen3-0.6B'
ğŸŸ© ConfigåˆæœŸåŒ–ã®å¾Œå‡¦ç† self=Config(model='/root/huggingface/Qwen3-0.6B', max_num_batched_tokens=16384, max_num_seqs=512, max_model_len=4096, gpu_memory_utilization=0.9, tensor_parallel_size=1, enforce_eager=True, hf_config=None, eos=-1, kvcache_block_size=256, num_kvcache_blocks=-1)
ğŸŸ© Initializing ModelRunner on rank 0
ğŸŸ© Qwen3ForCausalLMã‚’åˆæœŸåŒ–é–‹å§‹ config.vocab_size=151936 config.hidden_size=1024 config.tie_word_embeddings=True
ğŸŸ© Qwen3Modelã‚’åˆæœŸåŒ–é–‹å§‹ config.vocab_size=151936 config.hidden_size=1024 config.num_hidden_layers=28
ğŸŸ© å…¥åŠ›ã®åŸ‹ã‚è¾¼ã¿å±¤ã‚’åˆæœŸåŒ–é–‹å§‹ num_embeddings=151936, embedding_dim=1024
ğŸŸ¦ åˆ†æ•£ç’°å¢ƒæƒ…å ±å–å¾— self.tp_rank=0, self.tp_size=1
ğŸŸ¦ æ‹…å½“ã™ã‚‹èªå½™ã®ç¯„å›²è¨ˆç®— self.vocab_start_idx=0, self.vocab_end_idx=151936
ğŸŸ¦ æ‹…å½“ã™ã‚‹èªå½™ã®é‡ã¿è¡Œåˆ—åˆæœŸåŒ– self.weight.shape=torch.Size([151936, 1024])
ğŸŸ© å…¥åŠ›ã®åŸ‹ã‚è¾¼ã¿å±¤ã‚’åˆæœŸåŒ–å®Œäº†
ğŸŸ© Qwen3DecoderLayerã‚’åˆæœŸåŒ–é–‹å§‹ config.hidden_size=1024 config.num_attention_heads=16 config.num_key_value_heads=8 config.intermediate_size=3072 config.rms_norm_eps=1e-06 config.hidden_act='silu'
ğŸŸ© Qwen3Attentionã‚’åˆæœŸåŒ–é–‹å§‹ hidden_size: 1024, num_heads: 16, num_kv_heads: 8, max_position: 40960, head_dim: 128, rms_norm_eps: 1e-06, qkv_bias: False, rope_theta: 1000000, rope_scaling: None
ğŸŸ© QKVParallelLinearã‚’åˆæœŸåŒ–é–‹å§‹ hidden_size=1024, head_size=128, total_num_heads=16, total_num_kv_heads=8, bias=False
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã‚’åˆæœŸåŒ–é–‹å§‹ input_size=1024, output_size=4096, bias=False
ğŸŸ© ç·šå½¢å±¤ã®åŸºåº•ã‚¯ãƒ©ã‚¹ã‚’åˆæœŸåŒ–é–‹å§‹ input_size=1024, output_size=4096, bias=False, tp_dim=0
ğŸŸ© ç·šå½¢å±¤ã®åŸºåº•ã‚¯ãƒ©ã‚¹ã‚’åˆæœŸåŒ–å®Œäº†
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã‚’åˆæœŸåŒ–å®Œäº† self.tp_size=1
ğŸŸ© QKVParallelLinearã‚’åˆæœŸåŒ–å®Œäº† self.tp_size=1, self.num_heads=16, self.num_kv_heads=8
ğŸŸ© RowParallelLinearã‚’åˆæœŸåŒ–é–‹å§‹ input_size=2048, output_size=1024, bias=False
ğŸŸ© ç·šå½¢å±¤ã®åŸºåº•ã‚¯ãƒ©ã‚¹ã‚’åˆæœŸåŒ–é–‹å§‹ input_size=2048, output_size=1024, bias=False, tp_dim=1
ğŸŸ© ç·šå½¢å±¤ã®åŸºåº•ã‚¯ãƒ©ã‚¹ã‚’åˆæœŸåŒ–å®Œäº†
ğŸŸ© RowParallelLinearã‚’åˆæœŸåŒ–å®Œäº† self.tp_size=1
ğŸŸ© RoPEã‚’åˆæœŸåŒ– head_size=128, rotary_dim=128, max_position_embeddings=40960, base=1000000
ğŸŸ¦ å‘¨æ³¢æ•°è¨ˆç®— freqs.shape=torch.Size([40960, 64])
ğŸŸ¦ ã‚³ã‚µã‚¤ãƒ³ãƒ»ã‚µã‚¤ãƒ³ã‚­ãƒ£ãƒƒã‚·ãƒ¥è¨ˆç®— cache.shape=torch.Size([40960, 1, 128])
ğŸŸ© RoPEã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© Attentionå±¤ã‚’åˆæœŸåŒ– num_heads=16, head_dim=128, scale=0.08838834764831845, num_kv_heads=8
ğŸŸ© Attentionå±¤ã‚’åˆæœŸåŒ–å®Œäº†
ğŸŸ© RMSNormã‚’åˆæœŸåŒ– hidden_size=128, eps=1e-06
ğŸŸ© RMSNormã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© RMSNormã‚’åˆæœŸåŒ– hidden_size=128, eps=1e-06
ğŸŸ© RMSNormã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© Qwen3Attentionã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© Qwen3MLPã‚’åˆæœŸåŒ–é–‹å§‹ hidden_size=1024 intermediate_size=3072 hidden_act='silu'
ğŸŸ© MergedColumnParallelLinearã‚’åˆæœŸåŒ–é–‹å§‹ input_size=1024, output_sizes=[3072, 3072], bias=False
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã‚’åˆæœŸåŒ–é–‹å§‹ input_size=1024, output_size=6144, bias=False
ğŸŸ© ç·šå½¢å±¤ã®åŸºåº•ã‚¯ãƒ©ã‚¹ã‚’åˆæœŸåŒ–é–‹å§‹ input_size=1024, output_size=6144, bias=False, tp_dim=0
ğŸŸ© ç·šå½¢å±¤ã®åŸºåº•ã‚¯ãƒ©ã‚¹ã‚’åˆæœŸåŒ–å®Œäº†
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã‚’åˆæœŸåŒ–å®Œäº† self.tp_size=1
ğŸŸ© MergedColumnParallelLinearã‚’åˆæœŸåŒ–å®Œäº†
ğŸŸ© RowParallelLinearã‚’åˆæœŸåŒ–é–‹å§‹ input_size=3072, output_size=1024, bias=False
ğŸŸ© ç·šå½¢å±¤ã®åŸºåº•ã‚¯ãƒ©ã‚¹ã‚’åˆæœŸåŒ–é–‹å§‹ input_size=3072, output_size=1024, bias=False, tp_dim=1
ğŸŸ© ç·šå½¢å±¤ã®åŸºåº•ã‚¯ãƒ©ã‚¹ã‚’åˆæœŸåŒ–å®Œäº†
ğŸŸ© RowParallelLinearã‚’åˆæœŸåŒ–å®Œäº† self.tp_size=1
ğŸŸ© SwiGLUã‚’åˆæœŸåŒ–
ğŸŸ© Qwen3MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© RMSNormã‚’åˆæœŸåŒ– hidden_size=1024, eps=1e-06
ğŸŸ© RMSNormã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© RMSNormã‚’åˆæœŸåŒ– hidden_size=1024, eps=1e-06
ğŸŸ© RMSNormã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© Qwen3DecoderLayerã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© Qwen3DecoderLayerã‚’åˆæœŸåŒ–é–‹å§‹ config.hidden_size=1024 config.num_attention_heads=16 config.num_key_value_heads=8 config.intermediate_size=3072 config.rms_norm_eps=1e-06 config.hidden_act='silu'
ğŸŸ© Qwen3Attentionã‚’åˆæœŸåŒ–é–‹å§‹ hidden_size: 1024, num_heads: 16, num_kv_heads: 8, max_position: 40960, head_dim: 128, rms_norm_eps: 1e-06, qkv_bias: False, rope_theta: 1000000, rope_scaling: None
ğŸŸ© QKVParallelLinearã‚’åˆæœŸåŒ–é–‹å§‹ hidden_size=1024, head_size=128, total_num_heads=16, total_num_kv_heads=8, bias=False
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã‚’åˆæœŸåŒ–é–‹å§‹ input_size=1024, output_size=4096, bias=False
ğŸŸ© ç·šå½¢å±¤ã®åŸºåº•ã‚¯ãƒ©ã‚¹ã‚’åˆæœŸåŒ–é–‹å§‹ input_size=1024, output_size=4096, bias=False, tp_dim=0
ğŸŸ© ç·šå½¢å±¤ã®åŸºåº•ã‚¯ãƒ©ã‚¹ã‚’åˆæœŸåŒ–å®Œäº†
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã‚’åˆæœŸåŒ–å®Œäº† self.tp_size=1
ğŸŸ© QKVParallelLinearã‚’åˆæœŸåŒ–å®Œäº† self.tp_size=1, self.num_heads=16, self.num_kv_heads=8
ğŸŸ© RowParallelLinearã‚’åˆæœŸåŒ–é–‹å§‹ input_size=2048, output_size=1024, bias=False
ğŸŸ© ç·šå½¢å±¤ã®åŸºåº•ã‚¯ãƒ©ã‚¹ã‚’åˆæœŸåŒ–é–‹å§‹ input_size=2048, output_size=1024, bias=False, tp_dim=1
ğŸŸ© ç·šå½¢å±¤ã®åŸºåº•ã‚¯ãƒ©ã‚¹ã‚’åˆæœŸåŒ–å®Œäº†
ğŸŸ© RowParallelLinearã‚’åˆæœŸåŒ–å®Œäº† self.tp_size=1
ğŸŸ© Attentionå±¤ã‚’åˆæœŸåŒ– num_heads=16, head_dim=128, scale=0.08838834764831845, num_kv_heads=8
ğŸŸ© Attentionå±¤ã‚’åˆæœŸåŒ–å®Œäº†
ğŸŸ© RMSNormã‚’åˆæœŸåŒ– hidden_size=128, eps=1e-06
ğŸŸ© RMSNormã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© RMSNormã‚’åˆæœŸåŒ– hidden_size=128, eps=1e-06
ğŸŸ© RMSNormã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© Qwen3Attentionã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© Qwen3MLPã‚’åˆæœŸåŒ–é–‹å§‹ hidden_size=1024 intermediate_size=3072 hidden_act='silu'
ğŸŸ© MergedColumnParallelLinearã‚’åˆæœŸåŒ–é–‹å§‹ input_size=1024, output_sizes=[3072, 3072], bias=False
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã‚’åˆæœŸåŒ–é–‹å§‹ input_size=1024, output_size=6144, bias=False
ğŸŸ© ç·šå½¢å±¤ã®åŸºåº•ã‚¯ãƒ©ã‚¹ã‚’åˆæœŸåŒ–é–‹å§‹ input_size=1024, output_size=6144, bias=False, tp_dim=0
ğŸŸ© ç·šå½¢å±¤ã®åŸºåº•ã‚¯ãƒ©ã‚¹ã‚’åˆæœŸåŒ–å®Œäº†
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã‚’åˆæœŸåŒ–å®Œäº† self.tp_size=1
ğŸŸ© MergedColumnParallelLinearã‚’åˆæœŸåŒ–å®Œäº†
ğŸŸ© RowParallelLinearã‚’åˆæœŸåŒ–é–‹å§‹ input_size=3072, output_size=1024, bias=False
ğŸŸ© ç·šå½¢å±¤ã®åŸºåº•ã‚¯ãƒ©ã‚¹ã‚’åˆæœŸåŒ–é–‹å§‹ input_size=3072, output_size=1024, bias=False, tp_dim=1
ğŸŸ© ç·šå½¢å±¤ã®åŸºåº•ã‚¯ãƒ©ã‚¹ã‚’åˆæœŸåŒ–å®Œäº†
ğŸŸ© RowParallelLinearã‚’åˆæœŸåŒ–å®Œäº† self.tp_size=1
ğŸŸ© SwiGLUã‚’åˆæœŸåŒ–
ğŸŸ© Qwen3MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© RMSNormã‚’åˆæœŸåŒ– hidden_size=1024, eps=1e-06
ğŸŸ© RMSNormã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© RMSNormã‚’åˆæœŸåŒ– hidden_size=1024, eps=1e-06
ğŸŸ© RMSNormã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© Qwen3DecoderLayerã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© Qwen3DecoderLayerã‚’åˆæœŸåŒ–é–‹å§‹ config.hidden_size=1024 config.num_attention_heads=16 config.num_key_value_heads=8 config.intermediate_size=3072 config.rms_norm_eps=1e-06 config.hidden_act='silu'
ğŸŸ© Qwen3Attentionã‚’åˆæœŸåŒ–é–‹å§‹ hidden_size: 1024, num_heads: 16, num_kv_heads: 8, max_position: 40960, head_dim: 128, rms_norm_eps: 1e-06, qkv_bias: False, rope_theta: 1000000, rope_scaling: None
ğŸŸ© QKVParallelLinearã‚’åˆæœŸåŒ–é–‹å§‹ hidden_size=1024, head_size=128, total_num_heads=16, total_num_kv_heads=8, bias=False
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã‚’åˆæœŸåŒ–é–‹å§‹ input_size=1024, output_size=4096, bias=False
ğŸŸ© ç·šå½¢å±¤ã®åŸºåº•ã‚¯ãƒ©ã‚¹ã‚’åˆæœŸåŒ–é–‹å§‹ input_size=1024, output_size=4096, bias=False, tp_dim=0
ğŸŸ© ç·šå½¢å±¤ã®åŸºåº•ã‚¯ãƒ©ã‚¹ã‚’åˆæœŸåŒ–å®Œäº†
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã‚’åˆæœŸåŒ–å®Œäº† self.tp_size=1
ğŸŸ© QKVParallelLinearã‚’åˆæœŸåŒ–å®Œäº† self.tp_size=1, self.num_heads=16, self.num_kv_heads=8
ğŸŸ© RowParallelLinearã‚’åˆæœŸåŒ–é–‹å§‹ input_size=2048, output_size=1024, bias=False
ğŸŸ© ç·šå½¢å±¤ã®åŸºåº•ã‚¯ãƒ©ã‚¹ã‚’åˆæœŸåŒ–é–‹å§‹ input_size=2048, output_size=1024, bias=False, tp_dim=1
ğŸŸ© ç·šå½¢å±¤ã®åŸºåº•ã‚¯ãƒ©ã‚¹ã‚’åˆæœŸåŒ–å®Œäº†
ğŸŸ© RowParallelLinearã‚’åˆæœŸåŒ–å®Œäº† self.tp_size=1
ğŸŸ© Attentionå±¤ã‚’åˆæœŸåŒ– num_heads=16, head_dim=128, scale=0.08838834764831845, num_kv_heads=8
ğŸŸ© Attentionå±¤ã‚’åˆæœŸåŒ–å®Œäº†
ğŸŸ© RMSNormã‚’åˆæœŸåŒ– hidden_size=128, eps=1e-06
ğŸŸ© RMSNormã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© RMSNormã‚’åˆæœŸåŒ– hidden_size=128, eps=1e-06
ğŸŸ© RMSNormã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© Qwen3Attentionã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© Qwen3MLPã‚’åˆæœŸåŒ–é–‹å§‹ hidden_size=1024 intermediate_size=3072 hidden_act='silu'
ğŸŸ© MergedColumnParallelLinearã‚’åˆæœŸåŒ–é–‹å§‹ input_size=1024, output_sizes=[3072, 3072], bias=False
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã‚’åˆæœŸåŒ–é–‹å§‹ input_size=1024, output_size=6144, bias=False
ğŸŸ© ç·šå½¢å±¤ã®åŸºåº•ã‚¯ãƒ©ã‚¹ã‚’åˆæœŸåŒ–é–‹å§‹ input_size=1024, output_size=6144, bias=False, tp_dim=0
ğŸŸ© ç·šå½¢å±¤ã®åŸºåº•ã‚¯ãƒ©ã‚¹ã‚’åˆæœŸåŒ–å®Œäº†
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã‚’åˆæœŸåŒ–å®Œäº† self.tp_size=1
ğŸŸ© MergedColumnParallelLinearã‚’åˆæœŸåŒ–å®Œäº†
ğŸŸ© RowParallelLinearã‚’åˆæœŸåŒ–é–‹å§‹ input_size=3072, output_size=1024, bias=False
ğŸŸ© ç·šå½¢å±¤ã®åŸºåº•ã‚¯ãƒ©ã‚¹ã‚’åˆæœŸåŒ–é–‹å§‹ input_size=3072, output_size=1024, bias=False, tp_dim=1
ğŸŸ© ç·šå½¢å±¤ã®åŸºåº•ã‚¯ãƒ©ã‚¹ã‚’åˆæœŸåŒ–å®Œäº†
ğŸŸ© RowParallelLinearã‚’åˆæœŸåŒ–å®Œäº† self.tp_size=1
ğŸŸ© SwiGLUã‚’åˆæœŸåŒ–
ğŸŸ© Qwen3MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© RMSNormã‚’åˆæœŸåŒ– hidden_size=1024, eps=1e-06
ğŸŸ© RMSNormã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© RMSNormã‚’åˆæœŸåŒ– hidden_size=1024, eps=1e-06
ğŸŸ© RMSNormã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© Qwen3DecoderLayerã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© Qwen3DecoderLayerã‚’åˆæœŸåŒ–é–‹å§‹ config.hidden_size=1024 config.num_attention_heads=16 config.num_key_value_heads=8 config.intermediate_size=3072 config.rms_norm_eps=1e-06 config.hidden_act='silu'
ğŸŸ© Qwen3Attentionã‚’åˆæœŸåŒ–é–‹å§‹ hidden_size: 1024, num_heads: 16, num_kv_heads: 8, max_position: 40960, head_dim: 128, rms_norm_eps: 1e-06, qkv_bias: False, rope_theta: 1000000, rope_scaling: None
ğŸŸ© QKVParallelLinearã‚’åˆæœŸåŒ–é–‹å§‹ hidden_size=1024, head_size=128, total_num_heads=16, total_num_kv_heads=8, bias=False
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã‚’åˆæœŸåŒ–é–‹å§‹ input_size=1024, output_size=4096, bias=False
ğŸŸ© ç·šå½¢å±¤ã®åŸºåº•ã‚¯ãƒ©ã‚¹ã‚’åˆæœŸåŒ–é–‹å§‹ input_size=1024, output_size=4096, bias=False, tp_dim=0
ğŸŸ© ç·šå½¢å±¤ã®åŸºåº•ã‚¯ãƒ©ã‚¹ã‚’åˆæœŸåŒ–å®Œäº†
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã‚’åˆæœŸåŒ–å®Œäº† self.tp_size=1
ğŸŸ© QKVParallelLinearã‚’åˆæœŸåŒ–å®Œäº† self.tp_size=1, self.num_heads=16, self.num_kv_heads=8
ğŸŸ© RowParallelLinearã‚’åˆæœŸåŒ–é–‹å§‹ input_size=2048, output_size=1024, bias=False
ğŸŸ© ç·šå½¢å±¤ã®åŸºåº•ã‚¯ãƒ©ã‚¹ã‚’åˆæœŸåŒ–é–‹å§‹ input_size=2048, output_size=1024, bias=False, tp_dim=1
ğŸŸ© ç·šå½¢å±¤ã®åŸºåº•ã‚¯ãƒ©ã‚¹ã‚’åˆæœŸåŒ–å®Œäº†
ğŸŸ© RowParallelLinearã‚’åˆæœŸåŒ–å®Œäº† self.tp_size=1
ğŸŸ© Attentionå±¤ã‚’åˆæœŸåŒ– num_heads=16, head_dim=128, scale=0.08838834764831845, num_kv_heads=8
ğŸŸ© Attentionå±¤ã‚’åˆæœŸåŒ–å®Œäº†
ğŸŸ© RMSNormã‚’åˆæœŸåŒ– hidden_size=128, eps=1e-06
ğŸŸ© RMSNormã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© RMSNormã‚’åˆæœŸåŒ– hidden_size=128, eps=1e-06
ğŸŸ© RMSNormã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© Qwen3Attentionã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© Qwen3MLPã‚’åˆæœŸåŒ–é–‹å§‹ hidden_size=1024 intermediate_size=3072 hidden_act='silu'
ğŸŸ© MergedColumnParallelLinearã‚’åˆæœŸåŒ–é–‹å§‹ input_size=1024, output_sizes=[3072, 3072], bias=False
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã‚’åˆæœŸåŒ–é–‹å§‹ input_size=1024, output_size=6144, bias=False
ğŸŸ© ç·šå½¢å±¤ã®åŸºåº•ã‚¯ãƒ©ã‚¹ã‚’åˆæœŸåŒ–é–‹å§‹ input_size=1024, output_size=6144, bias=False, tp_dim=0
ğŸŸ© ç·šå½¢å±¤ã®åŸºåº•ã‚¯ãƒ©ã‚¹ã‚’åˆæœŸåŒ–å®Œäº†
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã‚’åˆæœŸåŒ–å®Œäº† self.tp_size=1
ğŸŸ© MergedColumnParallelLinearã‚’åˆæœŸåŒ–å®Œäº†
ğŸŸ© RowParallelLinearã‚’åˆæœŸåŒ–é–‹å§‹ input_size=3072, output_size=1024, bias=False
ğŸŸ© ç·šå½¢å±¤ã®åŸºåº•ã‚¯ãƒ©ã‚¹ã‚’åˆæœŸåŒ–é–‹å§‹ input_size=3072, output_size=1024, bias=False, tp_dim=1
ğŸŸ© ç·šå½¢å±¤ã®åŸºåº•ã‚¯ãƒ©ã‚¹ã‚’åˆæœŸåŒ–å®Œäº†
ğŸŸ© RowParallelLinearã‚’åˆæœŸåŒ–å®Œäº† self.tp_size=1
ğŸŸ© SwiGLUã‚’åˆæœŸåŒ–
ğŸŸ© Qwen3MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© RMSNormã‚’åˆæœŸåŒ– hidden_size=1024, eps=1e-06
ğŸŸ© RMSNormã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© RMSNormã‚’åˆæœŸåŒ– hidden_size=1024, eps=1e-06
ğŸŸ© RMSNormã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© Qwen3DecoderLayerã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© Qwen3DecoderLayerã‚’åˆæœŸåŒ–é–‹å§‹ config.hidden_size=1024 config.num_attention_heads=16 config.num_key_value_heads=8 config.intermediate_size=3072 config.rms_norm_eps=1e-06 config.hidden_act='silu'
ğŸŸ© Qwen3Attentionã‚’åˆæœŸåŒ–é–‹å§‹ hidden_size: 1024, num_heads: 16, num_kv_heads: 8, max_position: 40960, head_dim: 128, rms_norm_eps: 1e-06, qkv_bias: False, rope_theta: 1000000, rope_scaling: None
ğŸŸ© QKVParallelLinearã‚’åˆæœŸåŒ–é–‹å§‹ hidden_size=1024, head_size=128, total_num_heads=16, total_num_kv_heads=8, bias=False
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã‚’åˆæœŸåŒ–é–‹å§‹ input_size=1024, output_size=4096, bias=False
ğŸŸ© ç·šå½¢å±¤ã®åŸºåº•ã‚¯ãƒ©ã‚¹ã‚’åˆæœŸåŒ–é–‹å§‹ input_size=1024, output_size=4096, bias=False, tp_dim=0
ğŸŸ© ç·šå½¢å±¤ã®åŸºåº•ã‚¯ãƒ©ã‚¹ã‚’åˆæœŸåŒ–å®Œäº†
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã‚’åˆæœŸåŒ–å®Œäº† self.tp_size=1
ğŸŸ© QKVParallelLinearã‚’åˆæœŸåŒ–å®Œäº† self.tp_size=1, self.num_heads=16, self.num_kv_heads=8
ğŸŸ© RowParallelLinearã‚’åˆæœŸåŒ–é–‹å§‹ input_size=2048, output_size=1024, bias=False
ğŸŸ© ç·šå½¢å±¤ã®åŸºåº•ã‚¯ãƒ©ã‚¹ã‚’åˆæœŸåŒ–é–‹å§‹ input_size=2048, output_size=1024, bias=False, tp_dim=1
ğŸŸ© ç·šå½¢å±¤ã®åŸºåº•ã‚¯ãƒ©ã‚¹ã‚’åˆæœŸåŒ–å®Œäº†
ğŸŸ© RowParallelLinearã‚’åˆæœŸåŒ–å®Œäº† self.tp_size=1
ğŸŸ© Attentionå±¤ã‚’åˆæœŸåŒ– num_heads=16, head_dim=128, scale=0.08838834764831845, num_kv_heads=8
ğŸŸ© Attentionå±¤ã‚’åˆæœŸåŒ–å®Œäº†
ğŸŸ© RMSNormã‚’åˆæœŸåŒ– hidden_size=128, eps=1e-06
ğŸŸ© RMSNormã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© RMSNormã‚’åˆæœŸåŒ– hidden_size=128, eps=1e-06
ğŸŸ© RMSNormã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© Qwen3Attentionã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© Qwen3MLPã‚’åˆæœŸåŒ–é–‹å§‹ hidden_size=1024 intermediate_size=3072 hidden_act='silu'
ğŸŸ© MergedColumnParallelLinearã‚’åˆæœŸåŒ–é–‹å§‹ input_size=1024, output_sizes=[3072, 3072], bias=False
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã‚’åˆæœŸåŒ–é–‹å§‹ input_size=1024, output_size=6144, bias=False
ğŸŸ© ç·šå½¢å±¤ã®åŸºåº•ã‚¯ãƒ©ã‚¹ã‚’åˆæœŸåŒ–é–‹å§‹ input_size=1024, output_size=6144, bias=False, tp_dim=0
ğŸŸ© ç·šå½¢å±¤ã®åŸºåº•ã‚¯ãƒ©ã‚¹ã‚’åˆæœŸåŒ–å®Œäº†
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã‚’åˆæœŸåŒ–å®Œäº† self.tp_size=1
ğŸŸ© MergedColumnParallelLinearã‚’åˆæœŸåŒ–å®Œäº†
ğŸŸ© RowParallelLinearã‚’åˆæœŸåŒ–é–‹å§‹ input_size=3072, output_size=1024, bias=False
ğŸŸ© ç·šå½¢å±¤ã®åŸºåº•ã‚¯ãƒ©ã‚¹ã‚’åˆæœŸåŒ–é–‹å§‹ input_size=3072, output_size=1024, bias=False, tp_dim=1
ğŸŸ© ç·šå½¢å±¤ã®åŸºåº•ã‚¯ãƒ©ã‚¹ã‚’åˆæœŸåŒ–å®Œäº†
ğŸŸ© RowParallelLinearã‚’åˆæœŸåŒ–å®Œäº† self.tp_size=1
ğŸŸ© SwiGLUã‚’åˆæœŸåŒ–
ğŸŸ© Qwen3MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© RMSNormã‚’åˆæœŸåŒ– hidden_size=1024, eps=1e-06
ğŸŸ© RMSNormã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© RMSNormã‚’åˆæœŸåŒ– hidden_size=1024, eps=1e-06
ğŸŸ© RMSNormã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© Qwen3DecoderLayerã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© Qwen3DecoderLayerã‚’åˆæœŸåŒ–é–‹å§‹ config.hidden_size=1024 config.num_attention_heads=16 config.num_key_value_heads=8 config.intermediate_size=3072 config.rms_norm_eps=1e-06 config.hidden_act='silu'
ğŸŸ© Qwen3Attentionã‚’åˆæœŸåŒ–é–‹å§‹ hidden_size: 1024, num_heads: 16, num_kv_heads: 8, max_position: 40960, head_dim: 128, rms_norm_eps: 1e-06, qkv_bias: False, rope_theta: 1000000, rope_scaling: None
ğŸŸ© QKVParallelLinearã‚’åˆæœŸåŒ–é–‹å§‹ hidden_size=1024, head_size=128, total_num_heads=16, total_num_kv_heads=8, bias=False
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã‚’åˆæœŸåŒ–é–‹å§‹ input_size=1024, output_size=4096, bias=False
ğŸŸ© ç·šå½¢å±¤ã®åŸºåº•ã‚¯ãƒ©ã‚¹ã‚’åˆæœŸåŒ–é–‹å§‹ input_size=1024, output_size=4096, bias=False, tp_dim=0
ğŸŸ© ç·šå½¢å±¤ã®åŸºåº•ã‚¯ãƒ©ã‚¹ã‚’åˆæœŸåŒ–å®Œäº†
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã‚’åˆæœŸåŒ–å®Œäº† self.tp_size=1
ğŸŸ© QKVParallelLinearã‚’åˆæœŸåŒ–å®Œäº† self.tp_size=1, self.num_heads=16, self.num_kv_heads=8
ğŸŸ© RowParallelLinearã‚’åˆæœŸåŒ–é–‹å§‹ input_size=2048, output_size=1024, bias=False
ğŸŸ© ç·šå½¢å±¤ã®åŸºåº•ã‚¯ãƒ©ã‚¹ã‚’åˆæœŸåŒ–é–‹å§‹ input_size=2048, output_size=1024, bias=False, tp_dim=1
ğŸŸ© ç·šå½¢å±¤ã®åŸºåº•ã‚¯ãƒ©ã‚¹ã‚’åˆæœŸåŒ–å®Œäº†
ğŸŸ© RowParallelLinearã‚’åˆæœŸåŒ–å®Œäº† self.tp_size=1
ğŸŸ© Attentionå±¤ã‚’åˆæœŸåŒ– num_heads=16, head_dim=128, scale=0.08838834764831845, num_kv_heads=8
ğŸŸ© Attentionå±¤ã‚’åˆæœŸåŒ–å®Œäº†
ğŸŸ© RMSNormã‚’åˆæœŸåŒ– hidden_size=128, eps=1e-06
ğŸŸ© RMSNormã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© RMSNormã‚’åˆæœŸåŒ– hidden_size=128, eps=1e-06
ğŸŸ© RMSNormã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© Qwen3Attentionã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© Qwen3MLPã‚’åˆæœŸåŒ–é–‹å§‹ hidden_size=1024 intermediate_size=3072 hidden_act='silu'
ğŸŸ© MergedColumnParallelLinearã‚’åˆæœŸåŒ–é–‹å§‹ input_size=1024, output_sizes=[3072, 3072], bias=False
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã‚’åˆæœŸåŒ–é–‹å§‹ input_size=1024, output_size=6144, bias=False
ğŸŸ© ç·šå½¢å±¤ã®åŸºåº•ã‚¯ãƒ©ã‚¹ã‚’åˆæœŸåŒ–é–‹å§‹ input_size=1024, output_size=6144, bias=False, tp_dim=0
ğŸŸ© ç·šå½¢å±¤ã®åŸºåº•ã‚¯ãƒ©ã‚¹ã‚’åˆæœŸåŒ–å®Œäº†
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã‚’åˆæœŸåŒ–å®Œäº† self.tp_size=1
ğŸŸ© MergedColumnParallelLinearã‚’åˆæœŸåŒ–å®Œäº†
ğŸŸ© RowParallelLinearã‚’åˆæœŸåŒ–é–‹å§‹ input_size=3072, output_size=1024, bias=False
ğŸŸ© ç·šå½¢å±¤ã®åŸºåº•ã‚¯ãƒ©ã‚¹ã‚’åˆæœŸåŒ–é–‹å§‹ input_size=3072, output_size=1024, bias=False, tp_dim=1
ğŸŸ© ç·šå½¢å±¤ã®åŸºåº•ã‚¯ãƒ©ã‚¹ã‚’åˆæœŸåŒ–å®Œäº†
ğŸŸ© RowParallelLinearã‚’åˆæœŸåŒ–å®Œäº† self.tp_size=1
ğŸŸ© SwiGLUã‚’åˆæœŸåŒ–
ğŸŸ© Qwen3MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© RMSNormã‚’åˆæœŸåŒ– hidden_size=1024, eps=1e-06
ğŸŸ© RMSNormã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© RMSNormã‚’åˆæœŸåŒ– hidden_size=1024, eps=1e-06
ğŸŸ© RMSNormã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© Qwen3DecoderLayerã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© Qwen3DecoderLayerã‚’åˆæœŸåŒ–é–‹å§‹ config.hidden_size=1024 config.num_attention_heads=16 config.num_key_value_heads=8 config.intermediate_size=3072 config.rms_norm_eps=1e-06 config.hidden_act='silu'
ğŸŸ© Qwen3Attentionã‚’åˆæœŸåŒ–é–‹å§‹ hidden_size: 1024, num_heads: 16, num_kv_heads: 8, max_position: 40960, head_dim: 128, rms_norm_eps: 1e-06, qkv_bias: False, rope_theta: 1000000, rope_scaling: None
ğŸŸ© QKVParallelLinearã‚’åˆæœŸåŒ–é–‹å§‹ hidden_size=1024, head_size=128, total_num_heads=16, total_num_kv_heads=8, bias=False
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã‚’åˆæœŸåŒ–é–‹å§‹ input_size=1024, output_size=4096, bias=False
ğŸŸ© ç·šå½¢å±¤ã®åŸºåº•ã‚¯ãƒ©ã‚¹ã‚’åˆæœŸåŒ–é–‹å§‹ input_size=1024, output_size=4096, bias=False, tp_dim=0
ğŸŸ© ç·šå½¢å±¤ã®åŸºåº•ã‚¯ãƒ©ã‚¹ã‚’åˆæœŸåŒ–å®Œäº†
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã‚’åˆæœŸåŒ–å®Œäº† self.tp_size=1
ğŸŸ© QKVParallelLinearã‚’åˆæœŸåŒ–å®Œäº† self.tp_size=1, self.num_heads=16, self.num_kv_heads=8
ğŸŸ© RowParallelLinearã‚’åˆæœŸåŒ–é–‹å§‹ input_size=2048, output_size=1024, bias=False
ğŸŸ© ç·šå½¢å±¤ã®åŸºåº•ã‚¯ãƒ©ã‚¹ã‚’åˆæœŸåŒ–é–‹å§‹ input_size=2048, output_size=1024, bias=False, tp_dim=1
ğŸŸ© ç·šå½¢å±¤ã®åŸºåº•ã‚¯ãƒ©ã‚¹ã‚’åˆæœŸåŒ–å®Œäº†
ğŸŸ© RowParallelLinearã‚’åˆæœŸåŒ–å®Œäº† self.tp_size=1
ğŸŸ© Attentionå±¤ã‚’åˆæœŸåŒ– num_heads=16, head_dim=128, scale=0.08838834764831845, num_kv_heads=8
ğŸŸ© Attentionå±¤ã‚’åˆæœŸåŒ–å®Œäº†
ğŸŸ© RMSNormã‚’åˆæœŸåŒ– hidden_size=128, eps=1e-06
ğŸŸ© RMSNormã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© RMSNormã‚’åˆæœŸåŒ– hidden_size=128, eps=1e-06
ğŸŸ© RMSNormã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© Qwen3Attentionã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© Qwen3MLPã‚’åˆæœŸåŒ–é–‹å§‹ hidden_size=1024 intermediate_size=3072 hidden_act='silu'
ğŸŸ© MergedColumnParallelLinearã‚’åˆæœŸåŒ–é–‹å§‹ input_size=1024, output_sizes=[3072, 3072], bias=False
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã‚’åˆæœŸåŒ–é–‹å§‹ input_size=1024, output_size=6144, bias=False
ğŸŸ© ç·šå½¢å±¤ã®åŸºåº•ã‚¯ãƒ©ã‚¹ã‚’åˆæœŸåŒ–é–‹å§‹ input_size=1024, output_size=6144, bias=False, tp_dim=0
ğŸŸ© ç·šå½¢å±¤ã®åŸºåº•ã‚¯ãƒ©ã‚¹ã‚’åˆæœŸåŒ–å®Œäº†
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã‚’åˆæœŸåŒ–å®Œäº† self.tp_size=1
ğŸŸ© MergedColumnParallelLinearã‚’åˆæœŸåŒ–å®Œäº†
ğŸŸ© RowParallelLinearã‚’åˆæœŸåŒ–é–‹å§‹ input_size=3072, output_size=1024, bias=False
ğŸŸ© ç·šå½¢å±¤ã®åŸºåº•ã‚¯ãƒ©ã‚¹ã‚’åˆæœŸåŒ–é–‹å§‹ input_size=3072, output_size=1024, bias=False, tp_dim=1
ğŸŸ© ç·šå½¢å±¤ã®åŸºåº•ã‚¯ãƒ©ã‚¹ã‚’åˆæœŸåŒ–å®Œäº†
ğŸŸ© RowParallelLinearã‚’åˆæœŸåŒ–å®Œäº† self.tp_size=1
ğŸŸ© SwiGLUã‚’åˆæœŸåŒ–
ğŸŸ© Qwen3MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© RMSNormã‚’åˆæœŸåŒ– hidden_size=1024, eps=1e-06
ğŸŸ© RMSNormã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© RMSNormã‚’åˆæœŸåŒ– hidden_size=1024, eps=1e-06
ğŸŸ© RMSNormã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© Qwen3DecoderLayerã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© Qwen3DecoderLayerã‚’åˆæœŸåŒ–é–‹å§‹ config.hidden_size=1024 config.num_attention_heads=16 config.num_key_value_heads=8 config.intermediate_size=3072 config.rms_norm_eps=1e-06 config.hidden_act='silu'
ğŸŸ© Qwen3Attentionã‚’åˆæœŸåŒ–é–‹å§‹ hidden_size: 1024, num_heads: 16, num_kv_heads: 8, max_position: 40960, head_dim: 128, rms_norm_eps: 1e-06, qkv_bias: False, rope_theta: 1000000, rope_scaling: None
ğŸŸ© QKVParallelLinearã‚’åˆæœŸåŒ–é–‹å§‹ hidden_size=1024, head_size=128, total_num_heads=16, total_num_kv_heads=8, bias=False
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã‚’åˆæœŸåŒ–é–‹å§‹ input_size=1024, output_size=4096, bias=False
ğŸŸ© ç·šå½¢å±¤ã®åŸºåº•ã‚¯ãƒ©ã‚¹ã‚’åˆæœŸåŒ–é–‹å§‹ input_size=1024, output_size=4096, bias=False, tp_dim=0
ğŸŸ© ç·šå½¢å±¤ã®åŸºåº•ã‚¯ãƒ©ã‚¹ã‚’åˆæœŸåŒ–å®Œäº†
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã‚’åˆæœŸåŒ–å®Œäº† self.tp_size=1
ğŸŸ© QKVParallelLinearã‚’åˆæœŸåŒ–å®Œäº† self.tp_size=1, self.num_heads=16, self.num_kv_heads=8
ğŸŸ© RowParallelLinearã‚’åˆæœŸåŒ–é–‹å§‹ input_size=2048, output_size=1024, bias=False
ğŸŸ© ç·šå½¢å±¤ã®åŸºåº•ã‚¯ãƒ©ã‚¹ã‚’åˆæœŸåŒ–é–‹å§‹ input_size=2048, output_size=1024, bias=False, tp_dim=1
ğŸŸ© ç·šå½¢å±¤ã®åŸºåº•ã‚¯ãƒ©ã‚¹ã‚’åˆæœŸåŒ–å®Œäº†
ğŸŸ© RowParallelLinearã‚’åˆæœŸåŒ–å®Œäº† self.tp_size=1
ğŸŸ© Attentionå±¤ã‚’åˆæœŸåŒ– num_heads=16, head_dim=128, scale=0.08838834764831845, num_kv_heads=8
ğŸŸ© Attentionå±¤ã‚’åˆæœŸåŒ–å®Œäº†
ğŸŸ© RMSNormã‚’åˆæœŸåŒ– hidden_size=128, eps=1e-06
ğŸŸ© RMSNormã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© RMSNormã‚’åˆæœŸåŒ– hidden_size=128, eps=1e-06
ğŸŸ© RMSNormã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© Qwen3Attentionã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© Qwen3MLPã‚’åˆæœŸåŒ–é–‹å§‹ hidden_size=1024 intermediate_size=3072 hidden_act='silu'
ğŸŸ© MergedColumnParallelLinearã‚’åˆæœŸåŒ–é–‹å§‹ input_size=1024, output_sizes=[3072, 3072], bias=False
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã‚’åˆæœŸåŒ–é–‹å§‹ input_size=1024, output_size=6144, bias=False
ğŸŸ© ç·šå½¢å±¤ã®åŸºåº•ã‚¯ãƒ©ã‚¹ã‚’åˆæœŸåŒ–é–‹å§‹ input_size=1024, output_size=6144, bias=False, tp_dim=0
ğŸŸ© ç·šå½¢å±¤ã®åŸºåº•ã‚¯ãƒ©ã‚¹ã‚’åˆæœŸåŒ–å®Œäº†
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã‚’åˆæœŸåŒ–å®Œäº† self.tp_size=1
ğŸŸ© MergedColumnParallelLinearã‚’åˆæœŸåŒ–å®Œäº†
ğŸŸ© RowParallelLinearã‚’åˆæœŸåŒ–é–‹å§‹ input_size=3072, output_size=1024, bias=False
ğŸŸ© ç·šå½¢å±¤ã®åŸºåº•ã‚¯ãƒ©ã‚¹ã‚’åˆæœŸåŒ–é–‹å§‹ input_size=3072, output_size=1024, bias=False, tp_dim=1
ğŸŸ© ç·šå½¢å±¤ã®åŸºåº•ã‚¯ãƒ©ã‚¹ã‚’åˆæœŸåŒ–å®Œäº†
ğŸŸ© RowParallelLinearã‚’åˆæœŸåŒ–å®Œäº† self.tp_size=1
ğŸŸ© SwiGLUã‚’åˆæœŸåŒ–
ğŸŸ© Qwen3MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© RMSNormã‚’åˆæœŸåŒ– hidden_size=1024, eps=1e-06
ğŸŸ© RMSNormã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© RMSNormã‚’åˆæœŸåŒ– hidden_size=1024, eps=1e-06
ğŸŸ© RMSNormã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© Qwen3DecoderLayerã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© Qwen3DecoderLayerã‚’åˆæœŸåŒ–é–‹å§‹ config.hidden_size=1024 config.num_attention_heads=16 config.num_key_value_heads=8 config.intermediate_size=3072 config.rms_norm_eps=1e-06 config.hidden_act='silu'
ğŸŸ© Qwen3Attentionã‚’åˆæœŸåŒ–é–‹å§‹ hidden_size: 1024, num_heads: 16, num_kv_heads: 8, max_position: 40960, head_dim: 128, rms_norm_eps: 1e-06, qkv_bias: False, rope_theta: 1000000, rope_scaling: None
ğŸŸ© QKVParallelLinearã‚’åˆæœŸåŒ–é–‹å§‹ hidden_size=1024, head_size=128, total_num_heads=16, total_num_kv_heads=8, bias=False
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã‚’åˆæœŸåŒ–é–‹å§‹ input_size=1024, output_size=4096, bias=False
ğŸŸ© ç·šå½¢å±¤ã®åŸºåº•ã‚¯ãƒ©ã‚¹ã‚’åˆæœŸåŒ–é–‹å§‹ input_size=1024, output_size=4096, bias=False, tp_dim=0
ğŸŸ© ç·šå½¢å±¤ã®åŸºåº•ã‚¯ãƒ©ã‚¹ã‚’åˆæœŸåŒ–å®Œäº†
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã‚’åˆæœŸåŒ–å®Œäº† self.tp_size=1
ğŸŸ© QKVParallelLinearã‚’åˆæœŸåŒ–å®Œäº† self.tp_size=1, self.num_heads=16, self.num_kv_heads=8
ğŸŸ© RowParallelLinearã‚’åˆæœŸåŒ–é–‹å§‹ input_size=2048, output_size=1024, bias=False
ğŸŸ© ç·šå½¢å±¤ã®åŸºåº•ã‚¯ãƒ©ã‚¹ã‚’åˆæœŸåŒ–é–‹å§‹ input_size=2048, output_size=1024, bias=False, tp_dim=1
ğŸŸ© ç·šå½¢å±¤ã®åŸºåº•ã‚¯ãƒ©ã‚¹ã‚’åˆæœŸåŒ–å®Œäº†
ğŸŸ© RowParallelLinearã‚’åˆæœŸåŒ–å®Œäº† self.tp_size=1
ğŸŸ© Attentionå±¤ã‚’åˆæœŸåŒ– num_heads=16, head_dim=128, scale=0.08838834764831845, num_kv_heads=8
ğŸŸ© Attentionå±¤ã‚’åˆæœŸåŒ–å®Œäº†
ğŸŸ© RMSNormã‚’åˆæœŸåŒ– hidden_size=128, eps=1e-06
ğŸŸ© RMSNormã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© RMSNormã‚’åˆæœŸåŒ– hidden_size=128, eps=1e-06
ğŸŸ© RMSNormã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© Qwen3Attentionã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© Qwen3MLPã‚’åˆæœŸåŒ–é–‹å§‹ hidden_size=1024 intermediate_size=3072 hidden_act='silu'
ğŸŸ© MergedColumnParallelLinearã‚’åˆæœŸåŒ–é–‹å§‹ input_size=1024, output_sizes=[3072, 3072], bias=False
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã‚’åˆæœŸåŒ–é–‹å§‹ input_size=1024, output_size=6144, bias=False
ğŸŸ© ç·šå½¢å±¤ã®åŸºåº•ã‚¯ãƒ©ã‚¹ã‚’åˆæœŸåŒ–é–‹å§‹ input_size=1024, output_size=6144, bias=False, tp_dim=0
ğŸŸ© ç·šå½¢å±¤ã®åŸºåº•ã‚¯ãƒ©ã‚¹ã‚’åˆæœŸåŒ–å®Œäº†
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã‚’åˆæœŸåŒ–å®Œäº† self.tp_size=1
ğŸŸ© MergedColumnParallelLinearã‚’åˆæœŸåŒ–å®Œäº†
ğŸŸ© RowParallelLinearã‚’åˆæœŸåŒ–é–‹å§‹ input_size=3072, output_size=1024, bias=False
ğŸŸ© ç·šå½¢å±¤ã®åŸºåº•ã‚¯ãƒ©ã‚¹ã‚’åˆæœŸåŒ–é–‹å§‹ input_size=3072, output_size=1024, bias=False, tp_dim=1
ğŸŸ© ç·šå½¢å±¤ã®åŸºåº•ã‚¯ãƒ©ã‚¹ã‚’åˆæœŸåŒ–å®Œäº†
ğŸŸ© RowParallelLinearã‚’åˆæœŸåŒ–å®Œäº† self.tp_size=1
ğŸŸ© SwiGLUã‚’åˆæœŸåŒ–
ğŸŸ© Qwen3MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© RMSNormã‚’åˆæœŸåŒ– hidden_size=1024, eps=1e-06
ğŸŸ© RMSNormã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© RMSNormã‚’åˆæœŸåŒ– hidden_size=1024, eps=1e-06
ğŸŸ© RMSNormã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© Qwen3DecoderLayerã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© Qwen3DecoderLayerã‚’åˆæœŸåŒ–é–‹å§‹ config.hidden_size=1024 config.num_attention_heads=16 config.num_key_value_heads=8 config.intermediate_size=3072 config.rms_norm_eps=1e-06 config.hidden_act='silu'
ğŸŸ© Qwen3Attentionã‚’åˆæœŸåŒ–é–‹å§‹ hidden_size: 1024, num_heads: 16, num_kv_heads: 8, max_position: 40960, head_dim: 128, rms_norm_eps: 1e-06, qkv_bias: False, rope_theta: 1000000, rope_scaling: None
ğŸŸ© QKVParallelLinearã‚’åˆæœŸåŒ–é–‹å§‹ hidden_size=1024, head_size=128, total_num_heads=16, total_num_kv_heads=8, bias=False
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã‚’åˆæœŸåŒ–é–‹å§‹ input_size=1024, output_size=4096, bias=False
ğŸŸ© ç·šå½¢å±¤ã®åŸºåº•ã‚¯ãƒ©ã‚¹ã‚’åˆæœŸåŒ–é–‹å§‹ input_size=1024, output_size=4096, bias=False, tp_dim=0
ğŸŸ© ç·šå½¢å±¤ã®åŸºåº•ã‚¯ãƒ©ã‚¹ã‚’åˆæœŸåŒ–å®Œäº†
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã‚’åˆæœŸåŒ–å®Œäº† self.tp_size=1
ğŸŸ© QKVParallelLinearã‚’åˆæœŸåŒ–å®Œäº† self.tp_size=1, self.num_heads=16, self.num_kv_heads=8
ğŸŸ© RowParallelLinearã‚’åˆæœŸåŒ–é–‹å§‹ input_size=2048, output_size=1024, bias=False
ğŸŸ© ç·šå½¢å±¤ã®åŸºåº•ã‚¯ãƒ©ã‚¹ã‚’åˆæœŸåŒ–é–‹å§‹ input_size=2048, output_size=1024, bias=False, tp_dim=1
ğŸŸ© ç·šå½¢å±¤ã®åŸºåº•ã‚¯ãƒ©ã‚¹ã‚’åˆæœŸåŒ–å®Œäº†
ğŸŸ© RowParallelLinearã‚’åˆæœŸåŒ–å®Œäº† self.tp_size=1
ğŸŸ© Attentionå±¤ã‚’åˆæœŸåŒ– num_heads=16, head_dim=128, scale=0.08838834764831845, num_kv_heads=8
ğŸŸ© Attentionå±¤ã‚’åˆæœŸåŒ–å®Œäº†
ğŸŸ© RMSNormã‚’åˆæœŸåŒ– hidden_size=128, eps=1e-06
ğŸŸ© RMSNormã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© RMSNormã‚’åˆæœŸåŒ– hidden_size=128, eps=1e-06
ğŸŸ© RMSNormã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© Qwen3Attentionã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© Qwen3MLPã‚’åˆæœŸåŒ–é–‹å§‹ hidden_size=1024 intermediate_size=3072 hidden_act='silu'
ğŸŸ© MergedColumnParallelLinearã‚’åˆæœŸåŒ–é–‹å§‹ input_size=1024, output_sizes=[3072, 3072], bias=False
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã‚’åˆæœŸåŒ–é–‹å§‹ input_size=1024, output_size=6144, bias=False
ğŸŸ© ç·šå½¢å±¤ã®åŸºåº•ã‚¯ãƒ©ã‚¹ã‚’åˆæœŸåŒ–é–‹å§‹ input_size=1024, output_size=6144, bias=False, tp_dim=0
ğŸŸ© ç·šå½¢å±¤ã®åŸºåº•ã‚¯ãƒ©ã‚¹ã‚’åˆæœŸåŒ–å®Œäº†
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã‚’åˆæœŸåŒ–å®Œäº† self.tp_size=1
ğŸŸ© MergedColumnParallelLinearã‚’åˆæœŸåŒ–å®Œäº†
ğŸŸ© RowParallelLinearã‚’åˆæœŸåŒ–é–‹å§‹ input_size=3072, output_size=1024, bias=False
ğŸŸ© ç·šå½¢å±¤ã®åŸºåº•ã‚¯ãƒ©ã‚¹ã‚’åˆæœŸåŒ–é–‹å§‹ input_size=3072, output_size=1024, bias=False, tp_dim=1
ğŸŸ© ç·šå½¢å±¤ã®åŸºåº•ã‚¯ãƒ©ã‚¹ã‚’åˆæœŸåŒ–å®Œäº†
ğŸŸ© RowParallelLinearã‚’åˆæœŸåŒ–å®Œäº† self.tp_size=1
ğŸŸ© SwiGLUã‚’åˆæœŸåŒ–
ğŸŸ© Qwen3MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© RMSNormã‚’åˆæœŸåŒ– hidden_size=1024, eps=1e-06
ğŸŸ© RMSNormã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© RMSNormã‚’åˆæœŸåŒ– hidden_size=1024, eps=1e-06
ğŸŸ© RMSNormã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© Qwen3DecoderLayerã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© Qwen3DecoderLayerã‚’åˆæœŸåŒ–é–‹å§‹ config.hidden_size=1024 config.num_attention_heads=16 config.num_key_value_heads=8 config.intermediate_size=3072 config.rms_norm_eps=1e-06 config.hidden_act='silu'
ğŸŸ© Qwen3Attentionã‚’åˆæœŸåŒ–é–‹å§‹ hidden_size: 1024, num_heads: 16, num_kv_heads: 8, max_position: 40960, head_dim: 128, rms_norm_eps: 1e-06, qkv_bias: False, rope_theta: 1000000, rope_scaling: None
ğŸŸ© QKVParallelLinearã‚’åˆæœŸåŒ–é–‹å§‹ hidden_size=1024, head_size=128, total_num_heads=16, total_num_kv_heads=8, bias=False
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã‚’åˆæœŸåŒ–é–‹å§‹ input_size=1024, output_size=4096, bias=False
ğŸŸ© ç·šå½¢å±¤ã®åŸºåº•ã‚¯ãƒ©ã‚¹ã‚’åˆæœŸåŒ–é–‹å§‹ input_size=1024, output_size=4096, bias=False, tp_dim=0
ğŸŸ© ç·šå½¢å±¤ã®åŸºåº•ã‚¯ãƒ©ã‚¹ã‚’åˆæœŸåŒ–å®Œäº†
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã‚’åˆæœŸåŒ–å®Œäº† self.tp_size=1
ğŸŸ© QKVParallelLinearã‚’åˆæœŸåŒ–å®Œäº† self.tp_size=1, self.num_heads=16, self.num_kv_heads=8
ğŸŸ© RowParallelLinearã‚’åˆæœŸåŒ–é–‹å§‹ input_size=2048, output_size=1024, bias=False
ğŸŸ© ç·šå½¢å±¤ã®åŸºåº•ã‚¯ãƒ©ã‚¹ã‚’åˆæœŸåŒ–é–‹å§‹ input_size=2048, output_size=1024, bias=False, tp_dim=1
ğŸŸ© ç·šå½¢å±¤ã®åŸºåº•ã‚¯ãƒ©ã‚¹ã‚’åˆæœŸåŒ–å®Œäº†
ğŸŸ© RowParallelLinearã‚’åˆæœŸåŒ–å®Œäº† self.tp_size=1
ğŸŸ© Attentionå±¤ã‚’åˆæœŸåŒ– num_heads=16, head_dim=128, scale=0.08838834764831845, num_kv_heads=8
ğŸŸ© Attentionå±¤ã‚’åˆæœŸåŒ–å®Œäº†
ğŸŸ© RMSNormã‚’åˆæœŸåŒ– hidden_size=128, eps=1e-06
ğŸŸ© RMSNormã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© RMSNormã‚’åˆæœŸåŒ– hidden_size=128, eps=1e-06
ğŸŸ© RMSNormã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© Qwen3Attentionã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© Qwen3MLPã‚’åˆæœŸåŒ–é–‹å§‹ hidden_size=1024 intermediate_size=3072 hidden_act='silu'
ğŸŸ© MergedColumnParallelLinearã‚’åˆæœŸåŒ–é–‹å§‹ input_size=1024, output_sizes=[3072, 3072], bias=False
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã‚’åˆæœŸåŒ–é–‹å§‹ input_size=1024, output_size=6144, bias=False
ğŸŸ© ç·šå½¢å±¤ã®åŸºåº•ã‚¯ãƒ©ã‚¹ã‚’åˆæœŸåŒ–é–‹å§‹ input_size=1024, output_size=6144, bias=False, tp_dim=0
ğŸŸ© ç·šå½¢å±¤ã®åŸºåº•ã‚¯ãƒ©ã‚¹ã‚’åˆæœŸåŒ–å®Œäº†
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã‚’åˆæœŸåŒ–å®Œäº† self.tp_size=1
ğŸŸ© MergedColumnParallelLinearã‚’åˆæœŸåŒ–å®Œäº†
ğŸŸ© RowParallelLinearã‚’åˆæœŸåŒ–é–‹å§‹ input_size=3072, output_size=1024, bias=False
ğŸŸ© ç·šå½¢å±¤ã®åŸºåº•ã‚¯ãƒ©ã‚¹ã‚’åˆæœŸåŒ–é–‹å§‹ input_size=3072, output_size=1024, bias=False, tp_dim=1
ğŸŸ© ç·šå½¢å±¤ã®åŸºåº•ã‚¯ãƒ©ã‚¹ã‚’åˆæœŸåŒ–å®Œäº†
ğŸŸ© RowParallelLinearã‚’åˆæœŸåŒ–å®Œäº† self.tp_size=1
ğŸŸ© SwiGLUã‚’åˆæœŸåŒ–
ğŸŸ© Qwen3MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© RMSNormã‚’åˆæœŸåŒ– hidden_size=1024, eps=1e-06
ğŸŸ© RMSNormã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© RMSNormã‚’åˆæœŸåŒ– hidden_size=1024, eps=1e-06
ğŸŸ© RMSNormã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© Qwen3DecoderLayerã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© Qwen3DecoderLayerã‚’åˆæœŸåŒ–é–‹å§‹ config.hidden_size=1024 config.num_attention_heads=16 config.num_key_value_heads=8 config.intermediate_size=3072 config.rms_norm_eps=1e-06 config.hidden_act='silu'
ğŸŸ© Qwen3Attentionã‚’åˆæœŸåŒ–é–‹å§‹ hidden_size: 1024, num_heads: 16, num_kv_heads: 8, max_position: 40960, head_dim: 128, rms_norm_eps: 1e-06, qkv_bias: False, rope_theta: 1000000, rope_scaling: None
ğŸŸ© QKVParallelLinearã‚’åˆæœŸåŒ–é–‹å§‹ hidden_size=1024, head_size=128, total_num_heads=16, total_num_kv_heads=8, bias=False
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã‚’åˆæœŸåŒ–é–‹å§‹ input_size=1024, output_size=4096, bias=False
ğŸŸ© ç·šå½¢å±¤ã®åŸºåº•ã‚¯ãƒ©ã‚¹ã‚’åˆæœŸåŒ–é–‹å§‹ input_size=1024, output_size=4096, bias=False, tp_dim=0
ğŸŸ© ç·šå½¢å±¤ã®åŸºåº•ã‚¯ãƒ©ã‚¹ã‚’åˆæœŸåŒ–å®Œäº†
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã‚’åˆæœŸåŒ–å®Œäº† self.tp_size=1
ğŸŸ© QKVParallelLinearã‚’åˆæœŸåŒ–å®Œäº† self.tp_size=1, self.num_heads=16, self.num_kv_heads=8
ğŸŸ© RowParallelLinearã‚’åˆæœŸåŒ–é–‹å§‹ input_size=2048, output_size=1024, bias=False
ğŸŸ© ç·šå½¢å±¤ã®åŸºåº•ã‚¯ãƒ©ã‚¹ã‚’åˆæœŸåŒ–é–‹å§‹ input_size=2048, output_size=1024, bias=False, tp_dim=1
ğŸŸ© ç·šå½¢å±¤ã®åŸºåº•ã‚¯ãƒ©ã‚¹ã‚’åˆæœŸåŒ–å®Œäº†
ğŸŸ© RowParallelLinearã‚’åˆæœŸåŒ–å®Œäº† self.tp_size=1
ğŸŸ© Attentionå±¤ã‚’åˆæœŸåŒ– num_heads=16, head_dim=128, scale=0.08838834764831845, num_kv_heads=8
ğŸŸ© Attentionå±¤ã‚’åˆæœŸåŒ–å®Œäº†
ğŸŸ© RMSNormã‚’åˆæœŸåŒ– hidden_size=128, eps=1e-06
ğŸŸ© RMSNormã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© RMSNormã‚’åˆæœŸåŒ– hidden_size=128, eps=1e-06
ğŸŸ© RMSNormã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© Qwen3Attentionã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© Qwen3MLPã‚’åˆæœŸåŒ–é–‹å§‹ hidden_size=1024 intermediate_size=3072 hidden_act='silu'
ğŸŸ© MergedColumnParallelLinearã‚’åˆæœŸåŒ–é–‹å§‹ input_size=1024, output_sizes=[3072, 3072], bias=False
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã‚’åˆæœŸåŒ–é–‹å§‹ input_size=1024, output_size=6144, bias=False
ğŸŸ© ç·šå½¢å±¤ã®åŸºåº•ã‚¯ãƒ©ã‚¹ã‚’åˆæœŸåŒ–é–‹å§‹ input_size=1024, output_size=6144, bias=False, tp_dim=0
ğŸŸ© ç·šå½¢å±¤ã®åŸºåº•ã‚¯ãƒ©ã‚¹ã‚’åˆæœŸåŒ–å®Œäº†
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã‚’åˆæœŸåŒ–å®Œäº† self.tp_size=1
ğŸŸ© MergedColumnParallelLinearã‚’åˆæœŸåŒ–å®Œäº†
ğŸŸ© RowParallelLinearã‚’åˆæœŸåŒ–é–‹å§‹ input_size=3072, output_size=1024, bias=False
ğŸŸ© ç·šå½¢å±¤ã®åŸºåº•ã‚¯ãƒ©ã‚¹ã‚’åˆæœŸåŒ–é–‹å§‹ input_size=3072, output_size=1024, bias=False, tp_dim=1
ğŸŸ© ç·šå½¢å±¤ã®åŸºåº•ã‚¯ãƒ©ã‚¹ã‚’åˆæœŸåŒ–å®Œäº†
ğŸŸ© RowParallelLinearã‚’åˆæœŸåŒ–å®Œäº† self.tp_size=1
ğŸŸ© SwiGLUã‚’åˆæœŸåŒ–
ğŸŸ© Qwen3MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© RMSNormã‚’åˆæœŸåŒ– hidden_size=1024, eps=1e-06
ğŸŸ© RMSNormã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© RMSNormã‚’åˆæœŸåŒ– hidden_size=1024, eps=1e-06
ğŸŸ© RMSNormã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© Qwen3DecoderLayerã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© Qwen3DecoderLayerã‚’åˆæœŸåŒ–é–‹å§‹ config.hidden_size=1024 config.num_attention_heads=16 config.num_key_value_heads=8 config.intermediate_size=3072 config.rms_norm_eps=1e-06 config.hidden_act='silu'
ğŸŸ© Qwen3Attentionã‚’åˆæœŸåŒ–é–‹å§‹ hidden_size: 1024, num_heads: 16, num_kv_heads: 8, max_position: 40960, head_dim: 128, rms_norm_eps: 1e-06, qkv_bias: False, rope_theta: 1000000, rope_scaling: None
ğŸŸ© QKVParallelLinearã‚’åˆæœŸåŒ–é–‹å§‹ hidden_size=1024, head_size=128, total_num_heads=16, total_num_kv_heads=8, bias=False
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã‚’åˆæœŸåŒ–é–‹å§‹ input_size=1024, output_size=4096, bias=False
ğŸŸ© ç·šå½¢å±¤ã®åŸºåº•ã‚¯ãƒ©ã‚¹ã‚’åˆæœŸåŒ–é–‹å§‹ input_size=1024, output_size=4096, bias=False, tp_dim=0
ğŸŸ© ç·šå½¢å±¤ã®åŸºåº•ã‚¯ãƒ©ã‚¹ã‚’åˆæœŸåŒ–å®Œäº†
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã‚’åˆæœŸåŒ–å®Œäº† self.tp_size=1
ğŸŸ© QKVParallelLinearã‚’åˆæœŸåŒ–å®Œäº† self.tp_size=1, self.num_heads=16, self.num_kv_heads=8
ğŸŸ© RowParallelLinearã‚’åˆæœŸåŒ–é–‹å§‹ input_size=2048, output_size=1024, bias=False
ğŸŸ© ç·šå½¢å±¤ã®åŸºåº•ã‚¯ãƒ©ã‚¹ã‚’åˆæœŸåŒ–é–‹å§‹ input_size=2048, output_size=1024, bias=False, tp_dim=1
ğŸŸ© ç·šå½¢å±¤ã®åŸºåº•ã‚¯ãƒ©ã‚¹ã‚’åˆæœŸåŒ–å®Œäº†
ğŸŸ© RowParallelLinearã‚’åˆæœŸåŒ–å®Œäº† self.tp_size=1
ğŸŸ© Attentionå±¤ã‚’åˆæœŸåŒ– num_heads=16, head_dim=128, scale=0.08838834764831845, num_kv_heads=8
ğŸŸ© Attentionå±¤ã‚’åˆæœŸåŒ–å®Œäº†
ğŸŸ© RMSNormã‚’åˆæœŸåŒ– hidden_size=128, eps=1e-06
ğŸŸ© RMSNormã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© RMSNormã‚’åˆæœŸåŒ– hidden_size=128, eps=1e-06
ğŸŸ© RMSNormã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© Qwen3Attentionã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© Qwen3MLPã‚’åˆæœŸåŒ–é–‹å§‹ hidden_size=1024 intermediate_size=3072 hidden_act='silu'
ğŸŸ© MergedColumnParallelLinearã‚’åˆæœŸåŒ–é–‹å§‹ input_size=1024, output_sizes=[3072, 3072], bias=False
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã‚’åˆæœŸåŒ–é–‹å§‹ input_size=1024, output_size=6144, bias=False
ğŸŸ© ç·šå½¢å±¤ã®åŸºåº•ã‚¯ãƒ©ã‚¹ã‚’åˆæœŸåŒ–é–‹å§‹ input_size=1024, output_size=6144, bias=False, tp_dim=0
ğŸŸ© ç·šå½¢å±¤ã®åŸºåº•ã‚¯ãƒ©ã‚¹ã‚’åˆæœŸåŒ–å®Œäº†
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã‚’åˆæœŸåŒ–å®Œäº† self.tp_size=1
ğŸŸ© MergedColumnParallelLinearã‚’åˆæœŸåŒ–å®Œäº†
ğŸŸ© RowParallelLinearã‚’åˆæœŸåŒ–é–‹å§‹ input_size=3072, output_size=1024, bias=False
ğŸŸ© ç·šå½¢å±¤ã®åŸºåº•ã‚¯ãƒ©ã‚¹ã‚’åˆæœŸåŒ–é–‹å§‹ input_size=3072, output_size=1024, bias=False, tp_dim=1
ğŸŸ© ç·šå½¢å±¤ã®åŸºåº•ã‚¯ãƒ©ã‚¹ã‚’åˆæœŸåŒ–å®Œäº†
ğŸŸ© RowParallelLinearã‚’åˆæœŸåŒ–å®Œäº† self.tp_size=1
ğŸŸ© SwiGLUã‚’åˆæœŸåŒ–
ğŸŸ© Qwen3MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© RMSNormã‚’åˆæœŸåŒ– hidden_size=1024, eps=1e-06
ğŸŸ© RMSNormã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© RMSNormã‚’åˆæœŸåŒ– hidden_size=1024, eps=1e-06
ğŸŸ© RMSNormã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© Qwen3DecoderLayerã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© Qwen3DecoderLayerã‚’åˆæœŸåŒ–é–‹å§‹ config.hidden_size=1024 config.num_attention_heads=16 config.num_key_value_heads=8 config.intermediate_size=3072 config.rms_norm_eps=1e-06 config.hidden_act='silu'
ğŸŸ© Qwen3Attentionã‚’åˆæœŸåŒ–é–‹å§‹ hidden_size: 1024, num_heads: 16, num_kv_heads: 8, max_position: 40960, head_dim: 128, rms_norm_eps: 1e-06, qkv_bias: False, rope_theta: 1000000, rope_scaling: None
ğŸŸ© QKVParallelLinearã‚’åˆæœŸåŒ–é–‹å§‹ hidden_size=1024, head_size=128, total_num_heads=16, total_num_kv_heads=8, bias=False
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã‚’åˆæœŸåŒ–é–‹å§‹ input_size=1024, output_size=4096, bias=False
ğŸŸ© ç·šå½¢å±¤ã®åŸºåº•ã‚¯ãƒ©ã‚¹ã‚’åˆæœŸåŒ–é–‹å§‹ input_size=1024, output_size=4096, bias=False, tp_dim=0
ğŸŸ© ç·šå½¢å±¤ã®åŸºåº•ã‚¯ãƒ©ã‚¹ã‚’åˆæœŸåŒ–å®Œäº†
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã‚’åˆæœŸåŒ–å®Œäº† self.tp_size=1
ğŸŸ© QKVParallelLinearã‚’åˆæœŸåŒ–å®Œäº† self.tp_size=1, self.num_heads=16, self.num_kv_heads=8
ğŸŸ© RowParallelLinearã‚’åˆæœŸåŒ–é–‹å§‹ input_size=2048, output_size=1024, bias=False
ğŸŸ© ç·šå½¢å±¤ã®åŸºåº•ã‚¯ãƒ©ã‚¹ã‚’åˆæœŸåŒ–é–‹å§‹ input_size=2048, output_size=1024, bias=False, tp_dim=1
ğŸŸ© ç·šå½¢å±¤ã®åŸºåº•ã‚¯ãƒ©ã‚¹ã‚’åˆæœŸåŒ–å®Œäº†
ğŸŸ© RowParallelLinearã‚’åˆæœŸåŒ–å®Œäº† self.tp_size=1
ğŸŸ© Attentionå±¤ã‚’åˆæœŸåŒ– num_heads=16, head_dim=128, scale=0.08838834764831845, num_kv_heads=8
ğŸŸ© Attentionå±¤ã‚’åˆæœŸåŒ–å®Œäº†
ğŸŸ© RMSNormã‚’åˆæœŸåŒ– hidden_size=128, eps=1e-06
ğŸŸ© RMSNormã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© RMSNormã‚’åˆæœŸåŒ– hidden_size=128, eps=1e-06
ğŸŸ© RMSNormã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© Qwen3Attentionã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© Qwen3MLPã‚’åˆæœŸåŒ–é–‹å§‹ hidden_size=1024 intermediate_size=3072 hidden_act='silu'
ğŸŸ© MergedColumnParallelLinearã‚’åˆæœŸåŒ–é–‹å§‹ input_size=1024, output_sizes=[3072, 3072], bias=False
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã‚’åˆæœŸåŒ–é–‹å§‹ input_size=1024, output_size=6144, bias=False
ğŸŸ© ç·šå½¢å±¤ã®åŸºåº•ã‚¯ãƒ©ã‚¹ã‚’åˆæœŸåŒ–é–‹å§‹ input_size=1024, output_size=6144, bias=False, tp_dim=0
ğŸŸ© ç·šå½¢å±¤ã®åŸºåº•ã‚¯ãƒ©ã‚¹ã‚’åˆæœŸåŒ–å®Œäº†
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã‚’åˆæœŸåŒ–å®Œäº† self.tp_size=1
ğŸŸ© MergedColumnParallelLinearã‚’åˆæœŸåŒ–å®Œäº†
ğŸŸ© RowParallelLinearã‚’åˆæœŸåŒ–é–‹å§‹ input_size=3072, output_size=1024, bias=False
ğŸŸ© ç·šå½¢å±¤ã®åŸºåº•ã‚¯ãƒ©ã‚¹ã‚’åˆæœŸåŒ–é–‹å§‹ input_size=3072, output_size=1024, bias=False, tp_dim=1
ğŸŸ© ç·šå½¢å±¤ã®åŸºåº•ã‚¯ãƒ©ã‚¹ã‚’åˆæœŸåŒ–å®Œäº†
ğŸŸ© RowParallelLinearã‚’åˆæœŸåŒ–å®Œäº† self.tp_size=1
ğŸŸ© SwiGLUã‚’åˆæœŸåŒ–
ğŸŸ© Qwen3MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© RMSNormã‚’åˆæœŸåŒ– hidden_size=1024, eps=1e-06
ğŸŸ© RMSNormã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© RMSNormã‚’åˆæœŸåŒ– hidden_size=1024, eps=1e-06
ğŸŸ© RMSNormã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© Qwen3DecoderLayerã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© Qwen3DecoderLayerã‚’åˆæœŸåŒ–é–‹å§‹ config.hidden_size=1024 config.num_attention_heads=16 config.num_key_value_heads=8 config.intermediate_size=3072 config.rms_norm_eps=1e-06 config.hidden_act='silu'
ğŸŸ© Qwen3Attentionã‚’åˆæœŸåŒ–é–‹å§‹ hidden_size: 1024, num_heads: 16, num_kv_heads: 8, max_position: 40960, head_dim: 128, rms_norm_eps: 1e-06, qkv_bias: False, rope_theta: 1000000, rope_scaling: None
ğŸŸ© QKVParallelLinearã‚’åˆæœŸåŒ–é–‹å§‹ hidden_size=1024, head_size=128, total_num_heads=16, total_num_kv_heads=8, bias=False
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã‚’åˆæœŸåŒ–é–‹å§‹ input_size=1024, output_size=4096, bias=False
ğŸŸ© ç·šå½¢å±¤ã®åŸºåº•ã‚¯ãƒ©ã‚¹ã‚’åˆæœŸåŒ–é–‹å§‹ input_size=1024, output_size=4096, bias=False, tp_dim=0
ğŸŸ© ç·šå½¢å±¤ã®åŸºåº•ã‚¯ãƒ©ã‚¹ã‚’åˆæœŸåŒ–å®Œäº†
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã‚’åˆæœŸåŒ–å®Œäº† self.tp_size=1
ğŸŸ© QKVParallelLinearã‚’åˆæœŸåŒ–å®Œäº† self.tp_size=1, self.num_heads=16, self.num_kv_heads=8
ğŸŸ© RowParallelLinearã‚’åˆæœŸåŒ–é–‹å§‹ input_size=2048, output_size=1024, bias=False
ğŸŸ© ç·šå½¢å±¤ã®åŸºåº•ã‚¯ãƒ©ã‚¹ã‚’åˆæœŸåŒ–é–‹å§‹ input_size=2048, output_size=1024, bias=False, tp_dim=1
ğŸŸ© ç·šå½¢å±¤ã®åŸºåº•ã‚¯ãƒ©ã‚¹ã‚’åˆæœŸåŒ–å®Œäº†
ğŸŸ© RowParallelLinearã‚’åˆæœŸåŒ–å®Œäº† self.tp_size=1
ğŸŸ© Attentionå±¤ã‚’åˆæœŸåŒ– num_heads=16, head_dim=128, scale=0.08838834764831845, num_kv_heads=8
ğŸŸ© Attentionå±¤ã‚’åˆæœŸåŒ–å®Œäº†
ğŸŸ© RMSNormã‚’åˆæœŸåŒ– hidden_size=128, eps=1e-06
ğŸŸ© RMSNormã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© RMSNormã‚’åˆæœŸåŒ– hidden_size=128, eps=1e-06
ğŸŸ© RMSNormã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© Qwen3Attentionã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© Qwen3MLPã‚’åˆæœŸåŒ–é–‹å§‹ hidden_size=1024 intermediate_size=3072 hidden_act='silu'
ğŸŸ© MergedColumnParallelLinearã‚’åˆæœŸåŒ–é–‹å§‹ input_size=1024, output_sizes=[3072, 3072], bias=False
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã‚’åˆæœŸåŒ–é–‹å§‹ input_size=1024, output_size=6144, bias=False
ğŸŸ© ç·šå½¢å±¤ã®åŸºåº•ã‚¯ãƒ©ã‚¹ã‚’åˆæœŸåŒ–é–‹å§‹ input_size=1024, output_size=6144, bias=False, tp_dim=0
ğŸŸ© ç·šå½¢å±¤ã®åŸºåº•ã‚¯ãƒ©ã‚¹ã‚’åˆæœŸåŒ–å®Œäº†
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã‚’åˆæœŸåŒ–å®Œäº† self.tp_size=1
ğŸŸ© MergedColumnParallelLinearã‚’åˆæœŸåŒ–å®Œäº†
ğŸŸ© RowParallelLinearã‚’åˆæœŸåŒ–é–‹å§‹ input_size=3072, output_size=1024, bias=False
ğŸŸ© ç·šå½¢å±¤ã®åŸºåº•ã‚¯ãƒ©ã‚¹ã‚’åˆæœŸåŒ–é–‹å§‹ input_size=3072, output_size=1024, bias=False, tp_dim=1
ğŸŸ© ç·šå½¢å±¤ã®åŸºåº•ã‚¯ãƒ©ã‚¹ã‚’åˆæœŸåŒ–å®Œäº†
ğŸŸ© RowParallelLinearã‚’åˆæœŸåŒ–å®Œäº† self.tp_size=1
ğŸŸ© SwiGLUã‚’åˆæœŸåŒ–
ğŸŸ© Qwen3MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© RMSNormã‚’åˆæœŸåŒ– hidden_size=1024, eps=1e-06
ğŸŸ© RMSNormã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© RMSNormã‚’åˆæœŸåŒ– hidden_size=1024, eps=1e-06
ğŸŸ© RMSNormã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© Qwen3DecoderLayerã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© Qwen3DecoderLayerã‚’åˆæœŸåŒ–é–‹å§‹ config.hidden_size=1024 config.num_attention_heads=16 config.num_key_value_heads=8 config.intermediate_size=3072 config.rms_norm_eps=1e-06 config.hidden_act='silu'
ğŸŸ© Qwen3Attentionã‚’åˆæœŸåŒ–é–‹å§‹ hidden_size: 1024, num_heads: 16, num_kv_heads: 8, max_position: 40960, head_dim: 128, rms_norm_eps: 1e-06, qkv_bias: False, rope_theta: 1000000, rope_scaling: None
ğŸŸ© QKVParallelLinearã‚’åˆæœŸåŒ–é–‹å§‹ hidden_size=1024, head_size=128, total_num_heads=16, total_num_kv_heads=8, bias=False
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã‚’åˆæœŸåŒ–é–‹å§‹ input_size=1024, output_size=4096, bias=False
ğŸŸ© ç·šå½¢å±¤ã®åŸºåº•ã‚¯ãƒ©ã‚¹ã‚’åˆæœŸåŒ–é–‹å§‹ input_size=1024, output_size=4096, bias=False, tp_dim=0
ğŸŸ© ç·šå½¢å±¤ã®åŸºåº•ã‚¯ãƒ©ã‚¹ã‚’åˆæœŸåŒ–å®Œäº†
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã‚’åˆæœŸåŒ–å®Œäº† self.tp_size=1
ğŸŸ© QKVParallelLinearã‚’åˆæœŸåŒ–å®Œäº† self.tp_size=1, self.num_heads=16, self.num_kv_heads=8
ğŸŸ© RowParallelLinearã‚’åˆæœŸåŒ–é–‹å§‹ input_size=2048, output_size=1024, bias=False
ğŸŸ© ç·šå½¢å±¤ã®åŸºåº•ã‚¯ãƒ©ã‚¹ã‚’åˆæœŸåŒ–é–‹å§‹ input_size=2048, output_size=1024, bias=False, tp_dim=1
ğŸŸ© ç·šå½¢å±¤ã®åŸºåº•ã‚¯ãƒ©ã‚¹ã‚’åˆæœŸåŒ–å®Œäº†
ğŸŸ© RowParallelLinearã‚’åˆæœŸåŒ–å®Œäº† self.tp_size=1
ğŸŸ© Attentionå±¤ã‚’åˆæœŸåŒ– num_heads=16, head_dim=128, scale=0.08838834764831845, num_kv_heads=8
ğŸŸ© Attentionå±¤ã‚’åˆæœŸåŒ–å®Œäº†
ğŸŸ© RMSNormã‚’åˆæœŸåŒ– hidden_size=128, eps=1e-06
ğŸŸ© RMSNormã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© RMSNormã‚’åˆæœŸåŒ– hidden_size=128, eps=1e-06
ğŸŸ© RMSNormã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© Qwen3Attentionã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© Qwen3MLPã‚’åˆæœŸåŒ–é–‹å§‹ hidden_size=1024 intermediate_size=3072 hidden_act='silu'
ğŸŸ© MergedColumnParallelLinearã‚’åˆæœŸåŒ–é–‹å§‹ input_size=1024, output_sizes=[3072, 3072], bias=False
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã‚’åˆæœŸåŒ–é–‹å§‹ input_size=1024, output_size=6144, bias=False
ğŸŸ© ç·šå½¢å±¤ã®åŸºåº•ã‚¯ãƒ©ã‚¹ã‚’åˆæœŸåŒ–é–‹å§‹ input_size=1024, output_size=6144, bias=False, tp_dim=0
ğŸŸ© ç·šå½¢å±¤ã®åŸºåº•ã‚¯ãƒ©ã‚¹ã‚’åˆæœŸåŒ–å®Œäº†
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã‚’åˆæœŸåŒ–å®Œäº† self.tp_size=1
ğŸŸ© MergedColumnParallelLinearã‚’åˆæœŸåŒ–å®Œäº†
ğŸŸ© RowParallelLinearã‚’åˆæœŸåŒ–é–‹å§‹ input_size=3072, output_size=1024, bias=False
ğŸŸ© ç·šå½¢å±¤ã®åŸºåº•ã‚¯ãƒ©ã‚¹ã‚’åˆæœŸåŒ–é–‹å§‹ input_size=3072, output_size=1024, bias=False, tp_dim=1
ğŸŸ© ç·šå½¢å±¤ã®åŸºåº•ã‚¯ãƒ©ã‚¹ã‚’åˆæœŸåŒ–å®Œäº†
ğŸŸ© RowParallelLinearã‚’åˆæœŸåŒ–å®Œäº† self.tp_size=1
ğŸŸ© SwiGLUã‚’åˆæœŸåŒ–
ğŸŸ© Qwen3MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© RMSNormã‚’åˆæœŸåŒ– hidden_size=1024, eps=1e-06
ğŸŸ© RMSNormã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© RMSNormã‚’åˆæœŸåŒ– hidden_size=1024, eps=1e-06
ğŸŸ© RMSNormã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© Qwen3DecoderLayerã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© Qwen3DecoderLayerã‚’åˆæœŸåŒ–é–‹å§‹ config.hidden_size=1024 config.num_attention_heads=16 config.num_key_value_heads=8 config.intermediate_size=3072 config.rms_norm_eps=1e-06 config.hidden_act='silu'
ğŸŸ© Qwen3Attentionã‚’åˆæœŸåŒ–é–‹å§‹ hidden_size: 1024, num_heads: 16, num_kv_heads: 8, max_position: 40960, head_dim: 128, rms_norm_eps: 1e-06, qkv_bias: False, rope_theta: 1000000, rope_scaling: None
ğŸŸ© QKVParallelLinearã‚’åˆæœŸåŒ–é–‹å§‹ hidden_size=1024, head_size=128, total_num_heads=16, total_num_kv_heads=8, bias=False
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã‚’åˆæœŸåŒ–é–‹å§‹ input_size=1024, output_size=4096, bias=False
ğŸŸ© ç·šå½¢å±¤ã®åŸºåº•ã‚¯ãƒ©ã‚¹ã‚’åˆæœŸåŒ–é–‹å§‹ input_size=1024, output_size=4096, bias=False, tp_dim=0
ğŸŸ© ç·šå½¢å±¤ã®åŸºåº•ã‚¯ãƒ©ã‚¹ã‚’åˆæœŸåŒ–å®Œäº†
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã‚’åˆæœŸåŒ–å®Œäº† self.tp_size=1
ğŸŸ© QKVParallelLinearã‚’åˆæœŸåŒ–å®Œäº† self.tp_size=1, self.num_heads=16, self.num_kv_heads=8
ğŸŸ© RowParallelLinearã‚’åˆæœŸåŒ–é–‹å§‹ input_size=2048, output_size=1024, bias=False
ğŸŸ© ç·šå½¢å±¤ã®åŸºåº•ã‚¯ãƒ©ã‚¹ã‚’åˆæœŸåŒ–é–‹å§‹ input_size=2048, output_size=1024, bias=False, tp_dim=1
ğŸŸ© ç·šå½¢å±¤ã®åŸºåº•ã‚¯ãƒ©ã‚¹ã‚’åˆæœŸåŒ–å®Œäº†
ğŸŸ© RowParallelLinearã‚’åˆæœŸåŒ–å®Œäº† self.tp_size=1
ğŸŸ© Attentionå±¤ã‚’åˆæœŸåŒ– num_heads=16, head_dim=128, scale=0.08838834764831845, num_kv_heads=8
ğŸŸ© Attentionå±¤ã‚’åˆæœŸåŒ–å®Œäº†
ğŸŸ© RMSNormã‚’åˆæœŸåŒ– hidden_size=128, eps=1e-06
ğŸŸ© RMSNormã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© RMSNormã‚’åˆæœŸåŒ– hidden_size=128, eps=1e-06
ğŸŸ© RMSNormã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© Qwen3Attentionã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© Qwen3MLPã‚’åˆæœŸåŒ–é–‹å§‹ hidden_size=1024 intermediate_size=3072 hidden_act='silu'
ğŸŸ© MergedColumnParallelLinearã‚’åˆæœŸåŒ–é–‹å§‹ input_size=1024, output_sizes=[3072, 3072], bias=False
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã‚’åˆæœŸåŒ–é–‹å§‹ input_size=1024, output_size=6144, bias=False
ğŸŸ© ç·šå½¢å±¤ã®åŸºåº•ã‚¯ãƒ©ã‚¹ã‚’åˆæœŸåŒ–é–‹å§‹ input_size=1024, output_size=6144, bias=False, tp_dim=0
ğŸŸ© ç·šå½¢å±¤ã®åŸºåº•ã‚¯ãƒ©ã‚¹ã‚’åˆæœŸåŒ–å®Œäº†
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã‚’åˆæœŸåŒ–å®Œäº† self.tp_size=1
ğŸŸ© MergedColumnParallelLinearã‚’åˆæœŸåŒ–å®Œäº†
ğŸŸ© RowParallelLinearã‚’åˆæœŸåŒ–é–‹å§‹ input_size=3072, output_size=1024, bias=False
ğŸŸ© ç·šå½¢å±¤ã®åŸºåº•ã‚¯ãƒ©ã‚¹ã‚’åˆæœŸåŒ–é–‹å§‹ input_size=3072, output_size=1024, bias=False, tp_dim=1
ğŸŸ© ç·šå½¢å±¤ã®åŸºåº•ã‚¯ãƒ©ã‚¹ã‚’åˆæœŸåŒ–å®Œäº†
ğŸŸ© RowParallelLinearã‚’åˆæœŸåŒ–å®Œäº† self.tp_size=1
ğŸŸ© SwiGLUã‚’åˆæœŸåŒ–
ğŸŸ© Qwen3MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© RMSNormã‚’åˆæœŸåŒ– hidden_size=1024, eps=1e-06
ğŸŸ© RMSNormã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© RMSNormã‚’åˆæœŸåŒ– hidden_size=1024, eps=1e-06
ğŸŸ© RMSNormã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© Qwen3DecoderLayerã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© Qwen3DecoderLayerã‚’åˆæœŸåŒ–é–‹å§‹ config.hidden_size=1024 config.num_attention_heads=16 config.num_key_value_heads=8 config.intermediate_size=3072 config.rms_norm_eps=1e-06 config.hidden_act='silu'
ğŸŸ© Qwen3Attentionã‚’åˆæœŸåŒ–é–‹å§‹ hidden_size: 1024, num_heads: 16, num_kv_heads: 8, max_position: 40960, head_dim: 128, rms_norm_eps: 1e-06, qkv_bias: False, rope_theta: 1000000, rope_scaling: None
ğŸŸ© QKVParallelLinearã‚’åˆæœŸåŒ–é–‹å§‹ hidden_size=1024, head_size=128, total_num_heads=16, total_num_kv_heads=8, bias=False
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã‚’åˆæœŸåŒ–é–‹å§‹ input_size=1024, output_size=4096, bias=False
ğŸŸ© ç·šå½¢å±¤ã®åŸºåº•ã‚¯ãƒ©ã‚¹ã‚’åˆæœŸåŒ–é–‹å§‹ input_size=1024, output_size=4096, bias=False, tp_dim=0
ğŸŸ© ç·šå½¢å±¤ã®åŸºåº•ã‚¯ãƒ©ã‚¹ã‚’åˆæœŸåŒ–å®Œäº†
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã‚’åˆæœŸåŒ–å®Œäº† self.tp_size=1
ğŸŸ© QKVParallelLinearã‚’åˆæœŸåŒ–å®Œäº† self.tp_size=1, self.num_heads=16, self.num_kv_heads=8
ğŸŸ© RowParallelLinearã‚’åˆæœŸåŒ–é–‹å§‹ input_size=2048, output_size=1024, bias=False
ğŸŸ© ç·šå½¢å±¤ã®åŸºåº•ã‚¯ãƒ©ã‚¹ã‚’åˆæœŸåŒ–é–‹å§‹ input_size=2048, output_size=1024, bias=False, tp_dim=1
ğŸŸ© ç·šå½¢å±¤ã®åŸºåº•ã‚¯ãƒ©ã‚¹ã‚’åˆæœŸåŒ–å®Œäº†
ğŸŸ© RowParallelLinearã‚’åˆæœŸåŒ–å®Œäº† self.tp_size=1
ğŸŸ© Attentionå±¤ã‚’åˆæœŸåŒ– num_heads=16, head_dim=128, scale=0.08838834764831845, num_kv_heads=8
ğŸŸ© Attentionå±¤ã‚’åˆæœŸåŒ–å®Œäº†
ğŸŸ© RMSNormã‚’åˆæœŸåŒ– hidden_size=128, eps=1e-06
ğŸŸ© RMSNormã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© RMSNormã‚’åˆæœŸåŒ– hidden_size=128, eps=1e-06
ğŸŸ© RMSNormã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© Qwen3Attentionã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© Qwen3MLPã‚’åˆæœŸåŒ–é–‹å§‹ hidden_size=1024 intermediate_size=3072 hidden_act='silu'
ğŸŸ© MergedColumnParallelLinearã‚’åˆæœŸåŒ–é–‹å§‹ input_size=1024, output_sizes=[3072, 3072], bias=False
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã‚’åˆæœŸåŒ–é–‹å§‹ input_size=1024, output_size=6144, bias=False
ğŸŸ© ç·šå½¢å±¤ã®åŸºåº•ã‚¯ãƒ©ã‚¹ã‚’åˆæœŸåŒ–é–‹å§‹ input_size=1024, output_size=6144, bias=False, tp_dim=0
ğŸŸ© ç·šå½¢å±¤ã®åŸºåº•ã‚¯ãƒ©ã‚¹ã‚’åˆæœŸåŒ–å®Œäº†
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã‚’åˆæœŸåŒ–å®Œäº† self.tp_size=1
ğŸŸ© MergedColumnParallelLinearã‚’åˆæœŸåŒ–å®Œäº†
ğŸŸ© RowParallelLinearã‚’åˆæœŸåŒ–é–‹å§‹ input_size=3072, output_size=1024, bias=False
ğŸŸ© ç·šå½¢å±¤ã®åŸºåº•ã‚¯ãƒ©ã‚¹ã‚’åˆæœŸåŒ–é–‹å§‹ input_size=3072, output_size=1024, bias=False, tp_dim=1
ğŸŸ© ç·šå½¢å±¤ã®åŸºåº•ã‚¯ãƒ©ã‚¹ã‚’åˆæœŸåŒ–å®Œäº†
ğŸŸ© RowParallelLinearã‚’åˆæœŸåŒ–å®Œäº† self.tp_size=1
ğŸŸ© SwiGLUã‚’åˆæœŸåŒ–
ğŸŸ© Qwen3MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© RMSNormã‚’åˆæœŸåŒ– hidden_size=1024, eps=1e-06
ğŸŸ© RMSNormã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© RMSNormã‚’åˆæœŸåŒ– hidden_size=1024, eps=1e-06
ğŸŸ© RMSNormã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© Qwen3DecoderLayerã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© Qwen3DecoderLayerã‚’åˆæœŸåŒ–é–‹å§‹ config.hidden_size=1024 config.num_attention_heads=16 config.num_key_value_heads=8 config.intermediate_size=3072 config.rms_norm_eps=1e-06 config.hidden_act='silu'
ğŸŸ© Qwen3Attentionã‚’åˆæœŸåŒ–é–‹å§‹ hidden_size: 1024, num_heads: 16, num_kv_heads: 8, max_position: 40960, head_dim: 128, rms_norm_eps: 1e-06, qkv_bias: False, rope_theta: 1000000, rope_scaling: None
ğŸŸ© QKVParallelLinearã‚’åˆæœŸåŒ–é–‹å§‹ hidden_size=1024, head_size=128, total_num_heads=16, total_num_kv_heads=8, bias=False
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã‚’åˆæœŸåŒ–é–‹å§‹ input_size=1024, output_size=4096, bias=False
ğŸŸ© ç·šå½¢å±¤ã®åŸºåº•ã‚¯ãƒ©ã‚¹ã‚’åˆæœŸåŒ–é–‹å§‹ input_size=1024, output_size=4096, bias=False, tp_dim=0
ğŸŸ© ç·šå½¢å±¤ã®åŸºåº•ã‚¯ãƒ©ã‚¹ã‚’åˆæœŸåŒ–å®Œäº†
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã‚’åˆæœŸåŒ–å®Œäº† self.tp_size=1
ğŸŸ© QKVParallelLinearã‚’åˆæœŸåŒ–å®Œäº† self.tp_size=1, self.num_heads=16, self.num_kv_heads=8
ğŸŸ© RowParallelLinearã‚’åˆæœŸåŒ–é–‹å§‹ input_size=2048, output_size=1024, bias=False
ğŸŸ© ç·šå½¢å±¤ã®åŸºåº•ã‚¯ãƒ©ã‚¹ã‚’åˆæœŸåŒ–é–‹å§‹ input_size=2048, output_size=1024, bias=False, tp_dim=1
ğŸŸ© ç·šå½¢å±¤ã®åŸºåº•ã‚¯ãƒ©ã‚¹ã‚’åˆæœŸåŒ–å®Œäº†
ğŸŸ© RowParallelLinearã‚’åˆæœŸåŒ–å®Œäº† self.tp_size=1
ğŸŸ© Attentionå±¤ã‚’åˆæœŸåŒ– num_heads=16, head_dim=128, scale=0.08838834764831845, num_kv_heads=8
ğŸŸ© Attentionå±¤ã‚’åˆæœŸåŒ–å®Œäº†
ğŸŸ© RMSNormã‚’åˆæœŸåŒ– hidden_size=128, eps=1e-06
ğŸŸ© RMSNormã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© RMSNormã‚’åˆæœŸåŒ– hidden_size=128, eps=1e-06
ğŸŸ© RMSNormã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© Qwen3Attentionã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© Qwen3MLPã‚’åˆæœŸåŒ–é–‹å§‹ hidden_size=1024 intermediate_size=3072 hidden_act='silu'
ğŸŸ© MergedColumnParallelLinearã‚’åˆæœŸåŒ–é–‹å§‹ input_size=1024, output_sizes=[3072, 3072], bias=False
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã‚’åˆæœŸåŒ–é–‹å§‹ input_size=1024, output_size=6144, bias=False
ğŸŸ© ç·šå½¢å±¤ã®åŸºåº•ã‚¯ãƒ©ã‚¹ã‚’åˆæœŸåŒ–é–‹å§‹ input_size=1024, output_size=6144, bias=False, tp_dim=0
ğŸŸ© ç·šå½¢å±¤ã®åŸºåº•ã‚¯ãƒ©ã‚¹ã‚’åˆæœŸåŒ–å®Œäº†
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã‚’åˆæœŸåŒ–å®Œäº† self.tp_size=1
ğŸŸ© MergedColumnParallelLinearã‚’åˆæœŸåŒ–å®Œäº†
ğŸŸ© RowParallelLinearã‚’åˆæœŸåŒ–é–‹å§‹ input_size=3072, output_size=1024, bias=False
ğŸŸ© ç·šå½¢å±¤ã®åŸºåº•ã‚¯ãƒ©ã‚¹ã‚’åˆæœŸåŒ–é–‹å§‹ input_size=3072, output_size=1024, bias=False, tp_dim=1
ğŸŸ© ç·šå½¢å±¤ã®åŸºåº•ã‚¯ãƒ©ã‚¹ã‚’åˆæœŸåŒ–å®Œäº†
ğŸŸ© RowParallelLinearã‚’åˆæœŸåŒ–å®Œäº† self.tp_size=1
ğŸŸ© SwiGLUã‚’åˆæœŸåŒ–
ğŸŸ© Qwen3MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© RMSNormã‚’åˆæœŸåŒ– hidden_size=1024, eps=1e-06
ğŸŸ© RMSNormã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© RMSNormã‚’åˆæœŸåŒ– hidden_size=1024, eps=1e-06
ğŸŸ© RMSNormã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© Qwen3DecoderLayerã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© Qwen3DecoderLayerã‚’åˆæœŸåŒ–é–‹å§‹ config.hidden_size=1024 config.num_attention_heads=16 config.num_key_value_heads=8 config.intermediate_size=3072 config.rms_norm_eps=1e-06 config.hidden_act='silu'
ğŸŸ© Qwen3Attentionã‚’åˆæœŸåŒ–é–‹å§‹ hidden_size: 1024, num_heads: 16, num_kv_heads: 8, max_position: 40960, head_dim: 128, rms_norm_eps: 1e-06, qkv_bias: False, rope_theta: 1000000, rope_scaling: None
ğŸŸ© QKVParallelLinearã‚’åˆæœŸåŒ–é–‹å§‹ hidden_size=1024, head_size=128, total_num_heads=16, total_num_kv_heads=8, bias=False
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã‚’åˆæœŸåŒ–é–‹å§‹ input_size=1024, output_size=4096, bias=False
ğŸŸ© ç·šå½¢å±¤ã®åŸºåº•ã‚¯ãƒ©ã‚¹ã‚’åˆæœŸåŒ–é–‹å§‹ input_size=1024, output_size=4096, bias=False, tp_dim=0
ğŸŸ© ç·šå½¢å±¤ã®åŸºåº•ã‚¯ãƒ©ã‚¹ã‚’åˆæœŸåŒ–å®Œäº†
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã‚’åˆæœŸåŒ–å®Œäº† self.tp_size=1
ğŸŸ© QKVParallelLinearã‚’åˆæœŸåŒ–å®Œäº† self.tp_size=1, self.num_heads=16, self.num_kv_heads=8
ğŸŸ© RowParallelLinearã‚’åˆæœŸåŒ–é–‹å§‹ input_size=2048, output_size=1024, bias=False
ğŸŸ© ç·šå½¢å±¤ã®åŸºåº•ã‚¯ãƒ©ã‚¹ã‚’åˆæœŸåŒ–é–‹å§‹ input_size=2048, output_size=1024, bias=False, tp_dim=1
ğŸŸ© ç·šå½¢å±¤ã®åŸºåº•ã‚¯ãƒ©ã‚¹ã‚’åˆæœŸåŒ–å®Œäº†
ğŸŸ© RowParallelLinearã‚’åˆæœŸåŒ–å®Œäº† self.tp_size=1
ğŸŸ© Attentionå±¤ã‚’åˆæœŸåŒ– num_heads=16, head_dim=128, scale=0.08838834764831845, num_kv_heads=8
ğŸŸ© Attentionå±¤ã‚’åˆæœŸåŒ–å®Œäº†
ğŸŸ© RMSNormã‚’åˆæœŸåŒ– hidden_size=128, eps=1e-06
ğŸŸ© RMSNormã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© RMSNormã‚’åˆæœŸåŒ– hidden_size=128, eps=1e-06
ğŸŸ© RMSNormã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© Qwen3Attentionã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© Qwen3MLPã‚’åˆæœŸåŒ–é–‹å§‹ hidden_size=1024 intermediate_size=3072 hidden_act='silu'
ğŸŸ© MergedColumnParallelLinearã‚’åˆæœŸåŒ–é–‹å§‹ input_size=1024, output_sizes=[3072, 3072], bias=False
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã‚’åˆæœŸåŒ–é–‹å§‹ input_size=1024, output_size=6144, bias=False
ğŸŸ© ç·šå½¢å±¤ã®åŸºåº•ã‚¯ãƒ©ã‚¹ã‚’åˆæœŸåŒ–é–‹å§‹ input_size=1024, output_size=6144, bias=False, tp_dim=0
ğŸŸ© ç·šå½¢å±¤ã®åŸºåº•ã‚¯ãƒ©ã‚¹ã‚’åˆæœŸåŒ–å®Œäº†
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã‚’åˆæœŸåŒ–å®Œäº† self.tp_size=1
ğŸŸ© MergedColumnParallelLinearã‚’åˆæœŸåŒ–å®Œäº†
ğŸŸ© RowParallelLinearã‚’åˆæœŸåŒ–é–‹å§‹ input_size=3072, output_size=1024, bias=False
ğŸŸ© ç·šå½¢å±¤ã®åŸºåº•ã‚¯ãƒ©ã‚¹ã‚’åˆæœŸåŒ–é–‹å§‹ input_size=3072, output_size=1024, bias=False, tp_dim=1
ğŸŸ© ç·šå½¢å±¤ã®åŸºåº•ã‚¯ãƒ©ã‚¹ã‚’åˆæœŸåŒ–å®Œäº†
ğŸŸ© RowParallelLinearã‚’åˆæœŸåŒ–å®Œäº† self.tp_size=1
ğŸŸ© SwiGLUã‚’åˆæœŸåŒ–
ğŸŸ© Qwen3MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© RMSNormã‚’åˆæœŸåŒ– hidden_size=1024, eps=1e-06
ğŸŸ© RMSNormã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© RMSNormã‚’åˆæœŸåŒ– hidden_size=1024, eps=1e-06
ğŸŸ© RMSNormã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© Qwen3DecoderLayerã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© Qwen3DecoderLayerã‚’åˆæœŸåŒ–é–‹å§‹ config.hidden_size=1024 config.num_attention_heads=16 config.num_key_value_heads=8 config.intermediate_size=3072 config.rms_norm_eps=1e-06 config.hidden_act='silu'
ğŸŸ© Qwen3Attentionã‚’åˆæœŸåŒ–é–‹å§‹ hidden_size: 1024, num_heads: 16, num_kv_heads: 8, max_position: 40960, head_dim: 128, rms_norm_eps: 1e-06, qkv_bias: False, rope_theta: 1000000, rope_scaling: None
ğŸŸ© QKVParallelLinearã‚’åˆæœŸåŒ–é–‹å§‹ hidden_size=1024, head_size=128, total_num_heads=16, total_num_kv_heads=8, bias=False
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã‚’åˆæœŸåŒ–é–‹å§‹ input_size=1024, output_size=4096, bias=False
ğŸŸ© ç·šå½¢å±¤ã®åŸºåº•ã‚¯ãƒ©ã‚¹ã‚’åˆæœŸåŒ–é–‹å§‹ input_size=1024, output_size=4096, bias=False, tp_dim=0
ğŸŸ© ç·šå½¢å±¤ã®åŸºåº•ã‚¯ãƒ©ã‚¹ã‚’åˆæœŸåŒ–å®Œäº†
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã‚’åˆæœŸåŒ–å®Œäº† self.tp_size=1
ğŸŸ© QKVParallelLinearã‚’åˆæœŸåŒ–å®Œäº† self.tp_size=1, self.num_heads=16, self.num_kv_heads=8
ğŸŸ© RowParallelLinearã‚’åˆæœŸåŒ–é–‹å§‹ input_size=2048, output_size=1024, bias=False
ğŸŸ© ç·šå½¢å±¤ã®åŸºåº•ã‚¯ãƒ©ã‚¹ã‚’åˆæœŸåŒ–é–‹å§‹ input_size=2048, output_size=1024, bias=False, tp_dim=1
ğŸŸ© ç·šå½¢å±¤ã®åŸºåº•ã‚¯ãƒ©ã‚¹ã‚’åˆæœŸåŒ–å®Œäº†
ğŸŸ© RowParallelLinearã‚’åˆæœŸåŒ–å®Œäº† self.tp_size=1
ğŸŸ© Attentionå±¤ã‚’åˆæœŸåŒ– num_heads=16, head_dim=128, scale=0.08838834764831845, num_kv_heads=8
ğŸŸ© Attentionå±¤ã‚’åˆæœŸåŒ–å®Œäº†
ğŸŸ© RMSNormã‚’åˆæœŸåŒ– hidden_size=128, eps=1e-06
ğŸŸ© RMSNormã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© RMSNormã‚’åˆæœŸåŒ– hidden_size=128, eps=1e-06
ğŸŸ© RMSNormã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© Qwen3Attentionã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© Qwen3MLPã‚’åˆæœŸåŒ–é–‹å§‹ hidden_size=1024 intermediate_size=3072 hidden_act='silu'
ğŸŸ© MergedColumnParallelLinearã‚’åˆæœŸåŒ–é–‹å§‹ input_size=1024, output_sizes=[3072, 3072], bias=False
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã‚’åˆæœŸåŒ–é–‹å§‹ input_size=1024, output_size=6144, bias=False
ğŸŸ© ç·šå½¢å±¤ã®åŸºåº•ã‚¯ãƒ©ã‚¹ã‚’åˆæœŸåŒ–é–‹å§‹ input_size=1024, output_size=6144, bias=False, tp_dim=0
ğŸŸ© ç·šå½¢å±¤ã®åŸºåº•ã‚¯ãƒ©ã‚¹ã‚’åˆæœŸåŒ–å®Œäº†
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã‚’åˆæœŸåŒ–å®Œäº† self.tp_size=1
ğŸŸ© MergedColumnParallelLinearã‚’åˆæœŸåŒ–å®Œäº†
ğŸŸ© RowParallelLinearã‚’åˆæœŸåŒ–é–‹å§‹ input_size=3072, output_size=1024, bias=False
ğŸŸ© ç·šå½¢å±¤ã®åŸºåº•ã‚¯ãƒ©ã‚¹ã‚’åˆæœŸåŒ–é–‹å§‹ input_size=3072, output_size=1024, bias=False, tp_dim=1
ğŸŸ© ç·šå½¢å±¤ã®åŸºåº•ã‚¯ãƒ©ã‚¹ã‚’åˆæœŸåŒ–å®Œäº†
ğŸŸ© RowParallelLinearã‚’åˆæœŸåŒ–å®Œäº† self.tp_size=1
ğŸŸ© SwiGLUã‚’åˆæœŸåŒ–
ğŸŸ© Qwen3MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© RMSNormã‚’åˆæœŸåŒ– hidden_size=1024, eps=1e-06
ğŸŸ© RMSNormã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© RMSNormã‚’åˆæœŸåŒ– hidden_size=1024, eps=1e-06
ğŸŸ© RMSNormã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© Qwen3DecoderLayerã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© Qwen3DecoderLayerã‚’åˆæœŸåŒ–é–‹å§‹ config.hidden_size=1024 config.num_attention_heads=16 config.num_key_value_heads=8 config.intermediate_size=3072 config.rms_norm_eps=1e-06 config.hidden_act='silu'
ğŸŸ© Qwen3Attentionã‚’åˆæœŸåŒ–é–‹å§‹ hidden_size: 1024, num_heads: 16, num_kv_heads: 8, max_position: 40960, head_dim: 128, rms_norm_eps: 1e-06, qkv_bias: False, rope_theta: 1000000, rope_scaling: None
ğŸŸ© QKVParallelLinearã‚’åˆæœŸåŒ–é–‹å§‹ hidden_size=1024, head_size=128, total_num_heads=16, total_num_kv_heads=8, bias=False
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã‚’åˆæœŸåŒ–é–‹å§‹ input_size=1024, output_size=4096, bias=False
ğŸŸ© ç·šå½¢å±¤ã®åŸºåº•ã‚¯ãƒ©ã‚¹ã‚’åˆæœŸåŒ–é–‹å§‹ input_size=1024, output_size=4096, bias=False, tp_dim=0
ğŸŸ© ç·šå½¢å±¤ã®åŸºåº•ã‚¯ãƒ©ã‚¹ã‚’åˆæœŸåŒ–å®Œäº†
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã‚’åˆæœŸåŒ–å®Œäº† self.tp_size=1
ğŸŸ© QKVParallelLinearã‚’åˆæœŸåŒ–å®Œäº† self.tp_size=1, self.num_heads=16, self.num_kv_heads=8
ğŸŸ© RowParallelLinearã‚’åˆæœŸåŒ–é–‹å§‹ input_size=2048, output_size=1024, bias=False
ğŸŸ© ç·šå½¢å±¤ã®åŸºåº•ã‚¯ãƒ©ã‚¹ã‚’åˆæœŸåŒ–é–‹å§‹ input_size=2048, output_size=1024, bias=False, tp_dim=1
ğŸŸ© ç·šå½¢å±¤ã®åŸºåº•ã‚¯ãƒ©ã‚¹ã‚’åˆæœŸåŒ–å®Œäº†
ğŸŸ© RowParallelLinearã‚’åˆæœŸåŒ–å®Œäº† self.tp_size=1
ğŸŸ© Attentionå±¤ã‚’åˆæœŸåŒ– num_heads=16, head_dim=128, scale=0.08838834764831845, num_kv_heads=8
ğŸŸ© Attentionå±¤ã‚’åˆæœŸåŒ–å®Œäº†
ğŸŸ© RMSNormã‚’åˆæœŸåŒ– hidden_size=128, eps=1e-06
ğŸŸ© RMSNormã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© RMSNormã‚’åˆæœŸåŒ– hidden_size=128, eps=1e-06
ğŸŸ© RMSNormã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© Qwen3Attentionã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© Qwen3MLPã‚’åˆæœŸåŒ–é–‹å§‹ hidden_size=1024 intermediate_size=3072 hidden_act='silu'
ğŸŸ© MergedColumnParallelLinearã‚’åˆæœŸåŒ–é–‹å§‹ input_size=1024, output_sizes=[3072, 3072], bias=False
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã‚’åˆæœŸåŒ–é–‹å§‹ input_size=1024, output_size=6144, bias=False
ğŸŸ© ç·šå½¢å±¤ã®åŸºåº•ã‚¯ãƒ©ã‚¹ã‚’åˆæœŸåŒ–é–‹å§‹ input_size=1024, output_size=6144, bias=False, tp_dim=0
ğŸŸ© ç·šå½¢å±¤ã®åŸºåº•ã‚¯ãƒ©ã‚¹ã‚’åˆæœŸåŒ–å®Œäº†
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã‚’åˆæœŸåŒ–å®Œäº† self.tp_size=1
ğŸŸ© MergedColumnParallelLinearã‚’åˆæœŸåŒ–å®Œäº†
ğŸŸ© RowParallelLinearã‚’åˆæœŸåŒ–é–‹å§‹ input_size=3072, output_size=1024, bias=False
ğŸŸ© ç·šå½¢å±¤ã®åŸºåº•ã‚¯ãƒ©ã‚¹ã‚’åˆæœŸåŒ–é–‹å§‹ input_size=3072, output_size=1024, bias=False, tp_dim=1
ğŸŸ© ç·šå½¢å±¤ã®åŸºåº•ã‚¯ãƒ©ã‚¹ã‚’åˆæœŸåŒ–å®Œäº†
ğŸŸ© RowParallelLinearã‚’åˆæœŸåŒ–å®Œäº† self.tp_size=1
ğŸŸ© SwiGLUã‚’åˆæœŸåŒ–
ğŸŸ© Qwen3MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© RMSNormã‚’åˆæœŸåŒ– hidden_size=1024, eps=1e-06
ğŸŸ© RMSNormã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© RMSNormã‚’åˆæœŸåŒ– hidden_size=1024, eps=1e-06
ğŸŸ© RMSNormã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© Qwen3DecoderLayerã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© Qwen3DecoderLayerã‚’åˆæœŸåŒ–é–‹å§‹ config.hidden_size=1024 config.num_attention_heads=16 config.num_key_value_heads=8 config.intermediate_size=3072 config.rms_norm_eps=1e-06 config.hidden_act='silu'
ğŸŸ© Qwen3Attentionã‚’åˆæœŸåŒ–é–‹å§‹ hidden_size: 1024, num_heads: 16, num_kv_heads: 8, max_position: 40960, head_dim: 128, rms_norm_eps: 1e-06, qkv_bias: False, rope_theta: 1000000, rope_scaling: None
ğŸŸ© QKVParallelLinearã‚’åˆæœŸåŒ–é–‹å§‹ hidden_size=1024, head_size=128, total_num_heads=16, total_num_kv_heads=8, bias=False
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã‚’åˆæœŸåŒ–é–‹å§‹ input_size=1024, output_size=4096, bias=False
ğŸŸ© ç·šå½¢å±¤ã®åŸºåº•ã‚¯ãƒ©ã‚¹ã‚’åˆæœŸåŒ–é–‹å§‹ input_size=1024, output_size=4096, bias=False, tp_dim=0
ğŸŸ© ç·šå½¢å±¤ã®åŸºåº•ã‚¯ãƒ©ã‚¹ã‚’åˆæœŸåŒ–å®Œäº†
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã‚’åˆæœŸåŒ–å®Œäº† self.tp_size=1
ğŸŸ© QKVParallelLinearã‚’åˆæœŸåŒ–å®Œäº† self.tp_size=1, self.num_heads=16, self.num_kv_heads=8
ğŸŸ© RowParallelLinearã‚’åˆæœŸåŒ–é–‹å§‹ input_size=2048, output_size=1024, bias=False
ğŸŸ© ç·šå½¢å±¤ã®åŸºåº•ã‚¯ãƒ©ã‚¹ã‚’åˆæœŸåŒ–é–‹å§‹ input_size=2048, output_size=1024, bias=False, tp_dim=1
ğŸŸ© ç·šå½¢å±¤ã®åŸºåº•ã‚¯ãƒ©ã‚¹ã‚’åˆæœŸåŒ–å®Œäº†
ğŸŸ© RowParallelLinearã‚’åˆæœŸåŒ–å®Œäº† self.tp_size=1
ğŸŸ© Attentionå±¤ã‚’åˆæœŸåŒ– num_heads=16, head_dim=128, scale=0.08838834764831845, num_kv_heads=8
ğŸŸ© Attentionå±¤ã‚’åˆæœŸåŒ–å®Œäº†
ğŸŸ© RMSNormã‚’åˆæœŸåŒ– hidden_size=128, eps=1e-06
ğŸŸ© RMSNormã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© RMSNormã‚’åˆæœŸåŒ– hidden_size=128, eps=1e-06
ğŸŸ© RMSNormã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© Qwen3Attentionã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© Qwen3MLPã‚’åˆæœŸåŒ–é–‹å§‹ hidden_size=1024 intermediate_size=3072 hidden_act='silu'
ğŸŸ© MergedColumnParallelLinearã‚’åˆæœŸåŒ–é–‹å§‹ input_size=1024, output_sizes=[3072, 3072], bias=False
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã‚’åˆæœŸåŒ–é–‹å§‹ input_size=1024, output_size=6144, bias=False
ğŸŸ© ç·šå½¢å±¤ã®åŸºåº•ã‚¯ãƒ©ã‚¹ã‚’åˆæœŸåŒ–é–‹å§‹ input_size=1024, output_size=6144, bias=False, tp_dim=0
ğŸŸ© ç·šå½¢å±¤ã®åŸºåº•ã‚¯ãƒ©ã‚¹ã‚’åˆæœŸåŒ–å®Œäº†
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã‚’åˆæœŸåŒ–å®Œäº† self.tp_size=1
ğŸŸ© MergedColumnParallelLinearã‚’åˆæœŸåŒ–å®Œäº†
ğŸŸ© RowParallelLinearã‚’åˆæœŸåŒ–é–‹å§‹ input_size=3072, output_size=1024, bias=False
ğŸŸ© ç·šå½¢å±¤ã®åŸºåº•ã‚¯ãƒ©ã‚¹ã‚’åˆæœŸåŒ–é–‹å§‹ input_size=3072, output_size=1024, bias=False, tp_dim=1
ğŸŸ© ç·šå½¢å±¤ã®åŸºåº•ã‚¯ãƒ©ã‚¹ã‚’åˆæœŸåŒ–å®Œäº†
ğŸŸ© RowParallelLinearã‚’åˆæœŸåŒ–å®Œäº† self.tp_size=1
ğŸŸ© SwiGLUã‚’åˆæœŸåŒ–
ğŸŸ© Qwen3MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© RMSNormã‚’åˆæœŸåŒ– hidden_size=1024, eps=1e-06
ğŸŸ© RMSNormã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© RMSNormã‚’åˆæœŸåŒ– hidden_size=1024, eps=1e-06
ğŸŸ© RMSNormã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© Qwen3DecoderLayerã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© Qwen3DecoderLayerã‚’åˆæœŸåŒ–é–‹å§‹ config.hidden_size=1024 config.num_attention_heads=16 config.num_key_value_heads=8 config.intermediate_size=3072 config.rms_norm_eps=1e-06 config.hidden_act='silu'
ğŸŸ© Qwen3Attentionã‚’åˆæœŸåŒ–é–‹å§‹ hidden_size: 1024, num_heads: 16, num_kv_heads: 8, max_position: 40960, head_dim: 128, rms_norm_eps: 1e-06, qkv_bias: False, rope_theta: 1000000, rope_scaling: None
ğŸŸ© QKVParallelLinearã‚’åˆæœŸåŒ–é–‹å§‹ hidden_size=1024, head_size=128, total_num_heads=16, total_num_kv_heads=8, bias=False
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã‚’åˆæœŸåŒ–é–‹å§‹ input_size=1024, output_size=4096, bias=False
ğŸŸ© ç·šå½¢å±¤ã®åŸºåº•ã‚¯ãƒ©ã‚¹ã‚’åˆæœŸåŒ–é–‹å§‹ input_size=1024, output_size=4096, bias=False, tp_dim=0
ğŸŸ© ç·šå½¢å±¤ã®åŸºåº•ã‚¯ãƒ©ã‚¹ã‚’åˆæœŸåŒ–å®Œäº†
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã‚’åˆæœŸåŒ–å®Œäº† self.tp_size=1
ğŸŸ© QKVParallelLinearã‚’åˆæœŸåŒ–å®Œäº† self.tp_size=1, self.num_heads=16, self.num_kv_heads=8
ğŸŸ© RowParallelLinearã‚’åˆæœŸåŒ–é–‹å§‹ input_size=2048, output_size=1024, bias=False
ğŸŸ© ç·šå½¢å±¤ã®åŸºåº•ã‚¯ãƒ©ã‚¹ã‚’åˆæœŸåŒ–é–‹å§‹ input_size=2048, output_size=1024, bias=False, tp_dim=1
ğŸŸ© ç·šå½¢å±¤ã®åŸºåº•ã‚¯ãƒ©ã‚¹ã‚’åˆæœŸåŒ–å®Œäº†
ğŸŸ© RowParallelLinearã‚’åˆæœŸåŒ–å®Œäº† self.tp_size=1
ğŸŸ© Attentionå±¤ã‚’åˆæœŸåŒ– num_heads=16, head_dim=128, scale=0.08838834764831845, num_kv_heads=8
ğŸŸ© Attentionå±¤ã‚’åˆæœŸåŒ–å®Œäº†
ğŸŸ© RMSNormã‚’åˆæœŸåŒ– hidden_size=128, eps=1e-06
ğŸŸ© RMSNormã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© RMSNormã‚’åˆæœŸåŒ– hidden_size=128, eps=1e-06
ğŸŸ© RMSNormã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© Qwen3Attentionã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© Qwen3MLPã‚’åˆæœŸåŒ–é–‹å§‹ hidden_size=1024 intermediate_size=3072 hidden_act='silu'
ğŸŸ© MergedColumnParallelLinearã‚’åˆæœŸåŒ–é–‹å§‹ input_size=1024, output_sizes=[3072, 3072], bias=False
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã‚’åˆæœŸåŒ–é–‹å§‹ input_size=1024, output_size=6144, bias=False
ğŸŸ© ç·šå½¢å±¤ã®åŸºåº•ã‚¯ãƒ©ã‚¹ã‚’åˆæœŸåŒ–é–‹å§‹ input_size=1024, output_size=6144, bias=False, tp_dim=0
ğŸŸ© ç·šå½¢å±¤ã®åŸºåº•ã‚¯ãƒ©ã‚¹ã‚’åˆæœŸåŒ–å®Œäº†
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã‚’åˆæœŸåŒ–å®Œäº† self.tp_size=1
ğŸŸ© MergedColumnParallelLinearã‚’åˆæœŸåŒ–å®Œäº†
ğŸŸ© RowParallelLinearã‚’åˆæœŸåŒ–é–‹å§‹ input_size=3072, output_size=1024, bias=False
ğŸŸ© ç·šå½¢å±¤ã®åŸºåº•ã‚¯ãƒ©ã‚¹ã‚’åˆæœŸåŒ–é–‹å§‹ input_size=3072, output_size=1024, bias=False, tp_dim=1
ğŸŸ© ç·šå½¢å±¤ã®åŸºåº•ã‚¯ãƒ©ã‚¹ã‚’åˆæœŸåŒ–å®Œäº†
ğŸŸ© RowParallelLinearã‚’åˆæœŸåŒ–å®Œäº† self.tp_size=1
ğŸŸ© SwiGLUã‚’åˆæœŸåŒ–
ğŸŸ© Qwen3MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© RMSNormã‚’åˆæœŸåŒ– hidden_size=1024, eps=1e-06
ğŸŸ© RMSNormã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© RMSNormã‚’åˆæœŸåŒ– hidden_size=1024, eps=1e-06
ğŸŸ© RMSNormã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© Qwen3DecoderLayerã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© Qwen3DecoderLayerã‚’åˆæœŸåŒ–é–‹å§‹ config.hidden_size=1024 config.num_attention_heads=16 config.num_key_value_heads=8 config.intermediate_size=3072 config.rms_norm_eps=1e-06 config.hidden_act='silu'
ğŸŸ© Qwen3Attentionã‚’åˆæœŸåŒ–é–‹å§‹ hidden_size: 1024, num_heads: 16, num_kv_heads: 8, max_position: 40960, head_dim: 128, rms_norm_eps: 1e-06, qkv_bias: False, rope_theta: 1000000, rope_scaling: None
ğŸŸ© QKVParallelLinearã‚’åˆæœŸåŒ–é–‹å§‹ hidden_size=1024, head_size=128, total_num_heads=16, total_num_kv_heads=8, bias=False
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã‚’åˆæœŸåŒ–é–‹å§‹ input_size=1024, output_size=4096, bias=False
ğŸŸ© ç·šå½¢å±¤ã®åŸºåº•ã‚¯ãƒ©ã‚¹ã‚’åˆæœŸåŒ–é–‹å§‹ input_size=1024, output_size=4096, bias=False, tp_dim=0
ğŸŸ© ç·šå½¢å±¤ã®åŸºåº•ã‚¯ãƒ©ã‚¹ã‚’åˆæœŸåŒ–å®Œäº†
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã‚’åˆæœŸåŒ–å®Œäº† self.tp_size=1
ğŸŸ© QKVParallelLinearã‚’åˆæœŸåŒ–å®Œäº† self.tp_size=1, self.num_heads=16, self.num_kv_heads=8
ğŸŸ© RowParallelLinearã‚’åˆæœŸåŒ–é–‹å§‹ input_size=2048, output_size=1024, bias=False
ğŸŸ© ç·šå½¢å±¤ã®åŸºåº•ã‚¯ãƒ©ã‚¹ã‚’åˆæœŸåŒ–é–‹å§‹ input_size=2048, output_size=1024, bias=False, tp_dim=1
ğŸŸ© ç·šå½¢å±¤ã®åŸºåº•ã‚¯ãƒ©ã‚¹ã‚’åˆæœŸåŒ–å®Œäº†
ğŸŸ© RowParallelLinearã‚’åˆæœŸåŒ–å®Œäº† self.tp_size=1
ğŸŸ© Attentionå±¤ã‚’åˆæœŸåŒ– num_heads=16, head_dim=128, scale=0.08838834764831845, num_kv_heads=8
ğŸŸ© Attentionå±¤ã‚’åˆæœŸåŒ–å®Œäº†
ğŸŸ© RMSNormã‚’åˆæœŸåŒ– hidden_size=128, eps=1e-06
ğŸŸ© RMSNormã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© RMSNormã‚’åˆæœŸåŒ– hidden_size=128, eps=1e-06
ğŸŸ© RMSNormã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© Qwen3Attentionã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© Qwen3MLPã‚’åˆæœŸåŒ–é–‹å§‹ hidden_size=1024 intermediate_size=3072 hidden_act='silu'
ğŸŸ© MergedColumnParallelLinearã‚’åˆæœŸåŒ–é–‹å§‹ input_size=1024, output_sizes=[3072, 3072], bias=False
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã‚’åˆæœŸåŒ–é–‹å§‹ input_size=1024, output_size=6144, bias=False
ğŸŸ© ç·šå½¢å±¤ã®åŸºåº•ã‚¯ãƒ©ã‚¹ã‚’åˆæœŸåŒ–é–‹å§‹ input_size=1024, output_size=6144, bias=False, tp_dim=0
ğŸŸ© ç·šå½¢å±¤ã®åŸºåº•ã‚¯ãƒ©ã‚¹ã‚’åˆæœŸåŒ–å®Œäº†
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã‚’åˆæœŸåŒ–å®Œäº† self.tp_size=1
ğŸŸ© MergedColumnParallelLinearã‚’åˆæœŸåŒ–å®Œäº†
ğŸŸ© RowParallelLinearã‚’åˆæœŸåŒ–é–‹å§‹ input_size=3072, output_size=1024, bias=False
ğŸŸ© ç·šå½¢å±¤ã®åŸºåº•ã‚¯ãƒ©ã‚¹ã‚’åˆæœŸåŒ–é–‹å§‹ input_size=3072, output_size=1024, bias=False, tp_dim=1
ğŸŸ© ç·šå½¢å±¤ã®åŸºåº•ã‚¯ãƒ©ã‚¹ã‚’åˆæœŸåŒ–å®Œäº†
ğŸŸ© RowParallelLinearã‚’åˆæœŸåŒ–å®Œäº† self.tp_size=1
ğŸŸ© SwiGLUã‚’åˆæœŸåŒ–
ğŸŸ© Qwen3MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© RMSNormã‚’åˆæœŸåŒ– hidden_size=1024, eps=1e-06
ğŸŸ© RMSNormã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© RMSNormã‚’åˆæœŸåŒ– hidden_size=1024, eps=1e-06
ğŸŸ© RMSNormã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© Qwen3DecoderLayerã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© Qwen3DecoderLayerã‚’åˆæœŸåŒ–é–‹å§‹ config.hidden_size=1024 config.num_attention_heads=16 config.num_key_value_heads=8 config.intermediate_size=3072 config.rms_norm_eps=1e-06 config.hidden_act='silu'
ğŸŸ© Qwen3Attentionã‚’åˆæœŸåŒ–é–‹å§‹ hidden_size: 1024, num_heads: 16, num_kv_heads: 8, max_position: 40960, head_dim: 128, rms_norm_eps: 1e-06, qkv_bias: False, rope_theta: 1000000, rope_scaling: None
ğŸŸ© QKVParallelLinearã‚’åˆæœŸåŒ–é–‹å§‹ hidden_size=1024, head_size=128, total_num_heads=16, total_num_kv_heads=8, bias=False
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã‚’åˆæœŸåŒ–é–‹å§‹ input_size=1024, output_size=4096, bias=False
ğŸŸ© ç·šå½¢å±¤ã®åŸºåº•ã‚¯ãƒ©ã‚¹ã‚’åˆæœŸåŒ–é–‹å§‹ input_size=1024, output_size=4096, bias=False, tp_dim=0
ğŸŸ© ç·šå½¢å±¤ã®åŸºåº•ã‚¯ãƒ©ã‚¹ã‚’åˆæœŸåŒ–å®Œäº†
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã‚’åˆæœŸåŒ–å®Œäº† self.tp_size=1
ğŸŸ© QKVParallelLinearã‚’åˆæœŸåŒ–å®Œäº† self.tp_size=1, self.num_heads=16, self.num_kv_heads=8
ğŸŸ© RowParallelLinearã‚’åˆæœŸåŒ–é–‹å§‹ input_size=2048, output_size=1024, bias=False
ğŸŸ© ç·šå½¢å±¤ã®åŸºåº•ã‚¯ãƒ©ã‚¹ã‚’åˆæœŸåŒ–é–‹å§‹ input_size=2048, output_size=1024, bias=False, tp_dim=1
ğŸŸ© ç·šå½¢å±¤ã®åŸºåº•ã‚¯ãƒ©ã‚¹ã‚’åˆæœŸåŒ–å®Œäº†
ğŸŸ© RowParallelLinearã‚’åˆæœŸåŒ–å®Œäº† self.tp_size=1
ğŸŸ© Attentionå±¤ã‚’åˆæœŸåŒ– num_heads=16, head_dim=128, scale=0.08838834764831845, num_kv_heads=8
ğŸŸ© Attentionå±¤ã‚’åˆæœŸåŒ–å®Œäº†
ğŸŸ© RMSNormã‚’åˆæœŸåŒ– hidden_size=128, eps=1e-06
ğŸŸ© RMSNormã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© RMSNormã‚’åˆæœŸåŒ– hidden_size=128, eps=1e-06
ğŸŸ© RMSNormã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© Qwen3Attentionã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© Qwen3MLPã‚’åˆæœŸåŒ–é–‹å§‹ hidden_size=1024 intermediate_size=3072 hidden_act='silu'
ğŸŸ© MergedColumnParallelLinearã‚’åˆæœŸåŒ–é–‹å§‹ input_size=1024, output_sizes=[3072, 3072], bias=False
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã‚’åˆæœŸåŒ–é–‹å§‹ input_size=1024, output_size=6144, bias=False
ğŸŸ© ç·šå½¢å±¤ã®åŸºåº•ã‚¯ãƒ©ã‚¹ã‚’åˆæœŸåŒ–é–‹å§‹ input_size=1024, output_size=6144, bias=False, tp_dim=0
ğŸŸ© ç·šå½¢å±¤ã®åŸºåº•ã‚¯ãƒ©ã‚¹ã‚’åˆæœŸåŒ–å®Œäº†
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã‚’åˆæœŸåŒ–å®Œäº† self.tp_size=1
ğŸŸ© MergedColumnParallelLinearã‚’åˆæœŸåŒ–å®Œäº†
ğŸŸ© RowParallelLinearã‚’åˆæœŸåŒ–é–‹å§‹ input_size=3072, output_size=1024, bias=False
ğŸŸ© ç·šå½¢å±¤ã®åŸºåº•ã‚¯ãƒ©ã‚¹ã‚’åˆæœŸåŒ–é–‹å§‹ input_size=3072, output_size=1024, bias=False, tp_dim=1
ğŸŸ© ç·šå½¢å±¤ã®åŸºåº•ã‚¯ãƒ©ã‚¹ã‚’åˆæœŸåŒ–å®Œäº†
ğŸŸ© RowParallelLinearã‚’åˆæœŸåŒ–å®Œäº† self.tp_size=1
ğŸŸ© SwiGLUã‚’åˆæœŸåŒ–
ğŸŸ© Qwen3MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© RMSNormã‚’åˆæœŸåŒ– hidden_size=1024, eps=1e-06
ğŸŸ© RMSNormã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© RMSNormã‚’åˆæœŸåŒ– hidden_size=1024, eps=1e-06
ğŸŸ© RMSNormã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© Qwen3DecoderLayerã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© Qwen3DecoderLayerã‚’åˆæœŸåŒ–é–‹å§‹ config.hidden_size=1024 config.num_attention_heads=16 config.num_key_value_heads=8 config.intermediate_size=3072 config.rms_norm_eps=1e-06 config.hidden_act='silu'
ğŸŸ© Qwen3Attentionã‚’åˆæœŸåŒ–é–‹å§‹ hidden_size: 1024, num_heads: 16, num_kv_heads: 8, max_position: 40960, head_dim: 128, rms_norm_eps: 1e-06, qkv_bias: False, rope_theta: 1000000, rope_scaling: None
ğŸŸ© QKVParallelLinearã‚’åˆæœŸåŒ–é–‹å§‹ hidden_size=1024, head_size=128, total_num_heads=16, total_num_kv_heads=8, bias=False
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã‚’åˆæœŸåŒ–é–‹å§‹ input_size=1024, output_size=4096, bias=False
ğŸŸ© ç·šå½¢å±¤ã®åŸºåº•ã‚¯ãƒ©ã‚¹ã‚’åˆæœŸåŒ–é–‹å§‹ input_size=1024, output_size=4096, bias=False, tp_dim=0
ğŸŸ© ç·šå½¢å±¤ã®åŸºåº•ã‚¯ãƒ©ã‚¹ã‚’åˆæœŸåŒ–å®Œäº†
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã‚’åˆæœŸåŒ–å®Œäº† self.tp_size=1
ğŸŸ© QKVParallelLinearã‚’åˆæœŸåŒ–å®Œäº† self.tp_size=1, self.num_heads=16, self.num_kv_heads=8
ğŸŸ© RowParallelLinearã‚’åˆæœŸåŒ–é–‹å§‹ input_size=2048, output_size=1024, bias=False
ğŸŸ© ç·šå½¢å±¤ã®åŸºåº•ã‚¯ãƒ©ã‚¹ã‚’åˆæœŸåŒ–é–‹å§‹ input_size=2048, output_size=1024, bias=False, tp_dim=1
ğŸŸ© ç·šå½¢å±¤ã®åŸºåº•ã‚¯ãƒ©ã‚¹ã‚’åˆæœŸåŒ–å®Œäº†
ğŸŸ© RowParallelLinearã‚’åˆæœŸåŒ–å®Œäº† self.tp_size=1
ğŸŸ© Attentionå±¤ã‚’åˆæœŸåŒ– num_heads=16, head_dim=128, scale=0.08838834764831845, num_kv_heads=8
ğŸŸ© Attentionå±¤ã‚’åˆæœŸåŒ–å®Œäº†
ğŸŸ© RMSNormã‚’åˆæœŸåŒ– hidden_size=128, eps=1e-06
ğŸŸ© RMSNormã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© RMSNormã‚’åˆæœŸåŒ– hidden_size=128, eps=1e-06
ğŸŸ© RMSNormã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© Qwen3Attentionã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© Qwen3MLPã‚’åˆæœŸåŒ–é–‹å§‹ hidden_size=1024 intermediate_size=3072 hidden_act='silu'
ğŸŸ© MergedColumnParallelLinearã‚’åˆæœŸåŒ–é–‹å§‹ input_size=1024, output_sizes=[3072, 3072], bias=False
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã‚’åˆæœŸåŒ–é–‹å§‹ input_size=1024, output_size=6144, bias=False
ğŸŸ© ç·šå½¢å±¤ã®åŸºåº•ã‚¯ãƒ©ã‚¹ã‚’åˆæœŸåŒ–é–‹å§‹ input_size=1024, output_size=6144, bias=False, tp_dim=0
ğŸŸ© ç·šå½¢å±¤ã®åŸºåº•ã‚¯ãƒ©ã‚¹ã‚’åˆæœŸåŒ–å®Œäº†
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã‚’åˆæœŸåŒ–å®Œäº† self.tp_size=1
ğŸŸ© MergedColumnParallelLinearã‚’åˆæœŸåŒ–å®Œäº†
ğŸŸ© RowParallelLinearã‚’åˆæœŸåŒ–é–‹å§‹ input_size=3072, output_size=1024, bias=False
ğŸŸ© ç·šå½¢å±¤ã®åŸºåº•ã‚¯ãƒ©ã‚¹ã‚’åˆæœŸåŒ–é–‹å§‹ input_size=3072, output_size=1024, bias=False, tp_dim=1
ğŸŸ© ç·šå½¢å±¤ã®åŸºåº•ã‚¯ãƒ©ã‚¹ã‚’åˆæœŸåŒ–å®Œäº†
ğŸŸ© RowParallelLinearã‚’åˆæœŸåŒ–å®Œäº† self.tp_size=1
ğŸŸ© SwiGLUã‚’åˆæœŸåŒ–
ğŸŸ© Qwen3MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© RMSNormã‚’åˆæœŸåŒ– hidden_size=1024, eps=1e-06
ğŸŸ© RMSNormã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© RMSNormã‚’åˆæœŸåŒ– hidden_size=1024, eps=1e-06
ğŸŸ© RMSNormã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© Qwen3DecoderLayerã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© Qwen3DecoderLayerã‚’åˆæœŸåŒ–é–‹å§‹ config.hidden_size=1024 config.num_attention_heads=16 config.num_key_value_heads=8 config.intermediate_size=3072 config.rms_norm_eps=1e-06 config.hidden_act='silu'
ğŸŸ© Qwen3Attentionã‚’åˆæœŸåŒ–é–‹å§‹ hidden_size: 1024, num_heads: 16, num_kv_heads: 8, max_position: 40960, head_dim: 128, rms_norm_eps: 1e-06, qkv_bias: False, rope_theta: 1000000, rope_scaling: None
ğŸŸ© QKVParallelLinearã‚’åˆæœŸåŒ–é–‹å§‹ hidden_size=1024, head_size=128, total_num_heads=16, total_num_kv_heads=8, bias=False
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã‚’åˆæœŸåŒ–é–‹å§‹ input_size=1024, output_size=4096, bias=False
ğŸŸ© ç·šå½¢å±¤ã®åŸºåº•ã‚¯ãƒ©ã‚¹ã‚’åˆæœŸåŒ–é–‹å§‹ input_size=1024, output_size=4096, bias=False, tp_dim=0
ğŸŸ© ç·šå½¢å±¤ã®åŸºåº•ã‚¯ãƒ©ã‚¹ã‚’åˆæœŸåŒ–å®Œäº†
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã‚’åˆæœŸåŒ–å®Œäº† self.tp_size=1
ğŸŸ© QKVParallelLinearã‚’åˆæœŸåŒ–å®Œäº† self.tp_size=1, self.num_heads=16, self.num_kv_heads=8
ğŸŸ© RowParallelLinearã‚’åˆæœŸåŒ–é–‹å§‹ input_size=2048, output_size=1024, bias=False
ğŸŸ© ç·šå½¢å±¤ã®åŸºåº•ã‚¯ãƒ©ã‚¹ã‚’åˆæœŸåŒ–é–‹å§‹ input_size=2048, output_size=1024, bias=False, tp_dim=1
ğŸŸ© ç·šå½¢å±¤ã®åŸºåº•ã‚¯ãƒ©ã‚¹ã‚’åˆæœŸåŒ–å®Œäº†
ğŸŸ© RowParallelLinearã‚’åˆæœŸåŒ–å®Œäº† self.tp_size=1
ğŸŸ© Attentionå±¤ã‚’åˆæœŸåŒ– num_heads=16, head_dim=128, scale=0.08838834764831845, num_kv_heads=8
ğŸŸ© Attentionå±¤ã‚’åˆæœŸåŒ–å®Œäº†
ğŸŸ© RMSNormã‚’åˆæœŸåŒ– hidden_size=128, eps=1e-06
ğŸŸ© RMSNormã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© RMSNormã‚’åˆæœŸåŒ– hidden_size=128, eps=1e-06
ğŸŸ© RMSNormã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© Qwen3Attentionã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© Qwen3MLPã‚’åˆæœŸåŒ–é–‹å§‹ hidden_size=1024 intermediate_size=3072 hidden_act='silu'
ğŸŸ© MergedColumnParallelLinearã‚’åˆæœŸåŒ–é–‹å§‹ input_size=1024, output_sizes=[3072, 3072], bias=False
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã‚’åˆæœŸåŒ–é–‹å§‹ input_size=1024, output_size=6144, bias=False
ğŸŸ© ç·šå½¢å±¤ã®åŸºåº•ã‚¯ãƒ©ã‚¹ã‚’åˆæœŸåŒ–é–‹å§‹ input_size=1024, output_size=6144, bias=False, tp_dim=0
ğŸŸ© ç·šå½¢å±¤ã®åŸºåº•ã‚¯ãƒ©ã‚¹ã‚’åˆæœŸåŒ–å®Œäº†
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã‚’åˆæœŸåŒ–å®Œäº† self.tp_size=1
ğŸŸ© MergedColumnParallelLinearã‚’åˆæœŸåŒ–å®Œäº†
ğŸŸ© RowParallelLinearã‚’åˆæœŸåŒ–é–‹å§‹ input_size=3072, output_size=1024, bias=False
ğŸŸ© ç·šå½¢å±¤ã®åŸºåº•ã‚¯ãƒ©ã‚¹ã‚’åˆæœŸåŒ–é–‹å§‹ input_size=3072, output_size=1024, bias=False, tp_dim=1
ğŸŸ© ç·šå½¢å±¤ã®åŸºåº•ã‚¯ãƒ©ã‚¹ã‚’åˆæœŸåŒ–å®Œäº†
ğŸŸ© RowParallelLinearã‚’åˆæœŸåŒ–å®Œäº† self.tp_size=1
ğŸŸ© SwiGLUã‚’åˆæœŸåŒ–
ğŸŸ© Qwen3MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© RMSNormã‚’åˆæœŸåŒ– hidden_size=1024, eps=1e-06
ğŸŸ© RMSNormã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© RMSNormã‚’åˆæœŸåŒ– hidden_size=1024, eps=1e-06
ğŸŸ© RMSNormã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© Qwen3DecoderLayerã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© RMSNormã‚’åˆæœŸåŒ– hidden_size=1024, eps=1e-06
ğŸŸ© RMSNormã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© Qwen3Modelã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© å‡ºåŠ›ã®è¨€èªãƒ¢ãƒ‡ãƒ«ãƒ˜ãƒƒãƒ‰ã‚’åˆæœŸåŒ–é–‹å§‹ num_embeddings=151936, embedding_dim=1024, bias=False
ğŸŸ© å…¥åŠ›ã®åŸ‹ã‚è¾¼ã¿å±¤ã‚’åˆæœŸåŒ–é–‹å§‹ num_embeddings=151936, embedding_dim=1024
ğŸŸ¦ åˆ†æ•£ç’°å¢ƒæƒ…å ±å–å¾— self.tp_rank=0, self.tp_size=1
ğŸŸ¦ æ‹…å½“ã™ã‚‹èªå½™ã®ç¯„å›²è¨ˆç®— self.vocab_start_idx=0, self.vocab_end_idx=151936
ğŸŸ¦ æ‹…å½“ã™ã‚‹èªå½™ã®é‡ã¿è¡Œåˆ—åˆæœŸåŒ– self.weight.shape=torch.Size([151936, 1024])
ğŸŸ© å…¥åŠ›ã®åŸ‹ã‚è¾¼ã¿å±¤ã‚’åˆæœŸåŒ–å®Œäº†
ğŸŸ© å‡ºåŠ›ã®è¨€èªãƒ¢ãƒ‡ãƒ«ãƒ˜ãƒƒãƒ‰ã‚’åˆæœŸåŒ–å®Œäº†
ğŸŸ© Qwen3ForCausalLMã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© ãƒ¢ãƒ‡ãƒ«ã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿é–‹å§‹ path='/root/huggingface/Qwen3-0.6B'
ğŸŸ¦ é‡ã¿ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‡¦ç†ä¸­ file='/root/huggingface/Qwen3-0.6B/model.safetensors'
ğŸŸ¦ é‡ã¿ã‚’èª­ã¿è¾¼ã¿ä¸­ weight_name='lm_head.weight'
ğŸŸ¦ é€šå¸¸ã®é‡ã¿èª­ã¿è¾¼ã¿ weight_name='lm_head.weight'
ğŸŸ© åŸ‹ã‚è¾¼ã¿å±¤ã®é‡ã¿ã‚’ãƒ­ãƒ¼ãƒ‰é–‹å§‹ param.shape=torch.Size([151936, 1024]), loaded_weight.shape=torch.Size([151936, 1024])
ğŸŸ© åŸ‹ã‚è¾¼ã¿å±¤ã®é‡ã¿ã‚’ãƒ­ãƒ¼ãƒ‰å®Œäº† loaded_weight.shape=torch.Size([151936, 1024])
ğŸŸ¦ é‡ã¿ã‚’èª­ã¿è¾¼ã¿ä¸­ weight_name='model.embed_tokens.weight'
ğŸŸ¦ é€šå¸¸ã®é‡ã¿èª­ã¿è¾¼ã¿ weight_name='model.embed_tokens.weight'
ğŸŸ© åŸ‹ã‚è¾¼ã¿å±¤ã®é‡ã¿ã‚’ãƒ­ãƒ¼ãƒ‰é–‹å§‹ param.shape=torch.Size([151936, 1024]), loaded_weight.shape=torch.Size([151936, 1024])
ğŸŸ© åŸ‹ã‚è¾¼ã¿å±¤ã®é‡ã¿ã‚’ãƒ­ãƒ¼ãƒ‰å®Œäº† loaded_weight.shape=torch.Size([151936, 1024])
ğŸŸ¦ é‡ã¿ã‚’èª­ã¿è¾¼ã¿ä¸­ weight_name='model.layers.0.input_layernorm.weight'
ğŸŸ¦ é€šå¸¸ã®é‡ã¿èª­ã¿è¾¼ã¿ weight_name='model.layers.0.input_layernorm.weight'
ğŸŸ© é‡ã¿ã‚’èª­ã¿è¾¼ã¿ param.shape=torch.Size([1024]) loaded_weight.shape=torch.Size([1024])
ğŸŸ¦ é‡ã¿ã‚’èª­ã¿è¾¼ã¿ä¸­ weight_name='model.layers.0.mlp.down_proj.weight'
ğŸŸ¦ é€šå¸¸ã®é‡ã¿èª­ã¿è¾¼ã¿ weight_name='model.layers.0.mlp.down_proj.weight'
ğŸŸ© RowParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿é–‹å§‹ param.shape=torch.Size([1024, 3072]), loaded_weight.shape=torch.Size([1024, 3072])
ğŸŸ© RowParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿å®Œäº†
ğŸŸ¦ é‡ã¿ã‚’èª­ã¿è¾¼ã¿ä¸­ weight_name='model.layers.0.mlp.gate_proj.weight'
ğŸŸ¦ ãƒãƒƒãƒ”ãƒ³ã‚°ã«ã‚ˆã‚‹é‡ã¿ã‚’èª­ã¿è¾¼ã¿ weight_name='model.layers.0.mlp.gate_proj.weight' k='gate_proj'
ğŸŸ© MergedColumnParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿é–‹å§‹ param.shape=torch.Size([6144, 1024]), loaded_weight.shape=torch.Size([3072, 1024]), loaded_shard_id=0
ğŸŸ© MergedColumnParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿å®Œäº†
ğŸŸ¦ é‡ã¿ã‚’èª­ã¿è¾¼ã¿ä¸­ weight_name='model.layers.0.mlp.up_proj.weight'
ğŸŸ¦ ãƒãƒƒãƒ”ãƒ³ã‚°ã«ã‚ˆã‚‹é‡ã¿ã‚’èª­ã¿è¾¼ã¿ weight_name='model.layers.0.mlp.up_proj.weight' k='up_proj'
ğŸŸ© MergedColumnParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿é–‹å§‹ param.shape=torch.Size([6144, 1024]), loaded_weight.shape=torch.Size([3072, 1024]), loaded_shard_id=1
ğŸŸ© MergedColumnParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿å®Œäº†
ğŸŸ¦ é‡ã¿ã‚’èª­ã¿è¾¼ã¿ä¸­ weight_name='model.layers.0.post_attention_layernorm.weight'
ğŸŸ¦ é€šå¸¸ã®é‡ã¿èª­ã¿è¾¼ã¿ weight_name='model.layers.0.post_attention_layernorm.weight'
ğŸŸ© é‡ã¿ã‚’èª­ã¿è¾¼ã¿ param.shape=torch.Size([1024]) loaded_weight.shape=torch.Size([1024])
ğŸŸ¦ é‡ã¿ã‚’èª­ã¿è¾¼ã¿ä¸­ weight_name='model.layers.0.self_attn.k_norm.weight'
ğŸŸ¦ é€šå¸¸ã®é‡ã¿èª­ã¿è¾¼ã¿ weight_name='model.layers.0.self_attn.k_norm.weight'
ğŸŸ© é‡ã¿ã‚’èª­ã¿è¾¼ã¿ param.shape=torch.Size([128]) loaded_weight.shape=torch.Size([128])
ğŸŸ¦ é‡ã¿ã‚’èª­ã¿è¾¼ã¿ä¸­ weight_name='model.layers.0.self_attn.k_proj.weight'
ğŸŸ¦ ãƒãƒƒãƒ”ãƒ³ã‚°ã«ã‚ˆã‚‹é‡ã¿ã‚’èª­ã¿è¾¼ã¿ weight_name='model.layers.0.self_attn.k_proj.weight' k='k_proj'
ğŸŸ© QKVParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿é–‹å§‹ param.shape=torch.Size([4096, 1024]), loaded_weight.shape=torch.Size([1024, 1024]), loaded_shard_id='k'
ğŸŸ© QKVParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿å®Œäº†
ğŸŸ¦ é‡ã¿ã‚’èª­ã¿è¾¼ã¿ä¸­ weight_name='model.layers.0.self_attn.o_proj.weight'
ğŸŸ¦ é€šå¸¸ã®é‡ã¿èª­ã¿è¾¼ã¿ weight_name='model.layers.0.self_attn.o_proj.weight'
ğŸŸ© RowParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿é–‹å§‹ param.shape=torch.Size([1024, 2048]), loaded_weight.shape=torch.Size([1024, 2048])
ğŸŸ© RowParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿å®Œäº†
ğŸŸ¦ é‡ã¿ã‚’èª­ã¿è¾¼ã¿ä¸­ weight_name='model.layers.0.self_attn.q_norm.weight'
ğŸŸ¦ é€šå¸¸ã®é‡ã¿èª­ã¿è¾¼ã¿ weight_name='model.layers.0.self_attn.q_norm.weight'
ğŸŸ© é‡ã¿ã‚’èª­ã¿è¾¼ã¿ param.shape=torch.Size([128]) loaded_weight.shape=torch.Size([128])
ğŸŸ¦ é‡ã¿ã‚’èª­ã¿è¾¼ã¿ä¸­ weight_name='model.layers.0.self_attn.q_proj.weight'
ğŸŸ¦ ãƒãƒƒãƒ”ãƒ³ã‚°ã«ã‚ˆã‚‹é‡ã¿ã‚’èª­ã¿è¾¼ã¿ weight_name='model.layers.0.self_attn.q_proj.weight' k='q_proj'
ğŸŸ© QKVParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿é–‹å§‹ param.shape=torch.Size([4096, 1024]), loaded_weight.shape=torch.Size([2048, 1024]), loaded_shard_id='q'
ğŸŸ© QKVParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿å®Œäº†
ğŸŸ¦ é‡ã¿ã‚’èª­ã¿è¾¼ã¿ä¸­ weight_name='model.layers.0.self_attn.v_proj.weight'
ğŸŸ¦ ãƒãƒƒãƒ”ãƒ³ã‚°ã«ã‚ˆã‚‹é‡ã¿ã‚’èª­ã¿è¾¼ã¿ weight_name='model.layers.0.self_attn.v_proj.weight' k='v_proj'
ğŸŸ© QKVParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿é–‹å§‹ param.shape=torch.Size([4096, 1024]), loaded_weight.shape=torch.Size([1024, 1024]), loaded_shard_id='v'
ğŸŸ© QKVParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿å®Œäº†
ğŸŸ¦ é‡ã¿ã‚’èª­ã¿è¾¼ã¿ä¸­ weight_name='model.layers.1.input_layernorm.weight'
ğŸŸ¦ é€šå¸¸ã®é‡ã¿èª­ã¿è¾¼ã¿ weight_name='model.layers.1.input_layernorm.weight'
ğŸŸ© é‡ã¿ã‚’èª­ã¿è¾¼ã¿ param.shape=torch.Size([1024]) loaded_weight.shape=torch.Size([1024])
ğŸŸ¦ é‡ã¿ã‚’èª­ã¿è¾¼ã¿ä¸­ weight_name='model.layers.1.mlp.down_proj.weight'
ğŸŸ¦ é€šå¸¸ã®é‡ã¿èª­ã¿è¾¼ã¿ weight_name='model.layers.1.mlp.down_proj.weight'
ğŸŸ© RowParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿é–‹å§‹ param.shape=torch.Size([1024, 3072]), loaded_weight.shape=torch.Size([1024, 3072])
ğŸŸ© RowParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿å®Œäº†
ğŸŸ¦ é‡ã¿ã‚’èª­ã¿è¾¼ã¿ä¸­ weight_name='model.layers.1.mlp.gate_proj.weight'
ğŸŸ¦ ãƒãƒƒãƒ”ãƒ³ã‚°ã«ã‚ˆã‚‹é‡ã¿ã‚’èª­ã¿è¾¼ã¿ weight_name='model.layers.1.mlp.gate_proj.weight' k='gate_proj'
ğŸŸ© MergedColumnParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿é–‹å§‹ param.shape=torch.Size([6144, 1024]), loaded_weight.shape=torch.Size([3072, 1024]), loaded_shard_id=0
ğŸŸ© MergedColumnParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿å®Œäº†
ğŸŸ¦ é‡ã¿ã‚’èª­ã¿è¾¼ã¿ä¸­ weight_name='model.layers.1.mlp.up_proj.weight'
ğŸŸ¦ ãƒãƒƒãƒ”ãƒ³ã‚°ã«ã‚ˆã‚‹é‡ã¿ã‚’èª­ã¿è¾¼ã¿ weight_name='model.layers.1.mlp.up_proj.weight' k='up_proj'
ğŸŸ© MergedColumnParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿é–‹å§‹ param.shape=torch.Size([6144, 1024]), loaded_weight.shape=torch.Size([3072, 1024]), loaded_shard_id=1
ğŸŸ© MergedColumnParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿å®Œäº†
ğŸŸ¦ é‡ã¿ã‚’èª­ã¿è¾¼ã¿ä¸­ weight_name='model.layers.1.post_attention_layernorm.weight'
ğŸŸ¦ é€šå¸¸ã®é‡ã¿èª­ã¿è¾¼ã¿ weight_name='model.layers.1.post_attention_layernorm.weight'
ğŸŸ© é‡ã¿ã‚’èª­ã¿è¾¼ã¿ param.shape=torch.Size([1024]) loaded_weight.shape=torch.Size([1024])
ğŸŸ¦ é‡ã¿ã‚’èª­ã¿è¾¼ã¿ä¸­ weight_name='model.layers.1.self_attn.k_norm.weight'
ğŸŸ¦ é€šå¸¸ã®é‡ã¿èª­ã¿è¾¼ã¿ weight_name='model.layers.1.self_attn.k_norm.weight'
ğŸŸ© é‡ã¿ã‚’èª­ã¿è¾¼ã¿ param.shape=torch.Size([128]) loaded_weight.shape=torch.Size([128])
ğŸŸ¦ é‡ã¿ã‚’èª­ã¿è¾¼ã¿ä¸­ weight_name='model.layers.1.self_attn.k_proj.weight'
ğŸŸ¦ ãƒãƒƒãƒ”ãƒ³ã‚°ã«ã‚ˆã‚‹é‡ã¿ã‚’èª­ã¿è¾¼ã¿ weight_name='model.layers.1.self_attn.k_proj.weight' k='k_proj'
ğŸŸ© QKVParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿é–‹å§‹ param.shape=torch.Size([4096, 1024]), loaded_weight.shape=torch.Size([1024, 1024]), loaded_shard_id='k'
ğŸŸ© QKVParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿å®Œäº†
ğŸŸ¦ é‡ã¿ã‚’èª­ã¿è¾¼ã¿ä¸­ weight_name='model.layers.1.self_attn.o_proj.weight'
ğŸŸ¦ é€šå¸¸ã®é‡ã¿èª­ã¿è¾¼ã¿ weight_name='model.layers.1.self_attn.o_proj.weight'
ğŸŸ© RowParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿é–‹å§‹ param.shape=torch.Size([1024, 2048]), loaded_weight.shape=torch.Size([1024, 2048])
ğŸŸ© RowParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿å®Œäº†
ğŸŸ¦ é‡ã¿ã‚’èª­ã¿è¾¼ã¿ä¸­ weight_name='model.layers.1.self_attn.q_norm.weight'
ğŸŸ¦ é€šå¸¸ã®é‡ã¿èª­ã¿è¾¼ã¿ weight_name='model.layers.1.self_attn.q_norm.weight'
ğŸŸ© é‡ã¿ã‚’èª­ã¿è¾¼ã¿ param.shape=torch.Size([128]) loaded_weight.shape=torch.Size([128])
ğŸŸ¦ é‡ã¿ã‚’èª­ã¿è¾¼ã¿ä¸­ weight_name='model.layers.1.self_attn.q_proj.weight'
ğŸŸ¦ ãƒãƒƒãƒ”ãƒ³ã‚°ã«ã‚ˆã‚‹é‡ã¿ã‚’èª­ã¿è¾¼ã¿ weight_name='model.layers.1.self_attn.q_proj.weight' k='q_proj'
ğŸŸ© QKVParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿é–‹å§‹ param.shape=torch.Size([4096, 1024]), loaded_weight.shape=torch.Size([2048, 1024]), loaded_shard_id='q'
ğŸŸ© QKVParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿å®Œäº†
ğŸŸ¦ é‡ã¿ã‚’èª­ã¿è¾¼ã¿ä¸­ weight_name='model.layers.1.self_attn.v_proj.weight'
ğŸŸ¦ ãƒãƒƒãƒ”ãƒ³ã‚°ã«ã‚ˆã‚‹é‡ã¿ã‚’èª­ã¿è¾¼ã¿ weight_name='model.layers.1.self_attn.v_proj.weight' k='v_proj'
ğŸŸ© QKVParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿é–‹å§‹ param.shape=torch.Size([4096, 1024]), loaded_weight.shape=torch.Size([1024, 1024]), loaded_shard_id='v'
ğŸŸ© QKVParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿å®Œäº†
ğŸŸ¦ é‡ã¿ã‚’èª­ã¿è¾¼ã¿ä¸­ weight_name='model.layers.10.input_layernorm.weight'
ğŸŸ¦ é€šå¸¸ã®é‡ã¿èª­ã¿è¾¼ã¿ weight_name='model.layers.10.input_layernorm.weight'
ğŸŸ© é‡ã¿ã‚’èª­ã¿è¾¼ã¿ param.shape=torch.Size([1024]) loaded_weight.shape=torch.Size([1024])
ğŸŸ¦ é‡ã¿ã‚’èª­ã¿è¾¼ã¿ä¸­ weight_name='model.layers.10.mlp.down_proj.weight'
ğŸŸ¦ é€šå¸¸ã®é‡ã¿èª­ã¿è¾¼ã¿ weight_name='model.layers.10.mlp.down_proj.weight'
ğŸŸ© RowParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿é–‹å§‹ param.shape=torch.Size([1024, 3072]), loaded_weight.shape=torch.Size([1024, 3072])
ğŸŸ© RowParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿å®Œäº†
ğŸŸ¦ é‡ã¿ã‚’èª­ã¿è¾¼ã¿ä¸­ weight_name='model.layers.10.mlp.gate_proj.weight'
ğŸŸ¦ ãƒãƒƒãƒ”ãƒ³ã‚°ã«ã‚ˆã‚‹é‡ã¿ã‚’èª­ã¿è¾¼ã¿ weight_name='model.layers.10.mlp.gate_proj.weight' k='gate_proj'
ğŸŸ© MergedColumnParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿é–‹å§‹ param.shape=torch.Size([6144, 1024]), loaded_weight.shape=torch.Size([3072, 1024]), loaded_shard_id=0
ğŸŸ© MergedColumnParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿å®Œäº†
ğŸŸ¦ é‡ã¿ã‚’èª­ã¿è¾¼ã¿ä¸­ weight_name='model.layers.10.mlp.up_proj.weight'
ğŸŸ¦ ãƒãƒƒãƒ”ãƒ³ã‚°ã«ã‚ˆã‚‹é‡ã¿ã‚’èª­ã¿è¾¼ã¿ weight_name='model.layers.10.mlp.up_proj.weight' k='up_proj'
ğŸŸ© MergedColumnParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿é–‹å§‹ param.shape=torch.Size([6144, 1024]), loaded_weight.shape=torch.Size([3072, 1024]), loaded_shard_id=1
ğŸŸ© MergedColumnParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿å®Œäº†
ğŸŸ¦ é‡ã¿ã‚’èª­ã¿è¾¼ã¿ä¸­ weight_name='model.layers.10.post_attention_layernorm.weight'
ğŸŸ¦ é€šå¸¸ã®é‡ã¿èª­ã¿è¾¼ã¿ weight_name='model.layers.10.post_attention_layernorm.weight'
ğŸŸ© é‡ã¿ã‚’èª­ã¿è¾¼ã¿ param.shape=torch.Size([1024]) loaded_weight.shape=torch.Size([1024])
ğŸŸ¦ é‡ã¿ã‚’èª­ã¿è¾¼ã¿ä¸­ weight_name='model.layers.10.self_attn.k_norm.weight'
ğŸŸ¦ é€šå¸¸ã®é‡ã¿èª­ã¿è¾¼ã¿ weight_name='model.layers.10.self_attn.k_norm.weight'
ğŸŸ© é‡ã¿ã‚’èª­ã¿è¾¼ã¿ param.shape=torch.Size([128]) loaded_weight.shape=torch.Size([128])
ğŸŸ¦ é‡ã¿ã‚’èª­ã¿è¾¼ã¿ä¸­ weight_name='model.layers.10.self_attn.k_proj.weight'
ğŸŸ¦ ãƒãƒƒãƒ”ãƒ³ã‚°ã«ã‚ˆã‚‹é‡ã¿ã‚’èª­ã¿è¾¼ã¿ weight_name='model.layers.10.self_attn.k_proj.weight' k='k_proj'
ğŸŸ© QKVParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿é–‹å§‹ param.shape=torch.Size([4096, 1024]), loaded_weight.shape=torch.Size([1024, 1024]), loaded_shard_id='k'
ğŸŸ© QKVParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿å®Œäº†
ğŸŸ¦ é‡ã¿ã‚’èª­ã¿è¾¼ã¿ä¸­ weight_name='model.layers.10.self_attn.o_proj.weight'
ğŸŸ¦ é€šå¸¸ã®é‡ã¿èª­ã¿è¾¼ã¿ weight_name='model.layers.10.self_attn.o_proj.weight'
ğŸŸ© RowParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿é–‹å§‹ param.shape=torch.Size([1024, 2048]), loaded_weight.shape=torch.Size([1024, 2048])
ğŸŸ© RowParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿å®Œäº†
ğŸŸ¦ é‡ã¿ã‚’èª­ã¿è¾¼ã¿ä¸­ weight_name='model.layers.10.self_attn.q_norm.weight'
ğŸŸ¦ é€šå¸¸ã®é‡ã¿èª­ã¿è¾¼ã¿ weight_name='model.layers.10.self_attn.q_norm.weight'
ğŸŸ© é‡ã¿ã‚’èª­ã¿è¾¼ã¿ param.shape=torch.Size([128]) loaded_weight.shape=torch.Size([128])
ğŸŸ¦ é‡ã¿ã‚’èª­ã¿è¾¼ã¿ä¸­ weight_name='model.layers.10.self_attn.q_proj.weight'
ğŸŸ¦ ãƒãƒƒãƒ”ãƒ³ã‚°ã«ã‚ˆã‚‹é‡ã¿ã‚’èª­ã¿è¾¼ã¿ weight_name='model.layers.10.self_attn.q_proj.weight' k='q_proj'
ğŸŸ© QKVParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿é–‹å§‹ param.shape=torch.Size([4096, 1024]), loaded_weight.shape=torch.Size([2048, 1024]), loaded_shard_id='q'
ğŸŸ© QKVParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿å®Œäº†
ğŸŸ¦ é‡ã¿ã‚’èª­ã¿è¾¼ã¿ä¸­ weight_name='model.layers.10.self_attn.v_proj.weight'
ğŸŸ¦ ãƒãƒƒãƒ”ãƒ³ã‚°ã«ã‚ˆã‚‹é‡ã¿ã‚’èª­ã¿è¾¼ã¿ weight_name='model.layers.10.self_attn.v_proj.weight' k='v_proj'
ğŸŸ© QKVParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿é–‹å§‹ param.shape=torch.Size([4096, 1024]), loaded_weight.shape=torch.Size([1024, 1024]), loaded_shard_id='v'
ğŸŸ© QKVParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿å®Œäº†
ğŸŸ¦ é‡ã¿ã‚’èª­ã¿è¾¼ã¿ä¸­ weight_name='model.layers.11.input_layernorm.weight'
ğŸŸ¦ é€šå¸¸ã®é‡ã¿èª­ã¿è¾¼ã¿ weight_name='model.layers.11.input_layernorm.weight'
ğŸŸ© é‡ã¿ã‚’èª­ã¿è¾¼ã¿ param.shape=torch.Size([1024]) loaded_weight.shape=torch.Size([1024])
ğŸŸ¦ é‡ã¿ã‚’èª­ã¿è¾¼ã¿ä¸­ weight_name='model.layers.11.mlp.down_proj.weight'
ğŸŸ¦ é€šå¸¸ã®é‡ã¿èª­ã¿è¾¼ã¿ weight_name='model.layers.11.mlp.down_proj.weight'
ğŸŸ© RowParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿é–‹å§‹ param.shape=torch.Size([1024, 3072]), loaded_weight.shape=torch.Size([1024, 3072])
ğŸŸ© RowParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿å®Œäº†
ğŸŸ¦ é‡ã¿ã‚’èª­ã¿è¾¼ã¿ä¸­ weight_name='model.layers.11.mlp.gate_proj.weight'
ğŸŸ¦ ãƒãƒƒãƒ”ãƒ³ã‚°ã«ã‚ˆã‚‹é‡ã¿ã‚’èª­ã¿è¾¼ã¿ weight_name='model.layers.11.mlp.gate_proj.weight' k='gate_proj'
ğŸŸ© MergedColumnParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿é–‹å§‹ param.shape=torch.Size([6144, 1024]), loaded_weight.shape=torch.Size([3072, 1024]), loaded_shard_id=0
ğŸŸ© MergedColumnParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿å®Œäº†
ğŸŸ¦ é‡ã¿ã‚’èª­ã¿è¾¼ã¿ä¸­ weight_name='model.layers.11.mlp.up_proj.weight'
ğŸŸ¦ ãƒãƒƒãƒ”ãƒ³ã‚°ã«ã‚ˆã‚‹é‡ã¿ã‚’èª­ã¿è¾¼ã¿ weight_name='model.layers.11.mlp.up_proj.weight' k='up_proj'
ğŸŸ© MergedColumnParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿é–‹å§‹ param.shape=torch.Size([6144, 1024]), loaded_weight.shape=torch.Size([3072, 1024]), loaded_shard_id=1
ğŸŸ© MergedColumnParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿å®Œäº†
ğŸŸ¦ é‡ã¿ã‚’èª­ã¿è¾¼ã¿ä¸­ weight_name='model.layers.11.post_attention_layernorm.weight'
ğŸŸ¦ é€šå¸¸ã®é‡ã¿èª­ã¿è¾¼ã¿ weight_name='model.layers.11.post_attention_layernorm.weight'
ğŸŸ© é‡ã¿ã‚’èª­ã¿è¾¼ã¿ param.shape=torch.Size([1024]) loaded_weight.shape=torch.Size([1024])
ğŸŸ¦ é‡ã¿ã‚’èª­ã¿è¾¼ã¿ä¸­ weight_name='model.layers.11.self_attn.k_norm.weight'
ğŸŸ¦ é€šå¸¸ã®é‡ã¿èª­ã¿è¾¼ã¿ weight_name='model.layers.11.self_attn.k_norm.weight'
ğŸŸ© é‡ã¿ã‚’èª­ã¿è¾¼ã¿ param.shape=torch.Size([128]) loaded_weight.shape=torch.Size([128])
ğŸŸ¦ é‡ã¿ã‚’èª­ã¿è¾¼ã¿ä¸­ weight_name='model.layers.11.self_attn.k_proj.weight'
ğŸŸ¦ ãƒãƒƒãƒ”ãƒ³ã‚°ã«ã‚ˆã‚‹é‡ã¿ã‚’èª­ã¿è¾¼ã¿ weight_name='model.layers.11.self_attn.k_proj.weight' k='k_proj'
ğŸŸ© QKVParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿é–‹å§‹ param.shape=torch.Size([4096, 1024]), loaded_weight.shape=torch.Size([1024, 1024]), loaded_shard_id='k'
ğŸŸ© QKVParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿å®Œäº†
ğŸŸ¦ é‡ã¿ã‚’èª­ã¿è¾¼ã¿ä¸­ weight_name='model.layers.11.self_attn.o_proj.weight'
ğŸŸ¦ é€šå¸¸ã®é‡ã¿èª­ã¿è¾¼ã¿ weight_name='model.layers.11.self_attn.o_proj.weight'
ğŸŸ© RowParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿é–‹å§‹ param.shape=torch.Size([1024, 2048]), loaded_weight.shape=torch.Size([1024, 2048])
ğŸŸ© RowParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿å®Œäº†
ğŸŸ¦ é‡ã¿ã‚’èª­ã¿è¾¼ã¿ä¸­ weight_name='model.layers.11.self_attn.q_norm.weight'
ğŸŸ¦ é€šå¸¸ã®é‡ã¿èª­ã¿è¾¼ã¿ weight_name='model.layers.11.self_attn.q_norm.weight'
ğŸŸ© é‡ã¿ã‚’èª­ã¿è¾¼ã¿ param.shape=torch.Size([128]) loaded_weight.shape=torch.Size([128])
ğŸŸ¦ é‡ã¿ã‚’èª­ã¿è¾¼ã¿ä¸­ weight_name='model.layers.11.self_attn.q_proj.weight'
ğŸŸ¦ ãƒãƒƒãƒ”ãƒ³ã‚°ã«ã‚ˆã‚‹é‡ã¿ã‚’èª­ã¿è¾¼ã¿ weight_name='model.layers.11.self_attn.q_proj.weight' k='q_proj'
ğŸŸ© QKVParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿é–‹å§‹ param.shape=torch.Size([4096, 1024]), loaded_weight.shape=torch.Size([2048, 1024]), loaded_shard_id='q'
ğŸŸ© QKVParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿å®Œäº†
ğŸŸ¦ é‡ã¿ã‚’èª­ã¿è¾¼ã¿ä¸­ weight_name='model.layers.11.self_attn.v_proj.weight'
ğŸŸ¦ ãƒãƒƒãƒ”ãƒ³ã‚°ã«ã‚ˆã‚‹é‡ã¿ã‚’èª­ã¿è¾¼ã¿ weight_name='model.layers.11.self_attn.v_proj.weight' k='v_proj'
ğŸŸ© QKVParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿é–‹å§‹ param.shape=torch.Size([4096, 1024]), loaded_weight.shape=torch.Size([1024, 1024]), loaded_shard_id='v'
ğŸŸ© QKVParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿å®Œäº†
ğŸŸ¦ é‡ã¿ã‚’èª­ã¿è¾¼ã¿ä¸­ weight_name='model.layers.12.input_layernorm.weight'
ğŸŸ¦ é€šå¸¸ã®é‡ã¿èª­ã¿è¾¼ã¿ weight_name='model.layers.12.input_layernorm.weight'
ğŸŸ© é‡ã¿ã‚’èª­ã¿è¾¼ã¿ param.shape=torch.Size([1024]) loaded_weight.shape=torch.Size([1024])
ğŸŸ¦ é‡ã¿ã‚’èª­ã¿è¾¼ã¿ä¸­ weight_name='model.layers.12.mlp.down_proj.weight'
ğŸŸ¦ é€šå¸¸ã®é‡ã¿èª­ã¿è¾¼ã¿ weight_name='model.layers.12.mlp.down_proj.weight'
ğŸŸ© RowParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿é–‹å§‹ param.shape=torch.Size([1024, 3072]), loaded_weight.shape=torch.Size([1024, 3072])
ğŸŸ© RowParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿å®Œäº†
ğŸŸ¦ é‡ã¿ã‚’èª­ã¿è¾¼ã¿ä¸­ weight_name='model.layers.12.mlp.gate_proj.weight'
ğŸŸ¦ ãƒãƒƒãƒ”ãƒ³ã‚°ã«ã‚ˆã‚‹é‡ã¿ã‚’èª­ã¿è¾¼ã¿ weight_name='model.layers.12.mlp.gate_proj.weight' k='gate_proj'
ğŸŸ© MergedColumnParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿é–‹å§‹ param.shape=torch.Size([6144, 1024]), loaded_weight.shape=torch.Size([3072, 1024]), loaded_shard_id=0
ğŸŸ© MergedColumnParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿å®Œäº†
ğŸŸ¦ é‡ã¿ã‚’èª­ã¿è¾¼ã¿ä¸­ weight_name='model.layers.12.mlp.up_proj.weight'
ğŸŸ¦ ãƒãƒƒãƒ”ãƒ³ã‚°ã«ã‚ˆã‚‹é‡ã¿ã‚’èª­ã¿è¾¼ã¿ weight_name='model.layers.12.mlp.up_proj.weight' k='up_proj'
ğŸŸ© MergedColumnParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿é–‹å§‹ param.shape=torch.Size([6144, 1024]), loaded_weight.shape=torch.Size([3072, 1024]), loaded_shard_id=1
ğŸŸ© MergedColumnParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿å®Œäº†
ğŸŸ¦ é‡ã¿ã‚’èª­ã¿è¾¼ã¿ä¸­ weight_name='model.layers.12.post_attention_layernorm.weight'
ğŸŸ¦ é€šå¸¸ã®é‡ã¿èª­ã¿è¾¼ã¿ weight_name='model.layers.12.post_attention_layernorm.weight'
ğŸŸ© é‡ã¿ã‚’èª­ã¿è¾¼ã¿ param.shape=torch.Size([1024]) loaded_weight.shape=torch.Size([1024])
ğŸŸ¦ é‡ã¿ã‚’èª­ã¿è¾¼ã¿ä¸­ weight_name='model.layers.12.self_attn.k_norm.weight'
ğŸŸ¦ é€šå¸¸ã®é‡ã¿èª­ã¿è¾¼ã¿ weight_name='model.layers.12.self_attn.k_norm.weight'
ğŸŸ© é‡ã¿ã‚’èª­ã¿è¾¼ã¿ param.shape=torch.Size([128]) loaded_weight.shape=torch.Size([128])
ğŸŸ¦ é‡ã¿ã‚’èª­ã¿è¾¼ã¿ä¸­ weight_name='model.layers.12.self_attn.k_proj.weight'
ğŸŸ¦ ãƒãƒƒãƒ”ãƒ³ã‚°ã«ã‚ˆã‚‹é‡ã¿ã‚’èª­ã¿è¾¼ã¿ weight_name='model.layers.12.self_attn.k_proj.weight' k='k_proj'
ğŸŸ© QKVParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿é–‹å§‹ param.shape=torch.Size([4096, 1024]), loaded_weight.shape=torch.Size([1024, 1024]), loaded_shard_id='k'
ğŸŸ© QKVParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿å®Œäº†
ğŸŸ¦ é‡ã¿ã‚’èª­ã¿è¾¼ã¿ä¸­ weight_name='model.layers.12.self_attn.o_proj.weight'
ğŸŸ¦ é€šå¸¸ã®é‡ã¿èª­ã¿è¾¼ã¿ weight_name='model.layers.12.self_attn.o_proj.weight'
ğŸŸ© RowParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿é–‹å§‹ param.shape=torch.Size([1024, 2048]), loaded_weight.shape=torch.Size([1024, 2048])
ğŸŸ© RowParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿å®Œäº†
ğŸŸ¦ é‡ã¿ã‚’èª­ã¿è¾¼ã¿ä¸­ weight_name='model.layers.12.self_attn.q_norm.weight'
ğŸŸ¦ é€šå¸¸ã®é‡ã¿èª­ã¿è¾¼ã¿ weight_name='model.layers.12.self_attn.q_norm.weight'
ğŸŸ© é‡ã¿ã‚’èª­ã¿è¾¼ã¿ param.shape=torch.Size([128]) loaded_weight.shape=torch.Size([128])
ğŸŸ¦ é‡ã¿ã‚’èª­ã¿è¾¼ã¿ä¸­ weight_name='model.layers.12.self_attn.q_proj.weight'
ğŸŸ¦ ãƒãƒƒãƒ”ãƒ³ã‚°ã«ã‚ˆã‚‹é‡ã¿ã‚’èª­ã¿è¾¼ã¿ weight_name='model.layers.12.self_attn.q_proj.weight' k='q_proj'
ğŸŸ© QKVParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿é–‹å§‹ param.shape=torch.Size([4096, 1024]), loaded_weight.shape=torch.Size([2048, 1024]), loaded_shard_id='q'
ğŸŸ© QKVParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿å®Œäº†
ğŸŸ¦ é‡ã¿ã‚’èª­ã¿è¾¼ã¿ä¸­ weight_name='model.layers.12.self_attn.v_proj.weight'
ğŸŸ¦ ãƒãƒƒãƒ”ãƒ³ã‚°ã«ã‚ˆã‚‹é‡ã¿ã‚’èª­ã¿è¾¼ã¿ weight_name='model.layers.12.self_attn.v_proj.weight' k='v_proj'
ğŸŸ© QKVParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿é–‹å§‹ param.shape=torch.Size([4096, 1024]), loaded_weight.shape=torch.Size([1024, 1024]), loaded_shard_id='v'
ğŸŸ© QKVParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿å®Œäº†
ğŸŸ¦ é‡ã¿ã‚’èª­ã¿è¾¼ã¿ä¸­ weight_name='model.layers.13.input_layernorm.weight'
ğŸŸ¦ é€šå¸¸ã®é‡ã¿èª­ã¿è¾¼ã¿ weight_name='model.layers.13.input_layernorm.weight'
ğŸŸ© é‡ã¿ã‚’èª­ã¿è¾¼ã¿ param.shape=torch.Size([1024]) loaded_weight.shape=torch.Size([1024])
ğŸŸ¦ é‡ã¿ã‚’èª­ã¿è¾¼ã¿ä¸­ weight_name='model.layers.13.mlp.down_proj.weight'
ğŸŸ¦ é€šå¸¸ã®é‡ã¿èª­ã¿è¾¼ã¿ weight_name='model.layers.13.mlp.down_proj.weight'
ğŸŸ© RowParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿é–‹å§‹ param.shape=torch.Size([1024, 3072]), loaded_weight.shape=torch.Size([1024, 3072])
ğŸŸ© RowParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿å®Œäº†
ğŸŸ¦ é‡ã¿ã‚’èª­ã¿è¾¼ã¿ä¸­ weight_name='model.layers.13.mlp.gate_proj.weight'
ğŸŸ¦ ãƒãƒƒãƒ”ãƒ³ã‚°ã«ã‚ˆã‚‹é‡ã¿ã‚’èª­ã¿è¾¼ã¿ weight_name='model.layers.13.mlp.gate_proj.weight' k='gate_proj'
ğŸŸ© MergedColumnParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿é–‹å§‹ param.shape=torch.Size([6144, 1024]), loaded_weight.shape=torch.Size([3072, 1024]), loaded_shard_id=0
ğŸŸ© MergedColumnParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿å®Œäº†
ğŸŸ¦ é‡ã¿ã‚’èª­ã¿è¾¼ã¿ä¸­ weight_name='model.layers.13.mlp.up_proj.weight'
ğŸŸ¦ ãƒãƒƒãƒ”ãƒ³ã‚°ã«ã‚ˆã‚‹é‡ã¿ã‚’èª­ã¿è¾¼ã¿ weight_name='model.layers.13.mlp.up_proj.weight' k='up_proj'
ğŸŸ© MergedColumnParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿é–‹å§‹ param.shape=torch.Size([6144, 1024]), loaded_weight.shape=torch.Size([3072, 1024]), loaded_shard_id=1
ğŸŸ© MergedColumnParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿å®Œäº†
ğŸŸ¦ é‡ã¿ã‚’èª­ã¿è¾¼ã¿ä¸­ weight_name='model.layers.13.post_attention_layernorm.weight'
ğŸŸ¦ é€šå¸¸ã®é‡ã¿èª­ã¿è¾¼ã¿ weight_name='model.layers.13.post_attention_layernorm.weight'
ğŸŸ© é‡ã¿ã‚’èª­ã¿è¾¼ã¿ param.shape=torch.Size([1024]) loaded_weight.shape=torch.Size([1024])
ğŸŸ¦ é‡ã¿ã‚’èª­ã¿è¾¼ã¿ä¸­ weight_name='model.layers.13.self_attn.k_norm.weight'
ğŸŸ¦ é€šå¸¸ã®é‡ã¿èª­ã¿è¾¼ã¿ weight_name='model.layers.13.self_attn.k_norm.weight'
ğŸŸ© é‡ã¿ã‚’èª­ã¿è¾¼ã¿ param.shape=torch.Size([128]) loaded_weight.shape=torch.Size([128])
ğŸŸ¦ é‡ã¿ã‚’èª­ã¿è¾¼ã¿ä¸­ weight_name='model.layers.13.self_attn.k_proj.weight'
ğŸŸ¦ ãƒãƒƒãƒ”ãƒ³ã‚°ã«ã‚ˆã‚‹é‡ã¿ã‚’èª­ã¿è¾¼ã¿ weight_name='model.layers.13.self_attn.k_proj.weight' k='k_proj'
ğŸŸ© QKVParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿é–‹å§‹ param.shape=torch.Size([4096, 1024]), loaded_weight.shape=torch.Size([1024, 1024]), loaded_shard_id='k'
ğŸŸ© QKVParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿å®Œäº†
ğŸŸ¦ é‡ã¿ã‚’èª­ã¿è¾¼ã¿ä¸­ weight_name='model.layers.13.self_attn.o_proj.weight'
ğŸŸ¦ é€šå¸¸ã®é‡ã¿èª­ã¿è¾¼ã¿ weight_name='model.layers.13.self_attn.o_proj.weight'
ğŸŸ© RowParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿é–‹å§‹ param.shape=torch.Size([1024, 2048]), loaded_weight.shape=torch.Size([1024, 2048])
ğŸŸ© RowParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿å®Œäº†
ğŸŸ¦ é‡ã¿ã‚’èª­ã¿è¾¼ã¿ä¸­ weight_name='model.layers.13.self_attn.q_norm.weight'
ğŸŸ¦ é€šå¸¸ã®é‡ã¿èª­ã¿è¾¼ã¿ weight_name='model.layers.13.self_attn.q_norm.weight'
ğŸŸ© é‡ã¿ã‚’èª­ã¿è¾¼ã¿ param.shape=torch.Size([128]) loaded_weight.shape=torch.Size([128])
ğŸŸ¦ é‡ã¿ã‚’èª­ã¿è¾¼ã¿ä¸­ weight_name='model.layers.13.self_attn.q_proj.weight'
ğŸŸ¦ ãƒãƒƒãƒ”ãƒ³ã‚°ã«ã‚ˆã‚‹é‡ã¿ã‚’èª­ã¿è¾¼ã¿ weight_name='model.layers.13.self_attn.q_proj.weight' k='q_proj'
ğŸŸ© QKVParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿é–‹å§‹ param.shape=torch.Size([4096, 1024]), loaded_weight.shape=torch.Size([2048, 1024]), loaded_shard_id='q'
ğŸŸ© QKVParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿å®Œäº†
ğŸŸ¦ é‡ã¿ã‚’èª­ã¿è¾¼ã¿ä¸­ weight_name='model.layers.13.self_attn.v_proj.weight'
ğŸŸ¦ ãƒãƒƒãƒ”ãƒ³ã‚°ã«ã‚ˆã‚‹é‡ã¿ã‚’èª­ã¿è¾¼ã¿ weight_name='model.layers.13.self_attn.v_proj.weight' k='v_proj'
ğŸŸ© QKVParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿é–‹å§‹ param.shape=torch.Size([4096, 1024]), loaded_weight.shape=torch.Size([1024, 1024]), loaded_shard_id='v'
ğŸŸ© QKVParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿å®Œäº†
ğŸŸ¦ é‡ã¿ã‚’èª­ã¿è¾¼ã¿ä¸­ weight_name='model.layers.14.input_layernorm.weight'
ğŸŸ¦ é€šå¸¸ã®é‡ã¿èª­ã¿è¾¼ã¿ weight_name='model.layers.14.input_layernorm.weight'
ğŸŸ© é‡ã¿ã‚’èª­ã¿è¾¼ã¿ param.shape=torch.Size([1024]) loaded_weight.shape=torch.Size([1024])
ğŸŸ¦ é‡ã¿ã‚’èª­ã¿è¾¼ã¿ä¸­ weight_name='model.layers.14.mlp.down_proj.weight'
ğŸŸ¦ é€šå¸¸ã®é‡ã¿èª­ã¿è¾¼ã¿ weight_name='model.layers.14.mlp.down_proj.weight'
ğŸŸ© RowParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿é–‹å§‹ param.shape=torch.Size([1024, 3072]), loaded_weight.shape=torch.Size([1024, 3072])
ğŸŸ© RowParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿å®Œäº†
ğŸŸ¦ é‡ã¿ã‚’èª­ã¿è¾¼ã¿ä¸­ weight_name='model.layers.14.mlp.gate_proj.weight'
ğŸŸ¦ ãƒãƒƒãƒ”ãƒ³ã‚°ã«ã‚ˆã‚‹é‡ã¿ã‚’èª­ã¿è¾¼ã¿ weight_name='model.layers.14.mlp.gate_proj.weight' k='gate_proj'
ğŸŸ© MergedColumnParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿é–‹å§‹ param.shape=torch.Size([6144, 1024]), loaded_weight.shape=torch.Size([3072, 1024]), loaded_shard_id=0
ğŸŸ© MergedColumnParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿å®Œäº†
ğŸŸ¦ é‡ã¿ã‚’èª­ã¿è¾¼ã¿ä¸­ weight_name='model.layers.14.mlp.up_proj.weight'
ğŸŸ¦ ãƒãƒƒãƒ”ãƒ³ã‚°ã«ã‚ˆã‚‹é‡ã¿ã‚’èª­ã¿è¾¼ã¿ weight_name='model.layers.14.mlp.up_proj.weight' k='up_proj'
ğŸŸ© MergedColumnParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿é–‹å§‹ param.shape=torch.Size([6144, 1024]), loaded_weight.shape=torch.Size([3072, 1024]), loaded_shard_id=1
ğŸŸ© MergedColumnParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿å®Œäº†
ğŸŸ¦ é‡ã¿ã‚’èª­ã¿è¾¼ã¿ä¸­ weight_name='model.layers.14.post_attention_layernorm.weight'
ğŸŸ¦ é€šå¸¸ã®é‡ã¿èª­ã¿è¾¼ã¿ weight_name='model.layers.14.post_attention_layernorm.weight'
ğŸŸ© é‡ã¿ã‚’èª­ã¿è¾¼ã¿ param.shape=torch.Size([1024]) loaded_weight.shape=torch.Size([1024])
ğŸŸ¦ é‡ã¿ã‚’èª­ã¿è¾¼ã¿ä¸­ weight_name='model.layers.14.self_attn.k_norm.weight'
ğŸŸ¦ é€šå¸¸ã®é‡ã¿èª­ã¿è¾¼ã¿ weight_name='model.layers.14.self_attn.k_norm.weight'
ğŸŸ© é‡ã¿ã‚’èª­ã¿è¾¼ã¿ param.shape=torch.Size([128]) loaded_weight.shape=torch.Size([128])
ğŸŸ¦ é‡ã¿ã‚’èª­ã¿è¾¼ã¿ä¸­ weight_name='model.layers.14.self_attn.k_proj.weight'
ğŸŸ¦ ãƒãƒƒãƒ”ãƒ³ã‚°ã«ã‚ˆã‚‹é‡ã¿ã‚’èª­ã¿è¾¼ã¿ weight_name='model.layers.14.self_attn.k_proj.weight' k='k_proj'
ğŸŸ© QKVParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿é–‹å§‹ param.shape=torch.Size([4096, 1024]), loaded_weight.shape=torch.Size([1024, 1024]), loaded_shard_id='k'
ğŸŸ© QKVParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿å®Œäº†
ğŸŸ¦ é‡ã¿ã‚’èª­ã¿è¾¼ã¿ä¸­ weight_name='model.layers.14.self_attn.o_proj.weight'
ğŸŸ¦ é€šå¸¸ã®é‡ã¿èª­ã¿è¾¼ã¿ weight_name='model.layers.14.self_attn.o_proj.weight'
ğŸŸ© RowParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿é–‹å§‹ param.shape=torch.Size([1024, 2048]), loaded_weight.shape=torch.Size([1024, 2048])
ğŸŸ© RowParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿å®Œäº†
ğŸŸ¦ é‡ã¿ã‚’èª­ã¿è¾¼ã¿ä¸­ weight_name='model.layers.14.self_attn.q_norm.weight'
ğŸŸ¦ é€šå¸¸ã®é‡ã¿èª­ã¿è¾¼ã¿ weight_name='model.layers.14.self_attn.q_norm.weight'
ğŸŸ© é‡ã¿ã‚’èª­ã¿è¾¼ã¿ param.shape=torch.Size([128]) loaded_weight.shape=torch.Size([128])
ğŸŸ¦ é‡ã¿ã‚’èª­ã¿è¾¼ã¿ä¸­ weight_name='model.layers.14.self_attn.q_proj.weight'
ğŸŸ¦ ãƒãƒƒãƒ”ãƒ³ã‚°ã«ã‚ˆã‚‹é‡ã¿ã‚’èª­ã¿è¾¼ã¿ weight_name='model.layers.14.self_attn.q_proj.weight' k='q_proj'
ğŸŸ© QKVParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿é–‹å§‹ param.shape=torch.Size([4096, 1024]), loaded_weight.shape=torch.Size([2048, 1024]), loaded_shard_id='q'
ğŸŸ© QKVParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿å®Œäº†
ğŸŸ¦ é‡ã¿ã‚’èª­ã¿è¾¼ã¿ä¸­ weight_name='model.layers.14.self_attn.v_proj.weight'
ğŸŸ¦ ãƒãƒƒãƒ”ãƒ³ã‚°ã«ã‚ˆã‚‹é‡ã¿ã‚’èª­ã¿è¾¼ã¿ weight_name='model.layers.14.self_attn.v_proj.weight' k='v_proj'
ğŸŸ© QKVParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿é–‹å§‹ param.shape=torch.Size([4096, 1024]), loaded_weight.shape=torch.Size([1024, 1024]), loaded_shard_id='v'
ğŸŸ© QKVParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿å®Œäº†
ğŸŸ¦ é‡ã¿ã‚’èª­ã¿è¾¼ã¿ä¸­ weight_name='model.layers.15.input_layernorm.weight'
ğŸŸ¦ é€šå¸¸ã®é‡ã¿èª­ã¿è¾¼ã¿ weight_name='model.layers.15.input_layernorm.weight'
ğŸŸ© é‡ã¿ã‚’èª­ã¿è¾¼ã¿ param.shape=torch.Size([1024]) loaded_weight.shape=torch.Size([1024])
ğŸŸ¦ é‡ã¿ã‚’èª­ã¿è¾¼ã¿ä¸­ weight_name='model.layers.15.mlp.down_proj.weight'
ğŸŸ¦ é€šå¸¸ã®é‡ã¿èª­ã¿è¾¼ã¿ weight_name='model.layers.15.mlp.down_proj.weight'
ğŸŸ© RowParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿é–‹å§‹ param.shape=torch.Size([1024, 3072]), loaded_weight.shape=torch.Size([1024, 3072])
ğŸŸ© RowParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿å®Œäº†
ğŸŸ¦ é‡ã¿ã‚’èª­ã¿è¾¼ã¿ä¸­ weight_name='model.layers.15.mlp.gate_proj.weight'
ğŸŸ¦ ãƒãƒƒãƒ”ãƒ³ã‚°ã«ã‚ˆã‚‹é‡ã¿ã‚’èª­ã¿è¾¼ã¿ weight_name='model.layers.15.mlp.gate_proj.weight' k='gate_proj'
ğŸŸ© MergedColumnParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿é–‹å§‹ param.shape=torch.Size([6144, 1024]), loaded_weight.shape=torch.Size([3072, 1024]), loaded_shard_id=0
ğŸŸ© MergedColumnParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿å®Œäº†
ğŸŸ¦ é‡ã¿ã‚’èª­ã¿è¾¼ã¿ä¸­ weight_name='model.layers.15.mlp.up_proj.weight'
ğŸŸ¦ ãƒãƒƒãƒ”ãƒ³ã‚°ã«ã‚ˆã‚‹é‡ã¿ã‚’èª­ã¿è¾¼ã¿ weight_name='model.layers.15.mlp.up_proj.weight' k='up_proj'
ğŸŸ© MergedColumnParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿é–‹å§‹ param.shape=torch.Size([6144, 1024]), loaded_weight.shape=torch.Size([3072, 1024]), loaded_shard_id=1
ğŸŸ© MergedColumnParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿å®Œäº†
ğŸŸ¦ é‡ã¿ã‚’èª­ã¿è¾¼ã¿ä¸­ weight_name='model.layers.15.post_attention_layernorm.weight'
ğŸŸ¦ é€šå¸¸ã®é‡ã¿èª­ã¿è¾¼ã¿ weight_name='model.layers.15.post_attention_layernorm.weight'
ğŸŸ© é‡ã¿ã‚’èª­ã¿è¾¼ã¿ param.shape=torch.Size([1024]) loaded_weight.shape=torch.Size([1024])
ğŸŸ¦ é‡ã¿ã‚’èª­ã¿è¾¼ã¿ä¸­ weight_name='model.layers.15.self_attn.k_norm.weight'
ğŸŸ¦ é€šå¸¸ã®é‡ã¿èª­ã¿è¾¼ã¿ weight_name='model.layers.15.self_attn.k_norm.weight'
ğŸŸ© é‡ã¿ã‚’èª­ã¿è¾¼ã¿ param.shape=torch.Size([128]) loaded_weight.shape=torch.Size([128])
ğŸŸ¦ é‡ã¿ã‚’èª­ã¿è¾¼ã¿ä¸­ weight_name='model.layers.15.self_attn.k_proj.weight'
ğŸŸ¦ ãƒãƒƒãƒ”ãƒ³ã‚°ã«ã‚ˆã‚‹é‡ã¿ã‚’èª­ã¿è¾¼ã¿ weight_name='model.layers.15.self_attn.k_proj.weight' k='k_proj'
ğŸŸ© QKVParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿é–‹å§‹ param.shape=torch.Size([4096, 1024]), loaded_weight.shape=torch.Size([1024, 1024]), loaded_shard_id='k'
ğŸŸ© QKVParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿å®Œäº†
ğŸŸ¦ é‡ã¿ã‚’èª­ã¿è¾¼ã¿ä¸­ weight_name='model.layers.15.self_attn.o_proj.weight'
ğŸŸ¦ é€šå¸¸ã®é‡ã¿èª­ã¿è¾¼ã¿ weight_name='model.layers.15.self_attn.o_proj.weight'
ğŸŸ© RowParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿é–‹å§‹ param.shape=torch.Size([1024, 2048]), loaded_weight.shape=torch.Size([1024, 2048])
ğŸŸ© RowParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿å®Œäº†
ğŸŸ¦ é‡ã¿ã‚’èª­ã¿è¾¼ã¿ä¸­ weight_name='model.layers.15.self_attn.q_norm.weight'
ğŸŸ¦ é€šå¸¸ã®é‡ã¿èª­ã¿è¾¼ã¿ weight_name='model.layers.15.self_attn.q_norm.weight'
ğŸŸ© é‡ã¿ã‚’èª­ã¿è¾¼ã¿ param.shape=torch.Size([128]) loaded_weight.shape=torch.Size([128])
ğŸŸ¦ é‡ã¿ã‚’èª­ã¿è¾¼ã¿ä¸­ weight_name='model.layers.15.self_attn.q_proj.weight'
ğŸŸ¦ ãƒãƒƒãƒ”ãƒ³ã‚°ã«ã‚ˆã‚‹é‡ã¿ã‚’èª­ã¿è¾¼ã¿ weight_name='model.layers.15.self_attn.q_proj.weight' k='q_proj'
ğŸŸ© QKVParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿é–‹å§‹ param.shape=torch.Size([4096, 1024]), loaded_weight.shape=torch.Size([2048, 1024]), loaded_shard_id='q'
ğŸŸ© QKVParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿å®Œäº†
ğŸŸ¦ é‡ã¿ã‚’èª­ã¿è¾¼ã¿ä¸­ weight_name='model.layers.15.self_attn.v_proj.weight'
ğŸŸ¦ ãƒãƒƒãƒ”ãƒ³ã‚°ã«ã‚ˆã‚‹é‡ã¿ã‚’èª­ã¿è¾¼ã¿ weight_name='model.layers.15.self_attn.v_proj.weight' k='v_proj'
ğŸŸ© QKVParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿é–‹å§‹ param.shape=torch.Size([4096, 1024]), loaded_weight.shape=torch.Size([1024, 1024]), loaded_shard_id='v'
ğŸŸ© QKVParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿å®Œäº†
ğŸŸ¦ é‡ã¿ã‚’èª­ã¿è¾¼ã¿ä¸­ weight_name='model.layers.16.input_layernorm.weight'
ğŸŸ¦ é€šå¸¸ã®é‡ã¿èª­ã¿è¾¼ã¿ weight_name='model.layers.16.input_layernorm.weight'
ğŸŸ© é‡ã¿ã‚’èª­ã¿è¾¼ã¿ param.shape=torch.Size([1024]) loaded_weight.shape=torch.Size([1024])
ğŸŸ¦ é‡ã¿ã‚’èª­ã¿è¾¼ã¿ä¸­ weight_name='model.layers.16.mlp.down_proj.weight'
ğŸŸ¦ é€šå¸¸ã®é‡ã¿èª­ã¿è¾¼ã¿ weight_name='model.layers.16.mlp.down_proj.weight'
ğŸŸ© RowParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿é–‹å§‹ param.shape=torch.Size([1024, 3072]), loaded_weight.shape=torch.Size([1024, 3072])
ğŸŸ© RowParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿å®Œäº†
ğŸŸ¦ é‡ã¿ã‚’èª­ã¿è¾¼ã¿ä¸­ weight_name='model.layers.16.mlp.gate_proj.weight'
ğŸŸ¦ ãƒãƒƒãƒ”ãƒ³ã‚°ã«ã‚ˆã‚‹é‡ã¿ã‚’èª­ã¿è¾¼ã¿ weight_name='model.layers.16.mlp.gate_proj.weight' k='gate_proj'
ğŸŸ© MergedColumnParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿é–‹å§‹ param.shape=torch.Size([6144, 1024]), loaded_weight.shape=torch.Size([3072, 1024]), loaded_shard_id=0
ğŸŸ© MergedColumnParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿å®Œäº†
ğŸŸ¦ é‡ã¿ã‚’èª­ã¿è¾¼ã¿ä¸­ weight_name='model.layers.16.mlp.up_proj.weight'
ğŸŸ¦ ãƒãƒƒãƒ”ãƒ³ã‚°ã«ã‚ˆã‚‹é‡ã¿ã‚’èª­ã¿è¾¼ã¿ weight_name='model.layers.16.mlp.up_proj.weight' k='up_proj'
ğŸŸ© MergedColumnParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿é–‹å§‹ param.shape=torch.Size([6144, 1024]), loaded_weight.shape=torch.Size([3072, 1024]), loaded_shard_id=1
ğŸŸ© MergedColumnParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿å®Œäº†
ğŸŸ¦ é‡ã¿ã‚’èª­ã¿è¾¼ã¿ä¸­ weight_name='model.layers.16.post_attention_layernorm.weight'
ğŸŸ¦ é€šå¸¸ã®é‡ã¿èª­ã¿è¾¼ã¿ weight_name='model.layers.16.post_attention_layernorm.weight'
ğŸŸ© é‡ã¿ã‚’èª­ã¿è¾¼ã¿ param.shape=torch.Size([1024]) loaded_weight.shape=torch.Size([1024])
ğŸŸ¦ é‡ã¿ã‚’èª­ã¿è¾¼ã¿ä¸­ weight_name='model.layers.16.self_attn.k_norm.weight'
ğŸŸ¦ é€šå¸¸ã®é‡ã¿èª­ã¿è¾¼ã¿ weight_name='model.layers.16.self_attn.k_norm.weight'
ğŸŸ© é‡ã¿ã‚’èª­ã¿è¾¼ã¿ param.shape=torch.Size([128]) loaded_weight.shape=torch.Size([128])
ğŸŸ¦ é‡ã¿ã‚’èª­ã¿è¾¼ã¿ä¸­ weight_name='model.layers.16.self_attn.k_proj.weight'
ğŸŸ¦ ãƒãƒƒãƒ”ãƒ³ã‚°ã«ã‚ˆã‚‹é‡ã¿ã‚’èª­ã¿è¾¼ã¿ weight_name='model.layers.16.self_attn.k_proj.weight' k='k_proj'
ğŸŸ© QKVParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿é–‹å§‹ param.shape=torch.Size([4096, 1024]), loaded_weight.shape=torch.Size([1024, 1024]), loaded_shard_id='k'
ğŸŸ© QKVParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿å®Œäº†
ğŸŸ¦ é‡ã¿ã‚’èª­ã¿è¾¼ã¿ä¸­ weight_name='model.layers.16.self_attn.o_proj.weight'
ğŸŸ¦ é€šå¸¸ã®é‡ã¿èª­ã¿è¾¼ã¿ weight_name='model.layers.16.self_attn.o_proj.weight'
ğŸŸ© RowParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿é–‹å§‹ param.shape=torch.Size([1024, 2048]), loaded_weight.shape=torch.Size([1024, 2048])
ğŸŸ© RowParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿å®Œäº†
ğŸŸ¦ é‡ã¿ã‚’èª­ã¿è¾¼ã¿ä¸­ weight_name='model.layers.16.self_attn.q_norm.weight'
ğŸŸ¦ é€šå¸¸ã®é‡ã¿èª­ã¿è¾¼ã¿ weight_name='model.layers.16.self_attn.q_norm.weight'
ğŸŸ© é‡ã¿ã‚’èª­ã¿è¾¼ã¿ param.shape=torch.Size([128]) loaded_weight.shape=torch.Size([128])
ğŸŸ¦ é‡ã¿ã‚’èª­ã¿è¾¼ã¿ä¸­ weight_name='model.layers.16.self_attn.q_proj.weight'
ğŸŸ¦ ãƒãƒƒãƒ”ãƒ³ã‚°ã«ã‚ˆã‚‹é‡ã¿ã‚’èª­ã¿è¾¼ã¿ weight_name='model.layers.16.self_attn.q_proj.weight' k='q_proj'
ğŸŸ© QKVParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿é–‹å§‹ param.shape=torch.Size([4096, 1024]), loaded_weight.shape=torch.Size([2048, 1024]), loaded_shard_id='q'
ğŸŸ© QKVParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿å®Œäº†
ğŸŸ¦ é‡ã¿ã‚’èª­ã¿è¾¼ã¿ä¸­ weight_name='model.layers.16.self_attn.v_proj.weight'
ğŸŸ¦ ãƒãƒƒãƒ”ãƒ³ã‚°ã«ã‚ˆã‚‹é‡ã¿ã‚’èª­ã¿è¾¼ã¿ weight_name='model.layers.16.self_attn.v_proj.weight' k='v_proj'
ğŸŸ© QKVParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿é–‹å§‹ param.shape=torch.Size([4096, 1024]), loaded_weight.shape=torch.Size([1024, 1024]), loaded_shard_id='v'
ğŸŸ© QKVParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿å®Œäº†
ğŸŸ¦ é‡ã¿ã‚’èª­ã¿è¾¼ã¿ä¸­ weight_name='model.layers.17.input_layernorm.weight'
ğŸŸ¦ é€šå¸¸ã®é‡ã¿èª­ã¿è¾¼ã¿ weight_name='model.layers.17.input_layernorm.weight'
ğŸŸ© é‡ã¿ã‚’èª­ã¿è¾¼ã¿ param.shape=torch.Size([1024]) loaded_weight.shape=torch.Size([1024])
ğŸŸ¦ é‡ã¿ã‚’èª­ã¿è¾¼ã¿ä¸­ weight_name='model.layers.17.mlp.down_proj.weight'
ğŸŸ¦ é€šå¸¸ã®é‡ã¿èª­ã¿è¾¼ã¿ weight_name='model.layers.17.mlp.down_proj.weight'
ğŸŸ© RowParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿é–‹å§‹ param.shape=torch.Size([1024, 3072]), loaded_weight.shape=torch.Size([1024, 3072])
ğŸŸ© RowParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿å®Œäº†
ğŸŸ¦ é‡ã¿ã‚’èª­ã¿è¾¼ã¿ä¸­ weight_name='model.layers.17.mlp.gate_proj.weight'
ğŸŸ¦ ãƒãƒƒãƒ”ãƒ³ã‚°ã«ã‚ˆã‚‹é‡ã¿ã‚’èª­ã¿è¾¼ã¿ weight_name='model.layers.17.mlp.gate_proj.weight' k='gate_proj'
ğŸŸ© MergedColumnParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿é–‹å§‹ param.shape=torch.Size([6144, 1024]), loaded_weight.shape=torch.Size([3072, 1024]), loaded_shard_id=0
ğŸŸ© MergedColumnParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿å®Œäº†
ğŸŸ¦ é‡ã¿ã‚’èª­ã¿è¾¼ã¿ä¸­ weight_name='model.layers.17.mlp.up_proj.weight'
ğŸŸ¦ ãƒãƒƒãƒ”ãƒ³ã‚°ã«ã‚ˆã‚‹é‡ã¿ã‚’èª­ã¿è¾¼ã¿ weight_name='model.layers.17.mlp.up_proj.weight' k='up_proj'
ğŸŸ© MergedColumnParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿é–‹å§‹ param.shape=torch.Size([6144, 1024]), loaded_weight.shape=torch.Size([3072, 1024]), loaded_shard_id=1
ğŸŸ© MergedColumnParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿å®Œäº†
ğŸŸ¦ é‡ã¿ã‚’èª­ã¿è¾¼ã¿ä¸­ weight_name='model.layers.17.post_attention_layernorm.weight'
ğŸŸ¦ é€šå¸¸ã®é‡ã¿èª­ã¿è¾¼ã¿ weight_name='model.layers.17.post_attention_layernorm.weight'
ğŸŸ© é‡ã¿ã‚’èª­ã¿è¾¼ã¿ param.shape=torch.Size([1024]) loaded_weight.shape=torch.Size([1024])
ğŸŸ¦ é‡ã¿ã‚’èª­ã¿è¾¼ã¿ä¸­ weight_name='model.layers.17.self_attn.k_norm.weight'
ğŸŸ¦ é€šå¸¸ã®é‡ã¿èª­ã¿è¾¼ã¿ weight_name='model.layers.17.self_attn.k_norm.weight'
ğŸŸ© é‡ã¿ã‚’èª­ã¿è¾¼ã¿ param.shape=torch.Size([128]) loaded_weight.shape=torch.Size([128])
ğŸŸ¦ é‡ã¿ã‚’èª­ã¿è¾¼ã¿ä¸­ weight_name='model.layers.17.self_attn.k_proj.weight'
ğŸŸ¦ ãƒãƒƒãƒ”ãƒ³ã‚°ã«ã‚ˆã‚‹é‡ã¿ã‚’èª­ã¿è¾¼ã¿ weight_name='model.layers.17.self_attn.k_proj.weight' k='k_proj'
ğŸŸ© QKVParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿é–‹å§‹ param.shape=torch.Size([4096, 1024]), loaded_weight.shape=torch.Size([1024, 1024]), loaded_shard_id='k'
ğŸŸ© QKVParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿å®Œäº†
ğŸŸ¦ é‡ã¿ã‚’èª­ã¿è¾¼ã¿ä¸­ weight_name='model.layers.17.self_attn.o_proj.weight'
ğŸŸ¦ é€šå¸¸ã®é‡ã¿èª­ã¿è¾¼ã¿ weight_name='model.layers.17.self_attn.o_proj.weight'
ğŸŸ© RowParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿é–‹å§‹ param.shape=torch.Size([1024, 2048]), loaded_weight.shape=torch.Size([1024, 2048])
ğŸŸ© RowParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿å®Œäº†
ğŸŸ¦ é‡ã¿ã‚’èª­ã¿è¾¼ã¿ä¸­ weight_name='model.layers.17.self_attn.q_norm.weight'
ğŸŸ¦ é€šå¸¸ã®é‡ã¿èª­ã¿è¾¼ã¿ weight_name='model.layers.17.self_attn.q_norm.weight'
ğŸŸ© é‡ã¿ã‚’èª­ã¿è¾¼ã¿ param.shape=torch.Size([128]) loaded_weight.shape=torch.Size([128])
ğŸŸ¦ é‡ã¿ã‚’èª­ã¿è¾¼ã¿ä¸­ weight_name='model.layers.17.self_attn.q_proj.weight'
ğŸŸ¦ ãƒãƒƒãƒ”ãƒ³ã‚°ã«ã‚ˆã‚‹é‡ã¿ã‚’èª­ã¿è¾¼ã¿ weight_name='model.layers.17.self_attn.q_proj.weight' k='q_proj'
ğŸŸ© QKVParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿é–‹å§‹ param.shape=torch.Size([4096, 1024]), loaded_weight.shape=torch.Size([2048, 1024]), loaded_shard_id='q'
ğŸŸ© QKVParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿å®Œäº†
ğŸŸ¦ é‡ã¿ã‚’èª­ã¿è¾¼ã¿ä¸­ weight_name='model.layers.17.self_attn.v_proj.weight'
ğŸŸ¦ ãƒãƒƒãƒ”ãƒ³ã‚°ã«ã‚ˆã‚‹é‡ã¿ã‚’èª­ã¿è¾¼ã¿ weight_name='model.layers.17.self_attn.v_proj.weight' k='v_proj'
ğŸŸ© QKVParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿é–‹å§‹ param.shape=torch.Size([4096, 1024]), loaded_weight.shape=torch.Size([1024, 1024]), loaded_shard_id='v'
ğŸŸ© QKVParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿å®Œäº†
ğŸŸ¦ é‡ã¿ã‚’èª­ã¿è¾¼ã¿ä¸­ weight_name='model.layers.18.input_layernorm.weight'
ğŸŸ¦ é€šå¸¸ã®é‡ã¿èª­ã¿è¾¼ã¿ weight_name='model.layers.18.input_layernorm.weight'
ğŸŸ© é‡ã¿ã‚’èª­ã¿è¾¼ã¿ param.shape=torch.Size([1024]) loaded_weight.shape=torch.Size([1024])
ğŸŸ¦ é‡ã¿ã‚’èª­ã¿è¾¼ã¿ä¸­ weight_name='model.layers.18.mlp.down_proj.weight'
ğŸŸ¦ é€šå¸¸ã®é‡ã¿èª­ã¿è¾¼ã¿ weight_name='model.layers.18.mlp.down_proj.weight'
ğŸŸ© RowParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿é–‹å§‹ param.shape=torch.Size([1024, 3072]), loaded_weight.shape=torch.Size([1024, 3072])
ğŸŸ© RowParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿å®Œäº†
ğŸŸ¦ é‡ã¿ã‚’èª­ã¿è¾¼ã¿ä¸­ weight_name='model.layers.18.mlp.gate_proj.weight'
ğŸŸ¦ ãƒãƒƒãƒ”ãƒ³ã‚°ã«ã‚ˆã‚‹é‡ã¿ã‚’èª­ã¿è¾¼ã¿ weight_name='model.layers.18.mlp.gate_proj.weight' k='gate_proj'
ğŸŸ© MergedColumnParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿é–‹å§‹ param.shape=torch.Size([6144, 1024]), loaded_weight.shape=torch.Size([3072, 1024]), loaded_shard_id=0
ğŸŸ© MergedColumnParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿å®Œäº†
ğŸŸ¦ é‡ã¿ã‚’èª­ã¿è¾¼ã¿ä¸­ weight_name='model.layers.18.mlp.up_proj.weight'
ğŸŸ¦ ãƒãƒƒãƒ”ãƒ³ã‚°ã«ã‚ˆã‚‹é‡ã¿ã‚’èª­ã¿è¾¼ã¿ weight_name='model.layers.18.mlp.up_proj.weight' k='up_proj'
ğŸŸ© MergedColumnParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿é–‹å§‹ param.shape=torch.Size([6144, 1024]), loaded_weight.shape=torch.Size([3072, 1024]), loaded_shard_id=1
ğŸŸ© MergedColumnParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿å®Œäº†
ğŸŸ¦ é‡ã¿ã‚’èª­ã¿è¾¼ã¿ä¸­ weight_name='model.layers.18.post_attention_layernorm.weight'
ğŸŸ¦ é€šå¸¸ã®é‡ã¿èª­ã¿è¾¼ã¿ weight_name='model.layers.18.post_attention_layernorm.weight'
ğŸŸ© é‡ã¿ã‚’èª­ã¿è¾¼ã¿ param.shape=torch.Size([1024]) loaded_weight.shape=torch.Size([1024])
ğŸŸ¦ é‡ã¿ã‚’èª­ã¿è¾¼ã¿ä¸­ weight_name='model.layers.18.self_attn.k_norm.weight'
ğŸŸ¦ é€šå¸¸ã®é‡ã¿èª­ã¿è¾¼ã¿ weight_name='model.layers.18.self_attn.k_norm.weight'
ğŸŸ© é‡ã¿ã‚’èª­ã¿è¾¼ã¿ param.shape=torch.Size([128]) loaded_weight.shape=torch.Size([128])
ğŸŸ¦ é‡ã¿ã‚’èª­ã¿è¾¼ã¿ä¸­ weight_name='model.layers.18.self_attn.k_proj.weight'
ğŸŸ¦ ãƒãƒƒãƒ”ãƒ³ã‚°ã«ã‚ˆã‚‹é‡ã¿ã‚’èª­ã¿è¾¼ã¿ weight_name='model.layers.18.self_attn.k_proj.weight' k='k_proj'
ğŸŸ© QKVParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿é–‹å§‹ param.shape=torch.Size([4096, 1024]), loaded_weight.shape=torch.Size([1024, 1024]), loaded_shard_id='k'
ğŸŸ© QKVParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿å®Œäº†
ğŸŸ¦ é‡ã¿ã‚’èª­ã¿è¾¼ã¿ä¸­ weight_name='model.layers.18.self_attn.o_proj.weight'
ğŸŸ¦ é€šå¸¸ã®é‡ã¿èª­ã¿è¾¼ã¿ weight_name='model.layers.18.self_attn.o_proj.weight'
ğŸŸ© RowParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿é–‹å§‹ param.shape=torch.Size([1024, 2048]), loaded_weight.shape=torch.Size([1024, 2048])
ğŸŸ© RowParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿å®Œäº†
ğŸŸ¦ é‡ã¿ã‚’èª­ã¿è¾¼ã¿ä¸­ weight_name='model.layers.18.self_attn.q_norm.weight'
ğŸŸ¦ é€šå¸¸ã®é‡ã¿èª­ã¿è¾¼ã¿ weight_name='model.layers.18.self_attn.q_norm.weight'
ğŸŸ© é‡ã¿ã‚’èª­ã¿è¾¼ã¿ param.shape=torch.Size([128]) loaded_weight.shape=torch.Size([128])
ğŸŸ¦ é‡ã¿ã‚’èª­ã¿è¾¼ã¿ä¸­ weight_name='model.layers.18.self_attn.q_proj.weight'
ğŸŸ¦ ãƒãƒƒãƒ”ãƒ³ã‚°ã«ã‚ˆã‚‹é‡ã¿ã‚’èª­ã¿è¾¼ã¿ weight_name='model.layers.18.self_attn.q_proj.weight' k='q_proj'
ğŸŸ© QKVParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿é–‹å§‹ param.shape=torch.Size([4096, 1024]), loaded_weight.shape=torch.Size([2048, 1024]), loaded_shard_id='q'
ğŸŸ© QKVParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿å®Œäº†
ğŸŸ¦ é‡ã¿ã‚’èª­ã¿è¾¼ã¿ä¸­ weight_name='model.layers.18.self_attn.v_proj.weight'
ğŸŸ¦ ãƒãƒƒãƒ”ãƒ³ã‚°ã«ã‚ˆã‚‹é‡ã¿ã‚’èª­ã¿è¾¼ã¿ weight_name='model.layers.18.self_attn.v_proj.weight' k='v_proj'
ğŸŸ© QKVParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿é–‹å§‹ param.shape=torch.Size([4096, 1024]), loaded_weight.shape=torch.Size([1024, 1024]), loaded_shard_id='v'
ğŸŸ© QKVParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿å®Œäº†
ğŸŸ¦ é‡ã¿ã‚’èª­ã¿è¾¼ã¿ä¸­ weight_name='model.layers.19.input_layernorm.weight'
ğŸŸ¦ é€šå¸¸ã®é‡ã¿èª­ã¿è¾¼ã¿ weight_name='model.layers.19.input_layernorm.weight'
ğŸŸ© é‡ã¿ã‚’èª­ã¿è¾¼ã¿ param.shape=torch.Size([1024]) loaded_weight.shape=torch.Size([1024])
ğŸŸ¦ é‡ã¿ã‚’èª­ã¿è¾¼ã¿ä¸­ weight_name='model.layers.19.mlp.down_proj.weight'
ğŸŸ¦ é€šå¸¸ã®é‡ã¿èª­ã¿è¾¼ã¿ weight_name='model.layers.19.mlp.down_proj.weight'
ğŸŸ© RowParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿é–‹å§‹ param.shape=torch.Size([1024, 3072]), loaded_weight.shape=torch.Size([1024, 3072])
ğŸŸ© RowParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿å®Œäº†
ğŸŸ¦ é‡ã¿ã‚’èª­ã¿è¾¼ã¿ä¸­ weight_name='model.layers.19.mlp.gate_proj.weight'
ğŸŸ¦ ãƒãƒƒãƒ”ãƒ³ã‚°ã«ã‚ˆã‚‹é‡ã¿ã‚’èª­ã¿è¾¼ã¿ weight_name='model.layers.19.mlp.gate_proj.weight' k='gate_proj'
ğŸŸ© MergedColumnParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿é–‹å§‹ param.shape=torch.Size([6144, 1024]), loaded_weight.shape=torch.Size([3072, 1024]), loaded_shard_id=0
ğŸŸ© MergedColumnParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿å®Œäº†
ğŸŸ¦ é‡ã¿ã‚’èª­ã¿è¾¼ã¿ä¸­ weight_name='model.layers.19.mlp.up_proj.weight'
ğŸŸ¦ ãƒãƒƒãƒ”ãƒ³ã‚°ã«ã‚ˆã‚‹é‡ã¿ã‚’èª­ã¿è¾¼ã¿ weight_name='model.layers.19.mlp.up_proj.weight' k='up_proj'
ğŸŸ© MergedColumnParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿é–‹å§‹ param.shape=torch.Size([6144, 1024]), loaded_weight.shape=torch.Size([3072, 1024]), loaded_shard_id=1
ğŸŸ© MergedColumnParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿å®Œäº†
ğŸŸ¦ é‡ã¿ã‚’èª­ã¿è¾¼ã¿ä¸­ weight_name='model.layers.19.post_attention_layernorm.weight'
ğŸŸ¦ é€šå¸¸ã®é‡ã¿èª­ã¿è¾¼ã¿ weight_name='model.layers.19.post_attention_layernorm.weight'
ğŸŸ© é‡ã¿ã‚’èª­ã¿è¾¼ã¿ param.shape=torch.Size([1024]) loaded_weight.shape=torch.Size([1024])
ğŸŸ¦ é‡ã¿ã‚’èª­ã¿è¾¼ã¿ä¸­ weight_name='model.layers.19.self_attn.k_norm.weight'
ğŸŸ¦ é€šå¸¸ã®é‡ã¿èª­ã¿è¾¼ã¿ weight_name='model.layers.19.self_attn.k_norm.weight'
ğŸŸ© é‡ã¿ã‚’èª­ã¿è¾¼ã¿ param.shape=torch.Size([128]) loaded_weight.shape=torch.Size([128])
ğŸŸ¦ é‡ã¿ã‚’èª­ã¿è¾¼ã¿ä¸­ weight_name='model.layers.19.self_attn.k_proj.weight'
ğŸŸ¦ ãƒãƒƒãƒ”ãƒ³ã‚°ã«ã‚ˆã‚‹é‡ã¿ã‚’èª­ã¿è¾¼ã¿ weight_name='model.layers.19.self_attn.k_proj.weight' k='k_proj'
ğŸŸ© QKVParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿é–‹å§‹ param.shape=torch.Size([4096, 1024]), loaded_weight.shape=torch.Size([1024, 1024]), loaded_shard_id='k'
ğŸŸ© QKVParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿å®Œäº†
ğŸŸ¦ é‡ã¿ã‚’èª­ã¿è¾¼ã¿ä¸­ weight_name='model.layers.19.self_attn.o_proj.weight'
ğŸŸ¦ é€šå¸¸ã®é‡ã¿èª­ã¿è¾¼ã¿ weight_name='model.layers.19.self_attn.o_proj.weight'
ğŸŸ© RowParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿é–‹å§‹ param.shape=torch.Size([1024, 2048]), loaded_weight.shape=torch.Size([1024, 2048])
ğŸŸ© RowParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿å®Œäº†
ğŸŸ¦ é‡ã¿ã‚’èª­ã¿è¾¼ã¿ä¸­ weight_name='model.layers.19.self_attn.q_norm.weight'
ğŸŸ¦ é€šå¸¸ã®é‡ã¿èª­ã¿è¾¼ã¿ weight_name='model.layers.19.self_attn.q_norm.weight'
ğŸŸ© é‡ã¿ã‚’èª­ã¿è¾¼ã¿ param.shape=torch.Size([128]) loaded_weight.shape=torch.Size([128])
ğŸŸ¦ é‡ã¿ã‚’èª­ã¿è¾¼ã¿ä¸­ weight_name='model.layers.19.self_attn.q_proj.weight'
ğŸŸ¦ ãƒãƒƒãƒ”ãƒ³ã‚°ã«ã‚ˆã‚‹é‡ã¿ã‚’èª­ã¿è¾¼ã¿ weight_name='model.layers.19.self_attn.q_proj.weight' k='q_proj'
ğŸŸ© QKVParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿é–‹å§‹ param.shape=torch.Size([4096, 1024]), loaded_weight.shape=torch.Size([2048, 1024]), loaded_shard_id='q'
ğŸŸ© QKVParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿å®Œäº†
ğŸŸ¦ é‡ã¿ã‚’èª­ã¿è¾¼ã¿ä¸­ weight_name='model.layers.19.self_attn.v_proj.weight'
ğŸŸ¦ ãƒãƒƒãƒ”ãƒ³ã‚°ã«ã‚ˆã‚‹é‡ã¿ã‚’èª­ã¿è¾¼ã¿ weight_name='model.layers.19.self_attn.v_proj.weight' k='v_proj'
ğŸŸ© QKVParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿é–‹å§‹ param.shape=torch.Size([4096, 1024]), loaded_weight.shape=torch.Size([1024, 1024]), loaded_shard_id='v'
ğŸŸ© QKVParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿å®Œäº†
ğŸŸ¦ é‡ã¿ã‚’èª­ã¿è¾¼ã¿ä¸­ weight_name='model.layers.2.input_layernorm.weight'
ğŸŸ¦ é€šå¸¸ã®é‡ã¿èª­ã¿è¾¼ã¿ weight_name='model.layers.2.input_layernorm.weight'
ğŸŸ© é‡ã¿ã‚’èª­ã¿è¾¼ã¿ param.shape=torch.Size([1024]) loaded_weight.shape=torch.Size([1024])
ğŸŸ¦ é‡ã¿ã‚’èª­ã¿è¾¼ã¿ä¸­ weight_name='model.layers.2.mlp.down_proj.weight'
ğŸŸ¦ é€šå¸¸ã®é‡ã¿èª­ã¿è¾¼ã¿ weight_name='model.layers.2.mlp.down_proj.weight'
ğŸŸ© RowParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿é–‹å§‹ param.shape=torch.Size([1024, 3072]), loaded_weight.shape=torch.Size([1024, 3072])
ğŸŸ© RowParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿å®Œäº†
ğŸŸ¦ é‡ã¿ã‚’èª­ã¿è¾¼ã¿ä¸­ weight_name='model.layers.2.mlp.gate_proj.weight'
ğŸŸ¦ ãƒãƒƒãƒ”ãƒ³ã‚°ã«ã‚ˆã‚‹é‡ã¿ã‚’èª­ã¿è¾¼ã¿ weight_name='model.layers.2.mlp.gate_proj.weight' k='gate_proj'
ğŸŸ© MergedColumnParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿é–‹å§‹ param.shape=torch.Size([6144, 1024]), loaded_weight.shape=torch.Size([3072, 1024]), loaded_shard_id=0
ğŸŸ© MergedColumnParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿å®Œäº†
ğŸŸ¦ é‡ã¿ã‚’èª­ã¿è¾¼ã¿ä¸­ weight_name='model.layers.2.mlp.up_proj.weight'
ğŸŸ¦ ãƒãƒƒãƒ”ãƒ³ã‚°ã«ã‚ˆã‚‹é‡ã¿ã‚’èª­ã¿è¾¼ã¿ weight_name='model.layers.2.mlp.up_proj.weight' k='up_proj'
ğŸŸ© MergedColumnParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿é–‹å§‹ param.shape=torch.Size([6144, 1024]), loaded_weight.shape=torch.Size([3072, 1024]), loaded_shard_id=1
ğŸŸ© MergedColumnParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿å®Œäº†
ğŸŸ¦ é‡ã¿ã‚’èª­ã¿è¾¼ã¿ä¸­ weight_name='model.layers.2.post_attention_layernorm.weight'
ğŸŸ¦ é€šå¸¸ã®é‡ã¿èª­ã¿è¾¼ã¿ weight_name='model.layers.2.post_attention_layernorm.weight'
ğŸŸ© é‡ã¿ã‚’èª­ã¿è¾¼ã¿ param.shape=torch.Size([1024]) loaded_weight.shape=torch.Size([1024])
ğŸŸ¦ é‡ã¿ã‚’èª­ã¿è¾¼ã¿ä¸­ weight_name='model.layers.2.self_attn.k_norm.weight'
ğŸŸ¦ é€šå¸¸ã®é‡ã¿èª­ã¿è¾¼ã¿ weight_name='model.layers.2.self_attn.k_norm.weight'
ğŸŸ© é‡ã¿ã‚’èª­ã¿è¾¼ã¿ param.shape=torch.Size([128]) loaded_weight.shape=torch.Size([128])
ğŸŸ¦ é‡ã¿ã‚’èª­ã¿è¾¼ã¿ä¸­ weight_name='model.layers.2.self_attn.k_proj.weight'
ğŸŸ¦ ãƒãƒƒãƒ”ãƒ³ã‚°ã«ã‚ˆã‚‹é‡ã¿ã‚’èª­ã¿è¾¼ã¿ weight_name='model.layers.2.self_attn.k_proj.weight' k='k_proj'
ğŸŸ© QKVParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿é–‹å§‹ param.shape=torch.Size([4096, 1024]), loaded_weight.shape=torch.Size([1024, 1024]), loaded_shard_id='k'
ğŸŸ© QKVParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿å®Œäº†
ğŸŸ¦ é‡ã¿ã‚’èª­ã¿è¾¼ã¿ä¸­ weight_name='model.layers.2.self_attn.o_proj.weight'
ğŸŸ¦ é€šå¸¸ã®é‡ã¿èª­ã¿è¾¼ã¿ weight_name='model.layers.2.self_attn.o_proj.weight'
ğŸŸ© RowParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿é–‹å§‹ param.shape=torch.Size([1024, 2048]), loaded_weight.shape=torch.Size([1024, 2048])
ğŸŸ© RowParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿å®Œäº†
ğŸŸ¦ é‡ã¿ã‚’èª­ã¿è¾¼ã¿ä¸­ weight_name='model.layers.2.self_attn.q_norm.weight'
ğŸŸ¦ é€šå¸¸ã®é‡ã¿èª­ã¿è¾¼ã¿ weight_name='model.layers.2.self_attn.q_norm.weight'
ğŸŸ© é‡ã¿ã‚’èª­ã¿è¾¼ã¿ param.shape=torch.Size([128]) loaded_weight.shape=torch.Size([128])
ğŸŸ¦ é‡ã¿ã‚’èª­ã¿è¾¼ã¿ä¸­ weight_name='model.layers.2.self_attn.q_proj.weight'
ğŸŸ¦ ãƒãƒƒãƒ”ãƒ³ã‚°ã«ã‚ˆã‚‹é‡ã¿ã‚’èª­ã¿è¾¼ã¿ weight_name='model.layers.2.self_attn.q_proj.weight' k='q_proj'
ğŸŸ© QKVParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿é–‹å§‹ param.shape=torch.Size([4096, 1024]), loaded_weight.shape=torch.Size([2048, 1024]), loaded_shard_id='q'
ğŸŸ© QKVParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿å®Œäº†
ğŸŸ¦ é‡ã¿ã‚’èª­ã¿è¾¼ã¿ä¸­ weight_name='model.layers.2.self_attn.v_proj.weight'
ğŸŸ¦ ãƒãƒƒãƒ”ãƒ³ã‚°ã«ã‚ˆã‚‹é‡ã¿ã‚’èª­ã¿è¾¼ã¿ weight_name='model.layers.2.self_attn.v_proj.weight' k='v_proj'
ğŸŸ© QKVParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿é–‹å§‹ param.shape=torch.Size([4096, 1024]), loaded_weight.shape=torch.Size([1024, 1024]), loaded_shard_id='v'
ğŸŸ© QKVParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿å®Œäº†
ğŸŸ¦ é‡ã¿ã‚’èª­ã¿è¾¼ã¿ä¸­ weight_name='model.layers.20.input_layernorm.weight'
ğŸŸ¦ é€šå¸¸ã®é‡ã¿èª­ã¿è¾¼ã¿ weight_name='model.layers.20.input_layernorm.weight'
ğŸŸ© é‡ã¿ã‚’èª­ã¿è¾¼ã¿ param.shape=torch.Size([1024]) loaded_weight.shape=torch.Size([1024])
ğŸŸ¦ é‡ã¿ã‚’èª­ã¿è¾¼ã¿ä¸­ weight_name='model.layers.20.mlp.down_proj.weight'
ğŸŸ¦ é€šå¸¸ã®é‡ã¿èª­ã¿è¾¼ã¿ weight_name='model.layers.20.mlp.down_proj.weight'
ğŸŸ© RowParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿é–‹å§‹ param.shape=torch.Size([1024, 3072]), loaded_weight.shape=torch.Size([1024, 3072])
ğŸŸ© RowParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿å®Œäº†
ğŸŸ¦ é‡ã¿ã‚’èª­ã¿è¾¼ã¿ä¸­ weight_name='model.layers.20.mlp.gate_proj.weight'
ğŸŸ¦ ãƒãƒƒãƒ”ãƒ³ã‚°ã«ã‚ˆã‚‹é‡ã¿ã‚’èª­ã¿è¾¼ã¿ weight_name='model.layers.20.mlp.gate_proj.weight' k='gate_proj'
ğŸŸ© MergedColumnParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿é–‹å§‹ param.shape=torch.Size([6144, 1024]), loaded_weight.shape=torch.Size([3072, 1024]), loaded_shard_id=0
ğŸŸ© MergedColumnParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿å®Œäº†
ğŸŸ¦ é‡ã¿ã‚’èª­ã¿è¾¼ã¿ä¸­ weight_name='model.layers.20.mlp.up_proj.weight'
ğŸŸ¦ ãƒãƒƒãƒ”ãƒ³ã‚°ã«ã‚ˆã‚‹é‡ã¿ã‚’èª­ã¿è¾¼ã¿ weight_name='model.layers.20.mlp.up_proj.weight' k='up_proj'
ğŸŸ© MergedColumnParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿é–‹å§‹ param.shape=torch.Size([6144, 1024]), loaded_weight.shape=torch.Size([3072, 1024]), loaded_shard_id=1
ğŸŸ© MergedColumnParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿å®Œäº†
ğŸŸ¦ é‡ã¿ã‚’èª­ã¿è¾¼ã¿ä¸­ weight_name='model.layers.20.post_attention_layernorm.weight'
ğŸŸ¦ é€šå¸¸ã®é‡ã¿èª­ã¿è¾¼ã¿ weight_name='model.layers.20.post_attention_layernorm.weight'
ğŸŸ© é‡ã¿ã‚’èª­ã¿è¾¼ã¿ param.shape=torch.Size([1024]) loaded_weight.shape=torch.Size([1024])
ğŸŸ¦ é‡ã¿ã‚’èª­ã¿è¾¼ã¿ä¸­ weight_name='model.layers.20.self_attn.k_norm.weight'
ğŸŸ¦ é€šå¸¸ã®é‡ã¿èª­ã¿è¾¼ã¿ weight_name='model.layers.20.self_attn.k_norm.weight'
ğŸŸ© é‡ã¿ã‚’èª­ã¿è¾¼ã¿ param.shape=torch.Size([128]) loaded_weight.shape=torch.Size([128])
ğŸŸ¦ é‡ã¿ã‚’èª­ã¿è¾¼ã¿ä¸­ weight_name='model.layers.20.self_attn.k_proj.weight'
ğŸŸ¦ ãƒãƒƒãƒ”ãƒ³ã‚°ã«ã‚ˆã‚‹é‡ã¿ã‚’èª­ã¿è¾¼ã¿ weight_name='model.layers.20.self_attn.k_proj.weight' k='k_proj'
ğŸŸ© QKVParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿é–‹å§‹ param.shape=torch.Size([4096, 1024]), loaded_weight.shape=torch.Size([1024, 1024]), loaded_shard_id='k'
ğŸŸ© QKVParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿å®Œäº†
ğŸŸ¦ é‡ã¿ã‚’èª­ã¿è¾¼ã¿ä¸­ weight_name='model.layers.20.self_attn.o_proj.weight'
ğŸŸ¦ é€šå¸¸ã®é‡ã¿èª­ã¿è¾¼ã¿ weight_name='model.layers.20.self_attn.o_proj.weight'
ğŸŸ© RowParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿é–‹å§‹ param.shape=torch.Size([1024, 2048]), loaded_weight.shape=torch.Size([1024, 2048])
ğŸŸ© RowParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿å®Œäº†
ğŸŸ¦ é‡ã¿ã‚’èª­ã¿è¾¼ã¿ä¸­ weight_name='model.layers.20.self_attn.q_norm.weight'
ğŸŸ¦ é€šå¸¸ã®é‡ã¿èª­ã¿è¾¼ã¿ weight_name='model.layers.20.self_attn.q_norm.weight'
ğŸŸ© é‡ã¿ã‚’èª­ã¿è¾¼ã¿ param.shape=torch.Size([128]) loaded_weight.shape=torch.Size([128])
ğŸŸ¦ é‡ã¿ã‚’èª­ã¿è¾¼ã¿ä¸­ weight_name='model.layers.20.self_attn.q_proj.weight'
ğŸŸ¦ ãƒãƒƒãƒ”ãƒ³ã‚°ã«ã‚ˆã‚‹é‡ã¿ã‚’èª­ã¿è¾¼ã¿ weight_name='model.layers.20.self_attn.q_proj.weight' k='q_proj'
ğŸŸ© QKVParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿é–‹å§‹ param.shape=torch.Size([4096, 1024]), loaded_weight.shape=torch.Size([2048, 1024]), loaded_shard_id='q'
ğŸŸ© QKVParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿å®Œäº†
ğŸŸ¦ é‡ã¿ã‚’èª­ã¿è¾¼ã¿ä¸­ weight_name='model.layers.20.self_attn.v_proj.weight'
ğŸŸ¦ ãƒãƒƒãƒ”ãƒ³ã‚°ã«ã‚ˆã‚‹é‡ã¿ã‚’èª­ã¿è¾¼ã¿ weight_name='model.layers.20.self_attn.v_proj.weight' k='v_proj'
ğŸŸ© QKVParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿é–‹å§‹ param.shape=torch.Size([4096, 1024]), loaded_weight.shape=torch.Size([1024, 1024]), loaded_shard_id='v'
ğŸŸ© QKVParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿å®Œäº†
ğŸŸ¦ é‡ã¿ã‚’èª­ã¿è¾¼ã¿ä¸­ weight_name='model.layers.21.input_layernorm.weight'
ğŸŸ¦ é€šå¸¸ã®é‡ã¿èª­ã¿è¾¼ã¿ weight_name='model.layers.21.input_layernorm.weight'
ğŸŸ© é‡ã¿ã‚’èª­ã¿è¾¼ã¿ param.shape=torch.Size([1024]) loaded_weight.shape=torch.Size([1024])
ğŸŸ¦ é‡ã¿ã‚’èª­ã¿è¾¼ã¿ä¸­ weight_name='model.layers.21.mlp.down_proj.weight'
ğŸŸ¦ é€šå¸¸ã®é‡ã¿èª­ã¿è¾¼ã¿ weight_name='model.layers.21.mlp.down_proj.weight'
ğŸŸ© RowParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿é–‹å§‹ param.shape=torch.Size([1024, 3072]), loaded_weight.shape=torch.Size([1024, 3072])
ğŸŸ© RowParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿å®Œäº†
ğŸŸ¦ é‡ã¿ã‚’èª­ã¿è¾¼ã¿ä¸­ weight_name='model.layers.21.mlp.gate_proj.weight'
ğŸŸ¦ ãƒãƒƒãƒ”ãƒ³ã‚°ã«ã‚ˆã‚‹é‡ã¿ã‚’èª­ã¿è¾¼ã¿ weight_name='model.layers.21.mlp.gate_proj.weight' k='gate_proj'
ğŸŸ© MergedColumnParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿é–‹å§‹ param.shape=torch.Size([6144, 1024]), loaded_weight.shape=torch.Size([3072, 1024]), loaded_shard_id=0
ğŸŸ© MergedColumnParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿å®Œäº†
ğŸŸ¦ é‡ã¿ã‚’èª­ã¿è¾¼ã¿ä¸­ weight_name='model.layers.21.mlp.up_proj.weight'
ğŸŸ¦ ãƒãƒƒãƒ”ãƒ³ã‚°ã«ã‚ˆã‚‹é‡ã¿ã‚’èª­ã¿è¾¼ã¿ weight_name='model.layers.21.mlp.up_proj.weight' k='up_proj'
ğŸŸ© MergedColumnParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿é–‹å§‹ param.shape=torch.Size([6144, 1024]), loaded_weight.shape=torch.Size([3072, 1024]), loaded_shard_id=1
ğŸŸ© MergedColumnParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿å®Œäº†
ğŸŸ¦ é‡ã¿ã‚’èª­ã¿è¾¼ã¿ä¸­ weight_name='model.layers.21.post_attention_layernorm.weight'
ğŸŸ¦ é€šå¸¸ã®é‡ã¿èª­ã¿è¾¼ã¿ weight_name='model.layers.21.post_attention_layernorm.weight'
ğŸŸ© é‡ã¿ã‚’èª­ã¿è¾¼ã¿ param.shape=torch.Size([1024]) loaded_weight.shape=torch.Size([1024])
ğŸŸ¦ é‡ã¿ã‚’èª­ã¿è¾¼ã¿ä¸­ weight_name='model.layers.21.self_attn.k_norm.weight'
ğŸŸ¦ é€šå¸¸ã®é‡ã¿èª­ã¿è¾¼ã¿ weight_name='model.layers.21.self_attn.k_norm.weight'
ğŸŸ© é‡ã¿ã‚’èª­ã¿è¾¼ã¿ param.shape=torch.Size([128]) loaded_weight.shape=torch.Size([128])
ğŸŸ¦ é‡ã¿ã‚’èª­ã¿è¾¼ã¿ä¸­ weight_name='model.layers.21.self_attn.k_proj.weight'
ğŸŸ¦ ãƒãƒƒãƒ”ãƒ³ã‚°ã«ã‚ˆã‚‹é‡ã¿ã‚’èª­ã¿è¾¼ã¿ weight_name='model.layers.21.self_attn.k_proj.weight' k='k_proj'
ğŸŸ© QKVParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿é–‹å§‹ param.shape=torch.Size([4096, 1024]), loaded_weight.shape=torch.Size([1024, 1024]), loaded_shard_id='k'
ğŸŸ© QKVParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿å®Œäº†
ğŸŸ¦ é‡ã¿ã‚’èª­ã¿è¾¼ã¿ä¸­ weight_name='model.layers.21.self_attn.o_proj.weight'
ğŸŸ¦ é€šå¸¸ã®é‡ã¿èª­ã¿è¾¼ã¿ weight_name='model.layers.21.self_attn.o_proj.weight'
ğŸŸ© RowParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿é–‹å§‹ param.shape=torch.Size([1024, 2048]), loaded_weight.shape=torch.Size([1024, 2048])
ğŸŸ© RowParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿å®Œäº†
ğŸŸ¦ é‡ã¿ã‚’èª­ã¿è¾¼ã¿ä¸­ weight_name='model.layers.21.self_attn.q_norm.weight'
ğŸŸ¦ é€šå¸¸ã®é‡ã¿èª­ã¿è¾¼ã¿ weight_name='model.layers.21.self_attn.q_norm.weight'
ğŸŸ© é‡ã¿ã‚’èª­ã¿è¾¼ã¿ param.shape=torch.Size([128]) loaded_weight.shape=torch.Size([128])
ğŸŸ¦ é‡ã¿ã‚’èª­ã¿è¾¼ã¿ä¸­ weight_name='model.layers.21.self_attn.q_proj.weight'
ğŸŸ¦ ãƒãƒƒãƒ”ãƒ³ã‚°ã«ã‚ˆã‚‹é‡ã¿ã‚’èª­ã¿è¾¼ã¿ weight_name='model.layers.21.self_attn.q_proj.weight' k='q_proj'
ğŸŸ© QKVParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿é–‹å§‹ param.shape=torch.Size([4096, 1024]), loaded_weight.shape=torch.Size([2048, 1024]), loaded_shard_id='q'
ğŸŸ© QKVParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿å®Œäº†
ğŸŸ¦ é‡ã¿ã‚’èª­ã¿è¾¼ã¿ä¸­ weight_name='model.layers.21.self_attn.v_proj.weight'
ğŸŸ¦ ãƒãƒƒãƒ”ãƒ³ã‚°ã«ã‚ˆã‚‹é‡ã¿ã‚’èª­ã¿è¾¼ã¿ weight_name='model.layers.21.self_attn.v_proj.weight' k='v_proj'
ğŸŸ© QKVParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿é–‹å§‹ param.shape=torch.Size([4096, 1024]), loaded_weight.shape=torch.Size([1024, 1024]), loaded_shard_id='v'
ğŸŸ© QKVParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿å®Œäº†
ğŸŸ¦ é‡ã¿ã‚’èª­ã¿è¾¼ã¿ä¸­ weight_name='model.layers.22.input_layernorm.weight'
ğŸŸ¦ é€šå¸¸ã®é‡ã¿èª­ã¿è¾¼ã¿ weight_name='model.layers.22.input_layernorm.weight'
ğŸŸ© é‡ã¿ã‚’èª­ã¿è¾¼ã¿ param.shape=torch.Size([1024]) loaded_weight.shape=torch.Size([1024])
ğŸŸ¦ é‡ã¿ã‚’èª­ã¿è¾¼ã¿ä¸­ weight_name='model.layers.22.mlp.down_proj.weight'
ğŸŸ¦ é€šå¸¸ã®é‡ã¿èª­ã¿è¾¼ã¿ weight_name='model.layers.22.mlp.down_proj.weight'
ğŸŸ© RowParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿é–‹å§‹ param.shape=torch.Size([1024, 3072]), loaded_weight.shape=torch.Size([1024, 3072])
ğŸŸ© RowParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿å®Œäº†
ğŸŸ¦ é‡ã¿ã‚’èª­ã¿è¾¼ã¿ä¸­ weight_name='model.layers.22.mlp.gate_proj.weight'
ğŸŸ¦ ãƒãƒƒãƒ”ãƒ³ã‚°ã«ã‚ˆã‚‹é‡ã¿ã‚’èª­ã¿è¾¼ã¿ weight_name='model.layers.22.mlp.gate_proj.weight' k='gate_proj'
ğŸŸ© MergedColumnParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿é–‹å§‹ param.shape=torch.Size([6144, 1024]), loaded_weight.shape=torch.Size([3072, 1024]), loaded_shard_id=0
ğŸŸ© MergedColumnParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿å®Œäº†
ğŸŸ¦ é‡ã¿ã‚’èª­ã¿è¾¼ã¿ä¸­ weight_name='model.layers.22.mlp.up_proj.weight'
ğŸŸ¦ ãƒãƒƒãƒ”ãƒ³ã‚°ã«ã‚ˆã‚‹é‡ã¿ã‚’èª­ã¿è¾¼ã¿ weight_name='model.layers.22.mlp.up_proj.weight' k='up_proj'
ğŸŸ© MergedColumnParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿é–‹å§‹ param.shape=torch.Size([6144, 1024]), loaded_weight.shape=torch.Size([3072, 1024]), loaded_shard_id=1
ğŸŸ© MergedColumnParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿å®Œäº†
ğŸŸ¦ é‡ã¿ã‚’èª­ã¿è¾¼ã¿ä¸­ weight_name='model.layers.22.post_attention_layernorm.weight'
ğŸŸ¦ é€šå¸¸ã®é‡ã¿èª­ã¿è¾¼ã¿ weight_name='model.layers.22.post_attention_layernorm.weight'
ğŸŸ© é‡ã¿ã‚’èª­ã¿è¾¼ã¿ param.shape=torch.Size([1024]) loaded_weight.shape=torch.Size([1024])
ğŸŸ¦ é‡ã¿ã‚’èª­ã¿è¾¼ã¿ä¸­ weight_name='model.layers.22.self_attn.k_norm.weight'
ğŸŸ¦ é€šå¸¸ã®é‡ã¿èª­ã¿è¾¼ã¿ weight_name='model.layers.22.self_attn.k_norm.weight'
ğŸŸ© é‡ã¿ã‚’èª­ã¿è¾¼ã¿ param.shape=torch.Size([128]) loaded_weight.shape=torch.Size([128])
ğŸŸ¦ é‡ã¿ã‚’èª­ã¿è¾¼ã¿ä¸­ weight_name='model.layers.22.self_attn.k_proj.weight'
ğŸŸ¦ ãƒãƒƒãƒ”ãƒ³ã‚°ã«ã‚ˆã‚‹é‡ã¿ã‚’èª­ã¿è¾¼ã¿ weight_name='model.layers.22.self_attn.k_proj.weight' k='k_proj'
ğŸŸ© QKVParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿é–‹å§‹ param.shape=torch.Size([4096, 1024]), loaded_weight.shape=torch.Size([1024, 1024]), loaded_shard_id='k'
ğŸŸ© QKVParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿å®Œäº†
ğŸŸ¦ é‡ã¿ã‚’èª­ã¿è¾¼ã¿ä¸­ weight_name='model.layers.22.self_attn.o_proj.weight'
ğŸŸ¦ é€šå¸¸ã®é‡ã¿èª­ã¿è¾¼ã¿ weight_name='model.layers.22.self_attn.o_proj.weight'
ğŸŸ© RowParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿é–‹å§‹ param.shape=torch.Size([1024, 2048]), loaded_weight.shape=torch.Size([1024, 2048])
ğŸŸ© RowParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿å®Œäº†
ğŸŸ¦ é‡ã¿ã‚’èª­ã¿è¾¼ã¿ä¸­ weight_name='model.layers.22.self_attn.q_norm.weight'
ğŸŸ¦ é€šå¸¸ã®é‡ã¿èª­ã¿è¾¼ã¿ weight_name='model.layers.22.self_attn.q_norm.weight'
ğŸŸ© é‡ã¿ã‚’èª­ã¿è¾¼ã¿ param.shape=torch.Size([128]) loaded_weight.shape=torch.Size([128])
ğŸŸ¦ é‡ã¿ã‚’èª­ã¿è¾¼ã¿ä¸­ weight_name='model.layers.22.self_attn.q_proj.weight'
ğŸŸ¦ ãƒãƒƒãƒ”ãƒ³ã‚°ã«ã‚ˆã‚‹é‡ã¿ã‚’èª­ã¿è¾¼ã¿ weight_name='model.layers.22.self_attn.q_proj.weight' k='q_proj'
ğŸŸ© QKVParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿é–‹å§‹ param.shape=torch.Size([4096, 1024]), loaded_weight.shape=torch.Size([2048, 1024]), loaded_shard_id='q'
ğŸŸ© QKVParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿å®Œäº†
ğŸŸ¦ é‡ã¿ã‚’èª­ã¿è¾¼ã¿ä¸­ weight_name='model.layers.22.self_attn.v_proj.weight'
ğŸŸ¦ ãƒãƒƒãƒ”ãƒ³ã‚°ã«ã‚ˆã‚‹é‡ã¿ã‚’èª­ã¿è¾¼ã¿ weight_name='model.layers.22.self_attn.v_proj.weight' k='v_proj'
ğŸŸ© QKVParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿é–‹å§‹ param.shape=torch.Size([4096, 1024]), loaded_weight.shape=torch.Size([1024, 1024]), loaded_shard_id='v'
ğŸŸ© QKVParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿å®Œäº†
ğŸŸ¦ é‡ã¿ã‚’èª­ã¿è¾¼ã¿ä¸­ weight_name='model.layers.23.input_layernorm.weight'
ğŸŸ¦ é€šå¸¸ã®é‡ã¿èª­ã¿è¾¼ã¿ weight_name='model.layers.23.input_layernorm.weight'
ğŸŸ© é‡ã¿ã‚’èª­ã¿è¾¼ã¿ param.shape=torch.Size([1024]) loaded_weight.shape=torch.Size([1024])
ğŸŸ¦ é‡ã¿ã‚’èª­ã¿è¾¼ã¿ä¸­ weight_name='model.layers.23.mlp.down_proj.weight'
ğŸŸ¦ é€šå¸¸ã®é‡ã¿èª­ã¿è¾¼ã¿ weight_name='model.layers.23.mlp.down_proj.weight'
ğŸŸ© RowParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿é–‹å§‹ param.shape=torch.Size([1024, 3072]), loaded_weight.shape=torch.Size([1024, 3072])
ğŸŸ© RowParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿å®Œäº†
ğŸŸ¦ é‡ã¿ã‚’èª­ã¿è¾¼ã¿ä¸­ weight_name='model.layers.23.mlp.gate_proj.weight'
ğŸŸ¦ ãƒãƒƒãƒ”ãƒ³ã‚°ã«ã‚ˆã‚‹é‡ã¿ã‚’èª­ã¿è¾¼ã¿ weight_name='model.layers.23.mlp.gate_proj.weight' k='gate_proj'
ğŸŸ© MergedColumnParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿é–‹å§‹ param.shape=torch.Size([6144, 1024]), loaded_weight.shape=torch.Size([3072, 1024]), loaded_shard_id=0
ğŸŸ© MergedColumnParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿å®Œäº†
ğŸŸ¦ é‡ã¿ã‚’èª­ã¿è¾¼ã¿ä¸­ weight_name='model.layers.23.mlp.up_proj.weight'
ğŸŸ¦ ãƒãƒƒãƒ”ãƒ³ã‚°ã«ã‚ˆã‚‹é‡ã¿ã‚’èª­ã¿è¾¼ã¿ weight_name='model.layers.23.mlp.up_proj.weight' k='up_proj'
ğŸŸ© MergedColumnParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿é–‹å§‹ param.shape=torch.Size([6144, 1024]), loaded_weight.shape=torch.Size([3072, 1024]), loaded_shard_id=1
ğŸŸ© MergedColumnParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿å®Œäº†
ğŸŸ¦ é‡ã¿ã‚’èª­ã¿è¾¼ã¿ä¸­ weight_name='model.layers.23.post_attention_layernorm.weight'
ğŸŸ¦ é€šå¸¸ã®é‡ã¿èª­ã¿è¾¼ã¿ weight_name='model.layers.23.post_attention_layernorm.weight'
ğŸŸ© é‡ã¿ã‚’èª­ã¿è¾¼ã¿ param.shape=torch.Size([1024]) loaded_weight.shape=torch.Size([1024])
ğŸŸ¦ é‡ã¿ã‚’èª­ã¿è¾¼ã¿ä¸­ weight_name='model.layers.23.self_attn.k_norm.weight'
ğŸŸ¦ é€šå¸¸ã®é‡ã¿èª­ã¿è¾¼ã¿ weight_name='model.layers.23.self_attn.k_norm.weight'
ğŸŸ© é‡ã¿ã‚’èª­ã¿è¾¼ã¿ param.shape=torch.Size([128]) loaded_weight.shape=torch.Size([128])
ğŸŸ¦ é‡ã¿ã‚’èª­ã¿è¾¼ã¿ä¸­ weight_name='model.layers.23.self_attn.k_proj.weight'
ğŸŸ¦ ãƒãƒƒãƒ”ãƒ³ã‚°ã«ã‚ˆã‚‹é‡ã¿ã‚’èª­ã¿è¾¼ã¿ weight_name='model.layers.23.self_attn.k_proj.weight' k='k_proj'
ğŸŸ© QKVParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿é–‹å§‹ param.shape=torch.Size([4096, 1024]), loaded_weight.shape=torch.Size([1024, 1024]), loaded_shard_id='k'
ğŸŸ© QKVParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿å®Œäº†
ğŸŸ¦ é‡ã¿ã‚’èª­ã¿è¾¼ã¿ä¸­ weight_name='model.layers.23.self_attn.o_proj.weight'
ğŸŸ¦ é€šå¸¸ã®é‡ã¿èª­ã¿è¾¼ã¿ weight_name='model.layers.23.self_attn.o_proj.weight'
ğŸŸ© RowParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿é–‹å§‹ param.shape=torch.Size([1024, 2048]), loaded_weight.shape=torch.Size([1024, 2048])
ğŸŸ© RowParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿å®Œäº†
ğŸŸ¦ é‡ã¿ã‚’èª­ã¿è¾¼ã¿ä¸­ weight_name='model.layers.23.self_attn.q_norm.weight'
ğŸŸ¦ é€šå¸¸ã®é‡ã¿èª­ã¿è¾¼ã¿ weight_name='model.layers.23.self_attn.q_norm.weight'
ğŸŸ© é‡ã¿ã‚’èª­ã¿è¾¼ã¿ param.shape=torch.Size([128]) loaded_weight.shape=torch.Size([128])
ğŸŸ¦ é‡ã¿ã‚’èª­ã¿è¾¼ã¿ä¸­ weight_name='model.layers.23.self_attn.q_proj.weight'
ğŸŸ¦ ãƒãƒƒãƒ”ãƒ³ã‚°ã«ã‚ˆã‚‹é‡ã¿ã‚’èª­ã¿è¾¼ã¿ weight_name='model.layers.23.self_attn.q_proj.weight' k='q_proj'
ğŸŸ© QKVParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿é–‹å§‹ param.shape=torch.Size([4096, 1024]), loaded_weight.shape=torch.Size([2048, 1024]), loaded_shard_id='q'
ğŸŸ© QKVParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿å®Œäº†
ğŸŸ¦ é‡ã¿ã‚’èª­ã¿è¾¼ã¿ä¸­ weight_name='model.layers.23.self_attn.v_proj.weight'
ğŸŸ¦ ãƒãƒƒãƒ”ãƒ³ã‚°ã«ã‚ˆã‚‹é‡ã¿ã‚’èª­ã¿è¾¼ã¿ weight_name='model.layers.23.self_attn.v_proj.weight' k='v_proj'
ğŸŸ© QKVParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿é–‹å§‹ param.shape=torch.Size([4096, 1024]), loaded_weight.shape=torch.Size([1024, 1024]), loaded_shard_id='v'
ğŸŸ© QKVParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿å®Œäº†
ğŸŸ¦ é‡ã¿ã‚’èª­ã¿è¾¼ã¿ä¸­ weight_name='model.layers.24.input_layernorm.weight'
ğŸŸ¦ é€šå¸¸ã®é‡ã¿èª­ã¿è¾¼ã¿ weight_name='model.layers.24.input_layernorm.weight'
ğŸŸ© é‡ã¿ã‚’èª­ã¿è¾¼ã¿ param.shape=torch.Size([1024]) loaded_weight.shape=torch.Size([1024])
ğŸŸ¦ é‡ã¿ã‚’èª­ã¿è¾¼ã¿ä¸­ weight_name='model.layers.24.mlp.down_proj.weight'
ğŸŸ¦ é€šå¸¸ã®é‡ã¿èª­ã¿è¾¼ã¿ weight_name='model.layers.24.mlp.down_proj.weight'
ğŸŸ© RowParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿é–‹å§‹ param.shape=torch.Size([1024, 3072]), loaded_weight.shape=torch.Size([1024, 3072])
ğŸŸ© RowParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿å®Œäº†
ğŸŸ¦ é‡ã¿ã‚’èª­ã¿è¾¼ã¿ä¸­ weight_name='model.layers.24.mlp.gate_proj.weight'
ğŸŸ¦ ãƒãƒƒãƒ”ãƒ³ã‚°ã«ã‚ˆã‚‹é‡ã¿ã‚’èª­ã¿è¾¼ã¿ weight_name='model.layers.24.mlp.gate_proj.weight' k='gate_proj'
ğŸŸ© MergedColumnParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿é–‹å§‹ param.shape=torch.Size([6144, 1024]), loaded_weight.shape=torch.Size([3072, 1024]), loaded_shard_id=0
ğŸŸ© MergedColumnParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿å®Œäº†
ğŸŸ¦ é‡ã¿ã‚’èª­ã¿è¾¼ã¿ä¸­ weight_name='model.layers.24.mlp.up_proj.weight'
ğŸŸ¦ ãƒãƒƒãƒ”ãƒ³ã‚°ã«ã‚ˆã‚‹é‡ã¿ã‚’èª­ã¿è¾¼ã¿ weight_name='model.layers.24.mlp.up_proj.weight' k='up_proj'
ğŸŸ© MergedColumnParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿é–‹å§‹ param.shape=torch.Size([6144, 1024]), loaded_weight.shape=torch.Size([3072, 1024]), loaded_shard_id=1
ğŸŸ© MergedColumnParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿å®Œäº†
ğŸŸ¦ é‡ã¿ã‚’èª­ã¿è¾¼ã¿ä¸­ weight_name='model.layers.24.post_attention_layernorm.weight'
ğŸŸ¦ é€šå¸¸ã®é‡ã¿èª­ã¿è¾¼ã¿ weight_name='model.layers.24.post_attention_layernorm.weight'
ğŸŸ© é‡ã¿ã‚’èª­ã¿è¾¼ã¿ param.shape=torch.Size([1024]) loaded_weight.shape=torch.Size([1024])
ğŸŸ¦ é‡ã¿ã‚’èª­ã¿è¾¼ã¿ä¸­ weight_name='model.layers.24.self_attn.k_norm.weight'
ğŸŸ¦ é€šå¸¸ã®é‡ã¿èª­ã¿è¾¼ã¿ weight_name='model.layers.24.self_attn.k_norm.weight'
ğŸŸ© é‡ã¿ã‚’èª­ã¿è¾¼ã¿ param.shape=torch.Size([128]) loaded_weight.shape=torch.Size([128])
ğŸŸ¦ é‡ã¿ã‚’èª­ã¿è¾¼ã¿ä¸­ weight_name='model.layers.24.self_attn.k_proj.weight'
ğŸŸ¦ ãƒãƒƒãƒ”ãƒ³ã‚°ã«ã‚ˆã‚‹é‡ã¿ã‚’èª­ã¿è¾¼ã¿ weight_name='model.layers.24.self_attn.k_proj.weight' k='k_proj'
ğŸŸ© QKVParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿é–‹å§‹ param.shape=torch.Size([4096, 1024]), loaded_weight.shape=torch.Size([1024, 1024]), loaded_shard_id='k'
ğŸŸ© QKVParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿å®Œäº†
ğŸŸ¦ é‡ã¿ã‚’èª­ã¿è¾¼ã¿ä¸­ weight_name='model.layers.24.self_attn.o_proj.weight'
ğŸŸ¦ é€šå¸¸ã®é‡ã¿èª­ã¿è¾¼ã¿ weight_name='model.layers.24.self_attn.o_proj.weight'
ğŸŸ© RowParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿é–‹å§‹ param.shape=torch.Size([1024, 2048]), loaded_weight.shape=torch.Size([1024, 2048])
ğŸŸ© RowParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿å®Œäº†
ğŸŸ¦ é‡ã¿ã‚’èª­ã¿è¾¼ã¿ä¸­ weight_name='model.layers.24.self_attn.q_norm.weight'
ğŸŸ¦ é€šå¸¸ã®é‡ã¿èª­ã¿è¾¼ã¿ weight_name='model.layers.24.self_attn.q_norm.weight'
ğŸŸ© é‡ã¿ã‚’èª­ã¿è¾¼ã¿ param.shape=torch.Size([128]) loaded_weight.shape=torch.Size([128])
ğŸŸ¦ é‡ã¿ã‚’èª­ã¿è¾¼ã¿ä¸­ weight_name='model.layers.24.self_attn.q_proj.weight'
ğŸŸ¦ ãƒãƒƒãƒ”ãƒ³ã‚°ã«ã‚ˆã‚‹é‡ã¿ã‚’èª­ã¿è¾¼ã¿ weight_name='model.layers.24.self_attn.q_proj.weight' k='q_proj'
ğŸŸ© QKVParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿é–‹å§‹ param.shape=torch.Size([4096, 1024]), loaded_weight.shape=torch.Size([2048, 1024]), loaded_shard_id='q'
ğŸŸ© QKVParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿å®Œäº†
ğŸŸ¦ é‡ã¿ã‚’èª­ã¿è¾¼ã¿ä¸­ weight_name='model.layers.24.self_attn.v_proj.weight'
ğŸŸ¦ ãƒãƒƒãƒ”ãƒ³ã‚°ã«ã‚ˆã‚‹é‡ã¿ã‚’èª­ã¿è¾¼ã¿ weight_name='model.layers.24.self_attn.v_proj.weight' k='v_proj'
ğŸŸ© QKVParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿é–‹å§‹ param.shape=torch.Size([4096, 1024]), loaded_weight.shape=torch.Size([1024, 1024]), loaded_shard_id='v'
ğŸŸ© QKVParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿å®Œäº†
ğŸŸ¦ é‡ã¿ã‚’èª­ã¿è¾¼ã¿ä¸­ weight_name='model.layers.25.input_layernorm.weight'
ğŸŸ¦ é€šå¸¸ã®é‡ã¿èª­ã¿è¾¼ã¿ weight_name='model.layers.25.input_layernorm.weight'
ğŸŸ© é‡ã¿ã‚’èª­ã¿è¾¼ã¿ param.shape=torch.Size([1024]) loaded_weight.shape=torch.Size([1024])
ğŸŸ¦ é‡ã¿ã‚’èª­ã¿è¾¼ã¿ä¸­ weight_name='model.layers.25.mlp.down_proj.weight'
ğŸŸ¦ é€šå¸¸ã®é‡ã¿èª­ã¿è¾¼ã¿ weight_name='model.layers.25.mlp.down_proj.weight'
ğŸŸ© RowParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿é–‹å§‹ param.shape=torch.Size([1024, 3072]), loaded_weight.shape=torch.Size([1024, 3072])
ğŸŸ© RowParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿å®Œäº†
ğŸŸ¦ é‡ã¿ã‚’èª­ã¿è¾¼ã¿ä¸­ weight_name='model.layers.25.mlp.gate_proj.weight'
ğŸŸ¦ ãƒãƒƒãƒ”ãƒ³ã‚°ã«ã‚ˆã‚‹é‡ã¿ã‚’èª­ã¿è¾¼ã¿ weight_name='model.layers.25.mlp.gate_proj.weight' k='gate_proj'
ğŸŸ© MergedColumnParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿é–‹å§‹ param.shape=torch.Size([6144, 1024]), loaded_weight.shape=torch.Size([3072, 1024]), loaded_shard_id=0
ğŸŸ© MergedColumnParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿å®Œäº†
ğŸŸ¦ é‡ã¿ã‚’èª­ã¿è¾¼ã¿ä¸­ weight_name='model.layers.25.mlp.up_proj.weight'
ğŸŸ¦ ãƒãƒƒãƒ”ãƒ³ã‚°ã«ã‚ˆã‚‹é‡ã¿ã‚’èª­ã¿è¾¼ã¿ weight_name='model.layers.25.mlp.up_proj.weight' k='up_proj'
ğŸŸ© MergedColumnParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿é–‹å§‹ param.shape=torch.Size([6144, 1024]), loaded_weight.shape=torch.Size([3072, 1024]), loaded_shard_id=1
ğŸŸ© MergedColumnParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿å®Œäº†
ğŸŸ¦ é‡ã¿ã‚’èª­ã¿è¾¼ã¿ä¸­ weight_name='model.layers.25.post_attention_layernorm.weight'
ğŸŸ¦ é€šå¸¸ã®é‡ã¿èª­ã¿è¾¼ã¿ weight_name='model.layers.25.post_attention_layernorm.weight'
ğŸŸ© é‡ã¿ã‚’èª­ã¿è¾¼ã¿ param.shape=torch.Size([1024]) loaded_weight.shape=torch.Size([1024])
ğŸŸ¦ é‡ã¿ã‚’èª­ã¿è¾¼ã¿ä¸­ weight_name='model.layers.25.self_attn.k_norm.weight'
ğŸŸ¦ é€šå¸¸ã®é‡ã¿èª­ã¿è¾¼ã¿ weight_name='model.layers.25.self_attn.k_norm.weight'
ğŸŸ© é‡ã¿ã‚’èª­ã¿è¾¼ã¿ param.shape=torch.Size([128]) loaded_weight.shape=torch.Size([128])
ğŸŸ¦ é‡ã¿ã‚’èª­ã¿è¾¼ã¿ä¸­ weight_name='model.layers.25.self_attn.k_proj.weight'
ğŸŸ¦ ãƒãƒƒãƒ”ãƒ³ã‚°ã«ã‚ˆã‚‹é‡ã¿ã‚’èª­ã¿è¾¼ã¿ weight_name='model.layers.25.self_attn.k_proj.weight' k='k_proj'
ğŸŸ© QKVParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿é–‹å§‹ param.shape=torch.Size([4096, 1024]), loaded_weight.shape=torch.Size([1024, 1024]), loaded_shard_id='k'
ğŸŸ© QKVParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿å®Œäº†
ğŸŸ¦ é‡ã¿ã‚’èª­ã¿è¾¼ã¿ä¸­ weight_name='model.layers.25.self_attn.o_proj.weight'
ğŸŸ¦ é€šå¸¸ã®é‡ã¿èª­ã¿è¾¼ã¿ weight_name='model.layers.25.self_attn.o_proj.weight'
ğŸŸ© RowParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿é–‹å§‹ param.shape=torch.Size([1024, 2048]), loaded_weight.shape=torch.Size([1024, 2048])
ğŸŸ© RowParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿å®Œäº†
ğŸŸ¦ é‡ã¿ã‚’èª­ã¿è¾¼ã¿ä¸­ weight_name='model.layers.25.self_attn.q_norm.weight'
ğŸŸ¦ é€šå¸¸ã®é‡ã¿èª­ã¿è¾¼ã¿ weight_name='model.layers.25.self_attn.q_norm.weight'
ğŸŸ© é‡ã¿ã‚’èª­ã¿è¾¼ã¿ param.shape=torch.Size([128]) loaded_weight.shape=torch.Size([128])
ğŸŸ¦ é‡ã¿ã‚’èª­ã¿è¾¼ã¿ä¸­ weight_name='model.layers.25.self_attn.q_proj.weight'
ğŸŸ¦ ãƒãƒƒãƒ”ãƒ³ã‚°ã«ã‚ˆã‚‹é‡ã¿ã‚’èª­ã¿è¾¼ã¿ weight_name='model.layers.25.self_attn.q_proj.weight' k='q_proj'
ğŸŸ© QKVParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿é–‹å§‹ param.shape=torch.Size([4096, 1024]), loaded_weight.shape=torch.Size([2048, 1024]), loaded_shard_id='q'
ğŸŸ© QKVParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿å®Œäº†
ğŸŸ¦ é‡ã¿ã‚’èª­ã¿è¾¼ã¿ä¸­ weight_name='model.layers.25.self_attn.v_proj.weight'
ğŸŸ¦ ãƒãƒƒãƒ”ãƒ³ã‚°ã«ã‚ˆã‚‹é‡ã¿ã‚’èª­ã¿è¾¼ã¿ weight_name='model.layers.25.self_attn.v_proj.weight' k='v_proj'
ğŸŸ© QKVParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿é–‹å§‹ param.shape=torch.Size([4096, 1024]), loaded_weight.shape=torch.Size([1024, 1024]), loaded_shard_id='v'
ğŸŸ© QKVParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿å®Œäº†
ğŸŸ¦ é‡ã¿ã‚’èª­ã¿è¾¼ã¿ä¸­ weight_name='model.layers.26.input_layernorm.weight'
ğŸŸ¦ é€šå¸¸ã®é‡ã¿èª­ã¿è¾¼ã¿ weight_name='model.layers.26.input_layernorm.weight'
ğŸŸ© é‡ã¿ã‚’èª­ã¿è¾¼ã¿ param.shape=torch.Size([1024]) loaded_weight.shape=torch.Size([1024])
ğŸŸ¦ é‡ã¿ã‚’èª­ã¿è¾¼ã¿ä¸­ weight_name='model.layers.26.mlp.down_proj.weight'
ğŸŸ¦ é€šå¸¸ã®é‡ã¿èª­ã¿è¾¼ã¿ weight_name='model.layers.26.mlp.down_proj.weight'
ğŸŸ© RowParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿é–‹å§‹ param.shape=torch.Size([1024, 3072]), loaded_weight.shape=torch.Size([1024, 3072])
ğŸŸ© RowParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿å®Œäº†
ğŸŸ¦ é‡ã¿ã‚’èª­ã¿è¾¼ã¿ä¸­ weight_name='model.layers.26.mlp.gate_proj.weight'
ğŸŸ¦ ãƒãƒƒãƒ”ãƒ³ã‚°ã«ã‚ˆã‚‹é‡ã¿ã‚’èª­ã¿è¾¼ã¿ weight_name='model.layers.26.mlp.gate_proj.weight' k='gate_proj'
ğŸŸ© MergedColumnParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿é–‹å§‹ param.shape=torch.Size([6144, 1024]), loaded_weight.shape=torch.Size([3072, 1024]), loaded_shard_id=0
ğŸŸ© MergedColumnParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿å®Œäº†
ğŸŸ¦ é‡ã¿ã‚’èª­ã¿è¾¼ã¿ä¸­ weight_name='model.layers.26.mlp.up_proj.weight'
ğŸŸ¦ ãƒãƒƒãƒ”ãƒ³ã‚°ã«ã‚ˆã‚‹é‡ã¿ã‚’èª­ã¿è¾¼ã¿ weight_name='model.layers.26.mlp.up_proj.weight' k='up_proj'
ğŸŸ© MergedColumnParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿é–‹å§‹ param.shape=torch.Size([6144, 1024]), loaded_weight.shape=torch.Size([3072, 1024]), loaded_shard_id=1
ğŸŸ© MergedColumnParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿å®Œäº†
ğŸŸ¦ é‡ã¿ã‚’èª­ã¿è¾¼ã¿ä¸­ weight_name='model.layers.26.post_attention_layernorm.weight'
ğŸŸ¦ é€šå¸¸ã®é‡ã¿èª­ã¿è¾¼ã¿ weight_name='model.layers.26.post_attention_layernorm.weight'
ğŸŸ© é‡ã¿ã‚’èª­ã¿è¾¼ã¿ param.shape=torch.Size([1024]) loaded_weight.shape=torch.Size([1024])
ğŸŸ¦ é‡ã¿ã‚’èª­ã¿è¾¼ã¿ä¸­ weight_name='model.layers.26.self_attn.k_norm.weight'
ğŸŸ¦ é€šå¸¸ã®é‡ã¿èª­ã¿è¾¼ã¿ weight_name='model.layers.26.self_attn.k_norm.weight'
ğŸŸ© é‡ã¿ã‚’èª­ã¿è¾¼ã¿ param.shape=torch.Size([128]) loaded_weight.shape=torch.Size([128])
ğŸŸ¦ é‡ã¿ã‚’èª­ã¿è¾¼ã¿ä¸­ weight_name='model.layers.26.self_attn.k_proj.weight'
ğŸŸ¦ ãƒãƒƒãƒ”ãƒ³ã‚°ã«ã‚ˆã‚‹é‡ã¿ã‚’èª­ã¿è¾¼ã¿ weight_name='model.layers.26.self_attn.k_proj.weight' k='k_proj'
ğŸŸ© QKVParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿é–‹å§‹ param.shape=torch.Size([4096, 1024]), loaded_weight.shape=torch.Size([1024, 1024]), loaded_shard_id='k'
ğŸŸ© QKVParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿å®Œäº†
ğŸŸ¦ é‡ã¿ã‚’èª­ã¿è¾¼ã¿ä¸­ weight_name='model.layers.26.self_attn.o_proj.weight'
ğŸŸ¦ é€šå¸¸ã®é‡ã¿èª­ã¿è¾¼ã¿ weight_name='model.layers.26.self_attn.o_proj.weight'
ğŸŸ© RowParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿é–‹å§‹ param.shape=torch.Size([1024, 2048]), loaded_weight.shape=torch.Size([1024, 2048])
ğŸŸ© RowParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿å®Œäº†
ğŸŸ¦ é‡ã¿ã‚’èª­ã¿è¾¼ã¿ä¸­ weight_name='model.layers.26.self_attn.q_norm.weight'
ğŸŸ¦ é€šå¸¸ã®é‡ã¿èª­ã¿è¾¼ã¿ weight_name='model.layers.26.self_attn.q_norm.weight'
ğŸŸ© é‡ã¿ã‚’èª­ã¿è¾¼ã¿ param.shape=torch.Size([128]) loaded_weight.shape=torch.Size([128])
ğŸŸ¦ é‡ã¿ã‚’èª­ã¿è¾¼ã¿ä¸­ weight_name='model.layers.26.self_attn.q_proj.weight'
ğŸŸ¦ ãƒãƒƒãƒ”ãƒ³ã‚°ã«ã‚ˆã‚‹é‡ã¿ã‚’èª­ã¿è¾¼ã¿ weight_name='model.layers.26.self_attn.q_proj.weight' k='q_proj'
ğŸŸ© QKVParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿é–‹å§‹ param.shape=torch.Size([4096, 1024]), loaded_weight.shape=torch.Size([2048, 1024]), loaded_shard_id='q'
ğŸŸ© QKVParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿å®Œäº†
ğŸŸ¦ é‡ã¿ã‚’èª­ã¿è¾¼ã¿ä¸­ weight_name='model.layers.26.self_attn.v_proj.weight'
ğŸŸ¦ ãƒãƒƒãƒ”ãƒ³ã‚°ã«ã‚ˆã‚‹é‡ã¿ã‚’èª­ã¿è¾¼ã¿ weight_name='model.layers.26.self_attn.v_proj.weight' k='v_proj'
ğŸŸ© QKVParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿é–‹å§‹ param.shape=torch.Size([4096, 1024]), loaded_weight.shape=torch.Size([1024, 1024]), loaded_shard_id='v'
ğŸŸ© QKVParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿å®Œäº†
ğŸŸ¦ é‡ã¿ã‚’èª­ã¿è¾¼ã¿ä¸­ weight_name='model.layers.27.input_layernorm.weight'
ğŸŸ¦ é€šå¸¸ã®é‡ã¿èª­ã¿è¾¼ã¿ weight_name='model.layers.27.input_layernorm.weight'
ğŸŸ© é‡ã¿ã‚’èª­ã¿è¾¼ã¿ param.shape=torch.Size([1024]) loaded_weight.shape=torch.Size([1024])
ğŸŸ¦ é‡ã¿ã‚’èª­ã¿è¾¼ã¿ä¸­ weight_name='model.layers.27.mlp.down_proj.weight'
ğŸŸ¦ é€šå¸¸ã®é‡ã¿èª­ã¿è¾¼ã¿ weight_name='model.layers.27.mlp.down_proj.weight'
ğŸŸ© RowParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿é–‹å§‹ param.shape=torch.Size([1024, 3072]), loaded_weight.shape=torch.Size([1024, 3072])
ğŸŸ© RowParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿å®Œäº†
ğŸŸ¦ é‡ã¿ã‚’èª­ã¿è¾¼ã¿ä¸­ weight_name='model.layers.27.mlp.gate_proj.weight'
ğŸŸ¦ ãƒãƒƒãƒ”ãƒ³ã‚°ã«ã‚ˆã‚‹é‡ã¿ã‚’èª­ã¿è¾¼ã¿ weight_name='model.layers.27.mlp.gate_proj.weight' k='gate_proj'
ğŸŸ© MergedColumnParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿é–‹å§‹ param.shape=torch.Size([6144, 1024]), loaded_weight.shape=torch.Size([3072, 1024]), loaded_shard_id=0
ğŸŸ© MergedColumnParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿å®Œäº†
ğŸŸ¦ é‡ã¿ã‚’èª­ã¿è¾¼ã¿ä¸­ weight_name='model.layers.27.mlp.up_proj.weight'
ğŸŸ¦ ãƒãƒƒãƒ”ãƒ³ã‚°ã«ã‚ˆã‚‹é‡ã¿ã‚’èª­ã¿è¾¼ã¿ weight_name='model.layers.27.mlp.up_proj.weight' k='up_proj'
ğŸŸ© MergedColumnParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿é–‹å§‹ param.shape=torch.Size([6144, 1024]), loaded_weight.shape=torch.Size([3072, 1024]), loaded_shard_id=1
ğŸŸ© MergedColumnParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿å®Œäº†
ğŸŸ¦ é‡ã¿ã‚’èª­ã¿è¾¼ã¿ä¸­ weight_name='model.layers.27.post_attention_layernorm.weight'
ğŸŸ¦ é€šå¸¸ã®é‡ã¿èª­ã¿è¾¼ã¿ weight_name='model.layers.27.post_attention_layernorm.weight'
ğŸŸ© é‡ã¿ã‚’èª­ã¿è¾¼ã¿ param.shape=torch.Size([1024]) loaded_weight.shape=torch.Size([1024])
ğŸŸ¦ é‡ã¿ã‚’èª­ã¿è¾¼ã¿ä¸­ weight_name='model.layers.27.self_attn.k_norm.weight'
ğŸŸ¦ é€šå¸¸ã®é‡ã¿èª­ã¿è¾¼ã¿ weight_name='model.layers.27.self_attn.k_norm.weight'
ğŸŸ© é‡ã¿ã‚’èª­ã¿è¾¼ã¿ param.shape=torch.Size([128]) loaded_weight.shape=torch.Size([128])
ğŸŸ¦ é‡ã¿ã‚’èª­ã¿è¾¼ã¿ä¸­ weight_name='model.layers.27.self_attn.k_proj.weight'
ğŸŸ¦ ãƒãƒƒãƒ”ãƒ³ã‚°ã«ã‚ˆã‚‹é‡ã¿ã‚’èª­ã¿è¾¼ã¿ weight_name='model.layers.27.self_attn.k_proj.weight' k='k_proj'
ğŸŸ© QKVParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿é–‹å§‹ param.shape=torch.Size([4096, 1024]), loaded_weight.shape=torch.Size([1024, 1024]), loaded_shard_id='k'
ğŸŸ© QKVParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿å®Œäº†
ğŸŸ¦ é‡ã¿ã‚’èª­ã¿è¾¼ã¿ä¸­ weight_name='model.layers.27.self_attn.o_proj.weight'
ğŸŸ¦ é€šå¸¸ã®é‡ã¿èª­ã¿è¾¼ã¿ weight_name='model.layers.27.self_attn.o_proj.weight'
ğŸŸ© RowParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿é–‹å§‹ param.shape=torch.Size([1024, 2048]), loaded_weight.shape=torch.Size([1024, 2048])
ğŸŸ© RowParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿å®Œäº†
ğŸŸ¦ é‡ã¿ã‚’èª­ã¿è¾¼ã¿ä¸­ weight_name='model.layers.27.self_attn.q_norm.weight'
ğŸŸ¦ é€šå¸¸ã®é‡ã¿èª­ã¿è¾¼ã¿ weight_name='model.layers.27.self_attn.q_norm.weight'
ğŸŸ© é‡ã¿ã‚’èª­ã¿è¾¼ã¿ param.shape=torch.Size([128]) loaded_weight.shape=torch.Size([128])
ğŸŸ¦ é‡ã¿ã‚’èª­ã¿è¾¼ã¿ä¸­ weight_name='model.layers.27.self_attn.q_proj.weight'
ğŸŸ¦ ãƒãƒƒãƒ”ãƒ³ã‚°ã«ã‚ˆã‚‹é‡ã¿ã‚’èª­ã¿è¾¼ã¿ weight_name='model.layers.27.self_attn.q_proj.weight' k='q_proj'
ğŸŸ© QKVParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿é–‹å§‹ param.shape=torch.Size([4096, 1024]), loaded_weight.shape=torch.Size([2048, 1024]), loaded_shard_id='q'
ğŸŸ© QKVParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿å®Œäº†
ğŸŸ¦ é‡ã¿ã‚’èª­ã¿è¾¼ã¿ä¸­ weight_name='model.layers.27.self_attn.v_proj.weight'
ğŸŸ¦ ãƒãƒƒãƒ”ãƒ³ã‚°ã«ã‚ˆã‚‹é‡ã¿ã‚’èª­ã¿è¾¼ã¿ weight_name='model.layers.27.self_attn.v_proj.weight' k='v_proj'
ğŸŸ© QKVParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿é–‹å§‹ param.shape=torch.Size([4096, 1024]), loaded_weight.shape=torch.Size([1024, 1024]), loaded_shard_id='v'
ğŸŸ© QKVParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿å®Œäº†
ğŸŸ¦ é‡ã¿ã‚’èª­ã¿è¾¼ã¿ä¸­ weight_name='model.layers.3.input_layernorm.weight'
ğŸŸ¦ é€šå¸¸ã®é‡ã¿èª­ã¿è¾¼ã¿ weight_name='model.layers.3.input_layernorm.weight'
ğŸŸ© é‡ã¿ã‚’èª­ã¿è¾¼ã¿ param.shape=torch.Size([1024]) loaded_weight.shape=torch.Size([1024])
ğŸŸ¦ é‡ã¿ã‚’èª­ã¿è¾¼ã¿ä¸­ weight_name='model.layers.3.mlp.down_proj.weight'
ğŸŸ¦ é€šå¸¸ã®é‡ã¿èª­ã¿è¾¼ã¿ weight_name='model.layers.3.mlp.down_proj.weight'
ğŸŸ© RowParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿é–‹å§‹ param.shape=torch.Size([1024, 3072]), loaded_weight.shape=torch.Size([1024, 3072])
ğŸŸ© RowParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿å®Œäº†
ğŸŸ¦ é‡ã¿ã‚’èª­ã¿è¾¼ã¿ä¸­ weight_name='model.layers.3.mlp.gate_proj.weight'
ğŸŸ¦ ãƒãƒƒãƒ”ãƒ³ã‚°ã«ã‚ˆã‚‹é‡ã¿ã‚’èª­ã¿è¾¼ã¿ weight_name='model.layers.3.mlp.gate_proj.weight' k='gate_proj'
ğŸŸ© MergedColumnParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿é–‹å§‹ param.shape=torch.Size([6144, 1024]), loaded_weight.shape=torch.Size([3072, 1024]), loaded_shard_id=0
ğŸŸ© MergedColumnParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿å®Œäº†
ğŸŸ¦ é‡ã¿ã‚’èª­ã¿è¾¼ã¿ä¸­ weight_name='model.layers.3.mlp.up_proj.weight'
ğŸŸ¦ ãƒãƒƒãƒ”ãƒ³ã‚°ã«ã‚ˆã‚‹é‡ã¿ã‚’èª­ã¿è¾¼ã¿ weight_name='model.layers.3.mlp.up_proj.weight' k='up_proj'
ğŸŸ© MergedColumnParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿é–‹å§‹ param.shape=torch.Size([6144, 1024]), loaded_weight.shape=torch.Size([3072, 1024]), loaded_shard_id=1
ğŸŸ© MergedColumnParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿å®Œäº†
ğŸŸ¦ é‡ã¿ã‚’èª­ã¿è¾¼ã¿ä¸­ weight_name='model.layers.3.post_attention_layernorm.weight'
ğŸŸ¦ é€šå¸¸ã®é‡ã¿èª­ã¿è¾¼ã¿ weight_name='model.layers.3.post_attention_layernorm.weight'
ğŸŸ© é‡ã¿ã‚’èª­ã¿è¾¼ã¿ param.shape=torch.Size([1024]) loaded_weight.shape=torch.Size([1024])
ğŸŸ¦ é‡ã¿ã‚’èª­ã¿è¾¼ã¿ä¸­ weight_name='model.layers.3.self_attn.k_norm.weight'
ğŸŸ¦ é€šå¸¸ã®é‡ã¿èª­ã¿è¾¼ã¿ weight_name='model.layers.3.self_attn.k_norm.weight'
ğŸŸ© é‡ã¿ã‚’èª­ã¿è¾¼ã¿ param.shape=torch.Size([128]) loaded_weight.shape=torch.Size([128])
ğŸŸ¦ é‡ã¿ã‚’èª­ã¿è¾¼ã¿ä¸­ weight_name='model.layers.3.self_attn.k_proj.weight'
ğŸŸ¦ ãƒãƒƒãƒ”ãƒ³ã‚°ã«ã‚ˆã‚‹é‡ã¿ã‚’èª­ã¿è¾¼ã¿ weight_name='model.layers.3.self_attn.k_proj.weight' k='k_proj'
ğŸŸ© QKVParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿é–‹å§‹ param.shape=torch.Size([4096, 1024]), loaded_weight.shape=torch.Size([1024, 1024]), loaded_shard_id='k'
ğŸŸ© QKVParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿å®Œäº†
ğŸŸ¦ é‡ã¿ã‚’èª­ã¿è¾¼ã¿ä¸­ weight_name='model.layers.3.self_attn.o_proj.weight'
ğŸŸ¦ é€šå¸¸ã®é‡ã¿èª­ã¿è¾¼ã¿ weight_name='model.layers.3.self_attn.o_proj.weight'
ğŸŸ© RowParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿é–‹å§‹ param.shape=torch.Size([1024, 2048]), loaded_weight.shape=torch.Size([1024, 2048])
ğŸŸ© RowParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿å®Œäº†
ğŸŸ¦ é‡ã¿ã‚’èª­ã¿è¾¼ã¿ä¸­ weight_name='model.layers.3.self_attn.q_norm.weight'
ğŸŸ¦ é€šå¸¸ã®é‡ã¿èª­ã¿è¾¼ã¿ weight_name='model.layers.3.self_attn.q_norm.weight'
ğŸŸ© é‡ã¿ã‚’èª­ã¿è¾¼ã¿ param.shape=torch.Size([128]) loaded_weight.shape=torch.Size([128])
ğŸŸ¦ é‡ã¿ã‚’èª­ã¿è¾¼ã¿ä¸­ weight_name='model.layers.3.self_attn.q_proj.weight'
ğŸŸ¦ ãƒãƒƒãƒ”ãƒ³ã‚°ã«ã‚ˆã‚‹é‡ã¿ã‚’èª­ã¿è¾¼ã¿ weight_name='model.layers.3.self_attn.q_proj.weight' k='q_proj'
ğŸŸ© QKVParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿é–‹å§‹ param.shape=torch.Size([4096, 1024]), loaded_weight.shape=torch.Size([2048, 1024]), loaded_shard_id='q'
ğŸŸ© QKVParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿å®Œäº†
ğŸŸ¦ é‡ã¿ã‚’èª­ã¿è¾¼ã¿ä¸­ weight_name='model.layers.3.self_attn.v_proj.weight'
ğŸŸ¦ ãƒãƒƒãƒ”ãƒ³ã‚°ã«ã‚ˆã‚‹é‡ã¿ã‚’èª­ã¿è¾¼ã¿ weight_name='model.layers.3.self_attn.v_proj.weight' k='v_proj'
ğŸŸ© QKVParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿é–‹å§‹ param.shape=torch.Size([4096, 1024]), loaded_weight.shape=torch.Size([1024, 1024]), loaded_shard_id='v'
ğŸŸ© QKVParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿å®Œäº†
ğŸŸ¦ é‡ã¿ã‚’èª­ã¿è¾¼ã¿ä¸­ weight_name='model.layers.4.input_layernorm.weight'
ğŸŸ¦ é€šå¸¸ã®é‡ã¿èª­ã¿è¾¼ã¿ weight_name='model.layers.4.input_layernorm.weight'
ğŸŸ© é‡ã¿ã‚’èª­ã¿è¾¼ã¿ param.shape=torch.Size([1024]) loaded_weight.shape=torch.Size([1024])
ğŸŸ¦ é‡ã¿ã‚’èª­ã¿è¾¼ã¿ä¸­ weight_name='model.layers.4.mlp.down_proj.weight'
ğŸŸ¦ é€šå¸¸ã®é‡ã¿èª­ã¿è¾¼ã¿ weight_name='model.layers.4.mlp.down_proj.weight'
ğŸŸ© RowParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿é–‹å§‹ param.shape=torch.Size([1024, 3072]), loaded_weight.shape=torch.Size([1024, 3072])
ğŸŸ© RowParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿å®Œäº†
ğŸŸ¦ é‡ã¿ã‚’èª­ã¿è¾¼ã¿ä¸­ weight_name='model.layers.4.mlp.gate_proj.weight'
ğŸŸ¦ ãƒãƒƒãƒ”ãƒ³ã‚°ã«ã‚ˆã‚‹é‡ã¿ã‚’èª­ã¿è¾¼ã¿ weight_name='model.layers.4.mlp.gate_proj.weight' k='gate_proj'
ğŸŸ© MergedColumnParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿é–‹å§‹ param.shape=torch.Size([6144, 1024]), loaded_weight.shape=torch.Size([3072, 1024]), loaded_shard_id=0
ğŸŸ© MergedColumnParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿å®Œäº†
ğŸŸ¦ é‡ã¿ã‚’èª­ã¿è¾¼ã¿ä¸­ weight_name='model.layers.4.mlp.up_proj.weight'
ğŸŸ¦ ãƒãƒƒãƒ”ãƒ³ã‚°ã«ã‚ˆã‚‹é‡ã¿ã‚’èª­ã¿è¾¼ã¿ weight_name='model.layers.4.mlp.up_proj.weight' k='up_proj'
ğŸŸ© MergedColumnParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿é–‹å§‹ param.shape=torch.Size([6144, 1024]), loaded_weight.shape=torch.Size([3072, 1024]), loaded_shard_id=1
ğŸŸ© MergedColumnParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿å®Œäº†
ğŸŸ¦ é‡ã¿ã‚’èª­ã¿è¾¼ã¿ä¸­ weight_name='model.layers.4.post_attention_layernorm.weight'
ğŸŸ¦ é€šå¸¸ã®é‡ã¿èª­ã¿è¾¼ã¿ weight_name='model.layers.4.post_attention_layernorm.weight'
ğŸŸ© é‡ã¿ã‚’èª­ã¿è¾¼ã¿ param.shape=torch.Size([1024]) loaded_weight.shape=torch.Size([1024])
ğŸŸ¦ é‡ã¿ã‚’èª­ã¿è¾¼ã¿ä¸­ weight_name='model.layers.4.self_attn.k_norm.weight'
ğŸŸ¦ é€šå¸¸ã®é‡ã¿èª­ã¿è¾¼ã¿ weight_name='model.layers.4.self_attn.k_norm.weight'
ğŸŸ© é‡ã¿ã‚’èª­ã¿è¾¼ã¿ param.shape=torch.Size([128]) loaded_weight.shape=torch.Size([128])
ğŸŸ¦ é‡ã¿ã‚’èª­ã¿è¾¼ã¿ä¸­ weight_name='model.layers.4.self_attn.k_proj.weight'
ğŸŸ¦ ãƒãƒƒãƒ”ãƒ³ã‚°ã«ã‚ˆã‚‹é‡ã¿ã‚’èª­ã¿è¾¼ã¿ weight_name='model.layers.4.self_attn.k_proj.weight' k='k_proj'
ğŸŸ© QKVParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿é–‹å§‹ param.shape=torch.Size([4096, 1024]), loaded_weight.shape=torch.Size([1024, 1024]), loaded_shard_id='k'
ğŸŸ© QKVParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿å®Œäº†
ğŸŸ¦ é‡ã¿ã‚’èª­ã¿è¾¼ã¿ä¸­ weight_name='model.layers.4.self_attn.o_proj.weight'
ğŸŸ¦ é€šå¸¸ã®é‡ã¿èª­ã¿è¾¼ã¿ weight_name='model.layers.4.self_attn.o_proj.weight'
ğŸŸ© RowParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿é–‹å§‹ param.shape=torch.Size([1024, 2048]), loaded_weight.shape=torch.Size([1024, 2048])
ğŸŸ© RowParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿å®Œäº†
ğŸŸ¦ é‡ã¿ã‚’èª­ã¿è¾¼ã¿ä¸­ weight_name='model.layers.4.self_attn.q_norm.weight'
ğŸŸ¦ é€šå¸¸ã®é‡ã¿èª­ã¿è¾¼ã¿ weight_name='model.layers.4.self_attn.q_norm.weight'
ğŸŸ© é‡ã¿ã‚’èª­ã¿è¾¼ã¿ param.shape=torch.Size([128]) loaded_weight.shape=torch.Size([128])
ğŸŸ¦ é‡ã¿ã‚’èª­ã¿è¾¼ã¿ä¸­ weight_name='model.layers.4.self_attn.q_proj.weight'
ğŸŸ¦ ãƒãƒƒãƒ”ãƒ³ã‚°ã«ã‚ˆã‚‹é‡ã¿ã‚’èª­ã¿è¾¼ã¿ weight_name='model.layers.4.self_attn.q_proj.weight' k='q_proj'
ğŸŸ© QKVParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿é–‹å§‹ param.shape=torch.Size([4096, 1024]), loaded_weight.shape=torch.Size([2048, 1024]), loaded_shard_id='q'
ğŸŸ© QKVParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿å®Œäº†
ğŸŸ¦ é‡ã¿ã‚’èª­ã¿è¾¼ã¿ä¸­ weight_name='model.layers.4.self_attn.v_proj.weight'
ğŸŸ¦ ãƒãƒƒãƒ”ãƒ³ã‚°ã«ã‚ˆã‚‹é‡ã¿ã‚’èª­ã¿è¾¼ã¿ weight_name='model.layers.4.self_attn.v_proj.weight' k='v_proj'
ğŸŸ© QKVParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿é–‹å§‹ param.shape=torch.Size([4096, 1024]), loaded_weight.shape=torch.Size([1024, 1024]), loaded_shard_id='v'
ğŸŸ© QKVParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿å®Œäº†
ğŸŸ¦ é‡ã¿ã‚’èª­ã¿è¾¼ã¿ä¸­ weight_name='model.layers.5.input_layernorm.weight'
ğŸŸ¦ é€šå¸¸ã®é‡ã¿èª­ã¿è¾¼ã¿ weight_name='model.layers.5.input_layernorm.weight'
ğŸŸ© é‡ã¿ã‚’èª­ã¿è¾¼ã¿ param.shape=torch.Size([1024]) loaded_weight.shape=torch.Size([1024])
ğŸŸ¦ é‡ã¿ã‚’èª­ã¿è¾¼ã¿ä¸­ weight_name='model.layers.5.mlp.down_proj.weight'
ğŸŸ¦ é€šå¸¸ã®é‡ã¿èª­ã¿è¾¼ã¿ weight_name='model.layers.5.mlp.down_proj.weight'
ğŸŸ© RowParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿é–‹å§‹ param.shape=torch.Size([1024, 3072]), loaded_weight.shape=torch.Size([1024, 3072])
ğŸŸ© RowParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿å®Œäº†
ğŸŸ¦ é‡ã¿ã‚’èª­ã¿è¾¼ã¿ä¸­ weight_name='model.layers.5.mlp.gate_proj.weight'
ğŸŸ¦ ãƒãƒƒãƒ”ãƒ³ã‚°ã«ã‚ˆã‚‹é‡ã¿ã‚’èª­ã¿è¾¼ã¿ weight_name='model.layers.5.mlp.gate_proj.weight' k='gate_proj'
ğŸŸ© MergedColumnParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿é–‹å§‹ param.shape=torch.Size([6144, 1024]), loaded_weight.shape=torch.Size([3072, 1024]), loaded_shard_id=0
ğŸŸ© MergedColumnParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿å®Œäº†
ğŸŸ¦ é‡ã¿ã‚’èª­ã¿è¾¼ã¿ä¸­ weight_name='model.layers.5.mlp.up_proj.weight'
ğŸŸ¦ ãƒãƒƒãƒ”ãƒ³ã‚°ã«ã‚ˆã‚‹é‡ã¿ã‚’èª­ã¿è¾¼ã¿ weight_name='model.layers.5.mlp.up_proj.weight' k='up_proj'
ğŸŸ© MergedColumnParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿é–‹å§‹ param.shape=torch.Size([6144, 1024]), loaded_weight.shape=torch.Size([3072, 1024]), loaded_shard_id=1
ğŸŸ© MergedColumnParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿å®Œäº†
ğŸŸ¦ é‡ã¿ã‚’èª­ã¿è¾¼ã¿ä¸­ weight_name='model.layers.5.post_attention_layernorm.weight'
ğŸŸ¦ é€šå¸¸ã®é‡ã¿èª­ã¿è¾¼ã¿ weight_name='model.layers.5.post_attention_layernorm.weight'
ğŸŸ© é‡ã¿ã‚’èª­ã¿è¾¼ã¿ param.shape=torch.Size([1024]) loaded_weight.shape=torch.Size([1024])
ğŸŸ¦ é‡ã¿ã‚’èª­ã¿è¾¼ã¿ä¸­ weight_name='model.layers.5.self_attn.k_norm.weight'
ğŸŸ¦ é€šå¸¸ã®é‡ã¿èª­ã¿è¾¼ã¿ weight_name='model.layers.5.self_attn.k_norm.weight'
ğŸŸ© é‡ã¿ã‚’èª­ã¿è¾¼ã¿ param.shape=torch.Size([128]) loaded_weight.shape=torch.Size([128])
ğŸŸ¦ é‡ã¿ã‚’èª­ã¿è¾¼ã¿ä¸­ weight_name='model.layers.5.self_attn.k_proj.weight'
ğŸŸ¦ ãƒãƒƒãƒ”ãƒ³ã‚°ã«ã‚ˆã‚‹é‡ã¿ã‚’èª­ã¿è¾¼ã¿ weight_name='model.layers.5.self_attn.k_proj.weight' k='k_proj'
ğŸŸ© QKVParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿é–‹å§‹ param.shape=torch.Size([4096, 1024]), loaded_weight.shape=torch.Size([1024, 1024]), loaded_shard_id='k'
ğŸŸ© QKVParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿å®Œäº†
ğŸŸ¦ é‡ã¿ã‚’èª­ã¿è¾¼ã¿ä¸­ weight_name='model.layers.5.self_attn.o_proj.weight'
ğŸŸ¦ é€šå¸¸ã®é‡ã¿èª­ã¿è¾¼ã¿ weight_name='model.layers.5.self_attn.o_proj.weight'
ğŸŸ© RowParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿é–‹å§‹ param.shape=torch.Size([1024, 2048]), loaded_weight.shape=torch.Size([1024, 2048])
ğŸŸ© RowParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿å®Œäº†
ğŸŸ¦ é‡ã¿ã‚’èª­ã¿è¾¼ã¿ä¸­ weight_name='model.layers.5.self_attn.q_norm.weight'
ğŸŸ¦ é€šå¸¸ã®é‡ã¿èª­ã¿è¾¼ã¿ weight_name='model.layers.5.self_attn.q_norm.weight'
ğŸŸ© é‡ã¿ã‚’èª­ã¿è¾¼ã¿ param.shape=torch.Size([128]) loaded_weight.shape=torch.Size([128])
ğŸŸ¦ é‡ã¿ã‚’èª­ã¿è¾¼ã¿ä¸­ weight_name='model.layers.5.self_attn.q_proj.weight'
ğŸŸ¦ ãƒãƒƒãƒ”ãƒ³ã‚°ã«ã‚ˆã‚‹é‡ã¿ã‚’èª­ã¿è¾¼ã¿ weight_name='model.layers.5.self_attn.q_proj.weight' k='q_proj'
ğŸŸ© QKVParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿é–‹å§‹ param.shape=torch.Size([4096, 1024]), loaded_weight.shape=torch.Size([2048, 1024]), loaded_shard_id='q'
ğŸŸ© QKVParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿å®Œäº†
ğŸŸ¦ é‡ã¿ã‚’èª­ã¿è¾¼ã¿ä¸­ weight_name='model.layers.5.self_attn.v_proj.weight'
ğŸŸ¦ ãƒãƒƒãƒ”ãƒ³ã‚°ã«ã‚ˆã‚‹é‡ã¿ã‚’èª­ã¿è¾¼ã¿ weight_name='model.layers.5.self_attn.v_proj.weight' k='v_proj'
ğŸŸ© QKVParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿é–‹å§‹ param.shape=torch.Size([4096, 1024]), loaded_weight.shape=torch.Size([1024, 1024]), loaded_shard_id='v'
ğŸŸ© QKVParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿å®Œäº†
ğŸŸ¦ é‡ã¿ã‚’èª­ã¿è¾¼ã¿ä¸­ weight_name='model.layers.6.input_layernorm.weight'
ğŸŸ¦ é€šå¸¸ã®é‡ã¿èª­ã¿è¾¼ã¿ weight_name='model.layers.6.input_layernorm.weight'
ğŸŸ© é‡ã¿ã‚’èª­ã¿è¾¼ã¿ param.shape=torch.Size([1024]) loaded_weight.shape=torch.Size([1024])
ğŸŸ¦ é‡ã¿ã‚’èª­ã¿è¾¼ã¿ä¸­ weight_name='model.layers.6.mlp.down_proj.weight'
ğŸŸ¦ é€šå¸¸ã®é‡ã¿èª­ã¿è¾¼ã¿ weight_name='model.layers.6.mlp.down_proj.weight'
ğŸŸ© RowParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿é–‹å§‹ param.shape=torch.Size([1024, 3072]), loaded_weight.shape=torch.Size([1024, 3072])
ğŸŸ© RowParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿å®Œäº†
ğŸŸ¦ é‡ã¿ã‚’èª­ã¿è¾¼ã¿ä¸­ weight_name='model.layers.6.mlp.gate_proj.weight'
ğŸŸ¦ ãƒãƒƒãƒ”ãƒ³ã‚°ã«ã‚ˆã‚‹é‡ã¿ã‚’èª­ã¿è¾¼ã¿ weight_name='model.layers.6.mlp.gate_proj.weight' k='gate_proj'
ğŸŸ© MergedColumnParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿é–‹å§‹ param.shape=torch.Size([6144, 1024]), loaded_weight.shape=torch.Size([3072, 1024]), loaded_shard_id=0
ğŸŸ© MergedColumnParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿å®Œäº†
ğŸŸ¦ é‡ã¿ã‚’èª­ã¿è¾¼ã¿ä¸­ weight_name='model.layers.6.mlp.up_proj.weight'
ğŸŸ¦ ãƒãƒƒãƒ”ãƒ³ã‚°ã«ã‚ˆã‚‹é‡ã¿ã‚’èª­ã¿è¾¼ã¿ weight_name='model.layers.6.mlp.up_proj.weight' k='up_proj'
ğŸŸ© MergedColumnParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿é–‹å§‹ param.shape=torch.Size([6144, 1024]), loaded_weight.shape=torch.Size([3072, 1024]), loaded_shard_id=1
ğŸŸ© MergedColumnParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿å®Œäº†
ğŸŸ¦ é‡ã¿ã‚’èª­ã¿è¾¼ã¿ä¸­ weight_name='model.layers.6.post_attention_layernorm.weight'
ğŸŸ¦ é€šå¸¸ã®é‡ã¿èª­ã¿è¾¼ã¿ weight_name='model.layers.6.post_attention_layernorm.weight'
ğŸŸ© é‡ã¿ã‚’èª­ã¿è¾¼ã¿ param.shape=torch.Size([1024]) loaded_weight.shape=torch.Size([1024])
ğŸŸ¦ é‡ã¿ã‚’èª­ã¿è¾¼ã¿ä¸­ weight_name='model.layers.6.self_attn.k_norm.weight'
ğŸŸ¦ é€šå¸¸ã®é‡ã¿èª­ã¿è¾¼ã¿ weight_name='model.layers.6.self_attn.k_norm.weight'
ğŸŸ© é‡ã¿ã‚’èª­ã¿è¾¼ã¿ param.shape=torch.Size([128]) loaded_weight.shape=torch.Size([128])
ğŸŸ¦ é‡ã¿ã‚’èª­ã¿è¾¼ã¿ä¸­ weight_name='model.layers.6.self_attn.k_proj.weight'
ğŸŸ¦ ãƒãƒƒãƒ”ãƒ³ã‚°ã«ã‚ˆã‚‹é‡ã¿ã‚’èª­ã¿è¾¼ã¿ weight_name='model.layers.6.self_attn.k_proj.weight' k='k_proj'
ğŸŸ© QKVParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿é–‹å§‹ param.shape=torch.Size([4096, 1024]), loaded_weight.shape=torch.Size([1024, 1024]), loaded_shard_id='k'
ğŸŸ© QKVParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿å®Œäº†
ğŸŸ¦ é‡ã¿ã‚’èª­ã¿è¾¼ã¿ä¸­ weight_name='model.layers.6.self_attn.o_proj.weight'
ğŸŸ¦ é€šå¸¸ã®é‡ã¿èª­ã¿è¾¼ã¿ weight_name='model.layers.6.self_attn.o_proj.weight'
ğŸŸ© RowParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿é–‹å§‹ param.shape=torch.Size([1024, 2048]), loaded_weight.shape=torch.Size([1024, 2048])
ğŸŸ© RowParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿å®Œäº†
ğŸŸ¦ é‡ã¿ã‚’èª­ã¿è¾¼ã¿ä¸­ weight_name='model.layers.6.self_attn.q_norm.weight'
ğŸŸ¦ é€šå¸¸ã®é‡ã¿èª­ã¿è¾¼ã¿ weight_name='model.layers.6.self_attn.q_norm.weight'
ğŸŸ© é‡ã¿ã‚’èª­ã¿è¾¼ã¿ param.shape=torch.Size([128]) loaded_weight.shape=torch.Size([128])
ğŸŸ¦ é‡ã¿ã‚’èª­ã¿è¾¼ã¿ä¸­ weight_name='model.layers.6.self_attn.q_proj.weight'
ğŸŸ¦ ãƒãƒƒãƒ”ãƒ³ã‚°ã«ã‚ˆã‚‹é‡ã¿ã‚’èª­ã¿è¾¼ã¿ weight_name='model.layers.6.self_attn.q_proj.weight' k='q_proj'
ğŸŸ© QKVParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿é–‹å§‹ param.shape=torch.Size([4096, 1024]), loaded_weight.shape=torch.Size([2048, 1024]), loaded_shard_id='q'
ğŸŸ© QKVParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿å®Œäº†
ğŸŸ¦ é‡ã¿ã‚’èª­ã¿è¾¼ã¿ä¸­ weight_name='model.layers.6.self_attn.v_proj.weight'
ğŸŸ¦ ãƒãƒƒãƒ”ãƒ³ã‚°ã«ã‚ˆã‚‹é‡ã¿ã‚’èª­ã¿è¾¼ã¿ weight_name='model.layers.6.self_attn.v_proj.weight' k='v_proj'
ğŸŸ© QKVParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿é–‹å§‹ param.shape=torch.Size([4096, 1024]), loaded_weight.shape=torch.Size([1024, 1024]), loaded_shard_id='v'
ğŸŸ© QKVParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿å®Œäº†
ğŸŸ¦ é‡ã¿ã‚’èª­ã¿è¾¼ã¿ä¸­ weight_name='model.layers.7.input_layernorm.weight'
ğŸŸ¦ é€šå¸¸ã®é‡ã¿èª­ã¿è¾¼ã¿ weight_name='model.layers.7.input_layernorm.weight'
ğŸŸ© é‡ã¿ã‚’èª­ã¿è¾¼ã¿ param.shape=torch.Size([1024]) loaded_weight.shape=torch.Size([1024])
ğŸŸ¦ é‡ã¿ã‚’èª­ã¿è¾¼ã¿ä¸­ weight_name='model.layers.7.mlp.down_proj.weight'
ğŸŸ¦ é€šå¸¸ã®é‡ã¿èª­ã¿è¾¼ã¿ weight_name='model.layers.7.mlp.down_proj.weight'
ğŸŸ© RowParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿é–‹å§‹ param.shape=torch.Size([1024, 3072]), loaded_weight.shape=torch.Size([1024, 3072])
ğŸŸ© RowParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿å®Œäº†
ğŸŸ¦ é‡ã¿ã‚’èª­ã¿è¾¼ã¿ä¸­ weight_name='model.layers.7.mlp.gate_proj.weight'
ğŸŸ¦ ãƒãƒƒãƒ”ãƒ³ã‚°ã«ã‚ˆã‚‹é‡ã¿ã‚’èª­ã¿è¾¼ã¿ weight_name='model.layers.7.mlp.gate_proj.weight' k='gate_proj'
ğŸŸ© MergedColumnParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿é–‹å§‹ param.shape=torch.Size([6144, 1024]), loaded_weight.shape=torch.Size([3072, 1024]), loaded_shard_id=0
ğŸŸ© MergedColumnParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿å®Œäº†
ğŸŸ¦ é‡ã¿ã‚’èª­ã¿è¾¼ã¿ä¸­ weight_name='model.layers.7.mlp.up_proj.weight'
ğŸŸ¦ ãƒãƒƒãƒ”ãƒ³ã‚°ã«ã‚ˆã‚‹é‡ã¿ã‚’èª­ã¿è¾¼ã¿ weight_name='model.layers.7.mlp.up_proj.weight' k='up_proj'
ğŸŸ© MergedColumnParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿é–‹å§‹ param.shape=torch.Size([6144, 1024]), loaded_weight.shape=torch.Size([3072, 1024]), loaded_shard_id=1
ğŸŸ© MergedColumnParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿å®Œäº†
ğŸŸ¦ é‡ã¿ã‚’èª­ã¿è¾¼ã¿ä¸­ weight_name='model.layers.7.post_attention_layernorm.weight'
ğŸŸ¦ é€šå¸¸ã®é‡ã¿èª­ã¿è¾¼ã¿ weight_name='model.layers.7.post_attention_layernorm.weight'
ğŸŸ© é‡ã¿ã‚’èª­ã¿è¾¼ã¿ param.shape=torch.Size([1024]) loaded_weight.shape=torch.Size([1024])
ğŸŸ¦ é‡ã¿ã‚’èª­ã¿è¾¼ã¿ä¸­ weight_name='model.layers.7.self_attn.k_norm.weight'
ğŸŸ¦ é€šå¸¸ã®é‡ã¿èª­ã¿è¾¼ã¿ weight_name='model.layers.7.self_attn.k_norm.weight'
ğŸŸ© é‡ã¿ã‚’èª­ã¿è¾¼ã¿ param.shape=torch.Size([128]) loaded_weight.shape=torch.Size([128])
ğŸŸ¦ é‡ã¿ã‚’èª­ã¿è¾¼ã¿ä¸­ weight_name='model.layers.7.self_attn.k_proj.weight'
ğŸŸ¦ ãƒãƒƒãƒ”ãƒ³ã‚°ã«ã‚ˆã‚‹é‡ã¿ã‚’èª­ã¿è¾¼ã¿ weight_name='model.layers.7.self_attn.k_proj.weight' k='k_proj'
ğŸŸ© QKVParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿é–‹å§‹ param.shape=torch.Size([4096, 1024]), loaded_weight.shape=torch.Size([1024, 1024]), loaded_shard_id='k'
ğŸŸ© QKVParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿å®Œäº†
ğŸŸ¦ é‡ã¿ã‚’èª­ã¿è¾¼ã¿ä¸­ weight_name='model.layers.7.self_attn.o_proj.weight'
ğŸŸ¦ é€šå¸¸ã®é‡ã¿èª­ã¿è¾¼ã¿ weight_name='model.layers.7.self_attn.o_proj.weight'
ğŸŸ© RowParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿é–‹å§‹ param.shape=torch.Size([1024, 2048]), loaded_weight.shape=torch.Size([1024, 2048])
ğŸŸ© RowParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿å®Œäº†
ğŸŸ¦ é‡ã¿ã‚’èª­ã¿è¾¼ã¿ä¸­ weight_name='model.layers.7.self_attn.q_norm.weight'
ğŸŸ¦ é€šå¸¸ã®é‡ã¿èª­ã¿è¾¼ã¿ weight_name='model.layers.7.self_attn.q_norm.weight'
ğŸŸ© é‡ã¿ã‚’èª­ã¿è¾¼ã¿ param.shape=torch.Size([128]) loaded_weight.shape=torch.Size([128])
ğŸŸ¦ é‡ã¿ã‚’èª­ã¿è¾¼ã¿ä¸­ weight_name='model.layers.7.self_attn.q_proj.weight'
ğŸŸ¦ ãƒãƒƒãƒ”ãƒ³ã‚°ã«ã‚ˆã‚‹é‡ã¿ã‚’èª­ã¿è¾¼ã¿ weight_name='model.layers.7.self_attn.q_proj.weight' k='q_proj'
ğŸŸ© QKVParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿é–‹å§‹ param.shape=torch.Size([4096, 1024]), loaded_weight.shape=torch.Size([2048, 1024]), loaded_shard_id='q'
ğŸŸ© QKVParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿å®Œäº†
ğŸŸ¦ é‡ã¿ã‚’èª­ã¿è¾¼ã¿ä¸­ weight_name='model.layers.7.self_attn.v_proj.weight'
ğŸŸ¦ ãƒãƒƒãƒ”ãƒ³ã‚°ã«ã‚ˆã‚‹é‡ã¿ã‚’èª­ã¿è¾¼ã¿ weight_name='model.layers.7.self_attn.v_proj.weight' k='v_proj'
ğŸŸ© QKVParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿é–‹å§‹ param.shape=torch.Size([4096, 1024]), loaded_weight.shape=torch.Size([1024, 1024]), loaded_shard_id='v'
ğŸŸ© QKVParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿å®Œäº†
ğŸŸ¦ é‡ã¿ã‚’èª­ã¿è¾¼ã¿ä¸­ weight_name='model.layers.8.input_layernorm.weight'
ğŸŸ¦ é€šå¸¸ã®é‡ã¿èª­ã¿è¾¼ã¿ weight_name='model.layers.8.input_layernorm.weight'
ğŸŸ© é‡ã¿ã‚’èª­ã¿è¾¼ã¿ param.shape=torch.Size([1024]) loaded_weight.shape=torch.Size([1024])
ğŸŸ¦ é‡ã¿ã‚’èª­ã¿è¾¼ã¿ä¸­ weight_name='model.layers.8.mlp.down_proj.weight'
ğŸŸ¦ é€šå¸¸ã®é‡ã¿èª­ã¿è¾¼ã¿ weight_name='model.layers.8.mlp.down_proj.weight'
ğŸŸ© RowParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿é–‹å§‹ param.shape=torch.Size([1024, 3072]), loaded_weight.shape=torch.Size([1024, 3072])
ğŸŸ© RowParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿å®Œäº†
ğŸŸ¦ é‡ã¿ã‚’èª­ã¿è¾¼ã¿ä¸­ weight_name='model.layers.8.mlp.gate_proj.weight'
ğŸŸ¦ ãƒãƒƒãƒ”ãƒ³ã‚°ã«ã‚ˆã‚‹é‡ã¿ã‚’èª­ã¿è¾¼ã¿ weight_name='model.layers.8.mlp.gate_proj.weight' k='gate_proj'
ğŸŸ© MergedColumnParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿é–‹å§‹ param.shape=torch.Size([6144, 1024]), loaded_weight.shape=torch.Size([3072, 1024]), loaded_shard_id=0
ğŸŸ© MergedColumnParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿å®Œäº†
ğŸŸ¦ é‡ã¿ã‚’èª­ã¿è¾¼ã¿ä¸­ weight_name='model.layers.8.mlp.up_proj.weight'
ğŸŸ¦ ãƒãƒƒãƒ”ãƒ³ã‚°ã«ã‚ˆã‚‹é‡ã¿ã‚’èª­ã¿è¾¼ã¿ weight_name='model.layers.8.mlp.up_proj.weight' k='up_proj'
ğŸŸ© MergedColumnParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿é–‹å§‹ param.shape=torch.Size([6144, 1024]), loaded_weight.shape=torch.Size([3072, 1024]), loaded_shard_id=1
ğŸŸ© MergedColumnParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿å®Œäº†
ğŸŸ¦ é‡ã¿ã‚’èª­ã¿è¾¼ã¿ä¸­ weight_name='model.layers.8.post_attention_layernorm.weight'
ğŸŸ¦ é€šå¸¸ã®é‡ã¿èª­ã¿è¾¼ã¿ weight_name='model.layers.8.post_attention_layernorm.weight'
ğŸŸ© é‡ã¿ã‚’èª­ã¿è¾¼ã¿ param.shape=torch.Size([1024]) loaded_weight.shape=torch.Size([1024])
ğŸŸ¦ é‡ã¿ã‚’èª­ã¿è¾¼ã¿ä¸­ weight_name='model.layers.8.self_attn.k_norm.weight'
ğŸŸ¦ é€šå¸¸ã®é‡ã¿èª­ã¿è¾¼ã¿ weight_name='model.layers.8.self_attn.k_norm.weight'
ğŸŸ© é‡ã¿ã‚’èª­ã¿è¾¼ã¿ param.shape=torch.Size([128]) loaded_weight.shape=torch.Size([128])
ğŸŸ¦ é‡ã¿ã‚’èª­ã¿è¾¼ã¿ä¸­ weight_name='model.layers.8.self_attn.k_proj.weight'
ğŸŸ¦ ãƒãƒƒãƒ”ãƒ³ã‚°ã«ã‚ˆã‚‹é‡ã¿ã‚’èª­ã¿è¾¼ã¿ weight_name='model.layers.8.self_attn.k_proj.weight' k='k_proj'
ğŸŸ© QKVParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿é–‹å§‹ param.shape=torch.Size([4096, 1024]), loaded_weight.shape=torch.Size([1024, 1024]), loaded_shard_id='k'
ğŸŸ© QKVParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿å®Œäº†
ğŸŸ¦ é‡ã¿ã‚’èª­ã¿è¾¼ã¿ä¸­ weight_name='model.layers.8.self_attn.o_proj.weight'
ğŸŸ¦ é€šå¸¸ã®é‡ã¿èª­ã¿è¾¼ã¿ weight_name='model.layers.8.self_attn.o_proj.weight'
ğŸŸ© RowParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿é–‹å§‹ param.shape=torch.Size([1024, 2048]), loaded_weight.shape=torch.Size([1024, 2048])
ğŸŸ© RowParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿å®Œäº†
ğŸŸ¦ é‡ã¿ã‚’èª­ã¿è¾¼ã¿ä¸­ weight_name='model.layers.8.self_attn.q_norm.weight'
ğŸŸ¦ é€šå¸¸ã®é‡ã¿èª­ã¿è¾¼ã¿ weight_name='model.layers.8.self_attn.q_norm.weight'
ğŸŸ© é‡ã¿ã‚’èª­ã¿è¾¼ã¿ param.shape=torch.Size([128]) loaded_weight.shape=torch.Size([128])
ğŸŸ¦ é‡ã¿ã‚’èª­ã¿è¾¼ã¿ä¸­ weight_name='model.layers.8.self_attn.q_proj.weight'
ğŸŸ¦ ãƒãƒƒãƒ”ãƒ³ã‚°ã«ã‚ˆã‚‹é‡ã¿ã‚’èª­ã¿è¾¼ã¿ weight_name='model.layers.8.self_attn.q_proj.weight' k='q_proj'
ğŸŸ© QKVParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿é–‹å§‹ param.shape=torch.Size([4096, 1024]), loaded_weight.shape=torch.Size([2048, 1024]), loaded_shard_id='q'
ğŸŸ© QKVParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿å®Œäº†
ğŸŸ¦ é‡ã¿ã‚’èª­ã¿è¾¼ã¿ä¸­ weight_name='model.layers.8.self_attn.v_proj.weight'
ğŸŸ¦ ãƒãƒƒãƒ”ãƒ³ã‚°ã«ã‚ˆã‚‹é‡ã¿ã‚’èª­ã¿è¾¼ã¿ weight_name='model.layers.8.self_attn.v_proj.weight' k='v_proj'
ğŸŸ© QKVParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿é–‹å§‹ param.shape=torch.Size([4096, 1024]), loaded_weight.shape=torch.Size([1024, 1024]), loaded_shard_id='v'
ğŸŸ© QKVParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿å®Œäº†
ğŸŸ¦ é‡ã¿ã‚’èª­ã¿è¾¼ã¿ä¸­ weight_name='model.layers.9.input_layernorm.weight'
ğŸŸ¦ é€šå¸¸ã®é‡ã¿èª­ã¿è¾¼ã¿ weight_name='model.layers.9.input_layernorm.weight'
ğŸŸ© é‡ã¿ã‚’èª­ã¿è¾¼ã¿ param.shape=torch.Size([1024]) loaded_weight.shape=torch.Size([1024])
ğŸŸ¦ é‡ã¿ã‚’èª­ã¿è¾¼ã¿ä¸­ weight_name='model.layers.9.mlp.down_proj.weight'
ğŸŸ¦ é€šå¸¸ã®é‡ã¿èª­ã¿è¾¼ã¿ weight_name='model.layers.9.mlp.down_proj.weight'
ğŸŸ© RowParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿é–‹å§‹ param.shape=torch.Size([1024, 3072]), loaded_weight.shape=torch.Size([1024, 3072])
ğŸŸ© RowParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿å®Œäº†
ğŸŸ¦ é‡ã¿ã‚’èª­ã¿è¾¼ã¿ä¸­ weight_name='model.layers.9.mlp.gate_proj.weight'
ğŸŸ¦ ãƒãƒƒãƒ”ãƒ³ã‚°ã«ã‚ˆã‚‹é‡ã¿ã‚’èª­ã¿è¾¼ã¿ weight_name='model.layers.9.mlp.gate_proj.weight' k='gate_proj'
ğŸŸ© MergedColumnParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿é–‹å§‹ param.shape=torch.Size([6144, 1024]), loaded_weight.shape=torch.Size([3072, 1024]), loaded_shard_id=0
ğŸŸ© MergedColumnParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿å®Œäº†
ğŸŸ¦ é‡ã¿ã‚’èª­ã¿è¾¼ã¿ä¸­ weight_name='model.layers.9.mlp.up_proj.weight'
ğŸŸ¦ ãƒãƒƒãƒ”ãƒ³ã‚°ã«ã‚ˆã‚‹é‡ã¿ã‚’èª­ã¿è¾¼ã¿ weight_name='model.layers.9.mlp.up_proj.weight' k='up_proj'
ğŸŸ© MergedColumnParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿é–‹å§‹ param.shape=torch.Size([6144, 1024]), loaded_weight.shape=torch.Size([3072, 1024]), loaded_shard_id=1
ğŸŸ© MergedColumnParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿å®Œäº†
ğŸŸ¦ é‡ã¿ã‚’èª­ã¿è¾¼ã¿ä¸­ weight_name='model.layers.9.post_attention_layernorm.weight'
ğŸŸ¦ é€šå¸¸ã®é‡ã¿èª­ã¿è¾¼ã¿ weight_name='model.layers.9.post_attention_layernorm.weight'
ğŸŸ© é‡ã¿ã‚’èª­ã¿è¾¼ã¿ param.shape=torch.Size([1024]) loaded_weight.shape=torch.Size([1024])
ğŸŸ¦ é‡ã¿ã‚’èª­ã¿è¾¼ã¿ä¸­ weight_name='model.layers.9.self_attn.k_norm.weight'
ğŸŸ¦ é€šå¸¸ã®é‡ã¿èª­ã¿è¾¼ã¿ weight_name='model.layers.9.self_attn.k_norm.weight'
ğŸŸ© é‡ã¿ã‚’èª­ã¿è¾¼ã¿ param.shape=torch.Size([128]) loaded_weight.shape=torch.Size([128])
ğŸŸ¦ é‡ã¿ã‚’èª­ã¿è¾¼ã¿ä¸­ weight_name='model.layers.9.self_attn.k_proj.weight'
ğŸŸ¦ ãƒãƒƒãƒ”ãƒ³ã‚°ã«ã‚ˆã‚‹é‡ã¿ã‚’èª­ã¿è¾¼ã¿ weight_name='model.layers.9.self_attn.k_proj.weight' k='k_proj'
ğŸŸ© QKVParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿é–‹å§‹ param.shape=torch.Size([4096, 1024]), loaded_weight.shape=torch.Size([1024, 1024]), loaded_shard_id='k'
ğŸŸ© QKVParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿å®Œäº†
ğŸŸ¦ é‡ã¿ã‚’èª­ã¿è¾¼ã¿ä¸­ weight_name='model.layers.9.self_attn.o_proj.weight'
ğŸŸ¦ é€šå¸¸ã®é‡ã¿èª­ã¿è¾¼ã¿ weight_name='model.layers.9.self_attn.o_proj.weight'
ğŸŸ© RowParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿é–‹å§‹ param.shape=torch.Size([1024, 2048]), loaded_weight.shape=torch.Size([1024, 2048])
ğŸŸ© RowParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿å®Œäº†
ğŸŸ¦ é‡ã¿ã‚’èª­ã¿è¾¼ã¿ä¸­ weight_name='model.layers.9.self_attn.q_norm.weight'
ğŸŸ¦ é€šå¸¸ã®é‡ã¿èª­ã¿è¾¼ã¿ weight_name='model.layers.9.self_attn.q_norm.weight'
ğŸŸ© é‡ã¿ã‚’èª­ã¿è¾¼ã¿ param.shape=torch.Size([128]) loaded_weight.shape=torch.Size([128])
ğŸŸ¦ é‡ã¿ã‚’èª­ã¿è¾¼ã¿ä¸­ weight_name='model.layers.9.self_attn.q_proj.weight'
ğŸŸ¦ ãƒãƒƒãƒ”ãƒ³ã‚°ã«ã‚ˆã‚‹é‡ã¿ã‚’èª­ã¿è¾¼ã¿ weight_name='model.layers.9.self_attn.q_proj.weight' k='q_proj'
ğŸŸ© QKVParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿é–‹å§‹ param.shape=torch.Size([4096, 1024]), loaded_weight.shape=torch.Size([2048, 1024]), loaded_shard_id='q'
ğŸŸ© QKVParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿å®Œäº†
ğŸŸ¦ é‡ã¿ã‚’èª­ã¿è¾¼ã¿ä¸­ weight_name='model.layers.9.self_attn.v_proj.weight'
ğŸŸ¦ ãƒãƒƒãƒ”ãƒ³ã‚°ã«ã‚ˆã‚‹é‡ã¿ã‚’èª­ã¿è¾¼ã¿ weight_name='model.layers.9.self_attn.v_proj.weight' k='v_proj'
ğŸŸ© QKVParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿é–‹å§‹ param.shape=torch.Size([4096, 1024]), loaded_weight.shape=torch.Size([1024, 1024]), loaded_shard_id='v'
ğŸŸ© QKVParallelLinearã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿å®Œäº†
ğŸŸ¦ é‡ã¿ã‚’èª­ã¿è¾¼ã¿ä¸­ weight_name='model.norm.weight'
ğŸŸ¦ é€šå¸¸ã®é‡ã¿èª­ã¿è¾¼ã¿ weight_name='model.norm.weight'
ğŸŸ© é‡ã¿ã‚’èª­ã¿è¾¼ã¿ param.shape=torch.Size([1024]) loaded_weight.shape=torch.Size([1024])
ğŸŸ© ãƒ¢ãƒ‡ãƒ«ã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿å®Œäº†
ğŸŸ© Samplerã‚’åˆæœŸåŒ–
ğŸŸ© Samplerã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© Warming up model on rank 0
ğŸŸ© ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã‚’åˆæœŸåŒ– len(token_ids)=4096, sampling_params=SamplingParams(temperature=1.0, max_tokens=64, ignore_eos=False)
ğŸŸ© ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã‚’åˆæœŸåŒ–å®Œäº† self.seq_id=0
ğŸŸ© ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã‚’åˆæœŸåŒ– len(token_ids)=4096, sampling_params=SamplingParams(temperature=1.0, max_tokens=64, ignore_eos=False)
ğŸŸ© ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã‚’åˆæœŸåŒ–å®Œäº† self.seq_id=1
ğŸŸ© ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã‚’åˆæœŸåŒ– len(token_ids)=4096, sampling_params=SamplingParams(temperature=1.0, max_tokens=64, ignore_eos=False)
ğŸŸ© ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã‚’åˆæœŸåŒ–å®Œäº† self.seq_id=2
ğŸŸ© ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã‚’åˆæœŸåŒ– len(token_ids)=4096, sampling_params=SamplingParams(temperature=1.0, max_tokens=64, ignore_eos=False)
ğŸŸ© ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã‚’åˆæœŸåŒ–å®Œäº† self.seq_id=3
ğŸŸ© æ¨è«–ã®1ã‚µã‚¤ã‚¯ãƒ«ã‚’å®Ÿè¡Œé–‹å§‹ len(seqs)=4 is_prefill=True
ğŸŸ© ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’è¨­å®š is_prefill=True cu_seqlens_q=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32) cu_seqlens_k=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32) max_seqlen_q=4096 max_seqlen_k=4096 slot_mapping=tensor([], device='cuda:0', dtype=torch.int32) context_lens=None block_tables=None
ğŸŸ© ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æº–å‚™é–‹å§‹ len(seqs)=4
ğŸŸ© ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æº–å‚™å®Œäº† temperatures=tensor([1., 1., 1., 1.], device='cuda:0', dtype=torch.float32)
ğŸŸ© ãƒ¢ãƒ‡ãƒ«ã®é †ä¼æ¬ã‚’å®Ÿè¡Œé–‹å§‹ input_ids.shape=torch.Size([16384]) positions.shape=torch.Size([16384]) is_prefill=True
ğŸŸ© Qwen3ForCausalLMã®é †ä¼æ’­é–‹å§‹ input_ids.shape=torch.Size([16384]) positions.shape=torch.Size([16384])
ğŸŸ© Qwen3Modelã®é †ä¼æ’­é–‹å§‹ input_ids.shape=torch.Size([16384]) positions.shape=torch.Size([16384])
ğŸŸ© åŸ‹ã‚è¾¼ã¿å±¤ã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([16384])
ğŸŸ© åŸ‹ã‚è¾¼ã¿å±¤ã®é †ä¼æ’­å®Œäº† y.shape=torch.Size([16384, 1024])
ğŸŸ© Qwen3DecoderLayerã®é †ä¼æ’­ positions.shape=torch.Size([16384]) hidden_states.shape=torch.Size([16384, 1024]) residual is None=True
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([16384, 1024]), residual.shape if residual is not None else None=None
ğŸŸ© æ­£è¦åŒ–é–‹å§‹ x.shape=torch.Size([16384, 1024])
ğŸŸ© æ­£è¦åŒ–å®Œäº† x.shape=torch.Size([16384, 1024])
ğŸŸ© Qwen3Attentionã®é †ä¼æ’­ positions.shape=torch.Size([16384]) hidden_states.shape=torch.Size([16384, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([16384, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­å®Œäº† result.shape=torch.Size([16384, 4096])
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([16384, 16, 128]), residual.shape if residual is not None else None=None
ğŸŸ© æ­£è¦åŒ–é–‹å§‹ x.shape=torch.Size([16384, 16, 128])
ğŸŸ© æ­£è¦åŒ–å®Œäº† x.shape=torch.Size([16384, 16, 128])
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([16384, 8, 128]), residual.shape if residual is not None else None=None
ğŸŸ© æ­£è¦åŒ–é–‹å§‹ x.shape=torch.Size([16384, 8, 128])
ğŸŸ© æ­£è¦åŒ–å®Œäº† x.shape=torch.Size([16384, 8, 128])
ğŸŸ© RoPEã®é †ä¼æ’­é–‹å§‹ positions.shape=torch.Size([16384]), query.shape=torch.Size([16384, 16, 128]), key.shape=torch.Size([16384, 8, 128])
ğŸŸ© RoPEã‚’é©ç”¨é–‹å§‹ x.shape=torch.Size([16384, 16, 128]), cos.shape=torch.Size([16384, 1, 64]), sin.shape=torch.Size([16384, 1, 64])
ğŸŸ¦ ãƒãƒ£ãƒ³ã‚¯åŒ– x1.shape=torch.Size([16384, 16, 64]), x2.shape=torch.Size([16384, 16, 64])
ğŸŸ¦ å›è»¢è¡Œåˆ—é©ç”¨ y1.shape=torch.Size([16384, 16, 64]), y2.shape=torch.Size([16384, 16, 64])
ğŸŸ© RoPEã‚’é©ç”¨å®Œäº† result.shape=torch.Size([16384, 16, 128])
ğŸŸ© RoPEã‚’é©ç”¨é–‹å§‹ x.shape=torch.Size([16384, 8, 128]), cos.shape=torch.Size([16384, 1, 64]), sin.shape=torch.Size([16384, 1, 64])
ğŸŸ¦ ãƒãƒ£ãƒ³ã‚¯åŒ– x1.shape=torch.Size([16384, 8, 64]), x2.shape=torch.Size([16384, 8, 64])
ğŸŸ¦ å›è»¢è¡Œåˆ—é©ç”¨ y1.shape=torch.Size([16384, 8, 64]), y2.shape=torch.Size([16384, 8, 64])
ğŸŸ© RoPEã‚’é©ç”¨å®Œäº† result.shape=torch.Size([16384, 8, 128])
ğŸŸ© RoPEã®é †ä¼æ’­å®Œäº† query.shape=torch.Size([16384, 16, 128]), key.shape=torch.Size([16384, 8, 128])
ğŸŸ© Attentionå±¤ã®é †ä¼æ’­é–‹å§‹ q.shape=torch.Size([16384, 16, 128]), k.shape=torch.Size([16384, 8, 128]), v.shape=torch.Size([16384, 8, 128])
ğŸŸ© ç¾åœ¨ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆå–å¾— _CONTEXT=Context(is_prefill=True, cu_seqlens_q=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32), cu_seqlens_k=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32), max_seqlen_q=4096, max_seqlen_k=4096, slot_mapping=tensor([], device='cuda:0', dtype=torch.int32), context_lens=None, block_tables=None)
ğŸŸ¦ ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæƒ…å ±å–å¾— context.is_prefill=True, context.slot_mapping=tensor([], device='cuda:0', dtype=torch.int32), context.max_seqlen_q=4096, context.cu_seqlens_q=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32), context.max_seqlen_k=4096, context.cu_seqlens_k=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32), context.context_lens=None, context.block_tables=None
ğŸŸ¦ Prefillãƒ¢ãƒ¼ãƒ‰ã§FlashAttentionã‚’å®Ÿè¡Œ context.max_seqlen_q=4096, context.cu_seqlens_q=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32), context.max_seqlen_k=4096, context.cu_seqlens_k=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32)
ğŸŸ© Attentionå±¤ã®é †ä¼æ’­å®Œäº† o.shape=torch.Size([16384, 16, 128])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­é–‹å§‹ å…¥åŠ›å½¢çŠ¶: torch.Size([16384, 2048])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­å®Œäº† å‡ºåŠ›å½¢çŠ¶: torch.Size([16384, 1024])
ğŸŸ© Qwen3Attentionã®é †ä¼æ’­å®Œäº† output.shape=torch.Size([16384, 1024])
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([16384, 1024]), residual.shape if residual is not None else None=torch.Size([16384, 1024])
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨å®Œäº†
ğŸŸ© Qwen3MLPã®é †ä¼æ’­ x.shape=torch.Size([16384, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([16384, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­å®Œäº† result.shape=torch.Size([16384, 6144])
ğŸŸ© SwiGLUã‚’é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([16384, 6144])
ğŸŸ¦ ãƒãƒ£ãƒ³ã‚¯åŒ– x.shape=torch.Size([16384, 3072]), y.shape=torch.Size([16384, 3072])
ğŸŸ© SwiGLUã‚’é †ä¼æ’­å®Œäº† result.shape=torch.Size([16384, 3072])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­é–‹å§‹ å…¥åŠ›å½¢çŠ¶: torch.Size([16384, 3072])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­å®Œäº† å‡ºåŠ›å½¢çŠ¶: torch.Size([16384, 1024])
ğŸŸ© Qwen3MLPã®é †ä¼æ’­å®Œäº† x.shape=torch.Size([16384, 1024])
ğŸŸ© Qwen3DecoderLayerã®é †ä¼æ’­å®Œäº† hidden_states.shape=torch.Size([16384, 1024])
ğŸŸ© Qwen3DecoderLayerã®é †ä¼æ’­ positions.shape=torch.Size([16384]) hidden_states.shape=torch.Size([16384, 1024]) residual is None=False
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([16384, 1024]), residual.shape if residual is not None else None=torch.Size([16384, 1024])
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨å®Œäº†
ğŸŸ© Qwen3Attentionã®é †ä¼æ’­ positions.shape=torch.Size([16384]) hidden_states.shape=torch.Size([16384, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([16384, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­å®Œäº† result.shape=torch.Size([16384, 4096])
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([16384, 16, 128]), residual.shape if residual is not None else None=None
ğŸŸ© æ­£è¦åŒ–é–‹å§‹ x.shape=torch.Size([16384, 16, 128])
ğŸŸ© æ­£è¦åŒ–å®Œäº† x.shape=torch.Size([16384, 16, 128])
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([16384, 8, 128]), residual.shape if residual is not None else None=None
ğŸŸ© æ­£è¦åŒ–é–‹å§‹ x.shape=torch.Size([16384, 8, 128])
ğŸŸ© æ­£è¦åŒ–å®Œäº† x.shape=torch.Size([16384, 8, 128])
ğŸŸ© RoPEã®é †ä¼æ’­é–‹å§‹ positions.shape=torch.Size([16384]), query.shape=torch.Size([16384, 16, 128]), key.shape=torch.Size([16384, 8, 128])
ğŸŸ© RoPEã‚’é©ç”¨é–‹å§‹ x.shape=torch.Size([16384, 16, 128]), cos.shape=torch.Size([16384, 1, 64]), sin.shape=torch.Size([16384, 1, 64])
ğŸŸ¦ ãƒãƒ£ãƒ³ã‚¯åŒ– x1.shape=torch.Size([16384, 16, 64]), x2.shape=torch.Size([16384, 16, 64])
ğŸŸ¦ å›è»¢è¡Œåˆ—é©ç”¨ y1.shape=torch.Size([16384, 16, 64]), y2.shape=torch.Size([16384, 16, 64])
ğŸŸ© RoPEã‚’é©ç”¨å®Œäº† result.shape=torch.Size([16384, 16, 128])
ğŸŸ© RoPEã‚’é©ç”¨é–‹å§‹ x.shape=torch.Size([16384, 8, 128]), cos.shape=torch.Size([16384, 1, 64]), sin.shape=torch.Size([16384, 1, 64])
ğŸŸ¦ ãƒãƒ£ãƒ³ã‚¯åŒ– x1.shape=torch.Size([16384, 8, 64]), x2.shape=torch.Size([16384, 8, 64])
ğŸŸ¦ å›è»¢è¡Œåˆ—é©ç”¨ y1.shape=torch.Size([16384, 8, 64]), y2.shape=torch.Size([16384, 8, 64])
ğŸŸ© RoPEã‚’é©ç”¨å®Œäº† result.shape=torch.Size([16384, 8, 128])
ğŸŸ© RoPEã®é †ä¼æ’­å®Œäº† query.shape=torch.Size([16384, 16, 128]), key.shape=torch.Size([16384, 8, 128])
ğŸŸ© Attentionå±¤ã®é †ä¼æ’­é–‹å§‹ q.shape=torch.Size([16384, 16, 128]), k.shape=torch.Size([16384, 8, 128]), v.shape=torch.Size([16384, 8, 128])
ğŸŸ© ç¾åœ¨ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆå–å¾— _CONTEXT=Context(is_prefill=True, cu_seqlens_q=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32), cu_seqlens_k=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32), max_seqlen_q=4096, max_seqlen_k=4096, slot_mapping=tensor([], device='cuda:0', dtype=torch.int32), context_lens=None, block_tables=None)
ğŸŸ¦ ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæƒ…å ±å–å¾— context.is_prefill=True, context.slot_mapping=tensor([], device='cuda:0', dtype=torch.int32), context.max_seqlen_q=4096, context.cu_seqlens_q=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32), context.max_seqlen_k=4096, context.cu_seqlens_k=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32), context.context_lens=None, context.block_tables=None
ğŸŸ¦ Prefillãƒ¢ãƒ¼ãƒ‰ã§FlashAttentionã‚’å®Ÿè¡Œ context.max_seqlen_q=4096, context.cu_seqlens_q=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32), context.max_seqlen_k=4096, context.cu_seqlens_k=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32)
ğŸŸ© Attentionå±¤ã®é †ä¼æ’­å®Œäº† o.shape=torch.Size([16384, 16, 128])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­é–‹å§‹ å…¥åŠ›å½¢çŠ¶: torch.Size([16384, 2048])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­å®Œäº† å‡ºåŠ›å½¢çŠ¶: torch.Size([16384, 1024])
ğŸŸ© Qwen3Attentionã®é †ä¼æ’­å®Œäº† output.shape=torch.Size([16384, 1024])
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([16384, 1024]), residual.shape if residual is not None else None=torch.Size([16384, 1024])
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨å®Œäº†
ğŸŸ© Qwen3MLPã®é †ä¼æ’­ x.shape=torch.Size([16384, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([16384, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­å®Œäº† result.shape=torch.Size([16384, 6144])
ğŸŸ© SwiGLUã‚’é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([16384, 6144])
ğŸŸ¦ ãƒãƒ£ãƒ³ã‚¯åŒ– x.shape=torch.Size([16384, 3072]), y.shape=torch.Size([16384, 3072])
ğŸŸ© SwiGLUã‚’é †ä¼æ’­å®Œäº† result.shape=torch.Size([16384, 3072])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­é–‹å§‹ å…¥åŠ›å½¢çŠ¶: torch.Size([16384, 3072])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­å®Œäº† å‡ºåŠ›å½¢çŠ¶: torch.Size([16384, 1024])
ğŸŸ© Qwen3MLPã®é †ä¼æ’­å®Œäº† x.shape=torch.Size([16384, 1024])
ğŸŸ© Qwen3DecoderLayerã®é †ä¼æ’­å®Œäº† hidden_states.shape=torch.Size([16384, 1024])
ğŸŸ© Qwen3DecoderLayerã®é †ä¼æ’­ positions.shape=torch.Size([16384]) hidden_states.shape=torch.Size([16384, 1024]) residual is None=False
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([16384, 1024]), residual.shape if residual is not None else None=torch.Size([16384, 1024])
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨å®Œäº†
ğŸŸ© Qwen3Attentionã®é †ä¼æ’­ positions.shape=torch.Size([16384]) hidden_states.shape=torch.Size([16384, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([16384, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­å®Œäº† result.shape=torch.Size([16384, 4096])
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([16384, 16, 128]), residual.shape if residual is not None else None=None
ğŸŸ© æ­£è¦åŒ–é–‹å§‹ x.shape=torch.Size([16384, 16, 128])
ğŸŸ© æ­£è¦åŒ–å®Œäº† x.shape=torch.Size([16384, 16, 128])
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([16384, 8, 128]), residual.shape if residual is not None else None=None
ğŸŸ© æ­£è¦åŒ–é–‹å§‹ x.shape=torch.Size([16384, 8, 128])
ğŸŸ© æ­£è¦åŒ–å®Œäº† x.shape=torch.Size([16384, 8, 128])
ğŸŸ© RoPEã®é †ä¼æ’­é–‹å§‹ positions.shape=torch.Size([16384]), query.shape=torch.Size([16384, 16, 128]), key.shape=torch.Size([16384, 8, 128])
ğŸŸ© RoPEã‚’é©ç”¨é–‹å§‹ x.shape=torch.Size([16384, 16, 128]), cos.shape=torch.Size([16384, 1, 64]), sin.shape=torch.Size([16384, 1, 64])
ğŸŸ¦ ãƒãƒ£ãƒ³ã‚¯åŒ– x1.shape=torch.Size([16384, 16, 64]), x2.shape=torch.Size([16384, 16, 64])
ğŸŸ¦ å›è»¢è¡Œåˆ—é©ç”¨ y1.shape=torch.Size([16384, 16, 64]), y2.shape=torch.Size([16384, 16, 64])
ğŸŸ© RoPEã‚’é©ç”¨å®Œäº† result.shape=torch.Size([16384, 16, 128])
ğŸŸ© RoPEã‚’é©ç”¨é–‹å§‹ x.shape=torch.Size([16384, 8, 128]), cos.shape=torch.Size([16384, 1, 64]), sin.shape=torch.Size([16384, 1, 64])
ğŸŸ¦ ãƒãƒ£ãƒ³ã‚¯åŒ– x1.shape=torch.Size([16384, 8, 64]), x2.shape=torch.Size([16384, 8, 64])
ğŸŸ¦ å›è»¢è¡Œåˆ—é©ç”¨ y1.shape=torch.Size([16384, 8, 64]), y2.shape=torch.Size([16384, 8, 64])
ğŸŸ© RoPEã‚’é©ç”¨å®Œäº† result.shape=torch.Size([16384, 8, 128])
ğŸŸ© RoPEã®é †ä¼æ’­å®Œäº† query.shape=torch.Size([16384, 16, 128]), key.shape=torch.Size([16384, 8, 128])
ğŸŸ© Attentionå±¤ã®é †ä¼æ’­é–‹å§‹ q.shape=torch.Size([16384, 16, 128]), k.shape=torch.Size([16384, 8, 128]), v.shape=torch.Size([16384, 8, 128])
ğŸŸ© ç¾åœ¨ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆå–å¾— _CONTEXT=Context(is_prefill=True, cu_seqlens_q=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32), cu_seqlens_k=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32), max_seqlen_q=4096, max_seqlen_k=4096, slot_mapping=tensor([], device='cuda:0', dtype=torch.int32), context_lens=None, block_tables=None)
ğŸŸ¦ ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæƒ…å ±å–å¾— context.is_prefill=True, context.slot_mapping=tensor([], device='cuda:0', dtype=torch.int32), context.max_seqlen_q=4096, context.cu_seqlens_q=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32), context.max_seqlen_k=4096, context.cu_seqlens_k=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32), context.context_lens=None, context.block_tables=None
ğŸŸ¦ Prefillãƒ¢ãƒ¼ãƒ‰ã§FlashAttentionã‚’å®Ÿè¡Œ context.max_seqlen_q=4096, context.cu_seqlens_q=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32), context.max_seqlen_k=4096, context.cu_seqlens_k=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32)
ğŸŸ© Attentionå±¤ã®é †ä¼æ’­å®Œäº† o.shape=torch.Size([16384, 16, 128])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­é–‹å§‹ å…¥åŠ›å½¢çŠ¶: torch.Size([16384, 2048])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­å®Œäº† å‡ºåŠ›å½¢çŠ¶: torch.Size([16384, 1024])
ğŸŸ© Qwen3Attentionã®é †ä¼æ’­å®Œäº† output.shape=torch.Size([16384, 1024])
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([16384, 1024]), residual.shape if residual is not None else None=torch.Size([16384, 1024])
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨å®Œäº†
ğŸŸ© Qwen3MLPã®é †ä¼æ’­ x.shape=torch.Size([16384, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([16384, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­å®Œäº† result.shape=torch.Size([16384, 6144])
ğŸŸ© SwiGLUã‚’é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([16384, 6144])
ğŸŸ¦ ãƒãƒ£ãƒ³ã‚¯åŒ– x.shape=torch.Size([16384, 3072]), y.shape=torch.Size([16384, 3072])
ğŸŸ© SwiGLUã‚’é †ä¼æ’­å®Œäº† result.shape=torch.Size([16384, 3072])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­é–‹å§‹ å…¥åŠ›å½¢çŠ¶: torch.Size([16384, 3072])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­å®Œäº† å‡ºåŠ›å½¢çŠ¶: torch.Size([16384, 1024])
ğŸŸ© Qwen3MLPã®é †ä¼æ’­å®Œäº† x.shape=torch.Size([16384, 1024])
ğŸŸ© Qwen3DecoderLayerã®é †ä¼æ’­å®Œäº† hidden_states.shape=torch.Size([16384, 1024])
ğŸŸ© Qwen3DecoderLayerã®é †ä¼æ’­ positions.shape=torch.Size([16384]) hidden_states.shape=torch.Size([16384, 1024]) residual is None=False
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([16384, 1024]), residual.shape if residual is not None else None=torch.Size([16384, 1024])
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨å®Œäº†
ğŸŸ© Qwen3Attentionã®é †ä¼æ’­ positions.shape=torch.Size([16384]) hidden_states.shape=torch.Size([16384, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([16384, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­å®Œäº† result.shape=torch.Size([16384, 4096])
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([16384, 16, 128]), residual.shape if residual is not None else None=None
ğŸŸ© æ­£è¦åŒ–é–‹å§‹ x.shape=torch.Size([16384, 16, 128])
ğŸŸ© æ­£è¦åŒ–å®Œäº† x.shape=torch.Size([16384, 16, 128])
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([16384, 8, 128]), residual.shape if residual is not None else None=None
ğŸŸ© æ­£è¦åŒ–é–‹å§‹ x.shape=torch.Size([16384, 8, 128])
ğŸŸ© æ­£è¦åŒ–å®Œäº† x.shape=torch.Size([16384, 8, 128])
ğŸŸ© RoPEã®é †ä¼æ’­é–‹å§‹ positions.shape=torch.Size([16384]), query.shape=torch.Size([16384, 16, 128]), key.shape=torch.Size([16384, 8, 128])
ğŸŸ© RoPEã‚’é©ç”¨é–‹å§‹ x.shape=torch.Size([16384, 16, 128]), cos.shape=torch.Size([16384, 1, 64]), sin.shape=torch.Size([16384, 1, 64])
ğŸŸ¦ ãƒãƒ£ãƒ³ã‚¯åŒ– x1.shape=torch.Size([16384, 16, 64]), x2.shape=torch.Size([16384, 16, 64])
ğŸŸ¦ å›è»¢è¡Œåˆ—é©ç”¨ y1.shape=torch.Size([16384, 16, 64]), y2.shape=torch.Size([16384, 16, 64])
ğŸŸ© RoPEã‚’é©ç”¨å®Œäº† result.shape=torch.Size([16384, 16, 128])
ğŸŸ© RoPEã‚’é©ç”¨é–‹å§‹ x.shape=torch.Size([16384, 8, 128]), cos.shape=torch.Size([16384, 1, 64]), sin.shape=torch.Size([16384, 1, 64])
ğŸŸ¦ ãƒãƒ£ãƒ³ã‚¯åŒ– x1.shape=torch.Size([16384, 8, 64]), x2.shape=torch.Size([16384, 8, 64])
ğŸŸ¦ å›è»¢è¡Œåˆ—é©ç”¨ y1.shape=torch.Size([16384, 8, 64]), y2.shape=torch.Size([16384, 8, 64])
ğŸŸ© RoPEã‚’é©ç”¨å®Œäº† result.shape=torch.Size([16384, 8, 128])
ğŸŸ© RoPEã®é †ä¼æ’­å®Œäº† query.shape=torch.Size([16384, 16, 128]), key.shape=torch.Size([16384, 8, 128])
ğŸŸ© Attentionå±¤ã®é †ä¼æ’­é–‹å§‹ q.shape=torch.Size([16384, 16, 128]), k.shape=torch.Size([16384, 8, 128]), v.shape=torch.Size([16384, 8, 128])
ğŸŸ© ç¾åœ¨ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆå–å¾— _CONTEXT=Context(is_prefill=True, cu_seqlens_q=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32), cu_seqlens_k=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32), max_seqlen_q=4096, max_seqlen_k=4096, slot_mapping=tensor([], device='cuda:0', dtype=torch.int32), context_lens=None, block_tables=None)
ğŸŸ¦ ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæƒ…å ±å–å¾— context.is_prefill=True, context.slot_mapping=tensor([], device='cuda:0', dtype=torch.int32), context.max_seqlen_q=4096, context.cu_seqlens_q=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32), context.max_seqlen_k=4096, context.cu_seqlens_k=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32), context.context_lens=None, context.block_tables=None
ğŸŸ¦ Prefillãƒ¢ãƒ¼ãƒ‰ã§FlashAttentionã‚’å®Ÿè¡Œ context.max_seqlen_q=4096, context.cu_seqlens_q=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32), context.max_seqlen_k=4096, context.cu_seqlens_k=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32)
ğŸŸ© Attentionå±¤ã®é †ä¼æ’­å®Œäº† o.shape=torch.Size([16384, 16, 128])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­é–‹å§‹ å…¥åŠ›å½¢çŠ¶: torch.Size([16384, 2048])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­å®Œäº† å‡ºåŠ›å½¢çŠ¶: torch.Size([16384, 1024])
ğŸŸ© Qwen3Attentionã®é †ä¼æ’­å®Œäº† output.shape=torch.Size([16384, 1024])
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([16384, 1024]), residual.shape if residual is not None else None=torch.Size([16384, 1024])
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨å®Œäº†
ğŸŸ© Qwen3MLPã®é †ä¼æ’­ x.shape=torch.Size([16384, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([16384, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­å®Œäº† result.shape=torch.Size([16384, 6144])
ğŸŸ© SwiGLUã‚’é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([16384, 6144])
ğŸŸ¦ ãƒãƒ£ãƒ³ã‚¯åŒ– x.shape=torch.Size([16384, 3072]), y.shape=torch.Size([16384, 3072])
ğŸŸ© SwiGLUã‚’é †ä¼æ’­å®Œäº† result.shape=torch.Size([16384, 3072])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­é–‹å§‹ å…¥åŠ›å½¢çŠ¶: torch.Size([16384, 3072])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­å®Œäº† å‡ºåŠ›å½¢çŠ¶: torch.Size([16384, 1024])
ğŸŸ© Qwen3MLPã®é †ä¼æ’­å®Œäº† x.shape=torch.Size([16384, 1024])
ğŸŸ© Qwen3DecoderLayerã®é †ä¼æ’­å®Œäº† hidden_states.shape=torch.Size([16384, 1024])
ğŸŸ© Qwen3DecoderLayerã®é †ä¼æ’­ positions.shape=torch.Size([16384]) hidden_states.shape=torch.Size([16384, 1024]) residual is None=False
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([16384, 1024]), residual.shape if residual is not None else None=torch.Size([16384, 1024])
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨å®Œäº†
ğŸŸ© Qwen3Attentionã®é †ä¼æ’­ positions.shape=torch.Size([16384]) hidden_states.shape=torch.Size([16384, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([16384, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­å®Œäº† result.shape=torch.Size([16384, 4096])
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([16384, 16, 128]), residual.shape if residual is not None else None=None
ğŸŸ© æ­£è¦åŒ–é–‹å§‹ x.shape=torch.Size([16384, 16, 128])
ğŸŸ© æ­£è¦åŒ–å®Œäº† x.shape=torch.Size([16384, 16, 128])
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([16384, 8, 128]), residual.shape if residual is not None else None=None
ğŸŸ© æ­£è¦åŒ–é–‹å§‹ x.shape=torch.Size([16384, 8, 128])
ğŸŸ© æ­£è¦åŒ–å®Œäº† x.shape=torch.Size([16384, 8, 128])
ğŸŸ© RoPEã®é †ä¼æ’­é–‹å§‹ positions.shape=torch.Size([16384]), query.shape=torch.Size([16384, 16, 128]), key.shape=torch.Size([16384, 8, 128])
ğŸŸ© RoPEã‚’é©ç”¨é–‹å§‹ x.shape=torch.Size([16384, 16, 128]), cos.shape=torch.Size([16384, 1, 64]), sin.shape=torch.Size([16384, 1, 64])
ğŸŸ¦ ãƒãƒ£ãƒ³ã‚¯åŒ– x1.shape=torch.Size([16384, 16, 64]), x2.shape=torch.Size([16384, 16, 64])
ğŸŸ¦ å›è»¢è¡Œåˆ—é©ç”¨ y1.shape=torch.Size([16384, 16, 64]), y2.shape=torch.Size([16384, 16, 64])
ğŸŸ© RoPEã‚’é©ç”¨å®Œäº† result.shape=torch.Size([16384, 16, 128])
ğŸŸ© RoPEã‚’é©ç”¨é–‹å§‹ x.shape=torch.Size([16384, 8, 128]), cos.shape=torch.Size([16384, 1, 64]), sin.shape=torch.Size([16384, 1, 64])
ğŸŸ¦ ãƒãƒ£ãƒ³ã‚¯åŒ– x1.shape=torch.Size([16384, 8, 64]), x2.shape=torch.Size([16384, 8, 64])
ğŸŸ¦ å›è»¢è¡Œåˆ—é©ç”¨ y1.shape=torch.Size([16384, 8, 64]), y2.shape=torch.Size([16384, 8, 64])
ğŸŸ© RoPEã‚’é©ç”¨å®Œäº† result.shape=torch.Size([16384, 8, 128])
ğŸŸ© RoPEã®é †ä¼æ’­å®Œäº† query.shape=torch.Size([16384, 16, 128]), key.shape=torch.Size([16384, 8, 128])
ğŸŸ© Attentionå±¤ã®é †ä¼æ’­é–‹å§‹ q.shape=torch.Size([16384, 16, 128]), k.shape=torch.Size([16384, 8, 128]), v.shape=torch.Size([16384, 8, 128])
ğŸŸ© ç¾åœ¨ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆå–å¾— _CONTEXT=Context(is_prefill=True, cu_seqlens_q=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32), cu_seqlens_k=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32), max_seqlen_q=4096, max_seqlen_k=4096, slot_mapping=tensor([], device='cuda:0', dtype=torch.int32), context_lens=None, block_tables=None)
ğŸŸ¦ ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæƒ…å ±å–å¾— context.is_prefill=True, context.slot_mapping=tensor([], device='cuda:0', dtype=torch.int32), context.max_seqlen_q=4096, context.cu_seqlens_q=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32), context.max_seqlen_k=4096, context.cu_seqlens_k=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32), context.context_lens=None, context.block_tables=None
ğŸŸ¦ Prefillãƒ¢ãƒ¼ãƒ‰ã§FlashAttentionã‚’å®Ÿè¡Œ context.max_seqlen_q=4096, context.cu_seqlens_q=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32), context.max_seqlen_k=4096, context.cu_seqlens_k=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32)
ğŸŸ© Attentionå±¤ã®é †ä¼æ’­å®Œäº† o.shape=torch.Size([16384, 16, 128])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­é–‹å§‹ å…¥åŠ›å½¢çŠ¶: torch.Size([16384, 2048])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­å®Œäº† å‡ºåŠ›å½¢çŠ¶: torch.Size([16384, 1024])
ğŸŸ© Qwen3Attentionã®é †ä¼æ’­å®Œäº† output.shape=torch.Size([16384, 1024])
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([16384, 1024]), residual.shape if residual is not None else None=torch.Size([16384, 1024])
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨å®Œäº†
ğŸŸ© Qwen3MLPã®é †ä¼æ’­ x.shape=torch.Size([16384, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([16384, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­å®Œäº† result.shape=torch.Size([16384, 6144])
ğŸŸ© SwiGLUã‚’é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([16384, 6144])
ğŸŸ¦ ãƒãƒ£ãƒ³ã‚¯åŒ– x.shape=torch.Size([16384, 3072]), y.shape=torch.Size([16384, 3072])
ğŸŸ© SwiGLUã‚’é †ä¼æ’­å®Œäº† result.shape=torch.Size([16384, 3072])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­é–‹å§‹ å…¥åŠ›å½¢çŠ¶: torch.Size([16384, 3072])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­å®Œäº† å‡ºåŠ›å½¢çŠ¶: torch.Size([16384, 1024])
ğŸŸ© Qwen3MLPã®é †ä¼æ’­å®Œäº† x.shape=torch.Size([16384, 1024])
ğŸŸ© Qwen3DecoderLayerã®é †ä¼æ’­å®Œäº† hidden_states.shape=torch.Size([16384, 1024])
ğŸŸ© Qwen3DecoderLayerã®é †ä¼æ’­ positions.shape=torch.Size([16384]) hidden_states.shape=torch.Size([16384, 1024]) residual is None=False
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([16384, 1024]), residual.shape if residual is not None else None=torch.Size([16384, 1024])
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨å®Œäº†
ğŸŸ© Qwen3Attentionã®é †ä¼æ’­ positions.shape=torch.Size([16384]) hidden_states.shape=torch.Size([16384, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([16384, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­å®Œäº† result.shape=torch.Size([16384, 4096])
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([16384, 16, 128]), residual.shape if residual is not None else None=None
ğŸŸ© æ­£è¦åŒ–é–‹å§‹ x.shape=torch.Size([16384, 16, 128])
ğŸŸ© æ­£è¦åŒ–å®Œäº† x.shape=torch.Size([16384, 16, 128])
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([16384, 8, 128]), residual.shape if residual is not None else None=None
ğŸŸ© æ­£è¦åŒ–é–‹å§‹ x.shape=torch.Size([16384, 8, 128])
ğŸŸ© æ­£è¦åŒ–å®Œäº† x.shape=torch.Size([16384, 8, 128])
ğŸŸ© RoPEã®é †ä¼æ’­é–‹å§‹ positions.shape=torch.Size([16384]), query.shape=torch.Size([16384, 16, 128]), key.shape=torch.Size([16384, 8, 128])
ğŸŸ© RoPEã‚’é©ç”¨é–‹å§‹ x.shape=torch.Size([16384, 16, 128]), cos.shape=torch.Size([16384, 1, 64]), sin.shape=torch.Size([16384, 1, 64])
ğŸŸ¦ ãƒãƒ£ãƒ³ã‚¯åŒ– x1.shape=torch.Size([16384, 16, 64]), x2.shape=torch.Size([16384, 16, 64])
ğŸŸ¦ å›è»¢è¡Œåˆ—é©ç”¨ y1.shape=torch.Size([16384, 16, 64]), y2.shape=torch.Size([16384, 16, 64])
ğŸŸ© RoPEã‚’é©ç”¨å®Œäº† result.shape=torch.Size([16384, 16, 128])
ğŸŸ© RoPEã‚’é©ç”¨é–‹å§‹ x.shape=torch.Size([16384, 8, 128]), cos.shape=torch.Size([16384, 1, 64]), sin.shape=torch.Size([16384, 1, 64])
ğŸŸ¦ ãƒãƒ£ãƒ³ã‚¯åŒ– x1.shape=torch.Size([16384, 8, 64]), x2.shape=torch.Size([16384, 8, 64])
ğŸŸ¦ å›è»¢è¡Œåˆ—é©ç”¨ y1.shape=torch.Size([16384, 8, 64]), y2.shape=torch.Size([16384, 8, 64])
ğŸŸ© RoPEã‚’é©ç”¨å®Œäº† result.shape=torch.Size([16384, 8, 128])
ğŸŸ© RoPEã®é †ä¼æ’­å®Œäº† query.shape=torch.Size([16384, 16, 128]), key.shape=torch.Size([16384, 8, 128])
ğŸŸ© Attentionå±¤ã®é †ä¼æ’­é–‹å§‹ q.shape=torch.Size([16384, 16, 128]), k.shape=torch.Size([16384, 8, 128]), v.shape=torch.Size([16384, 8, 128])
ğŸŸ© ç¾åœ¨ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆå–å¾— _CONTEXT=Context(is_prefill=True, cu_seqlens_q=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32), cu_seqlens_k=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32), max_seqlen_q=4096, max_seqlen_k=4096, slot_mapping=tensor([], device='cuda:0', dtype=torch.int32), context_lens=None, block_tables=None)
ğŸŸ¦ ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæƒ…å ±å–å¾— context.is_prefill=True, context.slot_mapping=tensor([], device='cuda:0', dtype=torch.int32), context.max_seqlen_q=4096, context.cu_seqlens_q=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32), context.max_seqlen_k=4096, context.cu_seqlens_k=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32), context.context_lens=None, context.block_tables=None
ğŸŸ¦ Prefillãƒ¢ãƒ¼ãƒ‰ã§FlashAttentionã‚’å®Ÿè¡Œ context.max_seqlen_q=4096, context.cu_seqlens_q=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32), context.max_seqlen_k=4096, context.cu_seqlens_k=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32)
ğŸŸ© Attentionå±¤ã®é †ä¼æ’­å®Œäº† o.shape=torch.Size([16384, 16, 128])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­é–‹å§‹ å…¥åŠ›å½¢çŠ¶: torch.Size([16384, 2048])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­å®Œäº† å‡ºåŠ›å½¢çŠ¶: torch.Size([16384, 1024])
ğŸŸ© Qwen3Attentionã®é †ä¼æ’­å®Œäº† output.shape=torch.Size([16384, 1024])
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([16384, 1024]), residual.shape if residual is not None else None=torch.Size([16384, 1024])
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨å®Œäº†
ğŸŸ© Qwen3MLPã®é †ä¼æ’­ x.shape=torch.Size([16384, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([16384, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­å®Œäº† result.shape=torch.Size([16384, 6144])
ğŸŸ© SwiGLUã‚’é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([16384, 6144])
ğŸŸ¦ ãƒãƒ£ãƒ³ã‚¯åŒ– x.shape=torch.Size([16384, 3072]), y.shape=torch.Size([16384, 3072])
ğŸŸ© SwiGLUã‚’é †ä¼æ’­å®Œäº† result.shape=torch.Size([16384, 3072])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­é–‹å§‹ å…¥åŠ›å½¢çŠ¶: torch.Size([16384, 3072])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­å®Œäº† å‡ºåŠ›å½¢çŠ¶: torch.Size([16384, 1024])
ğŸŸ© Qwen3MLPã®é †ä¼æ’­å®Œäº† x.shape=torch.Size([16384, 1024])
ğŸŸ© Qwen3DecoderLayerã®é †ä¼æ’­å®Œäº† hidden_states.shape=torch.Size([16384, 1024])
ğŸŸ© Qwen3DecoderLayerã®é †ä¼æ’­ positions.shape=torch.Size([16384]) hidden_states.shape=torch.Size([16384, 1024]) residual is None=False
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([16384, 1024]), residual.shape if residual is not None else None=torch.Size([16384, 1024])
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨å®Œäº†
ğŸŸ© Qwen3Attentionã®é †ä¼æ’­ positions.shape=torch.Size([16384]) hidden_states.shape=torch.Size([16384, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([16384, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­å®Œäº† result.shape=torch.Size([16384, 4096])
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([16384, 16, 128]), residual.shape if residual is not None else None=None
ğŸŸ© æ­£è¦åŒ–é–‹å§‹ x.shape=torch.Size([16384, 16, 128])
ğŸŸ© æ­£è¦åŒ–å®Œäº† x.shape=torch.Size([16384, 16, 128])
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([16384, 8, 128]), residual.shape if residual is not None else None=None
ğŸŸ© æ­£è¦åŒ–é–‹å§‹ x.shape=torch.Size([16384, 8, 128])
ğŸŸ© æ­£è¦åŒ–å®Œäº† x.shape=torch.Size([16384, 8, 128])
ğŸŸ© RoPEã®é †ä¼æ’­é–‹å§‹ positions.shape=torch.Size([16384]), query.shape=torch.Size([16384, 16, 128]), key.shape=torch.Size([16384, 8, 128])
ğŸŸ© RoPEã‚’é©ç”¨é–‹å§‹ x.shape=torch.Size([16384, 16, 128]), cos.shape=torch.Size([16384, 1, 64]), sin.shape=torch.Size([16384, 1, 64])
ğŸŸ¦ ãƒãƒ£ãƒ³ã‚¯åŒ– x1.shape=torch.Size([16384, 16, 64]), x2.shape=torch.Size([16384, 16, 64])
ğŸŸ¦ å›è»¢è¡Œåˆ—é©ç”¨ y1.shape=torch.Size([16384, 16, 64]), y2.shape=torch.Size([16384, 16, 64])
ğŸŸ© RoPEã‚’é©ç”¨å®Œäº† result.shape=torch.Size([16384, 16, 128])
ğŸŸ© RoPEã‚’é©ç”¨é–‹å§‹ x.shape=torch.Size([16384, 8, 128]), cos.shape=torch.Size([16384, 1, 64]), sin.shape=torch.Size([16384, 1, 64])
ğŸŸ¦ ãƒãƒ£ãƒ³ã‚¯åŒ– x1.shape=torch.Size([16384, 8, 64]), x2.shape=torch.Size([16384, 8, 64])
ğŸŸ¦ å›è»¢è¡Œåˆ—é©ç”¨ y1.shape=torch.Size([16384, 8, 64]), y2.shape=torch.Size([16384, 8, 64])
ğŸŸ© RoPEã‚’é©ç”¨å®Œäº† result.shape=torch.Size([16384, 8, 128])
ğŸŸ© RoPEã®é †ä¼æ’­å®Œäº† query.shape=torch.Size([16384, 16, 128]), key.shape=torch.Size([16384, 8, 128])
ğŸŸ© Attentionå±¤ã®é †ä¼æ’­é–‹å§‹ q.shape=torch.Size([16384, 16, 128]), k.shape=torch.Size([16384, 8, 128]), v.shape=torch.Size([16384, 8, 128])
ğŸŸ© ç¾åœ¨ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆå–å¾— _CONTEXT=Context(is_prefill=True, cu_seqlens_q=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32), cu_seqlens_k=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32), max_seqlen_q=4096, max_seqlen_k=4096, slot_mapping=tensor([], device='cuda:0', dtype=torch.int32), context_lens=None, block_tables=None)
ğŸŸ¦ ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæƒ…å ±å–å¾— context.is_prefill=True, context.slot_mapping=tensor([], device='cuda:0', dtype=torch.int32), context.max_seqlen_q=4096, context.cu_seqlens_q=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32), context.max_seqlen_k=4096, context.cu_seqlens_k=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32), context.context_lens=None, context.block_tables=None
ğŸŸ¦ Prefillãƒ¢ãƒ¼ãƒ‰ã§FlashAttentionã‚’å®Ÿè¡Œ context.max_seqlen_q=4096, context.cu_seqlens_q=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32), context.max_seqlen_k=4096, context.cu_seqlens_k=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32)
ğŸŸ© Attentionå±¤ã®é †ä¼æ’­å®Œäº† o.shape=torch.Size([16384, 16, 128])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­é–‹å§‹ å…¥åŠ›å½¢çŠ¶: torch.Size([16384, 2048])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­å®Œäº† å‡ºåŠ›å½¢çŠ¶: torch.Size([16384, 1024])
ğŸŸ© Qwen3Attentionã®é †ä¼æ’­å®Œäº† output.shape=torch.Size([16384, 1024])
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([16384, 1024]), residual.shape if residual is not None else None=torch.Size([16384, 1024])
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨å®Œäº†
ğŸŸ© Qwen3MLPã®é †ä¼æ’­ x.shape=torch.Size([16384, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([16384, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­å®Œäº† result.shape=torch.Size([16384, 6144])
ğŸŸ© SwiGLUã‚’é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([16384, 6144])
ğŸŸ¦ ãƒãƒ£ãƒ³ã‚¯åŒ– x.shape=torch.Size([16384, 3072]), y.shape=torch.Size([16384, 3072])
ğŸŸ© SwiGLUã‚’é †ä¼æ’­å®Œäº† result.shape=torch.Size([16384, 3072])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­é–‹å§‹ å…¥åŠ›å½¢çŠ¶: torch.Size([16384, 3072])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­å®Œäº† å‡ºåŠ›å½¢çŠ¶: torch.Size([16384, 1024])
ğŸŸ© Qwen3MLPã®é †ä¼æ’­å®Œäº† x.shape=torch.Size([16384, 1024])
ğŸŸ© Qwen3DecoderLayerã®é †ä¼æ’­å®Œäº† hidden_states.shape=torch.Size([16384, 1024])
ğŸŸ© Qwen3DecoderLayerã®é †ä¼æ’­ positions.shape=torch.Size([16384]) hidden_states.shape=torch.Size([16384, 1024]) residual is None=False
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([16384, 1024]), residual.shape if residual is not None else None=torch.Size([16384, 1024])
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨å®Œäº†
ğŸŸ© Qwen3Attentionã®é †ä¼æ’­ positions.shape=torch.Size([16384]) hidden_states.shape=torch.Size([16384, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([16384, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­å®Œäº† result.shape=torch.Size([16384, 4096])
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([16384, 16, 128]), residual.shape if residual is not None else None=None
ğŸŸ© æ­£è¦åŒ–é–‹å§‹ x.shape=torch.Size([16384, 16, 128])
ğŸŸ© æ­£è¦åŒ–å®Œäº† x.shape=torch.Size([16384, 16, 128])
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([16384, 8, 128]), residual.shape if residual is not None else None=None
ğŸŸ© æ­£è¦åŒ–é–‹å§‹ x.shape=torch.Size([16384, 8, 128])
ğŸŸ© æ­£è¦åŒ–å®Œäº† x.shape=torch.Size([16384, 8, 128])
ğŸŸ© RoPEã®é †ä¼æ’­é–‹å§‹ positions.shape=torch.Size([16384]), query.shape=torch.Size([16384, 16, 128]), key.shape=torch.Size([16384, 8, 128])
ğŸŸ© RoPEã‚’é©ç”¨é–‹å§‹ x.shape=torch.Size([16384, 16, 128]), cos.shape=torch.Size([16384, 1, 64]), sin.shape=torch.Size([16384, 1, 64])
ğŸŸ¦ ãƒãƒ£ãƒ³ã‚¯åŒ– x1.shape=torch.Size([16384, 16, 64]), x2.shape=torch.Size([16384, 16, 64])
ğŸŸ¦ å›è»¢è¡Œåˆ—é©ç”¨ y1.shape=torch.Size([16384, 16, 64]), y2.shape=torch.Size([16384, 16, 64])
ğŸŸ© RoPEã‚’é©ç”¨å®Œäº† result.shape=torch.Size([16384, 16, 128])
ğŸŸ© RoPEã‚’é©ç”¨é–‹å§‹ x.shape=torch.Size([16384, 8, 128]), cos.shape=torch.Size([16384, 1, 64]), sin.shape=torch.Size([16384, 1, 64])
ğŸŸ¦ ãƒãƒ£ãƒ³ã‚¯åŒ– x1.shape=torch.Size([16384, 8, 64]), x2.shape=torch.Size([16384, 8, 64])
ğŸŸ¦ å›è»¢è¡Œåˆ—é©ç”¨ y1.shape=torch.Size([16384, 8, 64]), y2.shape=torch.Size([16384, 8, 64])
ğŸŸ© RoPEã‚’é©ç”¨å®Œäº† result.shape=torch.Size([16384, 8, 128])
ğŸŸ© RoPEã®é †ä¼æ’­å®Œäº† query.shape=torch.Size([16384, 16, 128]), key.shape=torch.Size([16384, 8, 128])
ğŸŸ© Attentionå±¤ã®é †ä¼æ’­é–‹å§‹ q.shape=torch.Size([16384, 16, 128]), k.shape=torch.Size([16384, 8, 128]), v.shape=torch.Size([16384, 8, 128])
ğŸŸ© ç¾åœ¨ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆå–å¾— _CONTEXT=Context(is_prefill=True, cu_seqlens_q=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32), cu_seqlens_k=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32), max_seqlen_q=4096, max_seqlen_k=4096, slot_mapping=tensor([], device='cuda:0', dtype=torch.int32), context_lens=None, block_tables=None)
ğŸŸ¦ ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæƒ…å ±å–å¾— context.is_prefill=True, context.slot_mapping=tensor([], device='cuda:0', dtype=torch.int32), context.max_seqlen_q=4096, context.cu_seqlens_q=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32), context.max_seqlen_k=4096, context.cu_seqlens_k=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32), context.context_lens=None, context.block_tables=None
ğŸŸ¦ Prefillãƒ¢ãƒ¼ãƒ‰ã§FlashAttentionã‚’å®Ÿè¡Œ context.max_seqlen_q=4096, context.cu_seqlens_q=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32), context.max_seqlen_k=4096, context.cu_seqlens_k=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32)
ğŸŸ© Attentionå±¤ã®é †ä¼æ’­å®Œäº† o.shape=torch.Size([16384, 16, 128])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­é–‹å§‹ å…¥åŠ›å½¢çŠ¶: torch.Size([16384, 2048])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­å®Œäº† å‡ºåŠ›å½¢çŠ¶: torch.Size([16384, 1024])
ğŸŸ© Qwen3Attentionã®é †ä¼æ’­å®Œäº† output.shape=torch.Size([16384, 1024])
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([16384, 1024]), residual.shape if residual is not None else None=torch.Size([16384, 1024])
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨å®Œäº†
ğŸŸ© Qwen3MLPã®é †ä¼æ’­ x.shape=torch.Size([16384, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([16384, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­å®Œäº† result.shape=torch.Size([16384, 6144])
ğŸŸ© SwiGLUã‚’é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([16384, 6144])
ğŸŸ¦ ãƒãƒ£ãƒ³ã‚¯åŒ– x.shape=torch.Size([16384, 3072]), y.shape=torch.Size([16384, 3072])
ğŸŸ© SwiGLUã‚’é †ä¼æ’­å®Œäº† result.shape=torch.Size([16384, 3072])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­é–‹å§‹ å…¥åŠ›å½¢çŠ¶: torch.Size([16384, 3072])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­å®Œäº† å‡ºåŠ›å½¢çŠ¶: torch.Size([16384, 1024])
ğŸŸ© Qwen3MLPã®é †ä¼æ’­å®Œäº† x.shape=torch.Size([16384, 1024])
ğŸŸ© Qwen3DecoderLayerã®é †ä¼æ’­å®Œäº† hidden_states.shape=torch.Size([16384, 1024])
ğŸŸ© Qwen3DecoderLayerã®é †ä¼æ’­ positions.shape=torch.Size([16384]) hidden_states.shape=torch.Size([16384, 1024]) residual is None=False
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([16384, 1024]), residual.shape if residual is not None else None=torch.Size([16384, 1024])
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨å®Œäº†
ğŸŸ© Qwen3Attentionã®é †ä¼æ’­ positions.shape=torch.Size([16384]) hidden_states.shape=torch.Size([16384, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([16384, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­å®Œäº† result.shape=torch.Size([16384, 4096])
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([16384, 16, 128]), residual.shape if residual is not None else None=None
ğŸŸ© æ­£è¦åŒ–é–‹å§‹ x.shape=torch.Size([16384, 16, 128])
ğŸŸ© æ­£è¦åŒ–å®Œäº† x.shape=torch.Size([16384, 16, 128])
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([16384, 8, 128]), residual.shape if residual is not None else None=None
ğŸŸ© æ­£è¦åŒ–é–‹å§‹ x.shape=torch.Size([16384, 8, 128])
ğŸŸ© æ­£è¦åŒ–å®Œäº† x.shape=torch.Size([16384, 8, 128])
ğŸŸ© RoPEã®é †ä¼æ’­é–‹å§‹ positions.shape=torch.Size([16384]), query.shape=torch.Size([16384, 16, 128]), key.shape=torch.Size([16384, 8, 128])
ğŸŸ© RoPEã‚’é©ç”¨é–‹å§‹ x.shape=torch.Size([16384, 16, 128]), cos.shape=torch.Size([16384, 1, 64]), sin.shape=torch.Size([16384, 1, 64])
ğŸŸ¦ ãƒãƒ£ãƒ³ã‚¯åŒ– x1.shape=torch.Size([16384, 16, 64]), x2.shape=torch.Size([16384, 16, 64])
ğŸŸ¦ å›è»¢è¡Œåˆ—é©ç”¨ y1.shape=torch.Size([16384, 16, 64]), y2.shape=torch.Size([16384, 16, 64])
ğŸŸ© RoPEã‚’é©ç”¨å®Œäº† result.shape=torch.Size([16384, 16, 128])
ğŸŸ© RoPEã‚’é©ç”¨é–‹å§‹ x.shape=torch.Size([16384, 8, 128]), cos.shape=torch.Size([16384, 1, 64]), sin.shape=torch.Size([16384, 1, 64])
ğŸŸ¦ ãƒãƒ£ãƒ³ã‚¯åŒ– x1.shape=torch.Size([16384, 8, 64]), x2.shape=torch.Size([16384, 8, 64])
ğŸŸ¦ å›è»¢è¡Œåˆ—é©ç”¨ y1.shape=torch.Size([16384, 8, 64]), y2.shape=torch.Size([16384, 8, 64])
ğŸŸ© RoPEã‚’é©ç”¨å®Œäº† result.shape=torch.Size([16384, 8, 128])
ğŸŸ© RoPEã®é †ä¼æ’­å®Œäº† query.shape=torch.Size([16384, 16, 128]), key.shape=torch.Size([16384, 8, 128])
ğŸŸ© Attentionå±¤ã®é †ä¼æ’­é–‹å§‹ q.shape=torch.Size([16384, 16, 128]), k.shape=torch.Size([16384, 8, 128]), v.shape=torch.Size([16384, 8, 128])
ğŸŸ© ç¾åœ¨ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆå–å¾— _CONTEXT=Context(is_prefill=True, cu_seqlens_q=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32), cu_seqlens_k=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32), max_seqlen_q=4096, max_seqlen_k=4096, slot_mapping=tensor([], device='cuda:0', dtype=torch.int32), context_lens=None, block_tables=None)
ğŸŸ¦ ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæƒ…å ±å–å¾— context.is_prefill=True, context.slot_mapping=tensor([], device='cuda:0', dtype=torch.int32), context.max_seqlen_q=4096, context.cu_seqlens_q=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32), context.max_seqlen_k=4096, context.cu_seqlens_k=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32), context.context_lens=None, context.block_tables=None
ğŸŸ¦ Prefillãƒ¢ãƒ¼ãƒ‰ã§FlashAttentionã‚’å®Ÿè¡Œ context.max_seqlen_q=4096, context.cu_seqlens_q=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32), context.max_seqlen_k=4096, context.cu_seqlens_k=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32)
ğŸŸ© Attentionå±¤ã®é †ä¼æ’­å®Œäº† o.shape=torch.Size([16384, 16, 128])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­é–‹å§‹ å…¥åŠ›å½¢çŠ¶: torch.Size([16384, 2048])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­å®Œäº† å‡ºåŠ›å½¢çŠ¶: torch.Size([16384, 1024])
ğŸŸ© Qwen3Attentionã®é †ä¼æ’­å®Œäº† output.shape=torch.Size([16384, 1024])
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([16384, 1024]), residual.shape if residual is not None else None=torch.Size([16384, 1024])
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨å®Œäº†
ğŸŸ© Qwen3MLPã®é †ä¼æ’­ x.shape=torch.Size([16384, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([16384, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­å®Œäº† result.shape=torch.Size([16384, 6144])
ğŸŸ© SwiGLUã‚’é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([16384, 6144])
ğŸŸ¦ ãƒãƒ£ãƒ³ã‚¯åŒ– x.shape=torch.Size([16384, 3072]), y.shape=torch.Size([16384, 3072])
ğŸŸ© SwiGLUã‚’é †ä¼æ’­å®Œäº† result.shape=torch.Size([16384, 3072])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­é–‹å§‹ å…¥åŠ›å½¢çŠ¶: torch.Size([16384, 3072])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­å®Œäº† å‡ºåŠ›å½¢çŠ¶: torch.Size([16384, 1024])
ğŸŸ© Qwen3MLPã®é †ä¼æ’­å®Œäº† x.shape=torch.Size([16384, 1024])
ğŸŸ© Qwen3DecoderLayerã®é †ä¼æ’­å®Œäº† hidden_states.shape=torch.Size([16384, 1024])
ğŸŸ© Qwen3DecoderLayerã®é †ä¼æ’­ positions.shape=torch.Size([16384]) hidden_states.shape=torch.Size([16384, 1024]) residual is None=False
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([16384, 1024]), residual.shape if residual is not None else None=torch.Size([16384, 1024])
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨å®Œäº†
ğŸŸ© Qwen3Attentionã®é †ä¼æ’­ positions.shape=torch.Size([16384]) hidden_states.shape=torch.Size([16384, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([16384, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­å®Œäº† result.shape=torch.Size([16384, 4096])
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([16384, 16, 128]), residual.shape if residual is not None else None=None
ğŸŸ© æ­£è¦åŒ–é–‹å§‹ x.shape=torch.Size([16384, 16, 128])
ğŸŸ© æ­£è¦åŒ–å®Œäº† x.shape=torch.Size([16384, 16, 128])
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([16384, 8, 128]), residual.shape if residual is not None else None=None
ğŸŸ© æ­£è¦åŒ–é–‹å§‹ x.shape=torch.Size([16384, 8, 128])
ğŸŸ© æ­£è¦åŒ–å®Œäº† x.shape=torch.Size([16384, 8, 128])
ğŸŸ© RoPEã®é †ä¼æ’­é–‹å§‹ positions.shape=torch.Size([16384]), query.shape=torch.Size([16384, 16, 128]), key.shape=torch.Size([16384, 8, 128])
ğŸŸ© RoPEã‚’é©ç”¨é–‹å§‹ x.shape=torch.Size([16384, 16, 128]), cos.shape=torch.Size([16384, 1, 64]), sin.shape=torch.Size([16384, 1, 64])
ğŸŸ¦ ãƒãƒ£ãƒ³ã‚¯åŒ– x1.shape=torch.Size([16384, 16, 64]), x2.shape=torch.Size([16384, 16, 64])
ğŸŸ¦ å›è»¢è¡Œåˆ—é©ç”¨ y1.shape=torch.Size([16384, 16, 64]), y2.shape=torch.Size([16384, 16, 64])
ğŸŸ© RoPEã‚’é©ç”¨å®Œäº† result.shape=torch.Size([16384, 16, 128])
ğŸŸ© RoPEã‚’é©ç”¨é–‹å§‹ x.shape=torch.Size([16384, 8, 128]), cos.shape=torch.Size([16384, 1, 64]), sin.shape=torch.Size([16384, 1, 64])
ğŸŸ¦ ãƒãƒ£ãƒ³ã‚¯åŒ– x1.shape=torch.Size([16384, 8, 64]), x2.shape=torch.Size([16384, 8, 64])
ğŸŸ¦ å›è»¢è¡Œåˆ—é©ç”¨ y1.shape=torch.Size([16384, 8, 64]), y2.shape=torch.Size([16384, 8, 64])
ğŸŸ© RoPEã‚’é©ç”¨å®Œäº† result.shape=torch.Size([16384, 8, 128])
ğŸŸ© RoPEã®é †ä¼æ’­å®Œäº† query.shape=torch.Size([16384, 16, 128]), key.shape=torch.Size([16384, 8, 128])
ğŸŸ© Attentionå±¤ã®é †ä¼æ’­é–‹å§‹ q.shape=torch.Size([16384, 16, 128]), k.shape=torch.Size([16384, 8, 128]), v.shape=torch.Size([16384, 8, 128])
ğŸŸ© ç¾åœ¨ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆå–å¾— _CONTEXT=Context(is_prefill=True, cu_seqlens_q=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32), cu_seqlens_k=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32), max_seqlen_q=4096, max_seqlen_k=4096, slot_mapping=tensor([], device='cuda:0', dtype=torch.int32), context_lens=None, block_tables=None)
ğŸŸ¦ ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæƒ…å ±å–å¾— context.is_prefill=True, context.slot_mapping=tensor([], device='cuda:0', dtype=torch.int32), context.max_seqlen_q=4096, context.cu_seqlens_q=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32), context.max_seqlen_k=4096, context.cu_seqlens_k=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32), context.context_lens=None, context.block_tables=None
ğŸŸ¦ Prefillãƒ¢ãƒ¼ãƒ‰ã§FlashAttentionã‚’å®Ÿè¡Œ context.max_seqlen_q=4096, context.cu_seqlens_q=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32), context.max_seqlen_k=4096, context.cu_seqlens_k=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32)
ğŸŸ© Attentionå±¤ã®é †ä¼æ’­å®Œäº† o.shape=torch.Size([16384, 16, 128])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­é–‹å§‹ å…¥åŠ›å½¢çŠ¶: torch.Size([16384, 2048])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­å®Œäº† å‡ºåŠ›å½¢çŠ¶: torch.Size([16384, 1024])
ğŸŸ© Qwen3Attentionã®é †ä¼æ’­å®Œäº† output.shape=torch.Size([16384, 1024])
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([16384, 1024]), residual.shape if residual is not None else None=torch.Size([16384, 1024])
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨å®Œäº†
ğŸŸ© Qwen3MLPã®é †ä¼æ’­ x.shape=torch.Size([16384, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([16384, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­å®Œäº† result.shape=torch.Size([16384, 6144])
ğŸŸ© SwiGLUã‚’é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([16384, 6144])
ğŸŸ¦ ãƒãƒ£ãƒ³ã‚¯åŒ– x.shape=torch.Size([16384, 3072]), y.shape=torch.Size([16384, 3072])
ğŸŸ© SwiGLUã‚’é †ä¼æ’­å®Œäº† result.shape=torch.Size([16384, 3072])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­é–‹å§‹ å…¥åŠ›å½¢çŠ¶: torch.Size([16384, 3072])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­å®Œäº† å‡ºåŠ›å½¢çŠ¶: torch.Size([16384, 1024])
ğŸŸ© Qwen3MLPã®é †ä¼æ’­å®Œäº† x.shape=torch.Size([16384, 1024])
ğŸŸ© Qwen3DecoderLayerã®é †ä¼æ’­å®Œäº† hidden_states.shape=torch.Size([16384, 1024])
ğŸŸ© Qwen3DecoderLayerã®é †ä¼æ’­ positions.shape=torch.Size([16384]) hidden_states.shape=torch.Size([16384, 1024]) residual is None=False
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([16384, 1024]), residual.shape if residual is not None else None=torch.Size([16384, 1024])
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨å®Œäº†
ğŸŸ© Qwen3Attentionã®é †ä¼æ’­ positions.shape=torch.Size([16384]) hidden_states.shape=torch.Size([16384, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([16384, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­å®Œäº† result.shape=torch.Size([16384, 4096])
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([16384, 16, 128]), residual.shape if residual is not None else None=None
ğŸŸ© æ­£è¦åŒ–é–‹å§‹ x.shape=torch.Size([16384, 16, 128])
ğŸŸ© æ­£è¦åŒ–å®Œäº† x.shape=torch.Size([16384, 16, 128])
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([16384, 8, 128]), residual.shape if residual is not None else None=None
ğŸŸ© æ­£è¦åŒ–é–‹å§‹ x.shape=torch.Size([16384, 8, 128])
ğŸŸ© æ­£è¦åŒ–å®Œäº† x.shape=torch.Size([16384, 8, 128])
ğŸŸ© RoPEã®é †ä¼æ’­é–‹å§‹ positions.shape=torch.Size([16384]), query.shape=torch.Size([16384, 16, 128]), key.shape=torch.Size([16384, 8, 128])
ğŸŸ© RoPEã‚’é©ç”¨é–‹å§‹ x.shape=torch.Size([16384, 16, 128]), cos.shape=torch.Size([16384, 1, 64]), sin.shape=torch.Size([16384, 1, 64])
ğŸŸ¦ ãƒãƒ£ãƒ³ã‚¯åŒ– x1.shape=torch.Size([16384, 16, 64]), x2.shape=torch.Size([16384, 16, 64])
ğŸŸ¦ å›è»¢è¡Œåˆ—é©ç”¨ y1.shape=torch.Size([16384, 16, 64]), y2.shape=torch.Size([16384, 16, 64])
ğŸŸ© RoPEã‚’é©ç”¨å®Œäº† result.shape=torch.Size([16384, 16, 128])
ğŸŸ© RoPEã‚’é©ç”¨é–‹å§‹ x.shape=torch.Size([16384, 8, 128]), cos.shape=torch.Size([16384, 1, 64]), sin.shape=torch.Size([16384, 1, 64])
ğŸŸ¦ ãƒãƒ£ãƒ³ã‚¯åŒ– x1.shape=torch.Size([16384, 8, 64]), x2.shape=torch.Size([16384, 8, 64])
ğŸŸ¦ å›è»¢è¡Œåˆ—é©ç”¨ y1.shape=torch.Size([16384, 8, 64]), y2.shape=torch.Size([16384, 8, 64])
ğŸŸ© RoPEã‚’é©ç”¨å®Œäº† result.shape=torch.Size([16384, 8, 128])
ğŸŸ© RoPEã®é †ä¼æ’­å®Œäº† query.shape=torch.Size([16384, 16, 128]), key.shape=torch.Size([16384, 8, 128])
ğŸŸ© Attentionå±¤ã®é †ä¼æ’­é–‹å§‹ q.shape=torch.Size([16384, 16, 128]), k.shape=torch.Size([16384, 8, 128]), v.shape=torch.Size([16384, 8, 128])
ğŸŸ© ç¾åœ¨ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆå–å¾— _CONTEXT=Context(is_prefill=True, cu_seqlens_q=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32), cu_seqlens_k=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32), max_seqlen_q=4096, max_seqlen_k=4096, slot_mapping=tensor([], device='cuda:0', dtype=torch.int32), context_lens=None, block_tables=None)
ğŸŸ¦ ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæƒ…å ±å–å¾— context.is_prefill=True, context.slot_mapping=tensor([], device='cuda:0', dtype=torch.int32), context.max_seqlen_q=4096, context.cu_seqlens_q=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32), context.max_seqlen_k=4096, context.cu_seqlens_k=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32), context.context_lens=None, context.block_tables=None
ğŸŸ¦ Prefillãƒ¢ãƒ¼ãƒ‰ã§FlashAttentionã‚’å®Ÿè¡Œ context.max_seqlen_q=4096, context.cu_seqlens_q=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32), context.max_seqlen_k=4096, context.cu_seqlens_k=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32)
ğŸŸ© Attentionå±¤ã®é †ä¼æ’­å®Œäº† o.shape=torch.Size([16384, 16, 128])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­é–‹å§‹ å…¥åŠ›å½¢çŠ¶: torch.Size([16384, 2048])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­å®Œäº† å‡ºåŠ›å½¢çŠ¶: torch.Size([16384, 1024])
ğŸŸ© Qwen3Attentionã®é †ä¼æ’­å®Œäº† output.shape=torch.Size([16384, 1024])
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([16384, 1024]), residual.shape if residual is not None else None=torch.Size([16384, 1024])
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨å®Œäº†
ğŸŸ© Qwen3MLPã®é †ä¼æ’­ x.shape=torch.Size([16384, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([16384, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­å®Œäº† result.shape=torch.Size([16384, 6144])
ğŸŸ© SwiGLUã‚’é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([16384, 6144])
ğŸŸ¦ ãƒãƒ£ãƒ³ã‚¯åŒ– x.shape=torch.Size([16384, 3072]), y.shape=torch.Size([16384, 3072])
ğŸŸ© SwiGLUã‚’é †ä¼æ’­å®Œäº† result.shape=torch.Size([16384, 3072])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­é–‹å§‹ å…¥åŠ›å½¢çŠ¶: torch.Size([16384, 3072])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­å®Œäº† å‡ºåŠ›å½¢çŠ¶: torch.Size([16384, 1024])
ğŸŸ© Qwen3MLPã®é †ä¼æ’­å®Œäº† x.shape=torch.Size([16384, 1024])
ğŸŸ© Qwen3DecoderLayerã®é †ä¼æ’­å®Œäº† hidden_states.shape=torch.Size([16384, 1024])
ğŸŸ© Qwen3DecoderLayerã®é †ä¼æ’­ positions.shape=torch.Size([16384]) hidden_states.shape=torch.Size([16384, 1024]) residual is None=False
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([16384, 1024]), residual.shape if residual is not None else None=torch.Size([16384, 1024])
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨å®Œäº†
ğŸŸ© Qwen3Attentionã®é †ä¼æ’­ positions.shape=torch.Size([16384]) hidden_states.shape=torch.Size([16384, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([16384, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­å®Œäº† result.shape=torch.Size([16384, 4096])
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([16384, 16, 128]), residual.shape if residual is not None else None=None
ğŸŸ© æ­£è¦åŒ–é–‹å§‹ x.shape=torch.Size([16384, 16, 128])
ğŸŸ© æ­£è¦åŒ–å®Œäº† x.shape=torch.Size([16384, 16, 128])
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([16384, 8, 128]), residual.shape if residual is not None else None=None
ğŸŸ© æ­£è¦åŒ–é–‹å§‹ x.shape=torch.Size([16384, 8, 128])
ğŸŸ© æ­£è¦åŒ–å®Œäº† x.shape=torch.Size([16384, 8, 128])
ğŸŸ© RoPEã®é †ä¼æ’­é–‹å§‹ positions.shape=torch.Size([16384]), query.shape=torch.Size([16384, 16, 128]), key.shape=torch.Size([16384, 8, 128])
ğŸŸ© RoPEã‚’é©ç”¨é–‹å§‹ x.shape=torch.Size([16384, 16, 128]), cos.shape=torch.Size([16384, 1, 64]), sin.shape=torch.Size([16384, 1, 64])
ğŸŸ¦ ãƒãƒ£ãƒ³ã‚¯åŒ– x1.shape=torch.Size([16384, 16, 64]), x2.shape=torch.Size([16384, 16, 64])
ğŸŸ¦ å›è»¢è¡Œåˆ—é©ç”¨ y1.shape=torch.Size([16384, 16, 64]), y2.shape=torch.Size([16384, 16, 64])
ğŸŸ© RoPEã‚’é©ç”¨å®Œäº† result.shape=torch.Size([16384, 16, 128])
ğŸŸ© RoPEã‚’é©ç”¨é–‹å§‹ x.shape=torch.Size([16384, 8, 128]), cos.shape=torch.Size([16384, 1, 64]), sin.shape=torch.Size([16384, 1, 64])
ğŸŸ¦ ãƒãƒ£ãƒ³ã‚¯åŒ– x1.shape=torch.Size([16384, 8, 64]), x2.shape=torch.Size([16384, 8, 64])
ğŸŸ¦ å›è»¢è¡Œåˆ—é©ç”¨ y1.shape=torch.Size([16384, 8, 64]), y2.shape=torch.Size([16384, 8, 64])
ğŸŸ© RoPEã‚’é©ç”¨å®Œäº† result.shape=torch.Size([16384, 8, 128])
ğŸŸ© RoPEã®é †ä¼æ’­å®Œäº† query.shape=torch.Size([16384, 16, 128]), key.shape=torch.Size([16384, 8, 128])
ğŸŸ© Attentionå±¤ã®é †ä¼æ’­é–‹å§‹ q.shape=torch.Size([16384, 16, 128]), k.shape=torch.Size([16384, 8, 128]), v.shape=torch.Size([16384, 8, 128])
ğŸŸ© ç¾åœ¨ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆå–å¾— _CONTEXT=Context(is_prefill=True, cu_seqlens_q=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32), cu_seqlens_k=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32), max_seqlen_q=4096, max_seqlen_k=4096, slot_mapping=tensor([], device='cuda:0', dtype=torch.int32), context_lens=None, block_tables=None)
ğŸŸ¦ ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæƒ…å ±å–å¾— context.is_prefill=True, context.slot_mapping=tensor([], device='cuda:0', dtype=torch.int32), context.max_seqlen_q=4096, context.cu_seqlens_q=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32), context.max_seqlen_k=4096, context.cu_seqlens_k=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32), context.context_lens=None, context.block_tables=None
ğŸŸ¦ Prefillãƒ¢ãƒ¼ãƒ‰ã§FlashAttentionã‚’å®Ÿè¡Œ context.max_seqlen_q=4096, context.cu_seqlens_q=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32), context.max_seqlen_k=4096, context.cu_seqlens_k=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32)
ğŸŸ© Attentionå±¤ã®é †ä¼æ’­å®Œäº† o.shape=torch.Size([16384, 16, 128])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­é–‹å§‹ å…¥åŠ›å½¢çŠ¶: torch.Size([16384, 2048])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­å®Œäº† å‡ºåŠ›å½¢çŠ¶: torch.Size([16384, 1024])
ğŸŸ© Qwen3Attentionã®é †ä¼æ’­å®Œäº† output.shape=torch.Size([16384, 1024])
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([16384, 1024]), residual.shape if residual is not None else None=torch.Size([16384, 1024])
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨å®Œäº†
ğŸŸ© Qwen3MLPã®é †ä¼æ’­ x.shape=torch.Size([16384, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([16384, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­å®Œäº† result.shape=torch.Size([16384, 6144])
ğŸŸ© SwiGLUã‚’é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([16384, 6144])
ğŸŸ¦ ãƒãƒ£ãƒ³ã‚¯åŒ– x.shape=torch.Size([16384, 3072]), y.shape=torch.Size([16384, 3072])
ğŸŸ© SwiGLUã‚’é †ä¼æ’­å®Œäº† result.shape=torch.Size([16384, 3072])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­é–‹å§‹ å…¥åŠ›å½¢çŠ¶: torch.Size([16384, 3072])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­å®Œäº† å‡ºåŠ›å½¢çŠ¶: torch.Size([16384, 1024])
ğŸŸ© Qwen3MLPã®é †ä¼æ’­å®Œäº† x.shape=torch.Size([16384, 1024])
ğŸŸ© Qwen3DecoderLayerã®é †ä¼æ’­å®Œäº† hidden_states.shape=torch.Size([16384, 1024])
ğŸŸ© Qwen3DecoderLayerã®é †ä¼æ’­ positions.shape=torch.Size([16384]) hidden_states.shape=torch.Size([16384, 1024]) residual is None=False
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([16384, 1024]), residual.shape if residual is not None else None=torch.Size([16384, 1024])
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨å®Œäº†
ğŸŸ© Qwen3Attentionã®é †ä¼æ’­ positions.shape=torch.Size([16384]) hidden_states.shape=torch.Size([16384, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([16384, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­å®Œäº† result.shape=torch.Size([16384, 4096])
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([16384, 16, 128]), residual.shape if residual is not None else None=None
ğŸŸ© æ­£è¦åŒ–é–‹å§‹ x.shape=torch.Size([16384, 16, 128])
ğŸŸ© æ­£è¦åŒ–å®Œäº† x.shape=torch.Size([16384, 16, 128])
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([16384, 8, 128]), residual.shape if residual is not None else None=None
ğŸŸ© æ­£è¦åŒ–é–‹å§‹ x.shape=torch.Size([16384, 8, 128])
ğŸŸ© æ­£è¦åŒ–å®Œäº† x.shape=torch.Size([16384, 8, 128])
ğŸŸ© RoPEã®é †ä¼æ’­é–‹å§‹ positions.shape=torch.Size([16384]), query.shape=torch.Size([16384, 16, 128]), key.shape=torch.Size([16384, 8, 128])
ğŸŸ© RoPEã‚’é©ç”¨é–‹å§‹ x.shape=torch.Size([16384, 16, 128]), cos.shape=torch.Size([16384, 1, 64]), sin.shape=torch.Size([16384, 1, 64])
ğŸŸ¦ ãƒãƒ£ãƒ³ã‚¯åŒ– x1.shape=torch.Size([16384, 16, 64]), x2.shape=torch.Size([16384, 16, 64])
ğŸŸ¦ å›è»¢è¡Œåˆ—é©ç”¨ y1.shape=torch.Size([16384, 16, 64]), y2.shape=torch.Size([16384, 16, 64])
ğŸŸ© RoPEã‚’é©ç”¨å®Œäº† result.shape=torch.Size([16384, 16, 128])
ğŸŸ© RoPEã‚’é©ç”¨é–‹å§‹ x.shape=torch.Size([16384, 8, 128]), cos.shape=torch.Size([16384, 1, 64]), sin.shape=torch.Size([16384, 1, 64])
ğŸŸ¦ ãƒãƒ£ãƒ³ã‚¯åŒ– x1.shape=torch.Size([16384, 8, 64]), x2.shape=torch.Size([16384, 8, 64])
ğŸŸ¦ å›è»¢è¡Œåˆ—é©ç”¨ y1.shape=torch.Size([16384, 8, 64]), y2.shape=torch.Size([16384, 8, 64])
ğŸŸ© RoPEã‚’é©ç”¨å®Œäº† result.shape=torch.Size([16384, 8, 128])
ğŸŸ© RoPEã®é †ä¼æ’­å®Œäº† query.shape=torch.Size([16384, 16, 128]), key.shape=torch.Size([16384, 8, 128])
ğŸŸ© Attentionå±¤ã®é †ä¼æ’­é–‹å§‹ q.shape=torch.Size([16384, 16, 128]), k.shape=torch.Size([16384, 8, 128]), v.shape=torch.Size([16384, 8, 128])
ğŸŸ© ç¾åœ¨ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆå–å¾— _CONTEXT=Context(is_prefill=True, cu_seqlens_q=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32), cu_seqlens_k=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32), max_seqlen_q=4096, max_seqlen_k=4096, slot_mapping=tensor([], device='cuda:0', dtype=torch.int32), context_lens=None, block_tables=None)
ğŸŸ¦ ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæƒ…å ±å–å¾— context.is_prefill=True, context.slot_mapping=tensor([], device='cuda:0', dtype=torch.int32), context.max_seqlen_q=4096, context.cu_seqlens_q=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32), context.max_seqlen_k=4096, context.cu_seqlens_k=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32), context.context_lens=None, context.block_tables=None
ğŸŸ¦ Prefillãƒ¢ãƒ¼ãƒ‰ã§FlashAttentionã‚’å®Ÿè¡Œ context.max_seqlen_q=4096, context.cu_seqlens_q=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32), context.max_seqlen_k=4096, context.cu_seqlens_k=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32)
ğŸŸ© Attentionå±¤ã®é †ä¼æ’­å®Œäº† o.shape=torch.Size([16384, 16, 128])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­é–‹å§‹ å…¥åŠ›å½¢çŠ¶: torch.Size([16384, 2048])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­å®Œäº† å‡ºåŠ›å½¢çŠ¶: torch.Size([16384, 1024])
ğŸŸ© Qwen3Attentionã®é †ä¼æ’­å®Œäº† output.shape=torch.Size([16384, 1024])
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([16384, 1024]), residual.shape if residual is not None else None=torch.Size([16384, 1024])
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨å®Œäº†
ğŸŸ© Qwen3MLPã®é †ä¼æ’­ x.shape=torch.Size([16384, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([16384, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­å®Œäº† result.shape=torch.Size([16384, 6144])
ğŸŸ© SwiGLUã‚’é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([16384, 6144])
ğŸŸ¦ ãƒãƒ£ãƒ³ã‚¯åŒ– x.shape=torch.Size([16384, 3072]), y.shape=torch.Size([16384, 3072])
ğŸŸ© SwiGLUã‚’é †ä¼æ’­å®Œäº† result.shape=torch.Size([16384, 3072])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­é–‹å§‹ å…¥åŠ›å½¢çŠ¶: torch.Size([16384, 3072])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­å®Œäº† å‡ºåŠ›å½¢çŠ¶: torch.Size([16384, 1024])
ğŸŸ© Qwen3MLPã®é †ä¼æ’­å®Œäº† x.shape=torch.Size([16384, 1024])
ğŸŸ© Qwen3DecoderLayerã®é †ä¼æ’­å®Œäº† hidden_states.shape=torch.Size([16384, 1024])
ğŸŸ© Qwen3DecoderLayerã®é †ä¼æ’­ positions.shape=torch.Size([16384]) hidden_states.shape=torch.Size([16384, 1024]) residual is None=False
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([16384, 1024]), residual.shape if residual is not None else None=torch.Size([16384, 1024])
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨å®Œäº†
ğŸŸ© Qwen3Attentionã®é †ä¼æ’­ positions.shape=torch.Size([16384]) hidden_states.shape=torch.Size([16384, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([16384, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­å®Œäº† result.shape=torch.Size([16384, 4096])
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([16384, 16, 128]), residual.shape if residual is not None else None=None
ğŸŸ© æ­£è¦åŒ–é–‹å§‹ x.shape=torch.Size([16384, 16, 128])
ğŸŸ© æ­£è¦åŒ–å®Œäº† x.shape=torch.Size([16384, 16, 128])
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([16384, 8, 128]), residual.shape if residual is not None else None=None
ğŸŸ© æ­£è¦åŒ–é–‹å§‹ x.shape=torch.Size([16384, 8, 128])
ğŸŸ© æ­£è¦åŒ–å®Œäº† x.shape=torch.Size([16384, 8, 128])
ğŸŸ© RoPEã®é †ä¼æ’­é–‹å§‹ positions.shape=torch.Size([16384]), query.shape=torch.Size([16384, 16, 128]), key.shape=torch.Size([16384, 8, 128])
ğŸŸ© RoPEã‚’é©ç”¨é–‹å§‹ x.shape=torch.Size([16384, 16, 128]), cos.shape=torch.Size([16384, 1, 64]), sin.shape=torch.Size([16384, 1, 64])
ğŸŸ¦ ãƒãƒ£ãƒ³ã‚¯åŒ– x1.shape=torch.Size([16384, 16, 64]), x2.shape=torch.Size([16384, 16, 64])
ğŸŸ¦ å›è»¢è¡Œåˆ—é©ç”¨ y1.shape=torch.Size([16384, 16, 64]), y2.shape=torch.Size([16384, 16, 64])
ğŸŸ© RoPEã‚’é©ç”¨å®Œäº† result.shape=torch.Size([16384, 16, 128])
ğŸŸ© RoPEã‚’é©ç”¨é–‹å§‹ x.shape=torch.Size([16384, 8, 128]), cos.shape=torch.Size([16384, 1, 64]), sin.shape=torch.Size([16384, 1, 64])
ğŸŸ¦ ãƒãƒ£ãƒ³ã‚¯åŒ– x1.shape=torch.Size([16384, 8, 64]), x2.shape=torch.Size([16384, 8, 64])
ğŸŸ¦ å›è»¢è¡Œåˆ—é©ç”¨ y1.shape=torch.Size([16384, 8, 64]), y2.shape=torch.Size([16384, 8, 64])
ğŸŸ© RoPEã‚’é©ç”¨å®Œäº† result.shape=torch.Size([16384, 8, 128])
ğŸŸ© RoPEã®é †ä¼æ’­å®Œäº† query.shape=torch.Size([16384, 16, 128]), key.shape=torch.Size([16384, 8, 128])
ğŸŸ© Attentionå±¤ã®é †ä¼æ’­é–‹å§‹ q.shape=torch.Size([16384, 16, 128]), k.shape=torch.Size([16384, 8, 128]), v.shape=torch.Size([16384, 8, 128])
ğŸŸ© ç¾åœ¨ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆå–å¾— _CONTEXT=Context(is_prefill=True, cu_seqlens_q=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32), cu_seqlens_k=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32), max_seqlen_q=4096, max_seqlen_k=4096, slot_mapping=tensor([], device='cuda:0', dtype=torch.int32), context_lens=None, block_tables=None)
ğŸŸ¦ ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæƒ…å ±å–å¾— context.is_prefill=True, context.slot_mapping=tensor([], device='cuda:0', dtype=torch.int32), context.max_seqlen_q=4096, context.cu_seqlens_q=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32), context.max_seqlen_k=4096, context.cu_seqlens_k=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32), context.context_lens=None, context.block_tables=None
ğŸŸ¦ Prefillãƒ¢ãƒ¼ãƒ‰ã§FlashAttentionã‚’å®Ÿè¡Œ context.max_seqlen_q=4096, context.cu_seqlens_q=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32), context.max_seqlen_k=4096, context.cu_seqlens_k=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32)
ğŸŸ© Attentionå±¤ã®é †ä¼æ’­å®Œäº† o.shape=torch.Size([16384, 16, 128])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­é–‹å§‹ å…¥åŠ›å½¢çŠ¶: torch.Size([16384, 2048])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­å®Œäº† å‡ºåŠ›å½¢çŠ¶: torch.Size([16384, 1024])
ğŸŸ© Qwen3Attentionã®é †ä¼æ’­å®Œäº† output.shape=torch.Size([16384, 1024])
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([16384, 1024]), residual.shape if residual is not None else None=torch.Size([16384, 1024])
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨å®Œäº†
ğŸŸ© Qwen3MLPã®é †ä¼æ’­ x.shape=torch.Size([16384, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([16384, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­å®Œäº† result.shape=torch.Size([16384, 6144])
ğŸŸ© SwiGLUã‚’é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([16384, 6144])
ğŸŸ¦ ãƒãƒ£ãƒ³ã‚¯åŒ– x.shape=torch.Size([16384, 3072]), y.shape=torch.Size([16384, 3072])
ğŸŸ© SwiGLUã‚’é †ä¼æ’­å®Œäº† result.shape=torch.Size([16384, 3072])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­é–‹å§‹ å…¥åŠ›å½¢çŠ¶: torch.Size([16384, 3072])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­å®Œäº† å‡ºåŠ›å½¢çŠ¶: torch.Size([16384, 1024])
ğŸŸ© Qwen3MLPã®é †ä¼æ’­å®Œäº† x.shape=torch.Size([16384, 1024])
ğŸŸ© Qwen3DecoderLayerã®é †ä¼æ’­å®Œäº† hidden_states.shape=torch.Size([16384, 1024])
ğŸŸ© Qwen3DecoderLayerã®é †ä¼æ’­ positions.shape=torch.Size([16384]) hidden_states.shape=torch.Size([16384, 1024]) residual is None=False
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([16384, 1024]), residual.shape if residual is not None else None=torch.Size([16384, 1024])
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨å®Œäº†
ğŸŸ© Qwen3Attentionã®é †ä¼æ’­ positions.shape=torch.Size([16384]) hidden_states.shape=torch.Size([16384, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([16384, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­å®Œäº† result.shape=torch.Size([16384, 4096])
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([16384, 16, 128]), residual.shape if residual is not None else None=None
ğŸŸ© æ­£è¦åŒ–é–‹å§‹ x.shape=torch.Size([16384, 16, 128])
ğŸŸ© æ­£è¦åŒ–å®Œäº† x.shape=torch.Size([16384, 16, 128])
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([16384, 8, 128]), residual.shape if residual is not None else None=None
ğŸŸ© æ­£è¦åŒ–é–‹å§‹ x.shape=torch.Size([16384, 8, 128])
ğŸŸ© æ­£è¦åŒ–å®Œäº† x.shape=torch.Size([16384, 8, 128])
ğŸŸ© RoPEã®é †ä¼æ’­é–‹å§‹ positions.shape=torch.Size([16384]), query.shape=torch.Size([16384, 16, 128]), key.shape=torch.Size([16384, 8, 128])
ğŸŸ© RoPEã‚’é©ç”¨é–‹å§‹ x.shape=torch.Size([16384, 16, 128]), cos.shape=torch.Size([16384, 1, 64]), sin.shape=torch.Size([16384, 1, 64])
ğŸŸ¦ ãƒãƒ£ãƒ³ã‚¯åŒ– x1.shape=torch.Size([16384, 16, 64]), x2.shape=torch.Size([16384, 16, 64])
ğŸŸ¦ å›è»¢è¡Œåˆ—é©ç”¨ y1.shape=torch.Size([16384, 16, 64]), y2.shape=torch.Size([16384, 16, 64])
ğŸŸ© RoPEã‚’é©ç”¨å®Œäº† result.shape=torch.Size([16384, 16, 128])
ğŸŸ© RoPEã‚’é©ç”¨é–‹å§‹ x.shape=torch.Size([16384, 8, 128]), cos.shape=torch.Size([16384, 1, 64]), sin.shape=torch.Size([16384, 1, 64])
ğŸŸ¦ ãƒãƒ£ãƒ³ã‚¯åŒ– x1.shape=torch.Size([16384, 8, 64]), x2.shape=torch.Size([16384, 8, 64])
ğŸŸ¦ å›è»¢è¡Œåˆ—é©ç”¨ y1.shape=torch.Size([16384, 8, 64]), y2.shape=torch.Size([16384, 8, 64])
ğŸŸ© RoPEã‚’é©ç”¨å®Œäº† result.shape=torch.Size([16384, 8, 128])
ğŸŸ© RoPEã®é †ä¼æ’­å®Œäº† query.shape=torch.Size([16384, 16, 128]), key.shape=torch.Size([16384, 8, 128])
ğŸŸ© Attentionå±¤ã®é †ä¼æ’­é–‹å§‹ q.shape=torch.Size([16384, 16, 128]), k.shape=torch.Size([16384, 8, 128]), v.shape=torch.Size([16384, 8, 128])
ğŸŸ© ç¾åœ¨ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆå–å¾— _CONTEXT=Context(is_prefill=True, cu_seqlens_q=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32), cu_seqlens_k=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32), max_seqlen_q=4096, max_seqlen_k=4096, slot_mapping=tensor([], device='cuda:0', dtype=torch.int32), context_lens=None, block_tables=None)
ğŸŸ¦ ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæƒ…å ±å–å¾— context.is_prefill=True, context.slot_mapping=tensor([], device='cuda:0', dtype=torch.int32), context.max_seqlen_q=4096, context.cu_seqlens_q=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32), context.max_seqlen_k=4096, context.cu_seqlens_k=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32), context.context_lens=None, context.block_tables=None
ğŸŸ¦ Prefillãƒ¢ãƒ¼ãƒ‰ã§FlashAttentionã‚’å®Ÿè¡Œ context.max_seqlen_q=4096, context.cu_seqlens_q=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32), context.max_seqlen_k=4096, context.cu_seqlens_k=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32)
ğŸŸ© Attentionå±¤ã®é †ä¼æ’­å®Œäº† o.shape=torch.Size([16384, 16, 128])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­é–‹å§‹ å…¥åŠ›å½¢çŠ¶: torch.Size([16384, 2048])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­å®Œäº† å‡ºåŠ›å½¢çŠ¶: torch.Size([16384, 1024])
ğŸŸ© Qwen3Attentionã®é †ä¼æ’­å®Œäº† output.shape=torch.Size([16384, 1024])
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([16384, 1024]), residual.shape if residual is not None else None=torch.Size([16384, 1024])
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨å®Œäº†
ğŸŸ© Qwen3MLPã®é †ä¼æ’­ x.shape=torch.Size([16384, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([16384, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­å®Œäº† result.shape=torch.Size([16384, 6144])
ğŸŸ© SwiGLUã‚’é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([16384, 6144])
ğŸŸ¦ ãƒãƒ£ãƒ³ã‚¯åŒ– x.shape=torch.Size([16384, 3072]), y.shape=torch.Size([16384, 3072])
ğŸŸ© SwiGLUã‚’é †ä¼æ’­å®Œäº† result.shape=torch.Size([16384, 3072])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­é–‹å§‹ å…¥åŠ›å½¢çŠ¶: torch.Size([16384, 3072])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­å®Œäº† å‡ºåŠ›å½¢çŠ¶: torch.Size([16384, 1024])
ğŸŸ© Qwen3MLPã®é †ä¼æ’­å®Œäº† x.shape=torch.Size([16384, 1024])
ğŸŸ© Qwen3DecoderLayerã®é †ä¼æ’­å®Œäº† hidden_states.shape=torch.Size([16384, 1024])
ğŸŸ© Qwen3DecoderLayerã®é †ä¼æ’­ positions.shape=torch.Size([16384]) hidden_states.shape=torch.Size([16384, 1024]) residual is None=False
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([16384, 1024]), residual.shape if residual is not None else None=torch.Size([16384, 1024])
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨å®Œäº†
ğŸŸ© Qwen3Attentionã®é †ä¼æ’­ positions.shape=torch.Size([16384]) hidden_states.shape=torch.Size([16384, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([16384, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­å®Œäº† result.shape=torch.Size([16384, 4096])
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([16384, 16, 128]), residual.shape if residual is not None else None=None
ğŸŸ© æ­£è¦åŒ–é–‹å§‹ x.shape=torch.Size([16384, 16, 128])
ğŸŸ© æ­£è¦åŒ–å®Œäº† x.shape=torch.Size([16384, 16, 128])
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([16384, 8, 128]), residual.shape if residual is not None else None=None
ğŸŸ© æ­£è¦åŒ–é–‹å§‹ x.shape=torch.Size([16384, 8, 128])
ğŸŸ© æ­£è¦åŒ–å®Œäº† x.shape=torch.Size([16384, 8, 128])
ğŸŸ© RoPEã®é †ä¼æ’­é–‹å§‹ positions.shape=torch.Size([16384]), query.shape=torch.Size([16384, 16, 128]), key.shape=torch.Size([16384, 8, 128])
ğŸŸ© RoPEã‚’é©ç”¨é–‹å§‹ x.shape=torch.Size([16384, 16, 128]), cos.shape=torch.Size([16384, 1, 64]), sin.shape=torch.Size([16384, 1, 64])
ğŸŸ¦ ãƒãƒ£ãƒ³ã‚¯åŒ– x1.shape=torch.Size([16384, 16, 64]), x2.shape=torch.Size([16384, 16, 64])
ğŸŸ¦ å›è»¢è¡Œåˆ—é©ç”¨ y1.shape=torch.Size([16384, 16, 64]), y2.shape=torch.Size([16384, 16, 64])
ğŸŸ© RoPEã‚’é©ç”¨å®Œäº† result.shape=torch.Size([16384, 16, 128])
ğŸŸ© RoPEã‚’é©ç”¨é–‹å§‹ x.shape=torch.Size([16384, 8, 128]), cos.shape=torch.Size([16384, 1, 64]), sin.shape=torch.Size([16384, 1, 64])
ğŸŸ¦ ãƒãƒ£ãƒ³ã‚¯åŒ– x1.shape=torch.Size([16384, 8, 64]), x2.shape=torch.Size([16384, 8, 64])
ğŸŸ¦ å›è»¢è¡Œåˆ—é©ç”¨ y1.shape=torch.Size([16384, 8, 64]), y2.shape=torch.Size([16384, 8, 64])
ğŸŸ© RoPEã‚’é©ç”¨å®Œäº† result.shape=torch.Size([16384, 8, 128])
ğŸŸ© RoPEã®é †ä¼æ’­å®Œäº† query.shape=torch.Size([16384, 16, 128]), key.shape=torch.Size([16384, 8, 128])
ğŸŸ© Attentionå±¤ã®é †ä¼æ’­é–‹å§‹ q.shape=torch.Size([16384, 16, 128]), k.shape=torch.Size([16384, 8, 128]), v.shape=torch.Size([16384, 8, 128])
ğŸŸ© ç¾åœ¨ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆå–å¾— _CONTEXT=Context(is_prefill=True, cu_seqlens_q=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32), cu_seqlens_k=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32), max_seqlen_q=4096, max_seqlen_k=4096, slot_mapping=tensor([], device='cuda:0', dtype=torch.int32), context_lens=None, block_tables=None)
ğŸŸ¦ ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæƒ…å ±å–å¾— context.is_prefill=True, context.slot_mapping=tensor([], device='cuda:0', dtype=torch.int32), context.max_seqlen_q=4096, context.cu_seqlens_q=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32), context.max_seqlen_k=4096, context.cu_seqlens_k=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32), context.context_lens=None, context.block_tables=None
ğŸŸ¦ Prefillãƒ¢ãƒ¼ãƒ‰ã§FlashAttentionã‚’å®Ÿè¡Œ context.max_seqlen_q=4096, context.cu_seqlens_q=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32), context.max_seqlen_k=4096, context.cu_seqlens_k=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32)
ğŸŸ© Attentionå±¤ã®é †ä¼æ’­å®Œäº† o.shape=torch.Size([16384, 16, 128])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­é–‹å§‹ å…¥åŠ›å½¢çŠ¶: torch.Size([16384, 2048])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­å®Œäº† å‡ºåŠ›å½¢çŠ¶: torch.Size([16384, 1024])
ğŸŸ© Qwen3Attentionã®é †ä¼æ’­å®Œäº† output.shape=torch.Size([16384, 1024])
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([16384, 1024]), residual.shape if residual is not None else None=torch.Size([16384, 1024])
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨å®Œäº†
ğŸŸ© Qwen3MLPã®é †ä¼æ’­ x.shape=torch.Size([16384, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([16384, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­å®Œäº† result.shape=torch.Size([16384, 6144])
ğŸŸ© SwiGLUã‚’é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([16384, 6144])
ğŸŸ¦ ãƒãƒ£ãƒ³ã‚¯åŒ– x.shape=torch.Size([16384, 3072]), y.shape=torch.Size([16384, 3072])
ğŸŸ© SwiGLUã‚’é †ä¼æ’­å®Œäº† result.shape=torch.Size([16384, 3072])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­é–‹å§‹ å…¥åŠ›å½¢çŠ¶: torch.Size([16384, 3072])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­å®Œäº† å‡ºåŠ›å½¢çŠ¶: torch.Size([16384, 1024])
ğŸŸ© Qwen3MLPã®é †ä¼æ’­å®Œäº† x.shape=torch.Size([16384, 1024])
ğŸŸ© Qwen3DecoderLayerã®é †ä¼æ’­å®Œäº† hidden_states.shape=torch.Size([16384, 1024])
ğŸŸ© Qwen3DecoderLayerã®é †ä¼æ’­ positions.shape=torch.Size([16384]) hidden_states.shape=torch.Size([16384, 1024]) residual is None=False
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([16384, 1024]), residual.shape if residual is not None else None=torch.Size([16384, 1024])
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨å®Œäº†
ğŸŸ© Qwen3Attentionã®é †ä¼æ’­ positions.shape=torch.Size([16384]) hidden_states.shape=torch.Size([16384, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([16384, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­å®Œäº† result.shape=torch.Size([16384, 4096])
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([16384, 16, 128]), residual.shape if residual is not None else None=None
ğŸŸ© æ­£è¦åŒ–é–‹å§‹ x.shape=torch.Size([16384, 16, 128])
ğŸŸ© æ­£è¦åŒ–å®Œäº† x.shape=torch.Size([16384, 16, 128])
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([16384, 8, 128]), residual.shape if residual is not None else None=None
ğŸŸ© æ­£è¦åŒ–é–‹å§‹ x.shape=torch.Size([16384, 8, 128])
ğŸŸ© æ­£è¦åŒ–å®Œäº† x.shape=torch.Size([16384, 8, 128])
ğŸŸ© RoPEã®é †ä¼æ’­é–‹å§‹ positions.shape=torch.Size([16384]), query.shape=torch.Size([16384, 16, 128]), key.shape=torch.Size([16384, 8, 128])
ğŸŸ© RoPEã‚’é©ç”¨é–‹å§‹ x.shape=torch.Size([16384, 16, 128]), cos.shape=torch.Size([16384, 1, 64]), sin.shape=torch.Size([16384, 1, 64])
ğŸŸ¦ ãƒãƒ£ãƒ³ã‚¯åŒ– x1.shape=torch.Size([16384, 16, 64]), x2.shape=torch.Size([16384, 16, 64])
ğŸŸ¦ å›è»¢è¡Œåˆ—é©ç”¨ y1.shape=torch.Size([16384, 16, 64]), y2.shape=torch.Size([16384, 16, 64])
ğŸŸ© RoPEã‚’é©ç”¨å®Œäº† result.shape=torch.Size([16384, 16, 128])
ğŸŸ© RoPEã‚’é©ç”¨é–‹å§‹ x.shape=torch.Size([16384, 8, 128]), cos.shape=torch.Size([16384, 1, 64]), sin.shape=torch.Size([16384, 1, 64])
ğŸŸ¦ ãƒãƒ£ãƒ³ã‚¯åŒ– x1.shape=torch.Size([16384, 8, 64]), x2.shape=torch.Size([16384, 8, 64])
ğŸŸ¦ å›è»¢è¡Œåˆ—é©ç”¨ y1.shape=torch.Size([16384, 8, 64]), y2.shape=torch.Size([16384, 8, 64])
ğŸŸ© RoPEã‚’é©ç”¨å®Œäº† result.shape=torch.Size([16384, 8, 128])
ğŸŸ© RoPEã®é †ä¼æ’­å®Œäº† query.shape=torch.Size([16384, 16, 128]), key.shape=torch.Size([16384, 8, 128])
ğŸŸ© Attentionå±¤ã®é †ä¼æ’­é–‹å§‹ q.shape=torch.Size([16384, 16, 128]), k.shape=torch.Size([16384, 8, 128]), v.shape=torch.Size([16384, 8, 128])
ğŸŸ© ç¾åœ¨ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆå–å¾— _CONTEXT=Context(is_prefill=True, cu_seqlens_q=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32), cu_seqlens_k=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32), max_seqlen_q=4096, max_seqlen_k=4096, slot_mapping=tensor([], device='cuda:0', dtype=torch.int32), context_lens=None, block_tables=None)
ğŸŸ¦ ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæƒ…å ±å–å¾— context.is_prefill=True, context.slot_mapping=tensor([], device='cuda:0', dtype=torch.int32), context.max_seqlen_q=4096, context.cu_seqlens_q=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32), context.max_seqlen_k=4096, context.cu_seqlens_k=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32), context.context_lens=None, context.block_tables=None
ğŸŸ¦ Prefillãƒ¢ãƒ¼ãƒ‰ã§FlashAttentionã‚’å®Ÿè¡Œ context.max_seqlen_q=4096, context.cu_seqlens_q=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32), context.max_seqlen_k=4096, context.cu_seqlens_k=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32)
ğŸŸ© Attentionå±¤ã®é †ä¼æ’­å®Œäº† o.shape=torch.Size([16384, 16, 128])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­é–‹å§‹ å…¥åŠ›å½¢çŠ¶: torch.Size([16384, 2048])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­å®Œäº† å‡ºåŠ›å½¢çŠ¶: torch.Size([16384, 1024])
ğŸŸ© Qwen3Attentionã®é †ä¼æ’­å®Œäº† output.shape=torch.Size([16384, 1024])
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([16384, 1024]), residual.shape if residual is not None else None=torch.Size([16384, 1024])
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨å®Œäº†
ğŸŸ© Qwen3MLPã®é †ä¼æ’­ x.shape=torch.Size([16384, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([16384, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­å®Œäº† result.shape=torch.Size([16384, 6144])
ğŸŸ© SwiGLUã‚’é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([16384, 6144])
ğŸŸ¦ ãƒãƒ£ãƒ³ã‚¯åŒ– x.shape=torch.Size([16384, 3072]), y.shape=torch.Size([16384, 3072])
ğŸŸ© SwiGLUã‚’é †ä¼æ’­å®Œäº† result.shape=torch.Size([16384, 3072])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­é–‹å§‹ å…¥åŠ›å½¢çŠ¶: torch.Size([16384, 3072])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­å®Œäº† å‡ºåŠ›å½¢çŠ¶: torch.Size([16384, 1024])
ğŸŸ© Qwen3MLPã®é †ä¼æ’­å®Œäº† x.shape=torch.Size([16384, 1024])
ğŸŸ© Qwen3DecoderLayerã®é †ä¼æ’­å®Œäº† hidden_states.shape=torch.Size([16384, 1024])
ğŸŸ© Qwen3DecoderLayerã®é †ä¼æ’­ positions.shape=torch.Size([16384]) hidden_states.shape=torch.Size([16384, 1024]) residual is None=False
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([16384, 1024]), residual.shape if residual is not None else None=torch.Size([16384, 1024])
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨å®Œäº†
ğŸŸ© Qwen3Attentionã®é †ä¼æ’­ positions.shape=torch.Size([16384]) hidden_states.shape=torch.Size([16384, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([16384, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­å®Œäº† result.shape=torch.Size([16384, 4096])
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([16384, 16, 128]), residual.shape if residual is not None else None=None
ğŸŸ© æ­£è¦åŒ–é–‹å§‹ x.shape=torch.Size([16384, 16, 128])
ğŸŸ© æ­£è¦åŒ–å®Œäº† x.shape=torch.Size([16384, 16, 128])
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([16384, 8, 128]), residual.shape if residual is not None else None=None
ğŸŸ© æ­£è¦åŒ–é–‹å§‹ x.shape=torch.Size([16384, 8, 128])
ğŸŸ© æ­£è¦åŒ–å®Œäº† x.shape=torch.Size([16384, 8, 128])
ğŸŸ© RoPEã®é †ä¼æ’­é–‹å§‹ positions.shape=torch.Size([16384]), query.shape=torch.Size([16384, 16, 128]), key.shape=torch.Size([16384, 8, 128])
ğŸŸ© RoPEã‚’é©ç”¨é–‹å§‹ x.shape=torch.Size([16384, 16, 128]), cos.shape=torch.Size([16384, 1, 64]), sin.shape=torch.Size([16384, 1, 64])
ğŸŸ¦ ãƒãƒ£ãƒ³ã‚¯åŒ– x1.shape=torch.Size([16384, 16, 64]), x2.shape=torch.Size([16384, 16, 64])
ğŸŸ¦ å›è»¢è¡Œåˆ—é©ç”¨ y1.shape=torch.Size([16384, 16, 64]), y2.shape=torch.Size([16384, 16, 64])
ğŸŸ© RoPEã‚’é©ç”¨å®Œäº† result.shape=torch.Size([16384, 16, 128])
ğŸŸ© RoPEã‚’é©ç”¨é–‹å§‹ x.shape=torch.Size([16384, 8, 128]), cos.shape=torch.Size([16384, 1, 64]), sin.shape=torch.Size([16384, 1, 64])
ğŸŸ¦ ãƒãƒ£ãƒ³ã‚¯åŒ– x1.shape=torch.Size([16384, 8, 64]), x2.shape=torch.Size([16384, 8, 64])
ğŸŸ¦ å›è»¢è¡Œåˆ—é©ç”¨ y1.shape=torch.Size([16384, 8, 64]), y2.shape=torch.Size([16384, 8, 64])
ğŸŸ© RoPEã‚’é©ç”¨å®Œäº† result.shape=torch.Size([16384, 8, 128])
ğŸŸ© RoPEã®é †ä¼æ’­å®Œäº† query.shape=torch.Size([16384, 16, 128]), key.shape=torch.Size([16384, 8, 128])
ğŸŸ© Attentionå±¤ã®é †ä¼æ’­é–‹å§‹ q.shape=torch.Size([16384, 16, 128]), k.shape=torch.Size([16384, 8, 128]), v.shape=torch.Size([16384, 8, 128])
ğŸŸ© ç¾åœ¨ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆå–å¾— _CONTEXT=Context(is_prefill=True, cu_seqlens_q=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32), cu_seqlens_k=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32), max_seqlen_q=4096, max_seqlen_k=4096, slot_mapping=tensor([], device='cuda:0', dtype=torch.int32), context_lens=None, block_tables=None)
ğŸŸ¦ ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæƒ…å ±å–å¾— context.is_prefill=True, context.slot_mapping=tensor([], device='cuda:0', dtype=torch.int32), context.max_seqlen_q=4096, context.cu_seqlens_q=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32), context.max_seqlen_k=4096, context.cu_seqlens_k=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32), context.context_lens=None, context.block_tables=None
ğŸŸ¦ Prefillãƒ¢ãƒ¼ãƒ‰ã§FlashAttentionã‚’å®Ÿè¡Œ context.max_seqlen_q=4096, context.cu_seqlens_q=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32), context.max_seqlen_k=4096, context.cu_seqlens_k=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32)
ğŸŸ© Attentionå±¤ã®é †ä¼æ’­å®Œäº† o.shape=torch.Size([16384, 16, 128])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­é–‹å§‹ å…¥åŠ›å½¢çŠ¶: torch.Size([16384, 2048])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­å®Œäº† å‡ºåŠ›å½¢çŠ¶: torch.Size([16384, 1024])
ğŸŸ© Qwen3Attentionã®é †ä¼æ’­å®Œäº† output.shape=torch.Size([16384, 1024])
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([16384, 1024]), residual.shape if residual is not None else None=torch.Size([16384, 1024])
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨å®Œäº†
ğŸŸ© Qwen3MLPã®é †ä¼æ’­ x.shape=torch.Size([16384, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([16384, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­å®Œäº† result.shape=torch.Size([16384, 6144])
ğŸŸ© SwiGLUã‚’é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([16384, 6144])
ğŸŸ¦ ãƒãƒ£ãƒ³ã‚¯åŒ– x.shape=torch.Size([16384, 3072]), y.shape=torch.Size([16384, 3072])
ğŸŸ© SwiGLUã‚’é †ä¼æ’­å®Œäº† result.shape=torch.Size([16384, 3072])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­é–‹å§‹ å…¥åŠ›å½¢çŠ¶: torch.Size([16384, 3072])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­å®Œäº† å‡ºåŠ›å½¢çŠ¶: torch.Size([16384, 1024])
ğŸŸ© Qwen3MLPã®é †ä¼æ’­å®Œäº† x.shape=torch.Size([16384, 1024])
ğŸŸ© Qwen3DecoderLayerã®é †ä¼æ’­å®Œäº† hidden_states.shape=torch.Size([16384, 1024])
ğŸŸ© Qwen3DecoderLayerã®é †ä¼æ’­ positions.shape=torch.Size([16384]) hidden_states.shape=torch.Size([16384, 1024]) residual is None=False
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([16384, 1024]), residual.shape if residual is not None else None=torch.Size([16384, 1024])
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨å®Œäº†
ğŸŸ© Qwen3Attentionã®é †ä¼æ’­ positions.shape=torch.Size([16384]) hidden_states.shape=torch.Size([16384, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([16384, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­å®Œäº† result.shape=torch.Size([16384, 4096])
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([16384, 16, 128]), residual.shape if residual is not None else None=None
ğŸŸ© æ­£è¦åŒ–é–‹å§‹ x.shape=torch.Size([16384, 16, 128])
ğŸŸ© æ­£è¦åŒ–å®Œäº† x.shape=torch.Size([16384, 16, 128])
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([16384, 8, 128]), residual.shape if residual is not None else None=None
ğŸŸ© æ­£è¦åŒ–é–‹å§‹ x.shape=torch.Size([16384, 8, 128])
ğŸŸ© æ­£è¦åŒ–å®Œäº† x.shape=torch.Size([16384, 8, 128])
ğŸŸ© RoPEã®é †ä¼æ’­é–‹å§‹ positions.shape=torch.Size([16384]), query.shape=torch.Size([16384, 16, 128]), key.shape=torch.Size([16384, 8, 128])
ğŸŸ© RoPEã‚’é©ç”¨é–‹å§‹ x.shape=torch.Size([16384, 16, 128]), cos.shape=torch.Size([16384, 1, 64]), sin.shape=torch.Size([16384, 1, 64])
ğŸŸ¦ ãƒãƒ£ãƒ³ã‚¯åŒ– x1.shape=torch.Size([16384, 16, 64]), x2.shape=torch.Size([16384, 16, 64])
ğŸŸ¦ å›è»¢è¡Œåˆ—é©ç”¨ y1.shape=torch.Size([16384, 16, 64]), y2.shape=torch.Size([16384, 16, 64])
ğŸŸ© RoPEã‚’é©ç”¨å®Œäº† result.shape=torch.Size([16384, 16, 128])
ğŸŸ© RoPEã‚’é©ç”¨é–‹å§‹ x.shape=torch.Size([16384, 8, 128]), cos.shape=torch.Size([16384, 1, 64]), sin.shape=torch.Size([16384, 1, 64])
ğŸŸ¦ ãƒãƒ£ãƒ³ã‚¯åŒ– x1.shape=torch.Size([16384, 8, 64]), x2.shape=torch.Size([16384, 8, 64])
ğŸŸ¦ å›è»¢è¡Œåˆ—é©ç”¨ y1.shape=torch.Size([16384, 8, 64]), y2.shape=torch.Size([16384, 8, 64])
ğŸŸ© RoPEã‚’é©ç”¨å®Œäº† result.shape=torch.Size([16384, 8, 128])
ğŸŸ© RoPEã®é †ä¼æ’­å®Œäº† query.shape=torch.Size([16384, 16, 128]), key.shape=torch.Size([16384, 8, 128])
ğŸŸ© Attentionå±¤ã®é †ä¼æ’­é–‹å§‹ q.shape=torch.Size([16384, 16, 128]), k.shape=torch.Size([16384, 8, 128]), v.shape=torch.Size([16384, 8, 128])
ğŸŸ© ç¾åœ¨ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆå–å¾— _CONTEXT=Context(is_prefill=True, cu_seqlens_q=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32), cu_seqlens_k=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32), max_seqlen_q=4096, max_seqlen_k=4096, slot_mapping=tensor([], device='cuda:0', dtype=torch.int32), context_lens=None, block_tables=None)
ğŸŸ¦ ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæƒ…å ±å–å¾— context.is_prefill=True, context.slot_mapping=tensor([], device='cuda:0', dtype=torch.int32), context.max_seqlen_q=4096, context.cu_seqlens_q=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32), context.max_seqlen_k=4096, context.cu_seqlens_k=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32), context.context_lens=None, context.block_tables=None
ğŸŸ¦ Prefillãƒ¢ãƒ¼ãƒ‰ã§FlashAttentionã‚’å®Ÿè¡Œ context.max_seqlen_q=4096, context.cu_seqlens_q=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32), context.max_seqlen_k=4096, context.cu_seqlens_k=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32)
ğŸŸ© Attentionå±¤ã®é †ä¼æ’­å®Œäº† o.shape=torch.Size([16384, 16, 128])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­é–‹å§‹ å…¥åŠ›å½¢çŠ¶: torch.Size([16384, 2048])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­å®Œäº† å‡ºåŠ›å½¢çŠ¶: torch.Size([16384, 1024])
ğŸŸ© Qwen3Attentionã®é †ä¼æ’­å®Œäº† output.shape=torch.Size([16384, 1024])
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([16384, 1024]), residual.shape if residual is not None else None=torch.Size([16384, 1024])
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨å®Œäº†
ğŸŸ© Qwen3MLPã®é †ä¼æ’­ x.shape=torch.Size([16384, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([16384, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­å®Œäº† result.shape=torch.Size([16384, 6144])
ğŸŸ© SwiGLUã‚’é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([16384, 6144])
ğŸŸ¦ ãƒãƒ£ãƒ³ã‚¯åŒ– x.shape=torch.Size([16384, 3072]), y.shape=torch.Size([16384, 3072])
ğŸŸ© SwiGLUã‚’é †ä¼æ’­å®Œäº† result.shape=torch.Size([16384, 3072])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­é–‹å§‹ å…¥åŠ›å½¢çŠ¶: torch.Size([16384, 3072])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­å®Œäº† å‡ºåŠ›å½¢çŠ¶: torch.Size([16384, 1024])
ğŸŸ© Qwen3MLPã®é †ä¼æ’­å®Œäº† x.shape=torch.Size([16384, 1024])
ğŸŸ© Qwen3DecoderLayerã®é †ä¼æ’­å®Œäº† hidden_states.shape=torch.Size([16384, 1024])
ğŸŸ© Qwen3DecoderLayerã®é †ä¼æ’­ positions.shape=torch.Size([16384]) hidden_states.shape=torch.Size([16384, 1024]) residual is None=False
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([16384, 1024]), residual.shape if residual is not None else None=torch.Size([16384, 1024])
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨å®Œäº†
ğŸŸ© Qwen3Attentionã®é †ä¼æ’­ positions.shape=torch.Size([16384]) hidden_states.shape=torch.Size([16384, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([16384, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­å®Œäº† result.shape=torch.Size([16384, 4096])
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([16384, 16, 128]), residual.shape if residual is not None else None=None
ğŸŸ© æ­£è¦åŒ–é–‹å§‹ x.shape=torch.Size([16384, 16, 128])
ğŸŸ© æ­£è¦åŒ–å®Œäº† x.shape=torch.Size([16384, 16, 128])
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([16384, 8, 128]), residual.shape if residual is not None else None=None
ğŸŸ© æ­£è¦åŒ–é–‹å§‹ x.shape=torch.Size([16384, 8, 128])
ğŸŸ© æ­£è¦åŒ–å®Œäº† x.shape=torch.Size([16384, 8, 128])
ğŸŸ© RoPEã®é †ä¼æ’­é–‹å§‹ positions.shape=torch.Size([16384]), query.shape=torch.Size([16384, 16, 128]), key.shape=torch.Size([16384, 8, 128])
ğŸŸ© RoPEã‚’é©ç”¨é–‹å§‹ x.shape=torch.Size([16384, 16, 128]), cos.shape=torch.Size([16384, 1, 64]), sin.shape=torch.Size([16384, 1, 64])
ğŸŸ¦ ãƒãƒ£ãƒ³ã‚¯åŒ– x1.shape=torch.Size([16384, 16, 64]), x2.shape=torch.Size([16384, 16, 64])
ğŸŸ¦ å›è»¢è¡Œåˆ—é©ç”¨ y1.shape=torch.Size([16384, 16, 64]), y2.shape=torch.Size([16384, 16, 64])
ğŸŸ© RoPEã‚’é©ç”¨å®Œäº† result.shape=torch.Size([16384, 16, 128])
ğŸŸ© RoPEã‚’é©ç”¨é–‹å§‹ x.shape=torch.Size([16384, 8, 128]), cos.shape=torch.Size([16384, 1, 64]), sin.shape=torch.Size([16384, 1, 64])
ğŸŸ¦ ãƒãƒ£ãƒ³ã‚¯åŒ– x1.shape=torch.Size([16384, 8, 64]), x2.shape=torch.Size([16384, 8, 64])
ğŸŸ¦ å›è»¢è¡Œåˆ—é©ç”¨ y1.shape=torch.Size([16384, 8, 64]), y2.shape=torch.Size([16384, 8, 64])
ğŸŸ© RoPEã‚’é©ç”¨å®Œäº† result.shape=torch.Size([16384, 8, 128])
ğŸŸ© RoPEã®é †ä¼æ’­å®Œäº† query.shape=torch.Size([16384, 16, 128]), key.shape=torch.Size([16384, 8, 128])
ğŸŸ© Attentionå±¤ã®é †ä¼æ’­é–‹å§‹ q.shape=torch.Size([16384, 16, 128]), k.shape=torch.Size([16384, 8, 128]), v.shape=torch.Size([16384, 8, 128])
ğŸŸ© ç¾åœ¨ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆå–å¾— _CONTEXT=Context(is_prefill=True, cu_seqlens_q=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32), cu_seqlens_k=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32), max_seqlen_q=4096, max_seqlen_k=4096, slot_mapping=tensor([], device='cuda:0', dtype=torch.int32), context_lens=None, block_tables=None)
ğŸŸ¦ ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæƒ…å ±å–å¾— context.is_prefill=True, context.slot_mapping=tensor([], device='cuda:0', dtype=torch.int32), context.max_seqlen_q=4096, context.cu_seqlens_q=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32), context.max_seqlen_k=4096, context.cu_seqlens_k=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32), context.context_lens=None, context.block_tables=None
ğŸŸ¦ Prefillãƒ¢ãƒ¼ãƒ‰ã§FlashAttentionã‚’å®Ÿè¡Œ context.max_seqlen_q=4096, context.cu_seqlens_q=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32), context.max_seqlen_k=4096, context.cu_seqlens_k=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32)
ğŸŸ© Attentionå±¤ã®é †ä¼æ’­å®Œäº† o.shape=torch.Size([16384, 16, 128])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­é–‹å§‹ å…¥åŠ›å½¢çŠ¶: torch.Size([16384, 2048])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­å®Œäº† å‡ºåŠ›å½¢çŠ¶: torch.Size([16384, 1024])
ğŸŸ© Qwen3Attentionã®é †ä¼æ’­å®Œäº† output.shape=torch.Size([16384, 1024])
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([16384, 1024]), residual.shape if residual is not None else None=torch.Size([16384, 1024])
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨å®Œäº†
ğŸŸ© Qwen3MLPã®é †ä¼æ’­ x.shape=torch.Size([16384, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([16384, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­å®Œäº† result.shape=torch.Size([16384, 6144])
ğŸŸ© SwiGLUã‚’é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([16384, 6144])
ğŸŸ¦ ãƒãƒ£ãƒ³ã‚¯åŒ– x.shape=torch.Size([16384, 3072]), y.shape=torch.Size([16384, 3072])
ğŸŸ© SwiGLUã‚’é †ä¼æ’­å®Œäº† result.shape=torch.Size([16384, 3072])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­é–‹å§‹ å…¥åŠ›å½¢çŠ¶: torch.Size([16384, 3072])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­å®Œäº† å‡ºåŠ›å½¢çŠ¶: torch.Size([16384, 1024])
ğŸŸ© Qwen3MLPã®é †ä¼æ’­å®Œäº† x.shape=torch.Size([16384, 1024])
ğŸŸ© Qwen3DecoderLayerã®é †ä¼æ’­å®Œäº† hidden_states.shape=torch.Size([16384, 1024])
ğŸŸ© Qwen3DecoderLayerã®é †ä¼æ’­ positions.shape=torch.Size([16384]) hidden_states.shape=torch.Size([16384, 1024]) residual is None=False
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([16384, 1024]), residual.shape if residual is not None else None=torch.Size([16384, 1024])
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨å®Œäº†
ğŸŸ© Qwen3Attentionã®é †ä¼æ’­ positions.shape=torch.Size([16384]) hidden_states.shape=torch.Size([16384, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([16384, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­å®Œäº† result.shape=torch.Size([16384, 4096])
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([16384, 16, 128]), residual.shape if residual is not None else None=None
ğŸŸ© æ­£è¦åŒ–é–‹å§‹ x.shape=torch.Size([16384, 16, 128])
ğŸŸ© æ­£è¦åŒ–å®Œäº† x.shape=torch.Size([16384, 16, 128])
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([16384, 8, 128]), residual.shape if residual is not None else None=None
ğŸŸ© æ­£è¦åŒ–é–‹å§‹ x.shape=torch.Size([16384, 8, 128])
ğŸŸ© æ­£è¦åŒ–å®Œäº† x.shape=torch.Size([16384, 8, 128])
ğŸŸ© RoPEã®é †ä¼æ’­é–‹å§‹ positions.shape=torch.Size([16384]), query.shape=torch.Size([16384, 16, 128]), key.shape=torch.Size([16384, 8, 128])
ğŸŸ© RoPEã‚’é©ç”¨é–‹å§‹ x.shape=torch.Size([16384, 16, 128]), cos.shape=torch.Size([16384, 1, 64]), sin.shape=torch.Size([16384, 1, 64])
ğŸŸ¦ ãƒãƒ£ãƒ³ã‚¯åŒ– x1.shape=torch.Size([16384, 16, 64]), x2.shape=torch.Size([16384, 16, 64])
ğŸŸ¦ å›è»¢è¡Œåˆ—é©ç”¨ y1.shape=torch.Size([16384, 16, 64]), y2.shape=torch.Size([16384, 16, 64])
ğŸŸ© RoPEã‚’é©ç”¨å®Œäº† result.shape=torch.Size([16384, 16, 128])
ğŸŸ© RoPEã‚’é©ç”¨é–‹å§‹ x.shape=torch.Size([16384, 8, 128]), cos.shape=torch.Size([16384, 1, 64]), sin.shape=torch.Size([16384, 1, 64])
ğŸŸ¦ ãƒãƒ£ãƒ³ã‚¯åŒ– x1.shape=torch.Size([16384, 8, 64]), x2.shape=torch.Size([16384, 8, 64])
ğŸŸ¦ å›è»¢è¡Œåˆ—é©ç”¨ y1.shape=torch.Size([16384, 8, 64]), y2.shape=torch.Size([16384, 8, 64])
ğŸŸ© RoPEã‚’é©ç”¨å®Œäº† result.shape=torch.Size([16384, 8, 128])
ğŸŸ© RoPEã®é †ä¼æ’­å®Œäº† query.shape=torch.Size([16384, 16, 128]), key.shape=torch.Size([16384, 8, 128])
ğŸŸ© Attentionå±¤ã®é †ä¼æ’­é–‹å§‹ q.shape=torch.Size([16384, 16, 128]), k.shape=torch.Size([16384, 8, 128]), v.shape=torch.Size([16384, 8, 128])
ğŸŸ© ç¾åœ¨ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆå–å¾— _CONTEXT=Context(is_prefill=True, cu_seqlens_q=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32), cu_seqlens_k=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32), max_seqlen_q=4096, max_seqlen_k=4096, slot_mapping=tensor([], device='cuda:0', dtype=torch.int32), context_lens=None, block_tables=None)
ğŸŸ¦ ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæƒ…å ±å–å¾— context.is_prefill=True, context.slot_mapping=tensor([], device='cuda:0', dtype=torch.int32), context.max_seqlen_q=4096, context.cu_seqlens_q=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32), context.max_seqlen_k=4096, context.cu_seqlens_k=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32), context.context_lens=None, context.block_tables=None
ğŸŸ¦ Prefillãƒ¢ãƒ¼ãƒ‰ã§FlashAttentionã‚’å®Ÿè¡Œ context.max_seqlen_q=4096, context.cu_seqlens_q=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32), context.max_seqlen_k=4096, context.cu_seqlens_k=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32)
ğŸŸ© Attentionå±¤ã®é †ä¼æ’­å®Œäº† o.shape=torch.Size([16384, 16, 128])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­é–‹å§‹ å…¥åŠ›å½¢çŠ¶: torch.Size([16384, 2048])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­å®Œäº† å‡ºåŠ›å½¢çŠ¶: torch.Size([16384, 1024])
ğŸŸ© Qwen3Attentionã®é †ä¼æ’­å®Œäº† output.shape=torch.Size([16384, 1024])
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([16384, 1024]), residual.shape if residual is not None else None=torch.Size([16384, 1024])
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨å®Œäº†
ğŸŸ© Qwen3MLPã®é †ä¼æ’­ x.shape=torch.Size([16384, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([16384, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­å®Œäº† result.shape=torch.Size([16384, 6144])
ğŸŸ© SwiGLUã‚’é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([16384, 6144])
ğŸŸ¦ ãƒãƒ£ãƒ³ã‚¯åŒ– x.shape=torch.Size([16384, 3072]), y.shape=torch.Size([16384, 3072])
ğŸŸ© SwiGLUã‚’é †ä¼æ’­å®Œäº† result.shape=torch.Size([16384, 3072])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­é–‹å§‹ å…¥åŠ›å½¢çŠ¶: torch.Size([16384, 3072])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­å®Œäº† å‡ºåŠ›å½¢çŠ¶: torch.Size([16384, 1024])
ğŸŸ© Qwen3MLPã®é †ä¼æ’­å®Œäº† x.shape=torch.Size([16384, 1024])
ğŸŸ© Qwen3DecoderLayerã®é †ä¼æ’­å®Œäº† hidden_states.shape=torch.Size([16384, 1024])
ğŸŸ© Qwen3DecoderLayerã®é †ä¼æ’­ positions.shape=torch.Size([16384]) hidden_states.shape=torch.Size([16384, 1024]) residual is None=False
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([16384, 1024]), residual.shape if residual is not None else None=torch.Size([16384, 1024])
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨å®Œäº†
ğŸŸ© Qwen3Attentionã®é †ä¼æ’­ positions.shape=torch.Size([16384]) hidden_states.shape=torch.Size([16384, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([16384, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­å®Œäº† result.shape=torch.Size([16384, 4096])
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([16384, 16, 128]), residual.shape if residual is not None else None=None
ğŸŸ© æ­£è¦åŒ–é–‹å§‹ x.shape=torch.Size([16384, 16, 128])
ğŸŸ© æ­£è¦åŒ–å®Œäº† x.shape=torch.Size([16384, 16, 128])
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([16384, 8, 128]), residual.shape if residual is not None else None=None
ğŸŸ© æ­£è¦åŒ–é–‹å§‹ x.shape=torch.Size([16384, 8, 128])
ğŸŸ© æ­£è¦åŒ–å®Œäº† x.shape=torch.Size([16384, 8, 128])
ğŸŸ© RoPEã®é †ä¼æ’­é–‹å§‹ positions.shape=torch.Size([16384]), query.shape=torch.Size([16384, 16, 128]), key.shape=torch.Size([16384, 8, 128])
ğŸŸ© RoPEã‚’é©ç”¨é–‹å§‹ x.shape=torch.Size([16384, 16, 128]), cos.shape=torch.Size([16384, 1, 64]), sin.shape=torch.Size([16384, 1, 64])
ğŸŸ¦ ãƒãƒ£ãƒ³ã‚¯åŒ– x1.shape=torch.Size([16384, 16, 64]), x2.shape=torch.Size([16384, 16, 64])
ğŸŸ¦ å›è»¢è¡Œåˆ—é©ç”¨ y1.shape=torch.Size([16384, 16, 64]), y2.shape=torch.Size([16384, 16, 64])
ğŸŸ© RoPEã‚’é©ç”¨å®Œäº† result.shape=torch.Size([16384, 16, 128])
ğŸŸ© RoPEã‚’é©ç”¨é–‹å§‹ x.shape=torch.Size([16384, 8, 128]), cos.shape=torch.Size([16384, 1, 64]), sin.shape=torch.Size([16384, 1, 64])
ğŸŸ¦ ãƒãƒ£ãƒ³ã‚¯åŒ– x1.shape=torch.Size([16384, 8, 64]), x2.shape=torch.Size([16384, 8, 64])
ğŸŸ¦ å›è»¢è¡Œåˆ—é©ç”¨ y1.shape=torch.Size([16384, 8, 64]), y2.shape=torch.Size([16384, 8, 64])
ğŸŸ© RoPEã‚’é©ç”¨å®Œäº† result.shape=torch.Size([16384, 8, 128])
ğŸŸ© RoPEã®é †ä¼æ’­å®Œäº† query.shape=torch.Size([16384, 16, 128]), key.shape=torch.Size([16384, 8, 128])
ğŸŸ© Attentionå±¤ã®é †ä¼æ’­é–‹å§‹ q.shape=torch.Size([16384, 16, 128]), k.shape=torch.Size([16384, 8, 128]), v.shape=torch.Size([16384, 8, 128])
ğŸŸ© ç¾åœ¨ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆå–å¾— _CONTEXT=Context(is_prefill=True, cu_seqlens_q=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32), cu_seqlens_k=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32), max_seqlen_q=4096, max_seqlen_k=4096, slot_mapping=tensor([], device='cuda:0', dtype=torch.int32), context_lens=None, block_tables=None)
ğŸŸ¦ ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæƒ…å ±å–å¾— context.is_prefill=True, context.slot_mapping=tensor([], device='cuda:0', dtype=torch.int32), context.max_seqlen_q=4096, context.cu_seqlens_q=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32), context.max_seqlen_k=4096, context.cu_seqlens_k=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32), context.context_lens=None, context.block_tables=None
ğŸŸ¦ Prefillãƒ¢ãƒ¼ãƒ‰ã§FlashAttentionã‚’å®Ÿè¡Œ context.max_seqlen_q=4096, context.cu_seqlens_q=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32), context.max_seqlen_k=4096, context.cu_seqlens_k=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32)
ğŸŸ© Attentionå±¤ã®é †ä¼æ’­å®Œäº† o.shape=torch.Size([16384, 16, 128])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­é–‹å§‹ å…¥åŠ›å½¢çŠ¶: torch.Size([16384, 2048])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­å®Œäº† å‡ºåŠ›å½¢çŠ¶: torch.Size([16384, 1024])
ğŸŸ© Qwen3Attentionã®é †ä¼æ’­å®Œäº† output.shape=torch.Size([16384, 1024])
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([16384, 1024]), residual.shape if residual is not None else None=torch.Size([16384, 1024])
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨å®Œäº†
ğŸŸ© Qwen3MLPã®é †ä¼æ’­ x.shape=torch.Size([16384, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([16384, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­å®Œäº† result.shape=torch.Size([16384, 6144])
ğŸŸ© SwiGLUã‚’é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([16384, 6144])
ğŸŸ¦ ãƒãƒ£ãƒ³ã‚¯åŒ– x.shape=torch.Size([16384, 3072]), y.shape=torch.Size([16384, 3072])
ğŸŸ© SwiGLUã‚’é †ä¼æ’­å®Œäº† result.shape=torch.Size([16384, 3072])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­é–‹å§‹ å…¥åŠ›å½¢çŠ¶: torch.Size([16384, 3072])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­å®Œäº† å‡ºåŠ›å½¢çŠ¶: torch.Size([16384, 1024])
ğŸŸ© Qwen3MLPã®é †ä¼æ’­å®Œäº† x.shape=torch.Size([16384, 1024])
ğŸŸ© Qwen3DecoderLayerã®é †ä¼æ’­å®Œäº† hidden_states.shape=torch.Size([16384, 1024])
ğŸŸ© Qwen3DecoderLayerã®é †ä¼æ’­ positions.shape=torch.Size([16384]) hidden_states.shape=torch.Size([16384, 1024]) residual is None=False
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([16384, 1024]), residual.shape if residual is not None else None=torch.Size([16384, 1024])
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨å®Œäº†
ğŸŸ© Qwen3Attentionã®é †ä¼æ’­ positions.shape=torch.Size([16384]) hidden_states.shape=torch.Size([16384, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([16384, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­å®Œäº† result.shape=torch.Size([16384, 4096])
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([16384, 16, 128]), residual.shape if residual is not None else None=None
ğŸŸ© æ­£è¦åŒ–é–‹å§‹ x.shape=torch.Size([16384, 16, 128])
ğŸŸ© æ­£è¦åŒ–å®Œäº† x.shape=torch.Size([16384, 16, 128])
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([16384, 8, 128]), residual.shape if residual is not None else None=None
ğŸŸ© æ­£è¦åŒ–é–‹å§‹ x.shape=torch.Size([16384, 8, 128])
ğŸŸ© æ­£è¦åŒ–å®Œäº† x.shape=torch.Size([16384, 8, 128])
ğŸŸ© RoPEã®é †ä¼æ’­é–‹å§‹ positions.shape=torch.Size([16384]), query.shape=torch.Size([16384, 16, 128]), key.shape=torch.Size([16384, 8, 128])
ğŸŸ© RoPEã‚’é©ç”¨é–‹å§‹ x.shape=torch.Size([16384, 16, 128]), cos.shape=torch.Size([16384, 1, 64]), sin.shape=torch.Size([16384, 1, 64])
ğŸŸ¦ ãƒãƒ£ãƒ³ã‚¯åŒ– x1.shape=torch.Size([16384, 16, 64]), x2.shape=torch.Size([16384, 16, 64])
ğŸŸ¦ å›è»¢è¡Œåˆ—é©ç”¨ y1.shape=torch.Size([16384, 16, 64]), y2.shape=torch.Size([16384, 16, 64])
ğŸŸ© RoPEã‚’é©ç”¨å®Œäº† result.shape=torch.Size([16384, 16, 128])
ğŸŸ© RoPEã‚’é©ç”¨é–‹å§‹ x.shape=torch.Size([16384, 8, 128]), cos.shape=torch.Size([16384, 1, 64]), sin.shape=torch.Size([16384, 1, 64])
ğŸŸ¦ ãƒãƒ£ãƒ³ã‚¯åŒ– x1.shape=torch.Size([16384, 8, 64]), x2.shape=torch.Size([16384, 8, 64])
ğŸŸ¦ å›è»¢è¡Œåˆ—é©ç”¨ y1.shape=torch.Size([16384, 8, 64]), y2.shape=torch.Size([16384, 8, 64])
ğŸŸ© RoPEã‚’é©ç”¨å®Œäº† result.shape=torch.Size([16384, 8, 128])
ğŸŸ© RoPEã®é †ä¼æ’­å®Œäº† query.shape=torch.Size([16384, 16, 128]), key.shape=torch.Size([16384, 8, 128])
ğŸŸ© Attentionå±¤ã®é †ä¼æ’­é–‹å§‹ q.shape=torch.Size([16384, 16, 128]), k.shape=torch.Size([16384, 8, 128]), v.shape=torch.Size([16384, 8, 128])
ğŸŸ© ç¾åœ¨ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆå–å¾— _CONTEXT=Context(is_prefill=True, cu_seqlens_q=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32), cu_seqlens_k=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32), max_seqlen_q=4096, max_seqlen_k=4096, slot_mapping=tensor([], device='cuda:0', dtype=torch.int32), context_lens=None, block_tables=None)
ğŸŸ¦ ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæƒ…å ±å–å¾— context.is_prefill=True, context.slot_mapping=tensor([], device='cuda:0', dtype=torch.int32), context.max_seqlen_q=4096, context.cu_seqlens_q=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32), context.max_seqlen_k=4096, context.cu_seqlens_k=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32), context.context_lens=None, context.block_tables=None
ğŸŸ¦ Prefillãƒ¢ãƒ¼ãƒ‰ã§FlashAttentionã‚’å®Ÿè¡Œ context.max_seqlen_q=4096, context.cu_seqlens_q=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32), context.max_seqlen_k=4096, context.cu_seqlens_k=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32)
ğŸŸ© Attentionå±¤ã®é †ä¼æ’­å®Œäº† o.shape=torch.Size([16384, 16, 128])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­é–‹å§‹ å…¥åŠ›å½¢çŠ¶: torch.Size([16384, 2048])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­å®Œäº† å‡ºåŠ›å½¢çŠ¶: torch.Size([16384, 1024])
ğŸŸ© Qwen3Attentionã®é †ä¼æ’­å®Œäº† output.shape=torch.Size([16384, 1024])
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([16384, 1024]), residual.shape if residual is not None else None=torch.Size([16384, 1024])
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨å®Œäº†
ğŸŸ© Qwen3MLPã®é †ä¼æ’­ x.shape=torch.Size([16384, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([16384, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­å®Œäº† result.shape=torch.Size([16384, 6144])
ğŸŸ© SwiGLUã‚’é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([16384, 6144])
ğŸŸ¦ ãƒãƒ£ãƒ³ã‚¯åŒ– x.shape=torch.Size([16384, 3072]), y.shape=torch.Size([16384, 3072])
ğŸŸ© SwiGLUã‚’é †ä¼æ’­å®Œäº† result.shape=torch.Size([16384, 3072])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­é–‹å§‹ å…¥åŠ›å½¢çŠ¶: torch.Size([16384, 3072])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­å®Œäº† å‡ºåŠ›å½¢çŠ¶: torch.Size([16384, 1024])
ğŸŸ© Qwen3MLPã®é †ä¼æ’­å®Œäº† x.shape=torch.Size([16384, 1024])
ğŸŸ© Qwen3DecoderLayerã®é †ä¼æ’­å®Œäº† hidden_states.shape=torch.Size([16384, 1024])
ğŸŸ© Qwen3DecoderLayerã®é †ä¼æ’­ positions.shape=torch.Size([16384]) hidden_states.shape=torch.Size([16384, 1024]) residual is None=False
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([16384, 1024]), residual.shape if residual is not None else None=torch.Size([16384, 1024])
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨å®Œäº†
ğŸŸ© Qwen3Attentionã®é †ä¼æ’­ positions.shape=torch.Size([16384]) hidden_states.shape=torch.Size([16384, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([16384, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­å®Œäº† result.shape=torch.Size([16384, 4096])
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([16384, 16, 128]), residual.shape if residual is not None else None=None
ğŸŸ© æ­£è¦åŒ–é–‹å§‹ x.shape=torch.Size([16384, 16, 128])
ğŸŸ© æ­£è¦åŒ–å®Œäº† x.shape=torch.Size([16384, 16, 128])
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([16384, 8, 128]), residual.shape if residual is not None else None=None
ğŸŸ© æ­£è¦åŒ–é–‹å§‹ x.shape=torch.Size([16384, 8, 128])
ğŸŸ© æ­£è¦åŒ–å®Œäº† x.shape=torch.Size([16384, 8, 128])
ğŸŸ© RoPEã®é †ä¼æ’­é–‹å§‹ positions.shape=torch.Size([16384]), query.shape=torch.Size([16384, 16, 128]), key.shape=torch.Size([16384, 8, 128])
ğŸŸ© RoPEã‚’é©ç”¨é–‹å§‹ x.shape=torch.Size([16384, 16, 128]), cos.shape=torch.Size([16384, 1, 64]), sin.shape=torch.Size([16384, 1, 64])
ğŸŸ¦ ãƒãƒ£ãƒ³ã‚¯åŒ– x1.shape=torch.Size([16384, 16, 64]), x2.shape=torch.Size([16384, 16, 64])
ğŸŸ¦ å›è»¢è¡Œåˆ—é©ç”¨ y1.shape=torch.Size([16384, 16, 64]), y2.shape=torch.Size([16384, 16, 64])
ğŸŸ© RoPEã‚’é©ç”¨å®Œäº† result.shape=torch.Size([16384, 16, 128])
ğŸŸ© RoPEã‚’é©ç”¨é–‹å§‹ x.shape=torch.Size([16384, 8, 128]), cos.shape=torch.Size([16384, 1, 64]), sin.shape=torch.Size([16384, 1, 64])
ğŸŸ¦ ãƒãƒ£ãƒ³ã‚¯åŒ– x1.shape=torch.Size([16384, 8, 64]), x2.shape=torch.Size([16384, 8, 64])
ğŸŸ¦ å›è»¢è¡Œåˆ—é©ç”¨ y1.shape=torch.Size([16384, 8, 64]), y2.shape=torch.Size([16384, 8, 64])
ğŸŸ© RoPEã‚’é©ç”¨å®Œäº† result.shape=torch.Size([16384, 8, 128])
ğŸŸ© RoPEã®é †ä¼æ’­å®Œäº† query.shape=torch.Size([16384, 16, 128]), key.shape=torch.Size([16384, 8, 128])
ğŸŸ© Attentionå±¤ã®é †ä¼æ’­é–‹å§‹ q.shape=torch.Size([16384, 16, 128]), k.shape=torch.Size([16384, 8, 128]), v.shape=torch.Size([16384, 8, 128])
ğŸŸ© ç¾åœ¨ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆå–å¾— _CONTEXT=Context(is_prefill=True, cu_seqlens_q=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32), cu_seqlens_k=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32), max_seqlen_q=4096, max_seqlen_k=4096, slot_mapping=tensor([], device='cuda:0', dtype=torch.int32), context_lens=None, block_tables=None)
ğŸŸ¦ ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæƒ…å ±å–å¾— context.is_prefill=True, context.slot_mapping=tensor([], device='cuda:0', dtype=torch.int32), context.max_seqlen_q=4096, context.cu_seqlens_q=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32), context.max_seqlen_k=4096, context.cu_seqlens_k=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32), context.context_lens=None, context.block_tables=None
ğŸŸ¦ Prefillãƒ¢ãƒ¼ãƒ‰ã§FlashAttentionã‚’å®Ÿè¡Œ context.max_seqlen_q=4096, context.cu_seqlens_q=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32), context.max_seqlen_k=4096, context.cu_seqlens_k=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32)
ğŸŸ© Attentionå±¤ã®é †ä¼æ’­å®Œäº† o.shape=torch.Size([16384, 16, 128])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­é–‹å§‹ å…¥åŠ›å½¢çŠ¶: torch.Size([16384, 2048])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­å®Œäº† å‡ºåŠ›å½¢çŠ¶: torch.Size([16384, 1024])
ğŸŸ© Qwen3Attentionã®é †ä¼æ’­å®Œäº† output.shape=torch.Size([16384, 1024])
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([16384, 1024]), residual.shape if residual is not None else None=torch.Size([16384, 1024])
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨å®Œäº†
ğŸŸ© Qwen3MLPã®é †ä¼æ’­ x.shape=torch.Size([16384, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([16384, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­å®Œäº† result.shape=torch.Size([16384, 6144])
ğŸŸ© SwiGLUã‚’é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([16384, 6144])
ğŸŸ¦ ãƒãƒ£ãƒ³ã‚¯åŒ– x.shape=torch.Size([16384, 3072]), y.shape=torch.Size([16384, 3072])
ğŸŸ© SwiGLUã‚’é †ä¼æ’­å®Œäº† result.shape=torch.Size([16384, 3072])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­é–‹å§‹ å…¥åŠ›å½¢çŠ¶: torch.Size([16384, 3072])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­å®Œäº† å‡ºåŠ›å½¢çŠ¶: torch.Size([16384, 1024])
ğŸŸ© Qwen3MLPã®é †ä¼æ’­å®Œäº† x.shape=torch.Size([16384, 1024])
ğŸŸ© Qwen3DecoderLayerã®é †ä¼æ’­å®Œäº† hidden_states.shape=torch.Size([16384, 1024])
ğŸŸ© Qwen3DecoderLayerã®é †ä¼æ’­ positions.shape=torch.Size([16384]) hidden_states.shape=torch.Size([16384, 1024]) residual is None=False
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([16384, 1024]), residual.shape if residual is not None else None=torch.Size([16384, 1024])
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨å®Œäº†
ğŸŸ© Qwen3Attentionã®é †ä¼æ’­ positions.shape=torch.Size([16384]) hidden_states.shape=torch.Size([16384, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([16384, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­å®Œäº† result.shape=torch.Size([16384, 4096])
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([16384, 16, 128]), residual.shape if residual is not None else None=None
ğŸŸ© æ­£è¦åŒ–é–‹å§‹ x.shape=torch.Size([16384, 16, 128])
ğŸŸ© æ­£è¦åŒ–å®Œäº† x.shape=torch.Size([16384, 16, 128])
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([16384, 8, 128]), residual.shape if residual is not None else None=None
ğŸŸ© æ­£è¦åŒ–é–‹å§‹ x.shape=torch.Size([16384, 8, 128])
ğŸŸ© æ­£è¦åŒ–å®Œäº† x.shape=torch.Size([16384, 8, 128])
ğŸŸ© RoPEã®é †ä¼æ’­é–‹å§‹ positions.shape=torch.Size([16384]), query.shape=torch.Size([16384, 16, 128]), key.shape=torch.Size([16384, 8, 128])
ğŸŸ© RoPEã‚’é©ç”¨é–‹å§‹ x.shape=torch.Size([16384, 16, 128]), cos.shape=torch.Size([16384, 1, 64]), sin.shape=torch.Size([16384, 1, 64])
ğŸŸ¦ ãƒãƒ£ãƒ³ã‚¯åŒ– x1.shape=torch.Size([16384, 16, 64]), x2.shape=torch.Size([16384, 16, 64])
ğŸŸ¦ å›è»¢è¡Œåˆ—é©ç”¨ y1.shape=torch.Size([16384, 16, 64]), y2.shape=torch.Size([16384, 16, 64])
ğŸŸ© RoPEã‚’é©ç”¨å®Œäº† result.shape=torch.Size([16384, 16, 128])
ğŸŸ© RoPEã‚’é©ç”¨é–‹å§‹ x.shape=torch.Size([16384, 8, 128]), cos.shape=torch.Size([16384, 1, 64]), sin.shape=torch.Size([16384, 1, 64])
ğŸŸ¦ ãƒãƒ£ãƒ³ã‚¯åŒ– x1.shape=torch.Size([16384, 8, 64]), x2.shape=torch.Size([16384, 8, 64])
ğŸŸ¦ å›è»¢è¡Œåˆ—é©ç”¨ y1.shape=torch.Size([16384, 8, 64]), y2.shape=torch.Size([16384, 8, 64])
ğŸŸ© RoPEã‚’é©ç”¨å®Œäº† result.shape=torch.Size([16384, 8, 128])
ğŸŸ© RoPEã®é †ä¼æ’­å®Œäº† query.shape=torch.Size([16384, 16, 128]), key.shape=torch.Size([16384, 8, 128])
ğŸŸ© Attentionå±¤ã®é †ä¼æ’­é–‹å§‹ q.shape=torch.Size([16384, 16, 128]), k.shape=torch.Size([16384, 8, 128]), v.shape=torch.Size([16384, 8, 128])
ğŸŸ© ç¾åœ¨ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆå–å¾— _CONTEXT=Context(is_prefill=True, cu_seqlens_q=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32), cu_seqlens_k=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32), max_seqlen_q=4096, max_seqlen_k=4096, slot_mapping=tensor([], device='cuda:0', dtype=torch.int32), context_lens=None, block_tables=None)
ğŸŸ¦ ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæƒ…å ±å–å¾— context.is_prefill=True, context.slot_mapping=tensor([], device='cuda:0', dtype=torch.int32), context.max_seqlen_q=4096, context.cu_seqlens_q=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32), context.max_seqlen_k=4096, context.cu_seqlens_k=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32), context.context_lens=None, context.block_tables=None
ğŸŸ¦ Prefillãƒ¢ãƒ¼ãƒ‰ã§FlashAttentionã‚’å®Ÿè¡Œ context.max_seqlen_q=4096, context.cu_seqlens_q=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32), context.max_seqlen_k=4096, context.cu_seqlens_k=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32)
ğŸŸ© Attentionå±¤ã®é †ä¼æ’­å®Œäº† o.shape=torch.Size([16384, 16, 128])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­é–‹å§‹ å…¥åŠ›å½¢çŠ¶: torch.Size([16384, 2048])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­å®Œäº† å‡ºåŠ›å½¢çŠ¶: torch.Size([16384, 1024])
ğŸŸ© Qwen3Attentionã®é †ä¼æ’­å®Œäº† output.shape=torch.Size([16384, 1024])
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([16384, 1024]), residual.shape if residual is not None else None=torch.Size([16384, 1024])
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨å®Œäº†
ğŸŸ© Qwen3MLPã®é †ä¼æ’­ x.shape=torch.Size([16384, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([16384, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­å®Œäº† result.shape=torch.Size([16384, 6144])
ğŸŸ© SwiGLUã‚’é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([16384, 6144])
ğŸŸ¦ ãƒãƒ£ãƒ³ã‚¯åŒ– x.shape=torch.Size([16384, 3072]), y.shape=torch.Size([16384, 3072])
ğŸŸ© SwiGLUã‚’é †ä¼æ’­å®Œäº† result.shape=torch.Size([16384, 3072])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­é–‹å§‹ å…¥åŠ›å½¢çŠ¶: torch.Size([16384, 3072])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­å®Œäº† å‡ºåŠ›å½¢çŠ¶: torch.Size([16384, 1024])
ğŸŸ© Qwen3MLPã®é †ä¼æ’­å®Œäº† x.shape=torch.Size([16384, 1024])
ğŸŸ© Qwen3DecoderLayerã®é †ä¼æ’­å®Œäº† hidden_states.shape=torch.Size([16384, 1024])
ğŸŸ© Qwen3DecoderLayerã®é †ä¼æ’­ positions.shape=torch.Size([16384]) hidden_states.shape=torch.Size([16384, 1024]) residual is None=False
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([16384, 1024]), residual.shape if residual is not None else None=torch.Size([16384, 1024])
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨å®Œäº†
ğŸŸ© Qwen3Attentionã®é †ä¼æ’­ positions.shape=torch.Size([16384]) hidden_states.shape=torch.Size([16384, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([16384, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­å®Œäº† result.shape=torch.Size([16384, 4096])
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([16384, 16, 128]), residual.shape if residual is not None else None=None
ğŸŸ© æ­£è¦åŒ–é–‹å§‹ x.shape=torch.Size([16384, 16, 128])
ğŸŸ© æ­£è¦åŒ–å®Œäº† x.shape=torch.Size([16384, 16, 128])
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([16384, 8, 128]), residual.shape if residual is not None else None=None
ğŸŸ© æ­£è¦åŒ–é–‹å§‹ x.shape=torch.Size([16384, 8, 128])
ğŸŸ© æ­£è¦åŒ–å®Œäº† x.shape=torch.Size([16384, 8, 128])
ğŸŸ© RoPEã®é †ä¼æ’­é–‹å§‹ positions.shape=torch.Size([16384]), query.shape=torch.Size([16384, 16, 128]), key.shape=torch.Size([16384, 8, 128])
ğŸŸ© RoPEã‚’é©ç”¨é–‹å§‹ x.shape=torch.Size([16384, 16, 128]), cos.shape=torch.Size([16384, 1, 64]), sin.shape=torch.Size([16384, 1, 64])
ğŸŸ¦ ãƒãƒ£ãƒ³ã‚¯åŒ– x1.shape=torch.Size([16384, 16, 64]), x2.shape=torch.Size([16384, 16, 64])
ğŸŸ¦ å›è»¢è¡Œåˆ—é©ç”¨ y1.shape=torch.Size([16384, 16, 64]), y2.shape=torch.Size([16384, 16, 64])
ğŸŸ© RoPEã‚’é©ç”¨å®Œäº† result.shape=torch.Size([16384, 16, 128])
ğŸŸ© RoPEã‚’é©ç”¨é–‹å§‹ x.shape=torch.Size([16384, 8, 128]), cos.shape=torch.Size([16384, 1, 64]), sin.shape=torch.Size([16384, 1, 64])
ğŸŸ¦ ãƒãƒ£ãƒ³ã‚¯åŒ– x1.shape=torch.Size([16384, 8, 64]), x2.shape=torch.Size([16384, 8, 64])
ğŸŸ¦ å›è»¢è¡Œåˆ—é©ç”¨ y1.shape=torch.Size([16384, 8, 64]), y2.shape=torch.Size([16384, 8, 64])
ğŸŸ© RoPEã‚’é©ç”¨å®Œäº† result.shape=torch.Size([16384, 8, 128])
ğŸŸ© RoPEã®é †ä¼æ’­å®Œäº† query.shape=torch.Size([16384, 16, 128]), key.shape=torch.Size([16384, 8, 128])
ğŸŸ© Attentionå±¤ã®é †ä¼æ’­é–‹å§‹ q.shape=torch.Size([16384, 16, 128]), k.shape=torch.Size([16384, 8, 128]), v.shape=torch.Size([16384, 8, 128])
ğŸŸ© ç¾åœ¨ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆå–å¾— _CONTEXT=Context(is_prefill=True, cu_seqlens_q=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32), cu_seqlens_k=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32), max_seqlen_q=4096, max_seqlen_k=4096, slot_mapping=tensor([], device='cuda:0', dtype=torch.int32), context_lens=None, block_tables=None)
ğŸŸ¦ ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæƒ…å ±å–å¾— context.is_prefill=True, context.slot_mapping=tensor([], device='cuda:0', dtype=torch.int32), context.max_seqlen_q=4096, context.cu_seqlens_q=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32), context.max_seqlen_k=4096, context.cu_seqlens_k=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32), context.context_lens=None, context.block_tables=None
ğŸŸ¦ Prefillãƒ¢ãƒ¼ãƒ‰ã§FlashAttentionã‚’å®Ÿè¡Œ context.max_seqlen_q=4096, context.cu_seqlens_q=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32), context.max_seqlen_k=4096, context.cu_seqlens_k=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32)
ğŸŸ© Attentionå±¤ã®é †ä¼æ’­å®Œäº† o.shape=torch.Size([16384, 16, 128])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­é–‹å§‹ å…¥åŠ›å½¢çŠ¶: torch.Size([16384, 2048])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­å®Œäº† å‡ºåŠ›å½¢çŠ¶: torch.Size([16384, 1024])
ğŸŸ© Qwen3Attentionã®é †ä¼æ’­å®Œäº† output.shape=torch.Size([16384, 1024])
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([16384, 1024]), residual.shape if residual is not None else None=torch.Size([16384, 1024])
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨å®Œäº†
ğŸŸ© Qwen3MLPã®é †ä¼æ’­ x.shape=torch.Size([16384, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([16384, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­å®Œäº† result.shape=torch.Size([16384, 6144])
ğŸŸ© SwiGLUã‚’é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([16384, 6144])
ğŸŸ¦ ãƒãƒ£ãƒ³ã‚¯åŒ– x.shape=torch.Size([16384, 3072]), y.shape=torch.Size([16384, 3072])
ğŸŸ© SwiGLUã‚’é †ä¼æ’­å®Œäº† result.shape=torch.Size([16384, 3072])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­é–‹å§‹ å…¥åŠ›å½¢çŠ¶: torch.Size([16384, 3072])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­å®Œäº† å‡ºåŠ›å½¢çŠ¶: torch.Size([16384, 1024])
ğŸŸ© Qwen3MLPã®é †ä¼æ’­å®Œäº† x.shape=torch.Size([16384, 1024])
ğŸŸ© Qwen3DecoderLayerã®é †ä¼æ’­å®Œäº† hidden_states.shape=torch.Size([16384, 1024])
ğŸŸ© Qwen3DecoderLayerã®é †ä¼æ’­ positions.shape=torch.Size([16384]) hidden_states.shape=torch.Size([16384, 1024]) residual is None=False
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([16384, 1024]), residual.shape if residual is not None else None=torch.Size([16384, 1024])
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨å®Œäº†
ğŸŸ© Qwen3Attentionã®é †ä¼æ’­ positions.shape=torch.Size([16384]) hidden_states.shape=torch.Size([16384, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([16384, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­å®Œäº† result.shape=torch.Size([16384, 4096])
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([16384, 16, 128]), residual.shape if residual is not None else None=None
ğŸŸ© æ­£è¦åŒ–é–‹å§‹ x.shape=torch.Size([16384, 16, 128])
ğŸŸ© æ­£è¦åŒ–å®Œäº† x.shape=torch.Size([16384, 16, 128])
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([16384, 8, 128]), residual.shape if residual is not None else None=None
ğŸŸ© æ­£è¦åŒ–é–‹å§‹ x.shape=torch.Size([16384, 8, 128])
ğŸŸ© æ­£è¦åŒ–å®Œäº† x.shape=torch.Size([16384, 8, 128])
ğŸŸ© RoPEã®é †ä¼æ’­é–‹å§‹ positions.shape=torch.Size([16384]), query.shape=torch.Size([16384, 16, 128]), key.shape=torch.Size([16384, 8, 128])
ğŸŸ© RoPEã‚’é©ç”¨é–‹å§‹ x.shape=torch.Size([16384, 16, 128]), cos.shape=torch.Size([16384, 1, 64]), sin.shape=torch.Size([16384, 1, 64])
ğŸŸ¦ ãƒãƒ£ãƒ³ã‚¯åŒ– x1.shape=torch.Size([16384, 16, 64]), x2.shape=torch.Size([16384, 16, 64])
ğŸŸ¦ å›è»¢è¡Œåˆ—é©ç”¨ y1.shape=torch.Size([16384, 16, 64]), y2.shape=torch.Size([16384, 16, 64])
ğŸŸ© RoPEã‚’é©ç”¨å®Œäº† result.shape=torch.Size([16384, 16, 128])
ğŸŸ© RoPEã‚’é©ç”¨é–‹å§‹ x.shape=torch.Size([16384, 8, 128]), cos.shape=torch.Size([16384, 1, 64]), sin.shape=torch.Size([16384, 1, 64])
ğŸŸ¦ ãƒãƒ£ãƒ³ã‚¯åŒ– x1.shape=torch.Size([16384, 8, 64]), x2.shape=torch.Size([16384, 8, 64])
ğŸŸ¦ å›è»¢è¡Œåˆ—é©ç”¨ y1.shape=torch.Size([16384, 8, 64]), y2.shape=torch.Size([16384, 8, 64])
ğŸŸ© RoPEã‚’é©ç”¨å®Œäº† result.shape=torch.Size([16384, 8, 128])
ğŸŸ© RoPEã®é †ä¼æ’­å®Œäº† query.shape=torch.Size([16384, 16, 128]), key.shape=torch.Size([16384, 8, 128])
ğŸŸ© Attentionå±¤ã®é †ä¼æ’­é–‹å§‹ q.shape=torch.Size([16384, 16, 128]), k.shape=torch.Size([16384, 8, 128]), v.shape=torch.Size([16384, 8, 128])
ğŸŸ© ç¾åœ¨ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆå–å¾— _CONTEXT=Context(is_prefill=True, cu_seqlens_q=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32), cu_seqlens_k=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32), max_seqlen_q=4096, max_seqlen_k=4096, slot_mapping=tensor([], device='cuda:0', dtype=torch.int32), context_lens=None, block_tables=None)
ğŸŸ¦ ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæƒ…å ±å–å¾— context.is_prefill=True, context.slot_mapping=tensor([], device='cuda:0', dtype=torch.int32), context.max_seqlen_q=4096, context.cu_seqlens_q=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32), context.max_seqlen_k=4096, context.cu_seqlens_k=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32), context.context_lens=None, context.block_tables=None
ğŸŸ¦ Prefillãƒ¢ãƒ¼ãƒ‰ã§FlashAttentionã‚’å®Ÿè¡Œ context.max_seqlen_q=4096, context.cu_seqlens_q=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32), context.max_seqlen_k=4096, context.cu_seqlens_k=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32)
ğŸŸ© Attentionå±¤ã®é †ä¼æ’­å®Œäº† o.shape=torch.Size([16384, 16, 128])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­é–‹å§‹ å…¥åŠ›å½¢çŠ¶: torch.Size([16384, 2048])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­å®Œäº† å‡ºåŠ›å½¢çŠ¶: torch.Size([16384, 1024])
ğŸŸ© Qwen3Attentionã®é †ä¼æ’­å®Œäº† output.shape=torch.Size([16384, 1024])
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([16384, 1024]), residual.shape if residual is not None else None=torch.Size([16384, 1024])
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨å®Œäº†
ğŸŸ© Qwen3MLPã®é †ä¼æ’­ x.shape=torch.Size([16384, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([16384, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­å®Œäº† result.shape=torch.Size([16384, 6144])
ğŸŸ© SwiGLUã‚’é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([16384, 6144])
ğŸŸ¦ ãƒãƒ£ãƒ³ã‚¯åŒ– x.shape=torch.Size([16384, 3072]), y.shape=torch.Size([16384, 3072])
ğŸŸ© SwiGLUã‚’é †ä¼æ’­å®Œäº† result.shape=torch.Size([16384, 3072])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­é–‹å§‹ å…¥åŠ›å½¢çŠ¶: torch.Size([16384, 3072])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­å®Œäº† å‡ºåŠ›å½¢çŠ¶: torch.Size([16384, 1024])
ğŸŸ© Qwen3MLPã®é †ä¼æ’­å®Œäº† x.shape=torch.Size([16384, 1024])
ğŸŸ© Qwen3DecoderLayerã®é †ä¼æ’­å®Œäº† hidden_states.shape=torch.Size([16384, 1024])
ğŸŸ© Qwen3DecoderLayerã®é †ä¼æ’­ positions.shape=torch.Size([16384]) hidden_states.shape=torch.Size([16384, 1024]) residual is None=False
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([16384, 1024]), residual.shape if residual is not None else None=torch.Size([16384, 1024])
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨å®Œäº†
ğŸŸ© Qwen3Attentionã®é †ä¼æ’­ positions.shape=torch.Size([16384]) hidden_states.shape=torch.Size([16384, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([16384, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­å®Œäº† result.shape=torch.Size([16384, 4096])
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([16384, 16, 128]), residual.shape if residual is not None else None=None
ğŸŸ© æ­£è¦åŒ–é–‹å§‹ x.shape=torch.Size([16384, 16, 128])
ğŸŸ© æ­£è¦åŒ–å®Œäº† x.shape=torch.Size([16384, 16, 128])
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([16384, 8, 128]), residual.shape if residual is not None else None=None
ğŸŸ© æ­£è¦åŒ–é–‹å§‹ x.shape=torch.Size([16384, 8, 128])
ğŸŸ© æ­£è¦åŒ–å®Œäº† x.shape=torch.Size([16384, 8, 128])
ğŸŸ© RoPEã®é †ä¼æ’­é–‹å§‹ positions.shape=torch.Size([16384]), query.shape=torch.Size([16384, 16, 128]), key.shape=torch.Size([16384, 8, 128])
ğŸŸ© RoPEã‚’é©ç”¨é–‹å§‹ x.shape=torch.Size([16384, 16, 128]), cos.shape=torch.Size([16384, 1, 64]), sin.shape=torch.Size([16384, 1, 64])
ğŸŸ¦ ãƒãƒ£ãƒ³ã‚¯åŒ– x1.shape=torch.Size([16384, 16, 64]), x2.shape=torch.Size([16384, 16, 64])
ğŸŸ¦ å›è»¢è¡Œåˆ—é©ç”¨ y1.shape=torch.Size([16384, 16, 64]), y2.shape=torch.Size([16384, 16, 64])
ğŸŸ© RoPEã‚’é©ç”¨å®Œäº† result.shape=torch.Size([16384, 16, 128])
ğŸŸ© RoPEã‚’é©ç”¨é–‹å§‹ x.shape=torch.Size([16384, 8, 128]), cos.shape=torch.Size([16384, 1, 64]), sin.shape=torch.Size([16384, 1, 64])
ğŸŸ¦ ãƒãƒ£ãƒ³ã‚¯åŒ– x1.shape=torch.Size([16384, 8, 64]), x2.shape=torch.Size([16384, 8, 64])
ğŸŸ¦ å›è»¢è¡Œåˆ—é©ç”¨ y1.shape=torch.Size([16384, 8, 64]), y2.shape=torch.Size([16384, 8, 64])
ğŸŸ© RoPEã‚’é©ç”¨å®Œäº† result.shape=torch.Size([16384, 8, 128])
ğŸŸ© RoPEã®é †ä¼æ’­å®Œäº† query.shape=torch.Size([16384, 16, 128]), key.shape=torch.Size([16384, 8, 128])
ğŸŸ© Attentionå±¤ã®é †ä¼æ’­é–‹å§‹ q.shape=torch.Size([16384, 16, 128]), k.shape=torch.Size([16384, 8, 128]), v.shape=torch.Size([16384, 8, 128])
ğŸŸ© ç¾åœ¨ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆå–å¾— _CONTEXT=Context(is_prefill=True, cu_seqlens_q=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32), cu_seqlens_k=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32), max_seqlen_q=4096, max_seqlen_k=4096, slot_mapping=tensor([], device='cuda:0', dtype=torch.int32), context_lens=None, block_tables=None)
ğŸŸ¦ ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæƒ…å ±å–å¾— context.is_prefill=True, context.slot_mapping=tensor([], device='cuda:0', dtype=torch.int32), context.max_seqlen_q=4096, context.cu_seqlens_q=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32), context.max_seqlen_k=4096, context.cu_seqlens_k=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32), context.context_lens=None, context.block_tables=None
ğŸŸ¦ Prefillãƒ¢ãƒ¼ãƒ‰ã§FlashAttentionã‚’å®Ÿè¡Œ context.max_seqlen_q=4096, context.cu_seqlens_q=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32), context.max_seqlen_k=4096, context.cu_seqlens_k=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32)
ğŸŸ© Attentionå±¤ã®é †ä¼æ’­å®Œäº† o.shape=torch.Size([16384, 16, 128])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­é–‹å§‹ å…¥åŠ›å½¢çŠ¶: torch.Size([16384, 2048])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­å®Œäº† å‡ºåŠ›å½¢çŠ¶: torch.Size([16384, 1024])
ğŸŸ© Qwen3Attentionã®é †ä¼æ’­å®Œäº† output.shape=torch.Size([16384, 1024])
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([16384, 1024]), residual.shape if residual is not None else None=torch.Size([16384, 1024])
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨å®Œäº†
ğŸŸ© Qwen3MLPã®é †ä¼æ’­ x.shape=torch.Size([16384, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([16384, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­å®Œäº† result.shape=torch.Size([16384, 6144])
ğŸŸ© SwiGLUã‚’é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([16384, 6144])
ğŸŸ¦ ãƒãƒ£ãƒ³ã‚¯åŒ– x.shape=torch.Size([16384, 3072]), y.shape=torch.Size([16384, 3072])
ğŸŸ© SwiGLUã‚’é †ä¼æ’­å®Œäº† result.shape=torch.Size([16384, 3072])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­é–‹å§‹ å…¥åŠ›å½¢çŠ¶: torch.Size([16384, 3072])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­å®Œäº† å‡ºåŠ›å½¢çŠ¶: torch.Size([16384, 1024])
ğŸŸ© Qwen3MLPã®é †ä¼æ’­å®Œäº† x.shape=torch.Size([16384, 1024])
ğŸŸ© Qwen3DecoderLayerã®é †ä¼æ’­å®Œäº† hidden_states.shape=torch.Size([16384, 1024])
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([16384, 1024]), residual.shape if residual is not None else None=torch.Size([16384, 1024])
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨å®Œäº†
ğŸŸ© Qwen3Modelã®é †ä¼æ’­å®Œäº† hidden_states.shape=torch.Size([16384, 1024])
ğŸŸ© Qwen3ForCausalLMã®é †ä¼æ’­å®Œäº† output.shape=torch.Size([16384, 1024])
ğŸŸ© Qwen3ForCausalLMã®ãƒ­ã‚¸ãƒƒãƒˆè¨ˆç®—é–‹å§‹ hidden_states.shape=torch.Size([16384, 1024])
ğŸŸ© å‡ºåŠ›ã®è¨€èªãƒ¢ãƒ‡ãƒ«ãƒ˜ãƒƒãƒ‰ã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([16384, 1024])
ğŸŸ© ç¾åœ¨ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆå–å¾— _CONTEXT=Context(is_prefill=True, cu_seqlens_q=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32), cu_seqlens_k=tensor([    0,  4096,  8192, 12288, 16384], device='cuda:0', dtype=torch.int32), max_seqlen_q=4096, max_seqlen_k=4096, slot_mapping=tensor([], device='cuda:0', dtype=torch.int32), context_lens=None, block_tables=None)
ğŸŸ¦ æœ€å¾Œã®ãƒˆãƒ¼ã‚¯ãƒ³ã‚’æŠ½å‡ºï¼ˆPrefillï¼‰ x.shape=torch.Size([4, 1024])
ğŸŸ© å‡ºåŠ›ã®è¨€èªãƒ¢ãƒ‡ãƒ«ãƒ˜ãƒƒãƒ‰ã®é †ä¼æ’­å®Œäº† logits.shape if logits is not None else None=torch.Size([4, 151936])
ğŸŸ© Qwen3ForCausalLMã®ãƒ­ã‚¸ãƒƒãƒˆè¨ˆç®—å®Œäº† logits.shape=torch.Size([4, 151936])
ğŸŸ© ãƒ¢ãƒ‡ãƒ«ã®é †ä¼æ¬ï¼ˆprefillï¼‰ã‚’å®Ÿè¡Œå®Œäº† result.shape=torch.Size([4, 151936])
ğŸŸ© ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°é–‹å§‹ logits.shape=torch.Size([4, 151936]), temperatures.shape=torch.Size([4])
ğŸŸ© ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°å®Œäº† sample_tokens.shape=torch.Size([4])
ğŸŸ© ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒªã‚»ãƒƒãƒˆ
ğŸŸ© æ¨è«–ã®1ã‚µã‚¤ã‚¯ãƒ«ã‚’å®Ÿè¡Œå®Œäº† (len(token_ids) if token_ids else None)=4
ğŸŸ© Warmed up model on rank 0
ğŸŸ© Allocating KV cache on rank 0
ğŸŸ© Allocated KV cache with 875 blocks on rank 0
ğŸŸ© Initialized ModelRunner on rank 0
ğŸŸ© ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ã‚’åˆæœŸåŒ– config.max_num_seqs=512, config.max_num_batched_tokens=16384 config.eos=151645, config.num_kvcache_blocks=875, config.kvcache_block_size=256
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼ã‚’åˆæœŸåŒ–é–‹å§‹ num_blocks=875 block_size=256
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=0
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=0
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=1
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=1
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=2
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=2
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=3
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=3
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=4
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=4
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=5
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=5
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=6
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=6
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=7
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=7
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=8
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=8
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=9
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=9
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=10
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=10
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=11
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=11
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=12
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=12
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=13
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=13
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=14
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=14
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=15
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=15
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=16
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=16
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=17
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=17
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=18
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=18
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=19
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=19
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=20
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=20
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=21
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=21
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=22
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=22
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=23
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=23
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=24
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=24
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=25
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=25
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=26
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=26
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=27
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=27
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=28
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=28
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=29
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=29
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=30
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=30
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=31
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=31
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=32
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=32
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=33
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=33
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=34
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=34
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=35
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=35
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=36
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=36
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=37
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=37
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=38
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=38
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=39
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=39
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=40
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=40
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=41
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=41
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=42
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=42
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=43
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=43
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=44
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=44
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=45
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=45
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=46
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=46
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=47
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=47
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=48
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=48
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=49
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=49
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=50
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=50
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=51
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=51
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=52
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=52
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=53
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=53
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=54
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=54
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=55
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=55
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=56
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=56
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=57
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=57
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=58
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=58
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=59
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=59
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=60
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=60
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=61
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=61
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=62
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=62
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=63
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=63
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=64
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=64
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=65
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=65
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=66
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=66
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=67
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=67
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=68
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=68
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=69
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=69
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=70
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=70
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=71
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=71
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=72
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=72
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=73
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=73
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=74
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=74
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=75
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=75
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=76
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=76
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=77
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=77
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=78
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=78
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=79
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=79
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=80
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=80
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=81
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=81
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=82
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=82
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=83
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=83
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=84
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=84
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=85
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=85
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=86
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=86
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=87
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=87
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=88
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=88
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=89
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=89
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=90
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=90
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=91
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=91
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=92
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=92
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=93
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=93
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=94
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=94
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=95
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=95
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=96
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=96
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=97
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=97
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=98
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=98
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=99
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=99
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=100
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=100
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=101
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=101
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=102
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=102
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=103
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=103
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=104
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=104
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=105
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=105
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=106
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=106
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=107
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=107
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=108
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=108
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=109
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=109
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=110
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=110
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=111
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=111
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=112
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=112
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=113
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=113
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=114
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=114
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=115
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=115
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=116
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=116
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=117
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=117
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=118
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=118
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=119
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=119
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=120
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=120
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=121
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=121
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=122
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=122
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=123
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=123
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=124
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=124
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=125
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=125
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=126
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=126
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=127
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=127
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=128
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=128
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=129
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=129
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=130
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=130
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=131
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=131
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=132
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=132
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=133
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=133
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=134
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=134
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=135
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=135
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=136
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=136
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=137
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=137
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=138
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=138
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=139
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=139
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=140
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=140
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=141
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=141
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=142
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=142
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=143
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=143
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=144
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=144
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=145
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=145
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=146
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=146
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=147
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=147
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=148
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=148
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=149
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=149
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=150
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=150
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=151
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=151
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=152
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=152
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=153
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=153
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=154
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=154
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=155
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=155
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=156
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=156
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=157
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=157
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=158
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=158
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=159
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=159
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=160
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=160
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=161
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=161
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=162
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=162
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=163
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=163
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=164
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=164
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=165
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=165
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=166
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=166
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=167
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=167
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=168
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=168
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=169
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=169
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=170
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=170
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=171
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=171
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=172
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=172
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=173
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=173
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=174
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=174
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=175
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=175
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=176
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=176
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=177
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=177
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=178
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=178
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=179
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=179
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=180
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=180
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=181
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=181
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=182
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=182
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=183
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=183
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=184
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=184
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=185
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=185
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=186
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=186
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=187
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=187
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=188
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=188
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=189
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=189
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=190
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=190
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=191
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=191
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=192
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=192
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=193
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=193
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=194
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=194
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=195
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=195
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=196
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=196
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=197
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=197
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=198
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=198
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=199
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=199
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=200
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=200
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=201
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=201
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=202
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=202
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=203
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=203
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=204
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=204
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=205
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=205
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=206
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=206
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=207
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=207
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=208
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=208
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=209
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=209
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=210
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=210
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=211
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=211
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=212
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=212
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=213
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=213
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=214
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=214
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=215
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=215
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=216
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=216
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=217
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=217
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=218
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=218
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=219
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=219
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=220
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=220
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=221
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=221
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=222
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=222
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=223
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=223
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=224
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=224
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=225
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=225
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=226
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=226
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=227
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=227
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=228
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=228
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=229
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=229
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=230
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=230
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=231
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=231
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=232
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=232
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=233
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=233
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=234
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=234
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=235
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=235
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=236
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=236
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=237
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=237
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=238
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=238
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=239
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=239
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=240
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=240
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=241
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=241
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=242
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=242
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=243
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=243
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=244
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=244
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=245
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=245
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=246
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=246
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=247
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=247
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=248
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=248
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=249
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=249
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=250
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=250
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=251
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=251
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=252
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=252
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=253
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=253
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=254
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=254
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=255
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=255
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=256
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=256
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=257
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=257
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=258
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=258
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=259
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=259
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=260
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=260
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=261
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=261
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=262
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=262
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=263
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=263
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=264
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=264
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=265
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=265
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=266
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=266
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=267
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=267
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=268
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=268
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=269
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=269
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=270
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=270
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=271
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=271
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=272
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=272
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=273
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=273
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=274
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=274
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=275
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=275
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=276
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=276
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=277
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=277
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=278
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=278
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=279
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=279
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=280
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=280
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=281
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=281
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=282
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=282
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=283
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=283
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=284
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=284
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=285
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=285
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=286
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=286
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=287
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=287
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=288
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=288
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=289
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=289
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=290
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=290
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=291
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=291
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=292
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=292
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=293
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=293
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=294
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=294
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=295
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=295
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=296
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=296
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=297
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=297
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=298
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=298
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=299
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=299
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=300
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=300
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=301
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=301
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=302
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=302
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=303
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=303
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=304
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=304
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=305
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=305
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=306
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=306
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=307
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=307
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=308
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=308
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=309
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=309
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=310
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=310
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=311
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=311
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=312
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=312
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=313
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=313
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=314
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=314
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=315
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=315
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=316
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=316
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=317
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=317
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=318
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=318
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=319
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=319
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=320
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=320
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=321
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=321
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=322
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=322
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=323
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=323
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=324
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=324
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=325
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=325
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=326
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=326
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=327
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=327
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=328
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=328
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=329
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=329
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=330
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=330
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=331
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=331
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=332
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=332
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=333
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=333
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=334
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=334
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=335
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=335
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=336
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=336
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=337
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=337
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=338
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=338
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=339
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=339
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=340
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=340
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=341
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=341
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=342
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=342
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=343
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=343
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=344
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=344
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=345
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=345
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=346
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=346
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=347
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=347
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=348
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=348
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=349
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=349
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=350
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=350
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=351
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=351
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=352
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=352
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=353
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=353
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=354
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=354
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=355
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=355
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=356
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=356
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=357
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=357
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=358
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=358
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=359
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=359
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=360
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=360
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=361
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=361
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=362
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=362
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=363
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=363
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=364
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=364
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=365
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=365
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=366
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=366
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=367
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=367
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=368
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=368
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=369
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=369
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=370
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=370
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=371
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=371
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=372
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=372
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=373
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=373
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=374
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=374
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=375
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=375
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=376
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=376
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=377
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=377
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=378
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=378
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=379
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=379
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=380
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=380
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=381
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=381
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=382
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=382
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=383
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=383
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=384
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=384
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=385
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=385
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=386
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=386
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=387
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=387
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=388
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=388
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=389
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=389
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=390
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=390
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=391
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=391
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=392
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=392
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=393
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=393
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=394
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=394
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=395
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=395
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=396
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=396
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=397
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=397
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=398
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=398
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=399
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=399
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=400
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=400
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=401
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=401
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=402
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=402
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=403
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=403
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=404
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=404
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=405
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=405
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=406
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=406
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=407
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=407
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=408
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=408
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=409
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=409
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=410
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=410
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=411
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=411
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=412
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=412
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=413
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=413
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=414
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=414
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=415
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=415
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=416
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=416
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=417
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=417
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=418
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=418
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=419
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=419
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=420
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=420
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=421
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=421
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=422
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=422
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=423
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=423
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=424
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=424
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=425
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=425
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=426
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=426
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=427
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=427
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=428
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=428
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=429
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=429
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=430
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=430
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=431
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=431
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=432
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=432
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=433
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=433
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=434
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=434
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=435
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=435
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=436
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=436
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=437
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=437
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=438
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=438
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=439
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=439
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=440
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=440
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=441
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=441
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=442
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=442
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=443
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=443
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=444
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=444
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=445
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=445
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=446
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=446
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=447
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=447
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=448
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=448
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=449
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=449
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=450
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=450
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=451
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=451
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=452
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=452
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=453
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=453
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=454
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=454
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=455
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=455
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=456
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=456
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=457
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=457
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=458
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=458
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=459
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=459
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=460
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=460
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=461
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=461
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=462
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=462
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=463
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=463
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=464
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=464
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=465
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=465
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=466
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=466
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=467
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=467
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=468
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=468
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=469
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=469
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=470
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=470
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=471
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=471
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=472
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=472
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=473
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=473
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=474
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=474
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=475
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=475
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=476
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=476
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=477
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=477
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=478
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=478
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=479
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=479
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=480
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=480
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=481
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=481
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=482
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=482
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=483
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=483
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=484
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=484
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=485
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=485
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=486
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=486
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=487
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=487
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=488
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=488
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=489
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=489
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=490
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=490
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=491
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=491
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=492
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=492
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=493
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=493
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=494
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=494
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=495
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=495
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=496
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=496
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=497
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=497
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=498
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=498
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=499
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=499
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=500
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=500
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=501
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=501
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=502
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=502
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=503
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=503
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=504
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=504
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=505
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=505
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=506
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=506
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=507
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=507
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=508
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=508
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=509
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=509
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=510
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=510
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=511
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=511
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=512
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=512
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=513
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=513
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=514
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=514
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=515
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=515
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=516
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=516
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=517
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=517
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=518
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=518
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=519
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=519
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=520
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=520
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=521
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=521
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=522
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=522
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=523
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=523
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=524
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=524
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=525
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=525
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=526
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=526
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=527
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=527
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=528
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=528
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=529
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=529
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=530
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=530
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=531
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=531
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=532
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=532
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=533
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=533
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=534
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=534
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=535
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=535
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=536
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=536
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=537
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=537
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=538
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=538
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=539
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=539
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=540
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=540
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=541
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=541
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=542
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=542
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=543
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=543
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=544
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=544
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=545
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=545
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=546
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=546
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=547
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=547
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=548
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=548
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=549
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=549
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=550
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=550
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=551
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=551
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=552
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=552
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=553
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=553
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=554
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=554
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=555
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=555
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=556
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=556
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=557
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=557
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=558
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=558
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=559
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=559
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=560
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=560
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=561
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=561
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=562
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=562
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=563
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=563
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=564
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=564
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=565
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=565
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=566
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=566
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=567
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=567
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=568
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=568
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=569
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=569
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=570
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=570
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=571
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=571
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=572
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=572
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=573
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=573
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=574
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=574
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=575
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=575
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=576
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=576
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=577
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=577
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=578
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=578
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=579
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=579
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=580
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=580
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=581
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=581
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=582
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=582
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=583
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=583
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=584
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=584
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=585
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=585
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=586
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=586
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=587
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=587
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=588
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=588
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=589
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=589
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=590
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=590
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=591
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=591
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=592
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=592
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=593
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=593
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=594
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=594
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=595
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=595
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=596
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=596
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=597
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=597
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=598
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=598
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=599
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=599
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=600
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=600
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=601
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=601
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=602
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=602
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=603
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=603
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=604
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=604
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=605
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=605
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=606
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=606
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=607
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=607
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=608
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=608
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=609
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=609
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=610
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=610
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=611
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=611
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=612
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=612
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=613
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=613
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=614
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=614
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=615
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=615
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=616
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=616
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=617
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=617
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=618
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=618
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=619
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=619
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=620
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=620
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=621
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=621
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=622
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=622
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=623
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=623
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=624
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=624
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=625
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=625
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=626
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=626
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=627
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=627
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=628
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=628
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=629
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=629
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=630
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=630
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=631
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=631
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=632
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=632
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=633
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=633
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=634
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=634
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=635
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=635
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=636
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=636
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=637
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=637
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=638
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=638
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=639
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=639
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=640
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=640
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=641
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=641
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=642
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=642
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=643
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=643
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=644
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=644
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=645
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=645
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=646
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=646
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=647
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=647
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=648
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=648
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=649
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=649
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=650
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=650
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=651
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=651
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=652
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=652
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=653
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=653
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=654
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=654
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=655
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=655
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=656
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=656
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=657
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=657
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=658
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=658
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=659
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=659
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=660
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=660
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=661
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=661
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=662
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=662
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=663
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=663
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=664
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=664
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=665
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=665
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=666
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=666
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=667
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=667
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=668
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=668
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=669
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=669
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=670
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=670
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=671
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=671
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=672
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=672
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=673
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=673
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=674
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=674
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=675
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=675
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=676
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=676
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=677
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=677
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=678
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=678
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=679
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=679
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=680
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=680
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=681
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=681
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=682
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=682
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=683
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=683
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=684
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=684
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=685
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=685
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=686
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=686
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=687
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=687
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=688
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=688
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=689
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=689
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=690
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=690
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=691
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=691
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=692
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=692
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=693
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=693
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=694
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=694
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=695
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=695
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=696
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=696
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=697
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=697
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=698
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=698
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=699
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=699
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=700
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=700
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=701
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=701
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=702
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=702
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=703
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=703
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=704
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=704
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=705
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=705
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=706
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=706
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=707
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=707
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=708
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=708
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=709
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=709
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=710
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=710
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=711
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=711
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=712
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=712
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=713
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=713
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=714
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=714
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=715
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=715
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=716
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=716
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=717
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=717
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=718
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=718
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=719
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=719
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=720
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=720
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=721
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=721
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=722
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=722
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=723
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=723
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=724
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=724
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=725
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=725
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=726
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=726
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=727
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=727
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=728
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=728
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=729
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=729
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=730
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=730
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=731
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=731
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=732
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=732
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=733
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=733
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=734
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=734
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=735
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=735
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=736
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=736
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=737
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=737
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=738
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=738
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=739
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=739
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=740
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=740
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=741
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=741
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=742
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=742
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=743
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=743
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=744
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=744
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=745
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=745
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=746
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=746
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=747
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=747
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=748
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=748
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=749
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=749
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=750
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=750
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=751
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=751
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=752
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=752
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=753
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=753
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=754
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=754
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=755
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=755
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=756
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=756
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=757
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=757
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=758
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=758
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=759
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=759
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=760
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=760
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=761
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=761
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=762
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=762
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=763
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=763
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=764
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=764
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=765
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=765
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=766
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=766
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=767
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=767
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=768
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=768
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=769
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=769
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=770
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=770
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=771
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=771
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=772
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=772
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=773
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=773
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=774
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=774
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=775
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=775
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=776
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=776
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=777
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=777
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=778
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=778
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=779
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=779
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=780
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=780
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=781
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=781
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=782
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=782
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=783
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=783
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=784
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=784
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=785
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=785
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=786
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=786
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=787
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=787
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=788
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=788
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=789
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=789
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=790
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=790
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=791
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=791
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=792
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=792
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=793
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=793
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=794
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=794
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=795
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=795
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=796
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=796
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=797
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=797
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=798
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=798
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=799
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=799
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=800
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=800
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=801
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=801
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=802
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=802
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=803
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=803
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=804
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=804
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=805
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=805
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=806
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=806
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=807
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=807
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=808
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=808
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=809
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=809
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=810
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=810
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=811
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=811
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=812
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=812
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=813
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=813
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=814
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=814
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=815
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=815
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=816
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=816
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=817
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=817
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=818
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=818
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=819
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=819
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=820
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=820
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=821
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=821
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=822
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=822
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=823
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=823
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=824
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=824
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=825
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=825
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=826
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=826
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=827
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=827
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=828
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=828
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=829
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=829
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=830
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=830
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=831
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=831
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=832
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=832
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=833
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=833
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=834
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=834
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=835
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=835
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=836
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=836
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=837
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=837
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=838
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=838
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=839
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=839
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=840
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=840
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=841
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=841
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=842
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=842
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=843
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=843
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=844
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=844
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=845
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=845
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=846
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=846
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=847
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=847
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=848
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=848
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=849
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=849
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=850
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=850
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=851
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=851
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=852
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=852
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=853
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=853
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=854
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=854
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=855
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=855
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=856
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=856
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=857
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=857
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=858
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=858
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=859
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=859
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=860
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=860
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=861
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=861
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=862
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=862
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=863
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=863
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=864
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=864
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=865
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=865
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=866
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=866
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=867
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=867
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=868
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=868
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=869
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=869
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=870
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=870
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=871
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=871
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=872
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=872
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=873
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=873
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–é–‹å§‹ block_id=874
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–å®Œäº† self.block_id=874
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼ã‚’åˆæœŸåŒ–å®Œäº†
ğŸŸ© ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ã‚’åˆæœŸåŒ–å®Œäº†
ğŸŸ© ã‚¨ãƒ³ã‚¸ãƒ³ã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© SamplingParamsåˆæœŸåŒ–ã®å¾Œå‡¦ç† self=SamplingParams(temperature=0.6, max_tokens=1, ignore_eos=False)
ğŸŸ© ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆã‚’é–‹å§‹ len(prompts)=1 sampling_params=SamplingParams(temperature=0.6, max_tokens=1, ignore_eos=False)
ğŸŸ© ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’ã‚­ãƒ¥ãƒ¼ã«ç™»éŒ²é–‹å§‹ prompt='Hello' sampling_params=SamplingParams(temperature=0.6, max_tokens=1, ignore_eos=False)
ğŸŸ© ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã‚’åˆæœŸåŒ– len(token_ids)=1, sampling_params=SamplingParams(temperature=0.6, max_tokens=1, ignore_eos=False)
ğŸŸ© ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã‚’åˆæœŸåŒ–å®Œäº† self.seq_id=4
ğŸŸ© ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ã«æ–°ã—ã„ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’è¿½åŠ  seq.seq_id=4
ğŸŸ© ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’ã‚­ãƒ¥ãƒ¼ã«ç™»éŒ²å®Œäº† seq.seq_id=4
ğŸŸ© ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ã®å®Œäº†çŠ¶æ…‹ã‚’ç¢ºèª result=False
ğŸŸ© ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ã«å‡¦ç†ã™ã¹ãã‚¿ã‚¹ã‚¯ãŒæ®‹ã£ã¦ã„ã‚‹ã‹ç¢ºèª res=False
ğŸŸ© æ¨è«–ã‚¨ãƒ³ã‚¸ãƒ³ã®1ã‚µã‚¤ã‚¯ãƒ«ã‚’å®Ÿè¡Œé–‹å§‹
ğŸŸ© ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒªãƒ³ã‚°ã‚’é–‹å§‹
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’å‰²ã‚Šå½“ã¦å¯èƒ½ã‹ç¢ºèª 4 seq.num_blocks=1 result=True
ğŸŸ© ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã«ãƒ–ãƒ­ãƒƒã‚¯ã‚’å‰²ã‚Šå½“ã¦é–‹å§‹ 4
ğŸŸ© è«–ç†ãƒ–ãƒ­ãƒƒã‚¯ã‚’å–å¾— i=0
ğŸŸ© è«–ç†ãƒ–ãƒ­ãƒƒã‚¯ã‚’å–å¾—å®Œäº† i=0 result=[9707]
ğŸŸ© ç©ºããƒ–ãƒ­ãƒƒã‚¯ã‚’å‰²ã‚Šå½“ã¦é–‹å§‹ block_id=0
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’ãƒªã‚»ãƒƒãƒˆ 0
ğŸŸ© ç©ºããƒ–ãƒ­ãƒƒã‚¯ã‚’å‰²ã‚Šå½“ã¦å®Œäº† block_id=0
ğŸŸ© ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã«ãƒ–ãƒ­ãƒƒã‚¯ã‚’å‰²ã‚Šå½“ã¦å®Œäº† 4 seq.block_table=[0]
ğŸŸ© ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒªãƒ³ã‚°å®Œäº†ï¼ˆPrefillï¼‰ len(scheduled_seqs)=1
ğŸŸ© æ¨è«–ã®1ã‚µã‚¤ã‚¯ãƒ«ã‚’å®Ÿè¡Œé–‹å§‹ len(seqs)=1 is_prefill=True
ğŸŸ© ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’è¨­å®š is_prefill=True cu_seqlens_q=tensor([0, 1], device='cuda:0', dtype=torch.int32) cu_seqlens_k=tensor([0, 1], device='cuda:0', dtype=torch.int32) max_seqlen_q=1 max_seqlen_k=1 slot_mapping=tensor([0], device='cuda:0', dtype=torch.int32) context_lens=None block_tables=None
ğŸŸ© ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æº–å‚™é–‹å§‹ len(seqs)=1
ğŸŸ© ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æº–å‚™å®Œäº† temperatures=tensor([0.6000], device='cuda:0')
ğŸŸ© ãƒ¢ãƒ‡ãƒ«ã®é †ä¼æ¬ã‚’å®Ÿè¡Œé–‹å§‹ input_ids.shape=torch.Size([1]) positions.shape=torch.Size([1]) is_prefill=True
ğŸŸ© Qwen3ForCausalLMã®é †ä¼æ’­é–‹å§‹ input_ids.shape=torch.Size([1]) positions.shape=torch.Size([1])
ğŸŸ© Qwen3Modelã®é †ä¼æ’­é–‹å§‹ input_ids.shape=torch.Size([1]) positions.shape=torch.Size([1])
ğŸŸ© åŸ‹ã‚è¾¼ã¿å±¤ã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1])
ğŸŸ© åŸ‹ã‚è¾¼ã¿å±¤ã®é †ä¼æ’­å®Œäº† y.shape=torch.Size([1, 1024])
ğŸŸ© Qwen3DecoderLayerã®é †ä¼æ’­ positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 1024]) residual is None=True
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 1024]), residual.shape if residual is not None else None=None
ğŸŸ© æ­£è¦åŒ–é–‹å§‹ x.shape=torch.Size([1, 1024])
ğŸŸ© æ­£è¦åŒ–å®Œäº† x.shape=torch.Size([1, 1024])
ğŸŸ© Qwen3Attentionã®é †ä¼æ’­ positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­å®Œäº† result.shape=torch.Size([1, 4096])
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 16, 128]), residual.shape if residual is not None else None=None
ğŸŸ© æ­£è¦åŒ–é–‹å§‹ x.shape=torch.Size([1, 16, 128])
ğŸŸ© æ­£è¦åŒ–å®Œäº† x.shape=torch.Size([1, 16, 128])
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 8, 128]), residual.shape if residual is not None else None=None
ğŸŸ© æ­£è¦åŒ–é–‹å§‹ x.shape=torch.Size([1, 8, 128])
ğŸŸ© æ­£è¦åŒ–å®Œäº† x.shape=torch.Size([1, 8, 128])
ğŸŸ© RoPEã®é †ä¼æ’­é–‹å§‹ positions.shape=torch.Size([1]), query.shape=torch.Size([1, 16, 128]), key.shape=torch.Size([1, 8, 128])
ğŸŸ© RoPEã‚’é©ç”¨é–‹å§‹ x.shape=torch.Size([1, 16, 128]), cos.shape=torch.Size([1, 1, 64]), sin.shape=torch.Size([1, 1, 64])
ğŸŸ¦ ãƒãƒ£ãƒ³ã‚¯åŒ– x1.shape=torch.Size([1, 16, 64]), x2.shape=torch.Size([1, 16, 64])
ğŸŸ¦ å›è»¢è¡Œåˆ—é©ç”¨ y1.shape=torch.Size([1, 16, 64]), y2.shape=torch.Size([1, 16, 64])
ğŸŸ© RoPEã‚’é©ç”¨å®Œäº† result.shape=torch.Size([1, 16, 128])
ğŸŸ© RoPEã‚’é©ç”¨é–‹å§‹ x.shape=torch.Size([1, 8, 128]), cos.shape=torch.Size([1, 1, 64]), sin.shape=torch.Size([1, 1, 64])
ğŸŸ¦ ãƒãƒ£ãƒ³ã‚¯åŒ– x1.shape=torch.Size([1, 8, 64]), x2.shape=torch.Size([1, 8, 64])
ğŸŸ¦ å›è»¢è¡Œåˆ—é©ç”¨ y1.shape=torch.Size([1, 8, 64]), y2.shape=torch.Size([1, 8, 64])
ğŸŸ© RoPEã‚’é©ç”¨å®Œäº† result.shape=torch.Size([1, 8, 128])
ğŸŸ© RoPEã®é †ä¼æ’­å®Œäº† query.shape=torch.Size([1, 16, 128]), key.shape=torch.Size([1, 8, 128])
ğŸŸ© Attentionå±¤ã®é †ä¼æ’­é–‹å§‹ q.shape=torch.Size([1, 16, 128]), k.shape=torch.Size([1, 8, 128]), v.shape=torch.Size([1, 8, 128])
ğŸŸ© ç¾åœ¨ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆå–å¾— _CONTEXT=Context(is_prefill=True, cu_seqlens_q=tensor([0, 1], device='cuda:0', dtype=torch.int32), cu_seqlens_k=tensor([0, 1], device='cuda:0', dtype=torch.int32), max_seqlen_q=1, max_seqlen_k=1, slot_mapping=tensor([0], device='cuda:0', dtype=torch.int32), context_lens=None, block_tables=None)
ğŸŸ¦ ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæƒ…å ±å–å¾— context.is_prefill=True, context.slot_mapping=tensor([0], device='cuda:0', dtype=torch.int32), context.max_seqlen_q=1, context.cu_seqlens_q=tensor([0, 1], device='cuda:0', dtype=torch.int32), context.max_seqlen_k=1, context.cu_seqlens_k=tensor([0, 1], device='cuda:0', dtype=torch.int32), context.context_lens=None, context.block_tables=None
ğŸŸ© KVã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ã‚­ãƒ¼ã¨ãƒãƒªãƒ¥ãƒ¼ã‚’ä¿å­˜é–‹å§‹ key.shape=torch.Size([1, 8, 128]), value.shape=torch.Size([1, 8, 128]), k_cache.shape=torch.Size([875, 256, 8, 128]), v_cache.shape=torch.Size([875, 256, 8, 128]), slot_mapping.shape=torch.Size([1])
ğŸŸ© KVã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ã‚­ãƒ¼ã¨ãƒãƒªãƒ¥ãƒ¼ã‚’ä¿å­˜å®Œäº†
ğŸŸ¦ Prefillãƒ¢ãƒ¼ãƒ‰ã§FlashAttentionã‚’å®Ÿè¡Œ context.max_seqlen_q=1, context.cu_seqlens_q=tensor([0, 1], device='cuda:0', dtype=torch.int32), context.max_seqlen_k=1, context.cu_seqlens_k=tensor([0, 1], device='cuda:0', dtype=torch.int32)
ğŸŸ© Attentionå±¤ã®é †ä¼æ’­å®Œäº† o.shape=torch.Size([1, 16, 128])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­é–‹å§‹ å…¥åŠ›å½¢çŠ¶: torch.Size([1, 2048])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­å®Œäº† å‡ºåŠ›å½¢çŠ¶: torch.Size([1, 1024])
ğŸŸ© Qwen3Attentionã®é †ä¼æ’­å®Œäº† output.shape=torch.Size([1, 1024])
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 1024]), residual.shape if residual is not None else None=torch.Size([1, 1024])
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨å®Œäº†
ğŸŸ© Qwen3MLPã®é †ä¼æ’­ x.shape=torch.Size([1, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­å®Œäº† result.shape=torch.Size([1, 6144])
ğŸŸ© SwiGLUã‚’é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 6144])
ğŸŸ¦ ãƒãƒ£ãƒ³ã‚¯åŒ– x.shape=torch.Size([1, 3072]), y.shape=torch.Size([1, 3072])
ğŸŸ© SwiGLUã‚’é †ä¼æ’­å®Œäº† result.shape=torch.Size([1, 3072])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­é–‹å§‹ å…¥åŠ›å½¢çŠ¶: torch.Size([1, 3072])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­å®Œäº† å‡ºåŠ›å½¢çŠ¶: torch.Size([1, 1024])
ğŸŸ© Qwen3MLPã®é †ä¼æ’­å®Œäº† x.shape=torch.Size([1, 1024])
ğŸŸ© Qwen3DecoderLayerã®é †ä¼æ’­å®Œäº† hidden_states.shape=torch.Size([1, 1024])
ğŸŸ© Qwen3DecoderLayerã®é †ä¼æ’­ positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 1024]) residual is None=False
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 1024]), residual.shape if residual is not None else None=torch.Size([1, 1024])
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨å®Œäº†
ğŸŸ© Qwen3Attentionã®é †ä¼æ’­ positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­å®Œäº† result.shape=torch.Size([1, 4096])
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 16, 128]), residual.shape if residual is not None else None=None
ğŸŸ© æ­£è¦åŒ–é–‹å§‹ x.shape=torch.Size([1, 16, 128])
ğŸŸ© æ­£è¦åŒ–å®Œäº† x.shape=torch.Size([1, 16, 128])
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 8, 128]), residual.shape if residual is not None else None=None
ğŸŸ© æ­£è¦åŒ–é–‹å§‹ x.shape=torch.Size([1, 8, 128])
ğŸŸ© æ­£è¦åŒ–å®Œäº† x.shape=torch.Size([1, 8, 128])
ğŸŸ© RoPEã®é †ä¼æ’­é–‹å§‹ positions.shape=torch.Size([1]), query.shape=torch.Size([1, 16, 128]), key.shape=torch.Size([1, 8, 128])
ğŸŸ© RoPEã‚’é©ç”¨é–‹å§‹ x.shape=torch.Size([1, 16, 128]), cos.shape=torch.Size([1, 1, 64]), sin.shape=torch.Size([1, 1, 64])
ğŸŸ¦ ãƒãƒ£ãƒ³ã‚¯åŒ– x1.shape=torch.Size([1, 16, 64]), x2.shape=torch.Size([1, 16, 64])
ğŸŸ¦ å›è»¢è¡Œåˆ—é©ç”¨ y1.shape=torch.Size([1, 16, 64]), y2.shape=torch.Size([1, 16, 64])
ğŸŸ© RoPEã‚’é©ç”¨å®Œäº† result.shape=torch.Size([1, 16, 128])
ğŸŸ© RoPEã‚’é©ç”¨é–‹å§‹ x.shape=torch.Size([1, 8, 128]), cos.shape=torch.Size([1, 1, 64]), sin.shape=torch.Size([1, 1, 64])
ğŸŸ¦ ãƒãƒ£ãƒ³ã‚¯åŒ– x1.shape=torch.Size([1, 8, 64]), x2.shape=torch.Size([1, 8, 64])
ğŸŸ¦ å›è»¢è¡Œåˆ—é©ç”¨ y1.shape=torch.Size([1, 8, 64]), y2.shape=torch.Size([1, 8, 64])
ğŸŸ© RoPEã‚’é©ç”¨å®Œäº† result.shape=torch.Size([1, 8, 128])
ğŸŸ© RoPEã®é †ä¼æ’­å®Œäº† query.shape=torch.Size([1, 16, 128]), key.shape=torch.Size([1, 8, 128])
ğŸŸ© Attentionå±¤ã®é †ä¼æ’­é–‹å§‹ q.shape=torch.Size([1, 16, 128]), k.shape=torch.Size([1, 8, 128]), v.shape=torch.Size([1, 8, 128])
ğŸŸ© ç¾åœ¨ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆå–å¾— _CONTEXT=Context(is_prefill=True, cu_seqlens_q=tensor([0, 1], device='cuda:0', dtype=torch.int32), cu_seqlens_k=tensor([0, 1], device='cuda:0', dtype=torch.int32), max_seqlen_q=1, max_seqlen_k=1, slot_mapping=tensor([0], device='cuda:0', dtype=torch.int32), context_lens=None, block_tables=None)
ğŸŸ¦ ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæƒ…å ±å–å¾— context.is_prefill=True, context.slot_mapping=tensor([0], device='cuda:0', dtype=torch.int32), context.max_seqlen_q=1, context.cu_seqlens_q=tensor([0, 1], device='cuda:0', dtype=torch.int32), context.max_seqlen_k=1, context.cu_seqlens_k=tensor([0, 1], device='cuda:0', dtype=torch.int32), context.context_lens=None, context.block_tables=None
ğŸŸ© KVã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ã‚­ãƒ¼ã¨ãƒãƒªãƒ¥ãƒ¼ã‚’ä¿å­˜é–‹å§‹ key.shape=torch.Size([1, 8, 128]), value.shape=torch.Size([1, 8, 128]), k_cache.shape=torch.Size([875, 256, 8, 128]), v_cache.shape=torch.Size([875, 256, 8, 128]), slot_mapping.shape=torch.Size([1])
ğŸŸ© KVã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ã‚­ãƒ¼ã¨ãƒãƒªãƒ¥ãƒ¼ã‚’ä¿å­˜å®Œäº†
ğŸŸ¦ Prefillãƒ¢ãƒ¼ãƒ‰ã§FlashAttentionã‚’å®Ÿè¡Œ context.max_seqlen_q=1, context.cu_seqlens_q=tensor([0, 1], device='cuda:0', dtype=torch.int32), context.max_seqlen_k=1, context.cu_seqlens_k=tensor([0, 1], device='cuda:0', dtype=torch.int32)
ğŸŸ© Attentionå±¤ã®é †ä¼æ’­å®Œäº† o.shape=torch.Size([1, 16, 128])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­é–‹å§‹ å…¥åŠ›å½¢çŠ¶: torch.Size([1, 2048])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­å®Œäº† å‡ºåŠ›å½¢çŠ¶: torch.Size([1, 1024])
ğŸŸ© Qwen3Attentionã®é †ä¼æ’­å®Œäº† output.shape=torch.Size([1, 1024])
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 1024]), residual.shape if residual is not None else None=torch.Size([1, 1024])
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨å®Œäº†
ğŸŸ© Qwen3MLPã®é †ä¼æ’­ x.shape=torch.Size([1, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­å®Œäº† result.shape=torch.Size([1, 6144])
ğŸŸ© SwiGLUã‚’é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 6144])
ğŸŸ¦ ãƒãƒ£ãƒ³ã‚¯åŒ– x.shape=torch.Size([1, 3072]), y.shape=torch.Size([1, 3072])
ğŸŸ© SwiGLUã‚’é †ä¼æ’­å®Œäº† result.shape=torch.Size([1, 3072])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­é–‹å§‹ å…¥åŠ›å½¢çŠ¶: torch.Size([1, 3072])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­å®Œäº† å‡ºåŠ›å½¢çŠ¶: torch.Size([1, 1024])
ğŸŸ© Qwen3MLPã®é †ä¼æ’­å®Œäº† x.shape=torch.Size([1, 1024])
ğŸŸ© Qwen3DecoderLayerã®é †ä¼æ’­å®Œäº† hidden_states.shape=torch.Size([1, 1024])
ğŸŸ© Qwen3DecoderLayerã®é †ä¼æ’­ positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 1024]) residual is None=False
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 1024]), residual.shape if residual is not None else None=torch.Size([1, 1024])
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨å®Œäº†
ğŸŸ© Qwen3Attentionã®é †ä¼æ’­ positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­å®Œäº† result.shape=torch.Size([1, 4096])
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 16, 128]), residual.shape if residual is not None else None=None
ğŸŸ© æ­£è¦åŒ–é–‹å§‹ x.shape=torch.Size([1, 16, 128])
ğŸŸ© æ­£è¦åŒ–å®Œäº† x.shape=torch.Size([1, 16, 128])
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 8, 128]), residual.shape if residual is not None else None=None
ğŸŸ© æ­£è¦åŒ–é–‹å§‹ x.shape=torch.Size([1, 8, 128])
ğŸŸ© æ­£è¦åŒ–å®Œäº† x.shape=torch.Size([1, 8, 128])
ğŸŸ© RoPEã®é †ä¼æ’­é–‹å§‹ positions.shape=torch.Size([1]), query.shape=torch.Size([1, 16, 128]), key.shape=torch.Size([1, 8, 128])
ğŸŸ© RoPEã‚’é©ç”¨é–‹å§‹ x.shape=torch.Size([1, 16, 128]), cos.shape=torch.Size([1, 1, 64]), sin.shape=torch.Size([1, 1, 64])
ğŸŸ¦ ãƒãƒ£ãƒ³ã‚¯åŒ– x1.shape=torch.Size([1, 16, 64]), x2.shape=torch.Size([1, 16, 64])
ğŸŸ¦ å›è»¢è¡Œåˆ—é©ç”¨ y1.shape=torch.Size([1, 16, 64]), y2.shape=torch.Size([1, 16, 64])
ğŸŸ© RoPEã‚’é©ç”¨å®Œäº† result.shape=torch.Size([1, 16, 128])
ğŸŸ© RoPEã‚’é©ç”¨é–‹å§‹ x.shape=torch.Size([1, 8, 128]), cos.shape=torch.Size([1, 1, 64]), sin.shape=torch.Size([1, 1, 64])
ğŸŸ¦ ãƒãƒ£ãƒ³ã‚¯åŒ– x1.shape=torch.Size([1, 8, 64]), x2.shape=torch.Size([1, 8, 64])
ğŸŸ¦ å›è»¢è¡Œåˆ—é©ç”¨ y1.shape=torch.Size([1, 8, 64]), y2.shape=torch.Size([1, 8, 64])
ğŸŸ© RoPEã‚’é©ç”¨å®Œäº† result.shape=torch.Size([1, 8, 128])
ğŸŸ© RoPEã®é †ä¼æ’­å®Œäº† query.shape=torch.Size([1, 16, 128]), key.shape=torch.Size([1, 8, 128])
ğŸŸ© Attentionå±¤ã®é †ä¼æ’­é–‹å§‹ q.shape=torch.Size([1, 16, 128]), k.shape=torch.Size([1, 8, 128]), v.shape=torch.Size([1, 8, 128])
ğŸŸ© ç¾åœ¨ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆå–å¾— _CONTEXT=Context(is_prefill=True, cu_seqlens_q=tensor([0, 1], device='cuda:0', dtype=torch.int32), cu_seqlens_k=tensor([0, 1], device='cuda:0', dtype=torch.int32), max_seqlen_q=1, max_seqlen_k=1, slot_mapping=tensor([0], device='cuda:0', dtype=torch.int32), context_lens=None, block_tables=None)
ğŸŸ¦ ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæƒ…å ±å–å¾— context.is_prefill=True, context.slot_mapping=tensor([0], device='cuda:0', dtype=torch.int32), context.max_seqlen_q=1, context.cu_seqlens_q=tensor([0, 1], device='cuda:0', dtype=torch.int32), context.max_seqlen_k=1, context.cu_seqlens_k=tensor([0, 1], device='cuda:0', dtype=torch.int32), context.context_lens=None, context.block_tables=None
ğŸŸ© KVã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ã‚­ãƒ¼ã¨ãƒãƒªãƒ¥ãƒ¼ã‚’ä¿å­˜é–‹å§‹ key.shape=torch.Size([1, 8, 128]), value.shape=torch.Size([1, 8, 128]), k_cache.shape=torch.Size([875, 256, 8, 128]), v_cache.shape=torch.Size([875, 256, 8, 128]), slot_mapping.shape=torch.Size([1])
ğŸŸ© KVã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ã‚­ãƒ¼ã¨ãƒãƒªãƒ¥ãƒ¼ã‚’ä¿å­˜å®Œäº†
ğŸŸ¦ Prefillãƒ¢ãƒ¼ãƒ‰ã§FlashAttentionã‚’å®Ÿè¡Œ context.max_seqlen_q=1, context.cu_seqlens_q=tensor([0, 1], device='cuda:0', dtype=torch.int32), context.max_seqlen_k=1, context.cu_seqlens_k=tensor([0, 1], device='cuda:0', dtype=torch.int32)
ğŸŸ© Attentionå±¤ã®é †ä¼æ’­å®Œäº† o.shape=torch.Size([1, 16, 128])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­é–‹å§‹ å…¥åŠ›å½¢çŠ¶: torch.Size([1, 2048])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­å®Œäº† å‡ºåŠ›å½¢çŠ¶: torch.Size([1, 1024])
ğŸŸ© Qwen3Attentionã®é †ä¼æ’­å®Œäº† output.shape=torch.Size([1, 1024])
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 1024]), residual.shape if residual is not None else None=torch.Size([1, 1024])
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨å®Œäº†
ğŸŸ© Qwen3MLPã®é †ä¼æ’­ x.shape=torch.Size([1, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­å®Œäº† result.shape=torch.Size([1, 6144])
ğŸŸ© SwiGLUã‚’é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 6144])
ğŸŸ¦ ãƒãƒ£ãƒ³ã‚¯åŒ– x.shape=torch.Size([1, 3072]), y.shape=torch.Size([1, 3072])
ğŸŸ© SwiGLUã‚’é †ä¼æ’­å®Œäº† result.shape=torch.Size([1, 3072])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­é–‹å§‹ å…¥åŠ›å½¢çŠ¶: torch.Size([1, 3072])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­å®Œäº† å‡ºåŠ›å½¢çŠ¶: torch.Size([1, 1024])
ğŸŸ© Qwen3MLPã®é †ä¼æ’­å®Œäº† x.shape=torch.Size([1, 1024])
ğŸŸ© Qwen3DecoderLayerã®é †ä¼æ’­å®Œäº† hidden_states.shape=torch.Size([1, 1024])
ğŸŸ© Qwen3DecoderLayerã®é †ä¼æ’­ positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 1024]) residual is None=False
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 1024]), residual.shape if residual is not None else None=torch.Size([1, 1024])
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨å®Œäº†
ğŸŸ© Qwen3Attentionã®é †ä¼æ’­ positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­å®Œäº† result.shape=torch.Size([1, 4096])
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 16, 128]), residual.shape if residual is not None else None=None
ğŸŸ© æ­£è¦åŒ–é–‹å§‹ x.shape=torch.Size([1, 16, 128])
ğŸŸ© æ­£è¦åŒ–å®Œäº† x.shape=torch.Size([1, 16, 128])
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 8, 128]), residual.shape if residual is not None else None=None
ğŸŸ© æ­£è¦åŒ–é–‹å§‹ x.shape=torch.Size([1, 8, 128])
ğŸŸ© æ­£è¦åŒ–å®Œäº† x.shape=torch.Size([1, 8, 128])
ğŸŸ© RoPEã®é †ä¼æ’­é–‹å§‹ positions.shape=torch.Size([1]), query.shape=torch.Size([1, 16, 128]), key.shape=torch.Size([1, 8, 128])
ğŸŸ© RoPEã‚’é©ç”¨é–‹å§‹ x.shape=torch.Size([1, 16, 128]), cos.shape=torch.Size([1, 1, 64]), sin.shape=torch.Size([1, 1, 64])
ğŸŸ¦ ãƒãƒ£ãƒ³ã‚¯åŒ– x1.shape=torch.Size([1, 16, 64]), x2.shape=torch.Size([1, 16, 64])
ğŸŸ¦ å›è»¢è¡Œåˆ—é©ç”¨ y1.shape=torch.Size([1, 16, 64]), y2.shape=torch.Size([1, 16, 64])
ğŸŸ© RoPEã‚’é©ç”¨å®Œäº† result.shape=torch.Size([1, 16, 128])
ğŸŸ© RoPEã‚’é©ç”¨é–‹å§‹ x.shape=torch.Size([1, 8, 128]), cos.shape=torch.Size([1, 1, 64]), sin.shape=torch.Size([1, 1, 64])
ğŸŸ¦ ãƒãƒ£ãƒ³ã‚¯åŒ– x1.shape=torch.Size([1, 8, 64]), x2.shape=torch.Size([1, 8, 64])
ğŸŸ¦ å›è»¢è¡Œåˆ—é©ç”¨ y1.shape=torch.Size([1, 8, 64]), y2.shape=torch.Size([1, 8, 64])
ğŸŸ© RoPEã‚’é©ç”¨å®Œäº† result.shape=torch.Size([1, 8, 128])
ğŸŸ© RoPEã®é †ä¼æ’­å®Œäº† query.shape=torch.Size([1, 16, 128]), key.shape=torch.Size([1, 8, 128])
ğŸŸ© Attentionå±¤ã®é †ä¼æ’­é–‹å§‹ q.shape=torch.Size([1, 16, 128]), k.shape=torch.Size([1, 8, 128]), v.shape=torch.Size([1, 8, 128])
ğŸŸ© ç¾åœ¨ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆå–å¾— _CONTEXT=Context(is_prefill=True, cu_seqlens_q=tensor([0, 1], device='cuda:0', dtype=torch.int32), cu_seqlens_k=tensor([0, 1], device='cuda:0', dtype=torch.int32), max_seqlen_q=1, max_seqlen_k=1, slot_mapping=tensor([0], device='cuda:0', dtype=torch.int32), context_lens=None, block_tables=None)
ğŸŸ¦ ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæƒ…å ±å–å¾— context.is_prefill=True, context.slot_mapping=tensor([0], device='cuda:0', dtype=torch.int32), context.max_seqlen_q=1, context.cu_seqlens_q=tensor([0, 1], device='cuda:0', dtype=torch.int32), context.max_seqlen_k=1, context.cu_seqlens_k=tensor([0, 1], device='cuda:0', dtype=torch.int32), context.context_lens=None, context.block_tables=None
ğŸŸ© KVã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ã‚­ãƒ¼ã¨ãƒãƒªãƒ¥ãƒ¼ã‚’ä¿å­˜é–‹å§‹ key.shape=torch.Size([1, 8, 128]), value.shape=torch.Size([1, 8, 128]), k_cache.shape=torch.Size([875, 256, 8, 128]), v_cache.shape=torch.Size([875, 256, 8, 128]), slot_mapping.shape=torch.Size([1])
ğŸŸ© KVã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ã‚­ãƒ¼ã¨ãƒãƒªãƒ¥ãƒ¼ã‚’ä¿å­˜å®Œäº†
ğŸŸ¦ Prefillãƒ¢ãƒ¼ãƒ‰ã§FlashAttentionã‚’å®Ÿè¡Œ context.max_seqlen_q=1, context.cu_seqlens_q=tensor([0, 1], device='cuda:0', dtype=torch.int32), context.max_seqlen_k=1, context.cu_seqlens_k=tensor([0, 1], device='cuda:0', dtype=torch.int32)
ğŸŸ© Attentionå±¤ã®é †ä¼æ’­å®Œäº† o.shape=torch.Size([1, 16, 128])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­é–‹å§‹ å…¥åŠ›å½¢çŠ¶: torch.Size([1, 2048])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­å®Œäº† å‡ºåŠ›å½¢çŠ¶: torch.Size([1, 1024])
ğŸŸ© Qwen3Attentionã®é †ä¼æ’­å®Œäº† output.shape=torch.Size([1, 1024])
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 1024]), residual.shape if residual is not None else None=torch.Size([1, 1024])
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨å®Œäº†
ğŸŸ© Qwen3MLPã®é †ä¼æ’­ x.shape=torch.Size([1, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­å®Œäº† result.shape=torch.Size([1, 6144])
ğŸŸ© SwiGLUã‚’é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 6144])
ğŸŸ¦ ãƒãƒ£ãƒ³ã‚¯åŒ– x.shape=torch.Size([1, 3072]), y.shape=torch.Size([1, 3072])
ğŸŸ© SwiGLUã‚’é †ä¼æ’­å®Œäº† result.shape=torch.Size([1, 3072])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­é–‹å§‹ å…¥åŠ›å½¢çŠ¶: torch.Size([1, 3072])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­å®Œäº† å‡ºåŠ›å½¢çŠ¶: torch.Size([1, 1024])
ğŸŸ© Qwen3MLPã®é †ä¼æ’­å®Œäº† x.shape=torch.Size([1, 1024])
ğŸŸ© Qwen3DecoderLayerã®é †ä¼æ’­å®Œäº† hidden_states.shape=torch.Size([1, 1024])
ğŸŸ© Qwen3DecoderLayerã®é †ä¼æ’­ positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 1024]) residual is None=False
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 1024]), residual.shape if residual is not None else None=torch.Size([1, 1024])
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨å®Œäº†
ğŸŸ© Qwen3Attentionã®é †ä¼æ’­ positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­å®Œäº† result.shape=torch.Size([1, 4096])
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 16, 128]), residual.shape if residual is not None else None=None
ğŸŸ© æ­£è¦åŒ–é–‹å§‹ x.shape=torch.Size([1, 16, 128])
ğŸŸ© æ­£è¦åŒ–å®Œäº† x.shape=torch.Size([1, 16, 128])
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 8, 128]), residual.shape if residual is not None else None=None
ğŸŸ© æ­£è¦åŒ–é–‹å§‹ x.shape=torch.Size([1, 8, 128])
ğŸŸ© æ­£è¦åŒ–å®Œäº† x.shape=torch.Size([1, 8, 128])
ğŸŸ© RoPEã®é †ä¼æ’­é–‹å§‹ positions.shape=torch.Size([1]), query.shape=torch.Size([1, 16, 128]), key.shape=torch.Size([1, 8, 128])
ğŸŸ© RoPEã‚’é©ç”¨é–‹å§‹ x.shape=torch.Size([1, 16, 128]), cos.shape=torch.Size([1, 1, 64]), sin.shape=torch.Size([1, 1, 64])
ğŸŸ¦ ãƒãƒ£ãƒ³ã‚¯åŒ– x1.shape=torch.Size([1, 16, 64]), x2.shape=torch.Size([1, 16, 64])
ğŸŸ¦ å›è»¢è¡Œåˆ—é©ç”¨ y1.shape=torch.Size([1, 16, 64]), y2.shape=torch.Size([1, 16, 64])
ğŸŸ© RoPEã‚’é©ç”¨å®Œäº† result.shape=torch.Size([1, 16, 128])
ğŸŸ© RoPEã‚’é©ç”¨é–‹å§‹ x.shape=torch.Size([1, 8, 128]), cos.shape=torch.Size([1, 1, 64]), sin.shape=torch.Size([1, 1, 64])
ğŸŸ¦ ãƒãƒ£ãƒ³ã‚¯åŒ– x1.shape=torch.Size([1, 8, 64]), x2.shape=torch.Size([1, 8, 64])
ğŸŸ¦ å›è»¢è¡Œåˆ—é©ç”¨ y1.shape=torch.Size([1, 8, 64]), y2.shape=torch.Size([1, 8, 64])
ğŸŸ© RoPEã‚’é©ç”¨å®Œäº† result.shape=torch.Size([1, 8, 128])
ğŸŸ© RoPEã®é †ä¼æ’­å®Œäº† query.shape=torch.Size([1, 16, 128]), key.shape=torch.Size([1, 8, 128])
ğŸŸ© Attentionå±¤ã®é †ä¼æ’­é–‹å§‹ q.shape=torch.Size([1, 16, 128]), k.shape=torch.Size([1, 8, 128]), v.shape=torch.Size([1, 8, 128])
ğŸŸ© ç¾åœ¨ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆå–å¾— _CONTEXT=Context(is_prefill=True, cu_seqlens_q=tensor([0, 1], device='cuda:0', dtype=torch.int32), cu_seqlens_k=tensor([0, 1], device='cuda:0', dtype=torch.int32), max_seqlen_q=1, max_seqlen_k=1, slot_mapping=tensor([0], device='cuda:0', dtype=torch.int32), context_lens=None, block_tables=None)
ğŸŸ¦ ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæƒ…å ±å–å¾— context.is_prefill=True, context.slot_mapping=tensor([0], device='cuda:0', dtype=torch.int32), context.max_seqlen_q=1, context.cu_seqlens_q=tensor([0, 1], device='cuda:0', dtype=torch.int32), context.max_seqlen_k=1, context.cu_seqlens_k=tensor([0, 1], device='cuda:0', dtype=torch.int32), context.context_lens=None, context.block_tables=None
ğŸŸ© KVã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ã‚­ãƒ¼ã¨ãƒãƒªãƒ¥ãƒ¼ã‚’ä¿å­˜é–‹å§‹ key.shape=torch.Size([1, 8, 128]), value.shape=torch.Size([1, 8, 128]), k_cache.shape=torch.Size([875, 256, 8, 128]), v_cache.shape=torch.Size([875, 256, 8, 128]), slot_mapping.shape=torch.Size([1])
ğŸŸ© KVã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ã‚­ãƒ¼ã¨ãƒãƒªãƒ¥ãƒ¼ã‚’ä¿å­˜å®Œäº†
ğŸŸ¦ Prefillãƒ¢ãƒ¼ãƒ‰ã§FlashAttentionã‚’å®Ÿè¡Œ context.max_seqlen_q=1, context.cu_seqlens_q=tensor([0, 1], device='cuda:0', dtype=torch.int32), context.max_seqlen_k=1, context.cu_seqlens_k=tensor([0, 1], device='cuda:0', dtype=torch.int32)
ğŸŸ© Attentionå±¤ã®é †ä¼æ’­å®Œäº† o.shape=torch.Size([1, 16, 128])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­é–‹å§‹ å…¥åŠ›å½¢çŠ¶: torch.Size([1, 2048])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­å®Œäº† å‡ºåŠ›å½¢çŠ¶: torch.Size([1, 1024])
ğŸŸ© Qwen3Attentionã®é †ä¼æ’­å®Œäº† output.shape=torch.Size([1, 1024])
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 1024]), residual.shape if residual is not None else None=torch.Size([1, 1024])
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨å®Œäº†
ğŸŸ© Qwen3MLPã®é †ä¼æ’­ x.shape=torch.Size([1, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­å®Œäº† result.shape=torch.Size([1, 6144])
ğŸŸ© SwiGLUã‚’é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 6144])
ğŸŸ¦ ãƒãƒ£ãƒ³ã‚¯åŒ– x.shape=torch.Size([1, 3072]), y.shape=torch.Size([1, 3072])
ğŸŸ© SwiGLUã‚’é †ä¼æ’­å®Œäº† result.shape=torch.Size([1, 3072])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­é–‹å§‹ å…¥åŠ›å½¢çŠ¶: torch.Size([1, 3072])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­å®Œäº† å‡ºåŠ›å½¢çŠ¶: torch.Size([1, 1024])
ğŸŸ© Qwen3MLPã®é †ä¼æ’­å®Œäº† x.shape=torch.Size([1, 1024])
ğŸŸ© Qwen3DecoderLayerã®é †ä¼æ’­å®Œäº† hidden_states.shape=torch.Size([1, 1024])
ğŸŸ© Qwen3DecoderLayerã®é †ä¼æ’­ positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 1024]) residual is None=False
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 1024]), residual.shape if residual is not None else None=torch.Size([1, 1024])
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨å®Œäº†
ğŸŸ© Qwen3Attentionã®é †ä¼æ’­ positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­å®Œäº† result.shape=torch.Size([1, 4096])
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 16, 128]), residual.shape if residual is not None else None=None
ğŸŸ© æ­£è¦åŒ–é–‹å§‹ x.shape=torch.Size([1, 16, 128])
ğŸŸ© æ­£è¦åŒ–å®Œäº† x.shape=torch.Size([1, 16, 128])
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 8, 128]), residual.shape if residual is not None else None=None
ğŸŸ© æ­£è¦åŒ–é–‹å§‹ x.shape=torch.Size([1, 8, 128])
ğŸŸ© æ­£è¦åŒ–å®Œäº† x.shape=torch.Size([1, 8, 128])
ğŸŸ© RoPEã®é †ä¼æ’­é–‹å§‹ positions.shape=torch.Size([1]), query.shape=torch.Size([1, 16, 128]), key.shape=torch.Size([1, 8, 128])
ğŸŸ© RoPEã‚’é©ç”¨é–‹å§‹ x.shape=torch.Size([1, 16, 128]), cos.shape=torch.Size([1, 1, 64]), sin.shape=torch.Size([1, 1, 64])
ğŸŸ¦ ãƒãƒ£ãƒ³ã‚¯åŒ– x1.shape=torch.Size([1, 16, 64]), x2.shape=torch.Size([1, 16, 64])
ğŸŸ¦ å›è»¢è¡Œåˆ—é©ç”¨ y1.shape=torch.Size([1, 16, 64]), y2.shape=torch.Size([1, 16, 64])
ğŸŸ© RoPEã‚’é©ç”¨å®Œäº† result.shape=torch.Size([1, 16, 128])
ğŸŸ© RoPEã‚’é©ç”¨é–‹å§‹ x.shape=torch.Size([1, 8, 128]), cos.shape=torch.Size([1, 1, 64]), sin.shape=torch.Size([1, 1, 64])
ğŸŸ¦ ãƒãƒ£ãƒ³ã‚¯åŒ– x1.shape=torch.Size([1, 8, 64]), x2.shape=torch.Size([1, 8, 64])
ğŸŸ¦ å›è»¢è¡Œåˆ—é©ç”¨ y1.shape=torch.Size([1, 8, 64]), y2.shape=torch.Size([1, 8, 64])
ğŸŸ© RoPEã‚’é©ç”¨å®Œäº† result.shape=torch.Size([1, 8, 128])
ğŸŸ© RoPEã®é †ä¼æ’­å®Œäº† query.shape=torch.Size([1, 16, 128]), key.shape=torch.Size([1, 8, 128])
ğŸŸ© Attentionå±¤ã®é †ä¼æ’­é–‹å§‹ q.shape=torch.Size([1, 16, 128]), k.shape=torch.Size([1, 8, 128]), v.shape=torch.Size([1, 8, 128])
ğŸŸ© ç¾åœ¨ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆå–å¾— _CONTEXT=Context(is_prefill=True, cu_seqlens_q=tensor([0, 1], device='cuda:0', dtype=torch.int32), cu_seqlens_k=tensor([0, 1], device='cuda:0', dtype=torch.int32), max_seqlen_q=1, max_seqlen_k=1, slot_mapping=tensor([0], device='cuda:0', dtype=torch.int32), context_lens=None, block_tables=None)
ğŸŸ¦ ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæƒ…å ±å–å¾— context.is_prefill=True, context.slot_mapping=tensor([0], device='cuda:0', dtype=torch.int32), context.max_seqlen_q=1, context.cu_seqlens_q=tensor([0, 1], device='cuda:0', dtype=torch.int32), context.max_seqlen_k=1, context.cu_seqlens_k=tensor([0, 1], device='cuda:0', dtype=torch.int32), context.context_lens=None, context.block_tables=None
ğŸŸ© KVã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ã‚­ãƒ¼ã¨ãƒãƒªãƒ¥ãƒ¼ã‚’ä¿å­˜é–‹å§‹ key.shape=torch.Size([1, 8, 128]), value.shape=torch.Size([1, 8, 128]), k_cache.shape=torch.Size([875, 256, 8, 128]), v_cache.shape=torch.Size([875, 256, 8, 128]), slot_mapping.shape=torch.Size([1])
ğŸŸ© KVã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ã‚­ãƒ¼ã¨ãƒãƒªãƒ¥ãƒ¼ã‚’ä¿å­˜å®Œäº†
ğŸŸ¦ Prefillãƒ¢ãƒ¼ãƒ‰ã§FlashAttentionã‚’å®Ÿè¡Œ context.max_seqlen_q=1, context.cu_seqlens_q=tensor([0, 1], device='cuda:0', dtype=torch.int32), context.max_seqlen_k=1, context.cu_seqlens_k=tensor([0, 1], device='cuda:0', dtype=torch.int32)
ğŸŸ© Attentionå±¤ã®é †ä¼æ’­å®Œäº† o.shape=torch.Size([1, 16, 128])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­é–‹å§‹ å…¥åŠ›å½¢çŠ¶: torch.Size([1, 2048])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­å®Œäº† å‡ºåŠ›å½¢çŠ¶: torch.Size([1, 1024])
ğŸŸ© Qwen3Attentionã®é †ä¼æ’­å®Œäº† output.shape=torch.Size([1, 1024])
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 1024]), residual.shape if residual is not None else None=torch.Size([1, 1024])
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨å®Œäº†
ğŸŸ© Qwen3MLPã®é †ä¼æ’­ x.shape=torch.Size([1, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­å®Œäº† result.shape=torch.Size([1, 6144])
ğŸŸ© SwiGLUã‚’é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 6144])
ğŸŸ¦ ãƒãƒ£ãƒ³ã‚¯åŒ– x.shape=torch.Size([1, 3072]), y.shape=torch.Size([1, 3072])
ğŸŸ© SwiGLUã‚’é †ä¼æ’­å®Œäº† result.shape=torch.Size([1, 3072])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­é–‹å§‹ å…¥åŠ›å½¢çŠ¶: torch.Size([1, 3072])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­å®Œäº† å‡ºåŠ›å½¢çŠ¶: torch.Size([1, 1024])
ğŸŸ© Qwen3MLPã®é †ä¼æ’­å®Œäº† x.shape=torch.Size([1, 1024])
ğŸŸ© Qwen3DecoderLayerã®é †ä¼æ’­å®Œäº† hidden_states.shape=torch.Size([1, 1024])
ğŸŸ© Qwen3DecoderLayerã®é †ä¼æ’­ positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 1024]) residual is None=False
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 1024]), residual.shape if residual is not None else None=torch.Size([1, 1024])
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨å®Œäº†
ğŸŸ© Qwen3Attentionã®é †ä¼æ’­ positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­å®Œäº† result.shape=torch.Size([1, 4096])
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 16, 128]), residual.shape if residual is not None else None=None
ğŸŸ© æ­£è¦åŒ–é–‹å§‹ x.shape=torch.Size([1, 16, 128])
ğŸŸ© æ­£è¦åŒ–å®Œäº† x.shape=torch.Size([1, 16, 128])
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 8, 128]), residual.shape if residual is not None else None=None
ğŸŸ© æ­£è¦åŒ–é–‹å§‹ x.shape=torch.Size([1, 8, 128])
ğŸŸ© æ­£è¦åŒ–å®Œäº† x.shape=torch.Size([1, 8, 128])
ğŸŸ© RoPEã®é †ä¼æ’­é–‹å§‹ positions.shape=torch.Size([1]), query.shape=torch.Size([1, 16, 128]), key.shape=torch.Size([1, 8, 128])
ğŸŸ© RoPEã‚’é©ç”¨é–‹å§‹ x.shape=torch.Size([1, 16, 128]), cos.shape=torch.Size([1, 1, 64]), sin.shape=torch.Size([1, 1, 64])
ğŸŸ¦ ãƒãƒ£ãƒ³ã‚¯åŒ– x1.shape=torch.Size([1, 16, 64]), x2.shape=torch.Size([1, 16, 64])
ğŸŸ¦ å›è»¢è¡Œåˆ—é©ç”¨ y1.shape=torch.Size([1, 16, 64]), y2.shape=torch.Size([1, 16, 64])
ğŸŸ© RoPEã‚’é©ç”¨å®Œäº† result.shape=torch.Size([1, 16, 128])
ğŸŸ© RoPEã‚’é©ç”¨é–‹å§‹ x.shape=torch.Size([1, 8, 128]), cos.shape=torch.Size([1, 1, 64]), sin.shape=torch.Size([1, 1, 64])
ğŸŸ¦ ãƒãƒ£ãƒ³ã‚¯åŒ– x1.shape=torch.Size([1, 8, 64]), x2.shape=torch.Size([1, 8, 64])
ğŸŸ¦ å›è»¢è¡Œåˆ—é©ç”¨ y1.shape=torch.Size([1, 8, 64]), y2.shape=torch.Size([1, 8, 64])
ğŸŸ© RoPEã‚’é©ç”¨å®Œäº† result.shape=torch.Size([1, 8, 128])
ğŸŸ© RoPEã®é †ä¼æ’­å®Œäº† query.shape=torch.Size([1, 16, 128]), key.shape=torch.Size([1, 8, 128])
ğŸŸ© Attentionå±¤ã®é †ä¼æ’­é–‹å§‹ q.shape=torch.Size([1, 16, 128]), k.shape=torch.Size([1, 8, 128]), v.shape=torch.Size([1, 8, 128])
ğŸŸ© ç¾åœ¨ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆå–å¾— _CONTEXT=Context(is_prefill=True, cu_seqlens_q=tensor([0, 1], device='cuda:0', dtype=torch.int32), cu_seqlens_k=tensor([0, 1], device='cuda:0', dtype=torch.int32), max_seqlen_q=1, max_seqlen_k=1, slot_mapping=tensor([0], device='cuda:0', dtype=torch.int32), context_lens=None, block_tables=None)
ğŸŸ¦ ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæƒ…å ±å–å¾— context.is_prefill=True, context.slot_mapping=tensor([0], device='cuda:0', dtype=torch.int32), context.max_seqlen_q=1, context.cu_seqlens_q=tensor([0, 1], device='cuda:0', dtype=torch.int32), context.max_seqlen_k=1, context.cu_seqlens_k=tensor([0, 1], device='cuda:0', dtype=torch.int32), context.context_lens=None, context.block_tables=None
ğŸŸ© KVã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ã‚­ãƒ¼ã¨ãƒãƒªãƒ¥ãƒ¼ã‚’ä¿å­˜é–‹å§‹ key.shape=torch.Size([1, 8, 128]), value.shape=torch.Size([1, 8, 128]), k_cache.shape=torch.Size([875, 256, 8, 128]), v_cache.shape=torch.Size([875, 256, 8, 128]), slot_mapping.shape=torch.Size([1])
ğŸŸ© KVã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ã‚­ãƒ¼ã¨ãƒãƒªãƒ¥ãƒ¼ã‚’ä¿å­˜å®Œäº†
ğŸŸ¦ Prefillãƒ¢ãƒ¼ãƒ‰ã§FlashAttentionã‚’å®Ÿè¡Œ context.max_seqlen_q=1, context.cu_seqlens_q=tensor([0, 1], device='cuda:0', dtype=torch.int32), context.max_seqlen_k=1, context.cu_seqlens_k=tensor([0, 1], device='cuda:0', dtype=torch.int32)
ğŸŸ© Attentionå±¤ã®é †ä¼æ’­å®Œäº† o.shape=torch.Size([1, 16, 128])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­é–‹å§‹ å…¥åŠ›å½¢çŠ¶: torch.Size([1, 2048])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­å®Œäº† å‡ºåŠ›å½¢çŠ¶: torch.Size([1, 1024])
ğŸŸ© Qwen3Attentionã®é †ä¼æ’­å®Œäº† output.shape=torch.Size([1, 1024])
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 1024]), residual.shape if residual is not None else None=torch.Size([1, 1024])
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨å®Œäº†
ğŸŸ© Qwen3MLPã®é †ä¼æ’­ x.shape=torch.Size([1, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­å®Œäº† result.shape=torch.Size([1, 6144])
ğŸŸ© SwiGLUã‚’é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 6144])
ğŸŸ¦ ãƒãƒ£ãƒ³ã‚¯åŒ– x.shape=torch.Size([1, 3072]), y.shape=torch.Size([1, 3072])
ğŸŸ© SwiGLUã‚’é †ä¼æ’­å®Œäº† result.shape=torch.Size([1, 3072])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­é–‹å§‹ å…¥åŠ›å½¢çŠ¶: torch.Size([1, 3072])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­å®Œäº† å‡ºåŠ›å½¢çŠ¶: torch.Size([1, 1024])
ğŸŸ© Qwen3MLPã®é †ä¼æ’­å®Œäº† x.shape=torch.Size([1, 1024])
ğŸŸ© Qwen3DecoderLayerã®é †ä¼æ’­å®Œäº† hidden_states.shape=torch.Size([1, 1024])
ğŸŸ© Qwen3DecoderLayerã®é †ä¼æ’­ positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 1024]) residual is None=False
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 1024]), residual.shape if residual is not None else None=torch.Size([1, 1024])
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨å®Œäº†
ğŸŸ© Qwen3Attentionã®é †ä¼æ’­ positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­å®Œäº† result.shape=torch.Size([1, 4096])
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 16, 128]), residual.shape if residual is not None else None=None
ğŸŸ© æ­£è¦åŒ–é–‹å§‹ x.shape=torch.Size([1, 16, 128])
ğŸŸ© æ­£è¦åŒ–å®Œäº† x.shape=torch.Size([1, 16, 128])
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 8, 128]), residual.shape if residual is not None else None=None
ğŸŸ© æ­£è¦åŒ–é–‹å§‹ x.shape=torch.Size([1, 8, 128])
ğŸŸ© æ­£è¦åŒ–å®Œäº† x.shape=torch.Size([1, 8, 128])
ğŸŸ© RoPEã®é †ä¼æ’­é–‹å§‹ positions.shape=torch.Size([1]), query.shape=torch.Size([1, 16, 128]), key.shape=torch.Size([1, 8, 128])
ğŸŸ© RoPEã‚’é©ç”¨é–‹å§‹ x.shape=torch.Size([1, 16, 128]), cos.shape=torch.Size([1, 1, 64]), sin.shape=torch.Size([1, 1, 64])
ğŸŸ¦ ãƒãƒ£ãƒ³ã‚¯åŒ– x1.shape=torch.Size([1, 16, 64]), x2.shape=torch.Size([1, 16, 64])
ğŸŸ¦ å›è»¢è¡Œåˆ—é©ç”¨ y1.shape=torch.Size([1, 16, 64]), y2.shape=torch.Size([1, 16, 64])
ğŸŸ© RoPEã‚’é©ç”¨å®Œäº† result.shape=torch.Size([1, 16, 128])
ğŸŸ© RoPEã‚’é©ç”¨é–‹å§‹ x.shape=torch.Size([1, 8, 128]), cos.shape=torch.Size([1, 1, 64]), sin.shape=torch.Size([1, 1, 64])
ğŸŸ¦ ãƒãƒ£ãƒ³ã‚¯åŒ– x1.shape=torch.Size([1, 8, 64]), x2.shape=torch.Size([1, 8, 64])
ğŸŸ¦ å›è»¢è¡Œåˆ—é©ç”¨ y1.shape=torch.Size([1, 8, 64]), y2.shape=torch.Size([1, 8, 64])
ğŸŸ© RoPEã‚’é©ç”¨å®Œäº† result.shape=torch.Size([1, 8, 128])
ğŸŸ© RoPEã®é †ä¼æ’­å®Œäº† query.shape=torch.Size([1, 16, 128]), key.shape=torch.Size([1, 8, 128])
ğŸŸ© Attentionå±¤ã®é †ä¼æ’­é–‹å§‹ q.shape=torch.Size([1, 16, 128]), k.shape=torch.Size([1, 8, 128]), v.shape=torch.Size([1, 8, 128])
ğŸŸ© ç¾åœ¨ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆå–å¾— _CONTEXT=Context(is_prefill=True, cu_seqlens_q=tensor([0, 1], device='cuda:0', dtype=torch.int32), cu_seqlens_k=tensor([0, 1], device='cuda:0', dtype=torch.int32), max_seqlen_q=1, max_seqlen_k=1, slot_mapping=tensor([0], device='cuda:0', dtype=torch.int32), context_lens=None, block_tables=None)
ğŸŸ¦ ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæƒ…å ±å–å¾— context.is_prefill=True, context.slot_mapping=tensor([0], device='cuda:0', dtype=torch.int32), context.max_seqlen_q=1, context.cu_seqlens_q=tensor([0, 1], device='cuda:0', dtype=torch.int32), context.max_seqlen_k=1, context.cu_seqlens_k=tensor([0, 1], device='cuda:0', dtype=torch.int32), context.context_lens=None, context.block_tables=None
ğŸŸ© KVã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ã‚­ãƒ¼ã¨ãƒãƒªãƒ¥ãƒ¼ã‚’ä¿å­˜é–‹å§‹ key.shape=torch.Size([1, 8, 128]), value.shape=torch.Size([1, 8, 128]), k_cache.shape=torch.Size([875, 256, 8, 128]), v_cache.shape=torch.Size([875, 256, 8, 128]), slot_mapping.shape=torch.Size([1])
ğŸŸ© KVã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ã‚­ãƒ¼ã¨ãƒãƒªãƒ¥ãƒ¼ã‚’ä¿å­˜å®Œäº†
ğŸŸ¦ Prefillãƒ¢ãƒ¼ãƒ‰ã§FlashAttentionã‚’å®Ÿè¡Œ context.max_seqlen_q=1, context.cu_seqlens_q=tensor([0, 1], device='cuda:0', dtype=torch.int32), context.max_seqlen_k=1, context.cu_seqlens_k=tensor([0, 1], device='cuda:0', dtype=torch.int32)
ğŸŸ© Attentionå±¤ã®é †ä¼æ’­å®Œäº† o.shape=torch.Size([1, 16, 128])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­é–‹å§‹ å…¥åŠ›å½¢çŠ¶: torch.Size([1, 2048])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­å®Œäº† å‡ºåŠ›å½¢çŠ¶: torch.Size([1, 1024])
ğŸŸ© Qwen3Attentionã®é †ä¼æ’­å®Œäº† output.shape=torch.Size([1, 1024])
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 1024]), residual.shape if residual is not None else None=torch.Size([1, 1024])
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨å®Œäº†
ğŸŸ© Qwen3MLPã®é †ä¼æ’­ x.shape=torch.Size([1, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­å®Œäº† result.shape=torch.Size([1, 6144])
ğŸŸ© SwiGLUã‚’é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 6144])
ğŸŸ¦ ãƒãƒ£ãƒ³ã‚¯åŒ– x.shape=torch.Size([1, 3072]), y.shape=torch.Size([1, 3072])
ğŸŸ© SwiGLUã‚’é †ä¼æ’­å®Œäº† result.shape=torch.Size([1, 3072])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­é–‹å§‹ å…¥åŠ›å½¢çŠ¶: torch.Size([1, 3072])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­å®Œäº† å‡ºåŠ›å½¢çŠ¶: torch.Size([1, 1024])
ğŸŸ© Qwen3MLPã®é †ä¼æ’­å®Œäº† x.shape=torch.Size([1, 1024])
ğŸŸ© Qwen3DecoderLayerã®é †ä¼æ’­å®Œäº† hidden_states.shape=torch.Size([1, 1024])
ğŸŸ© Qwen3DecoderLayerã®é †ä¼æ’­ positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 1024]) residual is None=False
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 1024]), residual.shape if residual is not None else None=torch.Size([1, 1024])
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨å®Œäº†
ğŸŸ© Qwen3Attentionã®é †ä¼æ’­ positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­å®Œäº† result.shape=torch.Size([1, 4096])
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 16, 128]), residual.shape if residual is not None else None=None
ğŸŸ© æ­£è¦åŒ–é–‹å§‹ x.shape=torch.Size([1, 16, 128])
ğŸŸ© æ­£è¦åŒ–å®Œäº† x.shape=torch.Size([1, 16, 128])
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 8, 128]), residual.shape if residual is not None else None=None
ğŸŸ© æ­£è¦åŒ–é–‹å§‹ x.shape=torch.Size([1, 8, 128])
ğŸŸ© æ­£è¦åŒ–å®Œäº† x.shape=torch.Size([1, 8, 128])
ğŸŸ© RoPEã®é †ä¼æ’­é–‹å§‹ positions.shape=torch.Size([1]), query.shape=torch.Size([1, 16, 128]), key.shape=torch.Size([1, 8, 128])
ğŸŸ© RoPEã‚’é©ç”¨é–‹å§‹ x.shape=torch.Size([1, 16, 128]), cos.shape=torch.Size([1, 1, 64]), sin.shape=torch.Size([1, 1, 64])
ğŸŸ¦ ãƒãƒ£ãƒ³ã‚¯åŒ– x1.shape=torch.Size([1, 16, 64]), x2.shape=torch.Size([1, 16, 64])
ğŸŸ¦ å›è»¢è¡Œåˆ—é©ç”¨ y1.shape=torch.Size([1, 16, 64]), y2.shape=torch.Size([1, 16, 64])
ğŸŸ© RoPEã‚’é©ç”¨å®Œäº† result.shape=torch.Size([1, 16, 128])
ğŸŸ© RoPEã‚’é©ç”¨é–‹å§‹ x.shape=torch.Size([1, 8, 128]), cos.shape=torch.Size([1, 1, 64]), sin.shape=torch.Size([1, 1, 64])
ğŸŸ¦ ãƒãƒ£ãƒ³ã‚¯åŒ– x1.shape=torch.Size([1, 8, 64]), x2.shape=torch.Size([1, 8, 64])
ğŸŸ¦ å›è»¢è¡Œåˆ—é©ç”¨ y1.shape=torch.Size([1, 8, 64]), y2.shape=torch.Size([1, 8, 64])
ğŸŸ© RoPEã‚’é©ç”¨å®Œäº† result.shape=torch.Size([1, 8, 128])
ğŸŸ© RoPEã®é †ä¼æ’­å®Œäº† query.shape=torch.Size([1, 16, 128]), key.shape=torch.Size([1, 8, 128])
ğŸŸ© Attentionå±¤ã®é †ä¼æ’­é–‹å§‹ q.shape=torch.Size([1, 16, 128]), k.shape=torch.Size([1, 8, 128]), v.shape=torch.Size([1, 8, 128])
ğŸŸ© ç¾åœ¨ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆå–å¾— _CONTEXT=Context(is_prefill=True, cu_seqlens_q=tensor([0, 1], device='cuda:0', dtype=torch.int32), cu_seqlens_k=tensor([0, 1], device='cuda:0', dtype=torch.int32), max_seqlen_q=1, max_seqlen_k=1, slot_mapping=tensor([0], device='cuda:0', dtype=torch.int32), context_lens=None, block_tables=None)
ğŸŸ¦ ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæƒ…å ±å–å¾— context.is_prefill=True, context.slot_mapping=tensor([0], device='cuda:0', dtype=torch.int32), context.max_seqlen_q=1, context.cu_seqlens_q=tensor([0, 1], device='cuda:0', dtype=torch.int32), context.max_seqlen_k=1, context.cu_seqlens_k=tensor([0, 1], device='cuda:0', dtype=torch.int32), context.context_lens=None, context.block_tables=None
ğŸŸ© KVã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ã‚­ãƒ¼ã¨ãƒãƒªãƒ¥ãƒ¼ã‚’ä¿å­˜é–‹å§‹ key.shape=torch.Size([1, 8, 128]), value.shape=torch.Size([1, 8, 128]), k_cache.shape=torch.Size([875, 256, 8, 128]), v_cache.shape=torch.Size([875, 256, 8, 128]), slot_mapping.shape=torch.Size([1])
ğŸŸ© KVã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ã‚­ãƒ¼ã¨ãƒãƒªãƒ¥ãƒ¼ã‚’ä¿å­˜å®Œäº†
ğŸŸ¦ Prefillãƒ¢ãƒ¼ãƒ‰ã§FlashAttentionã‚’å®Ÿè¡Œ context.max_seqlen_q=1, context.cu_seqlens_q=tensor([0, 1], device='cuda:0', dtype=torch.int32), context.max_seqlen_k=1, context.cu_seqlens_k=tensor([0, 1], device='cuda:0', dtype=torch.int32)
ğŸŸ© Attentionå±¤ã®é †ä¼æ’­å®Œäº† o.shape=torch.Size([1, 16, 128])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­é–‹å§‹ å…¥åŠ›å½¢çŠ¶: torch.Size([1, 2048])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­å®Œäº† å‡ºåŠ›å½¢çŠ¶: torch.Size([1, 1024])
ğŸŸ© Qwen3Attentionã®é †ä¼æ’­å®Œäº† output.shape=torch.Size([1, 1024])
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 1024]), residual.shape if residual is not None else None=torch.Size([1, 1024])
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨å®Œäº†
ğŸŸ© Qwen3MLPã®é †ä¼æ’­ x.shape=torch.Size([1, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­å®Œäº† result.shape=torch.Size([1, 6144])
ğŸŸ© SwiGLUã‚’é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 6144])
ğŸŸ¦ ãƒãƒ£ãƒ³ã‚¯åŒ– x.shape=torch.Size([1, 3072]), y.shape=torch.Size([1, 3072])
ğŸŸ© SwiGLUã‚’é †ä¼æ’­å®Œäº† result.shape=torch.Size([1, 3072])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­é–‹å§‹ å…¥åŠ›å½¢çŠ¶: torch.Size([1, 3072])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­å®Œäº† å‡ºåŠ›å½¢çŠ¶: torch.Size([1, 1024])
ğŸŸ© Qwen3MLPã®é †ä¼æ’­å®Œäº† x.shape=torch.Size([1, 1024])
ğŸŸ© Qwen3DecoderLayerã®é †ä¼æ’­å®Œäº† hidden_states.shape=torch.Size([1, 1024])
ğŸŸ© Qwen3DecoderLayerã®é †ä¼æ’­ positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 1024]) residual is None=False
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 1024]), residual.shape if residual is not None else None=torch.Size([1, 1024])
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨å®Œäº†
ğŸŸ© Qwen3Attentionã®é †ä¼æ’­ positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­å®Œäº† result.shape=torch.Size([1, 4096])
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 16, 128]), residual.shape if residual is not None else None=None
ğŸŸ© æ­£è¦åŒ–é–‹å§‹ x.shape=torch.Size([1, 16, 128])
ğŸŸ© æ­£è¦åŒ–å®Œäº† x.shape=torch.Size([1, 16, 128])
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 8, 128]), residual.shape if residual is not None else None=None
ğŸŸ© æ­£è¦åŒ–é–‹å§‹ x.shape=torch.Size([1, 8, 128])
ğŸŸ© æ­£è¦åŒ–å®Œäº† x.shape=torch.Size([1, 8, 128])
ğŸŸ© RoPEã®é †ä¼æ’­é–‹å§‹ positions.shape=torch.Size([1]), query.shape=torch.Size([1, 16, 128]), key.shape=torch.Size([1, 8, 128])
ğŸŸ© RoPEã‚’é©ç”¨é–‹å§‹ x.shape=torch.Size([1, 16, 128]), cos.shape=torch.Size([1, 1, 64]), sin.shape=torch.Size([1, 1, 64])
ğŸŸ¦ ãƒãƒ£ãƒ³ã‚¯åŒ– x1.shape=torch.Size([1, 16, 64]), x2.shape=torch.Size([1, 16, 64])
ğŸŸ¦ å›è»¢è¡Œåˆ—é©ç”¨ y1.shape=torch.Size([1, 16, 64]), y2.shape=torch.Size([1, 16, 64])
ğŸŸ© RoPEã‚’é©ç”¨å®Œäº† result.shape=torch.Size([1, 16, 128])
ğŸŸ© RoPEã‚’é©ç”¨é–‹å§‹ x.shape=torch.Size([1, 8, 128]), cos.shape=torch.Size([1, 1, 64]), sin.shape=torch.Size([1, 1, 64])
ğŸŸ¦ ãƒãƒ£ãƒ³ã‚¯åŒ– x1.shape=torch.Size([1, 8, 64]), x2.shape=torch.Size([1, 8, 64])
ğŸŸ¦ å›è»¢è¡Œåˆ—é©ç”¨ y1.shape=torch.Size([1, 8, 64]), y2.shape=torch.Size([1, 8, 64])
ğŸŸ© RoPEã‚’é©ç”¨å®Œäº† result.shape=torch.Size([1, 8, 128])
ğŸŸ© RoPEã®é †ä¼æ’­å®Œäº† query.shape=torch.Size([1, 16, 128]), key.shape=torch.Size([1, 8, 128])
ğŸŸ© Attentionå±¤ã®é †ä¼æ’­é–‹å§‹ q.shape=torch.Size([1, 16, 128]), k.shape=torch.Size([1, 8, 128]), v.shape=torch.Size([1, 8, 128])
ğŸŸ© ç¾åœ¨ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆå–å¾— _CONTEXT=Context(is_prefill=True, cu_seqlens_q=tensor([0, 1], device='cuda:0', dtype=torch.int32), cu_seqlens_k=tensor([0, 1], device='cuda:0', dtype=torch.int32), max_seqlen_q=1, max_seqlen_k=1, slot_mapping=tensor([0], device='cuda:0', dtype=torch.int32), context_lens=None, block_tables=None)
ğŸŸ¦ ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæƒ…å ±å–å¾— context.is_prefill=True, context.slot_mapping=tensor([0], device='cuda:0', dtype=torch.int32), context.max_seqlen_q=1, context.cu_seqlens_q=tensor([0, 1], device='cuda:0', dtype=torch.int32), context.max_seqlen_k=1, context.cu_seqlens_k=tensor([0, 1], device='cuda:0', dtype=torch.int32), context.context_lens=None, context.block_tables=None
ğŸŸ© KVã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ã‚­ãƒ¼ã¨ãƒãƒªãƒ¥ãƒ¼ã‚’ä¿å­˜é–‹å§‹ key.shape=torch.Size([1, 8, 128]), value.shape=torch.Size([1, 8, 128]), k_cache.shape=torch.Size([875, 256, 8, 128]), v_cache.shape=torch.Size([875, 256, 8, 128]), slot_mapping.shape=torch.Size([1])
ğŸŸ© KVã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ã‚­ãƒ¼ã¨ãƒãƒªãƒ¥ãƒ¼ã‚’ä¿å­˜å®Œäº†
ğŸŸ¦ Prefillãƒ¢ãƒ¼ãƒ‰ã§FlashAttentionã‚’å®Ÿè¡Œ context.max_seqlen_q=1, context.cu_seqlens_q=tensor([0, 1], device='cuda:0', dtype=torch.int32), context.max_seqlen_k=1, context.cu_seqlens_k=tensor([0, 1], device='cuda:0', dtype=torch.int32)
ğŸŸ© Attentionå±¤ã®é †ä¼æ’­å®Œäº† o.shape=torch.Size([1, 16, 128])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­é–‹å§‹ å…¥åŠ›å½¢çŠ¶: torch.Size([1, 2048])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­å®Œäº† å‡ºåŠ›å½¢çŠ¶: torch.Size([1, 1024])
ğŸŸ© Qwen3Attentionã®é †ä¼æ’­å®Œäº† output.shape=torch.Size([1, 1024])
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 1024]), residual.shape if residual is not None else None=torch.Size([1, 1024])
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨å®Œäº†
ğŸŸ© Qwen3MLPã®é †ä¼æ’­ x.shape=torch.Size([1, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­å®Œäº† result.shape=torch.Size([1, 6144])
ğŸŸ© SwiGLUã‚’é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 6144])
ğŸŸ¦ ãƒãƒ£ãƒ³ã‚¯åŒ– x.shape=torch.Size([1, 3072]), y.shape=torch.Size([1, 3072])
ğŸŸ© SwiGLUã‚’é †ä¼æ’­å®Œäº† result.shape=torch.Size([1, 3072])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­é–‹å§‹ å…¥åŠ›å½¢çŠ¶: torch.Size([1, 3072])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­å®Œäº† å‡ºåŠ›å½¢çŠ¶: torch.Size([1, 1024])
ğŸŸ© Qwen3MLPã®é †ä¼æ’­å®Œäº† x.shape=torch.Size([1, 1024])
ğŸŸ© Qwen3DecoderLayerã®é †ä¼æ’­å®Œäº† hidden_states.shape=torch.Size([1, 1024])
ğŸŸ© Qwen3DecoderLayerã®é †ä¼æ’­ positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 1024]) residual is None=False
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 1024]), residual.shape if residual is not None else None=torch.Size([1, 1024])
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨å®Œäº†
ğŸŸ© Qwen3Attentionã®é †ä¼æ’­ positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­å®Œäº† result.shape=torch.Size([1, 4096])
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 16, 128]), residual.shape if residual is not None else None=None
ğŸŸ© æ­£è¦åŒ–é–‹å§‹ x.shape=torch.Size([1, 16, 128])
ğŸŸ© æ­£è¦åŒ–å®Œäº† x.shape=torch.Size([1, 16, 128])
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 8, 128]), residual.shape if residual is not None else None=None
ğŸŸ© æ­£è¦åŒ–é–‹å§‹ x.shape=torch.Size([1, 8, 128])
ğŸŸ© æ­£è¦åŒ–å®Œäº† x.shape=torch.Size([1, 8, 128])
ğŸŸ© RoPEã®é †ä¼æ’­é–‹å§‹ positions.shape=torch.Size([1]), query.shape=torch.Size([1, 16, 128]), key.shape=torch.Size([1, 8, 128])
ğŸŸ© RoPEã‚’é©ç”¨é–‹å§‹ x.shape=torch.Size([1, 16, 128]), cos.shape=torch.Size([1, 1, 64]), sin.shape=torch.Size([1, 1, 64])
ğŸŸ¦ ãƒãƒ£ãƒ³ã‚¯åŒ– x1.shape=torch.Size([1, 16, 64]), x2.shape=torch.Size([1, 16, 64])
ğŸŸ¦ å›è»¢è¡Œåˆ—é©ç”¨ y1.shape=torch.Size([1, 16, 64]), y2.shape=torch.Size([1, 16, 64])
ğŸŸ© RoPEã‚’é©ç”¨å®Œäº† result.shape=torch.Size([1, 16, 128])
ğŸŸ© RoPEã‚’é©ç”¨é–‹å§‹ x.shape=torch.Size([1, 8, 128]), cos.shape=torch.Size([1, 1, 64]), sin.shape=torch.Size([1, 1, 64])
ğŸŸ¦ ãƒãƒ£ãƒ³ã‚¯åŒ– x1.shape=torch.Size([1, 8, 64]), x2.shape=torch.Size([1, 8, 64])
ğŸŸ¦ å›è»¢è¡Œåˆ—é©ç”¨ y1.shape=torch.Size([1, 8, 64]), y2.shape=torch.Size([1, 8, 64])
ğŸŸ© RoPEã‚’é©ç”¨å®Œäº† result.shape=torch.Size([1, 8, 128])
ğŸŸ© RoPEã®é †ä¼æ’­å®Œäº† query.shape=torch.Size([1, 16, 128]), key.shape=torch.Size([1, 8, 128])
ğŸŸ© Attentionå±¤ã®é †ä¼æ’­é–‹å§‹ q.shape=torch.Size([1, 16, 128]), k.shape=torch.Size([1, 8, 128]), v.shape=torch.Size([1, 8, 128])
ğŸŸ© ç¾åœ¨ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆå–å¾— _CONTEXT=Context(is_prefill=True, cu_seqlens_q=tensor([0, 1], device='cuda:0', dtype=torch.int32), cu_seqlens_k=tensor([0, 1], device='cuda:0', dtype=torch.int32), max_seqlen_q=1, max_seqlen_k=1, slot_mapping=tensor([0], device='cuda:0', dtype=torch.int32), context_lens=None, block_tables=None)
ğŸŸ¦ ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæƒ…å ±å–å¾— context.is_prefill=True, context.slot_mapping=tensor([0], device='cuda:0', dtype=torch.int32), context.max_seqlen_q=1, context.cu_seqlens_q=tensor([0, 1], device='cuda:0', dtype=torch.int32), context.max_seqlen_k=1, context.cu_seqlens_k=tensor([0, 1], device='cuda:0', dtype=torch.int32), context.context_lens=None, context.block_tables=None
ğŸŸ© KVã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ã‚­ãƒ¼ã¨ãƒãƒªãƒ¥ãƒ¼ã‚’ä¿å­˜é–‹å§‹ key.shape=torch.Size([1, 8, 128]), value.shape=torch.Size([1, 8, 128]), k_cache.shape=torch.Size([875, 256, 8, 128]), v_cache.shape=torch.Size([875, 256, 8, 128]), slot_mapping.shape=torch.Size([1])
ğŸŸ© KVã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ã‚­ãƒ¼ã¨ãƒãƒªãƒ¥ãƒ¼ã‚’ä¿å­˜å®Œäº†
ğŸŸ¦ Prefillãƒ¢ãƒ¼ãƒ‰ã§FlashAttentionã‚’å®Ÿè¡Œ context.max_seqlen_q=1, context.cu_seqlens_q=tensor([0, 1], device='cuda:0', dtype=torch.int32), context.max_seqlen_k=1, context.cu_seqlens_k=tensor([0, 1], device='cuda:0', dtype=torch.int32)
ğŸŸ© Attentionå±¤ã®é †ä¼æ’­å®Œäº† o.shape=torch.Size([1, 16, 128])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­é–‹å§‹ å…¥åŠ›å½¢çŠ¶: torch.Size([1, 2048])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­å®Œäº† å‡ºåŠ›å½¢çŠ¶: torch.Size([1, 1024])
ğŸŸ© Qwen3Attentionã®é †ä¼æ’­å®Œäº† output.shape=torch.Size([1, 1024])
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 1024]), residual.shape if residual is not None else None=torch.Size([1, 1024])
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨å®Œäº†
ğŸŸ© Qwen3MLPã®é †ä¼æ’­ x.shape=torch.Size([1, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­å®Œäº† result.shape=torch.Size([1, 6144])
ğŸŸ© SwiGLUã‚’é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 6144])
ğŸŸ¦ ãƒãƒ£ãƒ³ã‚¯åŒ– x.shape=torch.Size([1, 3072]), y.shape=torch.Size([1, 3072])
ğŸŸ© SwiGLUã‚’é †ä¼æ’­å®Œäº† result.shape=torch.Size([1, 3072])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­é–‹å§‹ å…¥åŠ›å½¢çŠ¶: torch.Size([1, 3072])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­å®Œäº† å‡ºåŠ›å½¢çŠ¶: torch.Size([1, 1024])
ğŸŸ© Qwen3MLPã®é †ä¼æ’­å®Œäº† x.shape=torch.Size([1, 1024])
ğŸŸ© Qwen3DecoderLayerã®é †ä¼æ’­å®Œäº† hidden_states.shape=torch.Size([1, 1024])
ğŸŸ© Qwen3DecoderLayerã®é †ä¼æ’­ positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 1024]) residual is None=False
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 1024]), residual.shape if residual is not None else None=torch.Size([1, 1024])
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨å®Œäº†
ğŸŸ© Qwen3Attentionã®é †ä¼æ’­ positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­å®Œäº† result.shape=torch.Size([1, 4096])
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 16, 128]), residual.shape if residual is not None else None=None
ğŸŸ© æ­£è¦åŒ–é–‹å§‹ x.shape=torch.Size([1, 16, 128])
ğŸŸ© æ­£è¦åŒ–å®Œäº† x.shape=torch.Size([1, 16, 128])
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 8, 128]), residual.shape if residual is not None else None=None
ğŸŸ© æ­£è¦åŒ–é–‹å§‹ x.shape=torch.Size([1, 8, 128])
ğŸŸ© æ­£è¦åŒ–å®Œäº† x.shape=torch.Size([1, 8, 128])
ğŸŸ© RoPEã®é †ä¼æ’­é–‹å§‹ positions.shape=torch.Size([1]), query.shape=torch.Size([1, 16, 128]), key.shape=torch.Size([1, 8, 128])
ğŸŸ© RoPEã‚’é©ç”¨é–‹å§‹ x.shape=torch.Size([1, 16, 128]), cos.shape=torch.Size([1, 1, 64]), sin.shape=torch.Size([1, 1, 64])
ğŸŸ¦ ãƒãƒ£ãƒ³ã‚¯åŒ– x1.shape=torch.Size([1, 16, 64]), x2.shape=torch.Size([1, 16, 64])
ğŸŸ¦ å›è»¢è¡Œåˆ—é©ç”¨ y1.shape=torch.Size([1, 16, 64]), y2.shape=torch.Size([1, 16, 64])
ğŸŸ© RoPEã‚’é©ç”¨å®Œäº† result.shape=torch.Size([1, 16, 128])
ğŸŸ© RoPEã‚’é©ç”¨é–‹å§‹ x.shape=torch.Size([1, 8, 128]), cos.shape=torch.Size([1, 1, 64]), sin.shape=torch.Size([1, 1, 64])
ğŸŸ¦ ãƒãƒ£ãƒ³ã‚¯åŒ– x1.shape=torch.Size([1, 8, 64]), x2.shape=torch.Size([1, 8, 64])
ğŸŸ¦ å›è»¢è¡Œåˆ—é©ç”¨ y1.shape=torch.Size([1, 8, 64]), y2.shape=torch.Size([1, 8, 64])
ğŸŸ© RoPEã‚’é©ç”¨å®Œäº† result.shape=torch.Size([1, 8, 128])
ğŸŸ© RoPEã®é †ä¼æ’­å®Œäº† query.shape=torch.Size([1, 16, 128]), key.shape=torch.Size([1, 8, 128])
ğŸŸ© Attentionå±¤ã®é †ä¼æ’­é–‹å§‹ q.shape=torch.Size([1, 16, 128]), k.shape=torch.Size([1, 8, 128]), v.shape=torch.Size([1, 8, 128])
ğŸŸ© ç¾åœ¨ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆå–å¾— _CONTEXT=Context(is_prefill=True, cu_seqlens_q=tensor([0, 1], device='cuda:0', dtype=torch.int32), cu_seqlens_k=tensor([0, 1], device='cuda:0', dtype=torch.int32), max_seqlen_q=1, max_seqlen_k=1, slot_mapping=tensor([0], device='cuda:0', dtype=torch.int32), context_lens=None, block_tables=None)
ğŸŸ¦ ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæƒ…å ±å–å¾— context.is_prefill=True, context.slot_mapping=tensor([0], device='cuda:0', dtype=torch.int32), context.max_seqlen_q=1, context.cu_seqlens_q=tensor([0, 1], device='cuda:0', dtype=torch.int32), context.max_seqlen_k=1, context.cu_seqlens_k=tensor([0, 1], device='cuda:0', dtype=torch.int32), context.context_lens=None, context.block_tables=None
ğŸŸ© KVã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ã‚­ãƒ¼ã¨ãƒãƒªãƒ¥ãƒ¼ã‚’ä¿å­˜é–‹å§‹ key.shape=torch.Size([1, 8, 128]), value.shape=torch.Size([1, 8, 128]), k_cache.shape=torch.Size([875, 256, 8, 128]), v_cache.shape=torch.Size([875, 256, 8, 128]), slot_mapping.shape=torch.Size([1])
ğŸŸ© KVã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ã‚­ãƒ¼ã¨ãƒãƒªãƒ¥ãƒ¼ã‚’ä¿å­˜å®Œäº†
ğŸŸ¦ Prefillãƒ¢ãƒ¼ãƒ‰ã§FlashAttentionã‚’å®Ÿè¡Œ context.max_seqlen_q=1, context.cu_seqlens_q=tensor([0, 1], device='cuda:0', dtype=torch.int32), context.max_seqlen_k=1, context.cu_seqlens_k=tensor([0, 1], device='cuda:0', dtype=torch.int32)
ğŸŸ© Attentionå±¤ã®é †ä¼æ’­å®Œäº† o.shape=torch.Size([1, 16, 128])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­é–‹å§‹ å…¥åŠ›å½¢çŠ¶: torch.Size([1, 2048])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­å®Œäº† å‡ºåŠ›å½¢çŠ¶: torch.Size([1, 1024])
ğŸŸ© Qwen3Attentionã®é †ä¼æ’­å®Œäº† output.shape=torch.Size([1, 1024])
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 1024]), residual.shape if residual is not None else None=torch.Size([1, 1024])
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨å®Œäº†
ğŸŸ© Qwen3MLPã®é †ä¼æ’­ x.shape=torch.Size([1, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­å®Œäº† result.shape=torch.Size([1, 6144])
ğŸŸ© SwiGLUã‚’é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 6144])
ğŸŸ¦ ãƒãƒ£ãƒ³ã‚¯åŒ– x.shape=torch.Size([1, 3072]), y.shape=torch.Size([1, 3072])
ğŸŸ© SwiGLUã‚’é †ä¼æ’­å®Œäº† result.shape=torch.Size([1, 3072])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­é–‹å§‹ å…¥åŠ›å½¢çŠ¶: torch.Size([1, 3072])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­å®Œäº† å‡ºåŠ›å½¢çŠ¶: torch.Size([1, 1024])
ğŸŸ© Qwen3MLPã®é †ä¼æ’­å®Œäº† x.shape=torch.Size([1, 1024])
ğŸŸ© Qwen3DecoderLayerã®é †ä¼æ’­å®Œäº† hidden_states.shape=torch.Size([1, 1024])
ğŸŸ© Qwen3DecoderLayerã®é †ä¼æ’­ positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 1024]) residual is None=False
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 1024]), residual.shape if residual is not None else None=torch.Size([1, 1024])
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨å®Œäº†
ğŸŸ© Qwen3Attentionã®é †ä¼æ’­ positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­å®Œäº† result.shape=torch.Size([1, 4096])
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 16, 128]), residual.shape if residual is not None else None=None
ğŸŸ© æ­£è¦åŒ–é–‹å§‹ x.shape=torch.Size([1, 16, 128])
ğŸŸ© æ­£è¦åŒ–å®Œäº† x.shape=torch.Size([1, 16, 128])
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 8, 128]), residual.shape if residual is not None else None=None
ğŸŸ© æ­£è¦åŒ–é–‹å§‹ x.shape=torch.Size([1, 8, 128])
ğŸŸ© æ­£è¦åŒ–å®Œäº† x.shape=torch.Size([1, 8, 128])
ğŸŸ© RoPEã®é †ä¼æ’­é–‹å§‹ positions.shape=torch.Size([1]), query.shape=torch.Size([1, 16, 128]), key.shape=torch.Size([1, 8, 128])
ğŸŸ© RoPEã‚’é©ç”¨é–‹å§‹ x.shape=torch.Size([1, 16, 128]), cos.shape=torch.Size([1, 1, 64]), sin.shape=torch.Size([1, 1, 64])
ğŸŸ¦ ãƒãƒ£ãƒ³ã‚¯åŒ– x1.shape=torch.Size([1, 16, 64]), x2.shape=torch.Size([1, 16, 64])
ğŸŸ¦ å›è»¢è¡Œåˆ—é©ç”¨ y1.shape=torch.Size([1, 16, 64]), y2.shape=torch.Size([1, 16, 64])
ğŸŸ© RoPEã‚’é©ç”¨å®Œäº† result.shape=torch.Size([1, 16, 128])
ğŸŸ© RoPEã‚’é©ç”¨é–‹å§‹ x.shape=torch.Size([1, 8, 128]), cos.shape=torch.Size([1, 1, 64]), sin.shape=torch.Size([1, 1, 64])
ğŸŸ¦ ãƒãƒ£ãƒ³ã‚¯åŒ– x1.shape=torch.Size([1, 8, 64]), x2.shape=torch.Size([1, 8, 64])
ğŸŸ¦ å›è»¢è¡Œåˆ—é©ç”¨ y1.shape=torch.Size([1, 8, 64]), y2.shape=torch.Size([1, 8, 64])
ğŸŸ© RoPEã‚’é©ç”¨å®Œäº† result.shape=torch.Size([1, 8, 128])
ğŸŸ© RoPEã®é †ä¼æ’­å®Œäº† query.shape=torch.Size([1, 16, 128]), key.shape=torch.Size([1, 8, 128])
ğŸŸ© Attentionå±¤ã®é †ä¼æ’­é–‹å§‹ q.shape=torch.Size([1, 16, 128]), k.shape=torch.Size([1, 8, 128]), v.shape=torch.Size([1, 8, 128])
ğŸŸ© ç¾åœ¨ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆå–å¾— _CONTEXT=Context(is_prefill=True, cu_seqlens_q=tensor([0, 1], device='cuda:0', dtype=torch.int32), cu_seqlens_k=tensor([0, 1], device='cuda:0', dtype=torch.int32), max_seqlen_q=1, max_seqlen_k=1, slot_mapping=tensor([0], device='cuda:0', dtype=torch.int32), context_lens=None, block_tables=None)
ğŸŸ¦ ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæƒ…å ±å–å¾— context.is_prefill=True, context.slot_mapping=tensor([0], device='cuda:0', dtype=torch.int32), context.max_seqlen_q=1, context.cu_seqlens_q=tensor([0, 1], device='cuda:0', dtype=torch.int32), context.max_seqlen_k=1, context.cu_seqlens_k=tensor([0, 1], device='cuda:0', dtype=torch.int32), context.context_lens=None, context.block_tables=None
ğŸŸ© KVã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ã‚­ãƒ¼ã¨ãƒãƒªãƒ¥ãƒ¼ã‚’ä¿å­˜é–‹å§‹ key.shape=torch.Size([1, 8, 128]), value.shape=torch.Size([1, 8, 128]), k_cache.shape=torch.Size([875, 256, 8, 128]), v_cache.shape=torch.Size([875, 256, 8, 128]), slot_mapping.shape=torch.Size([1])
ğŸŸ© KVã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ã‚­ãƒ¼ã¨ãƒãƒªãƒ¥ãƒ¼ã‚’ä¿å­˜å®Œäº†
ğŸŸ¦ Prefillãƒ¢ãƒ¼ãƒ‰ã§FlashAttentionã‚’å®Ÿè¡Œ context.max_seqlen_q=1, context.cu_seqlens_q=tensor([0, 1], device='cuda:0', dtype=torch.int32), context.max_seqlen_k=1, context.cu_seqlens_k=tensor([0, 1], device='cuda:0', dtype=torch.int32)
ğŸŸ© Attentionå±¤ã®é †ä¼æ’­å®Œäº† o.shape=torch.Size([1, 16, 128])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­é–‹å§‹ å…¥åŠ›å½¢çŠ¶: torch.Size([1, 2048])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­å®Œäº† å‡ºåŠ›å½¢çŠ¶: torch.Size([1, 1024])
ğŸŸ© Qwen3Attentionã®é †ä¼æ’­å®Œäº† output.shape=torch.Size([1, 1024])
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 1024]), residual.shape if residual is not None else None=torch.Size([1, 1024])
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨å®Œäº†
ğŸŸ© Qwen3MLPã®é †ä¼æ’­ x.shape=torch.Size([1, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­å®Œäº† result.shape=torch.Size([1, 6144])
ğŸŸ© SwiGLUã‚’é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 6144])
ğŸŸ¦ ãƒãƒ£ãƒ³ã‚¯åŒ– x.shape=torch.Size([1, 3072]), y.shape=torch.Size([1, 3072])
ğŸŸ© SwiGLUã‚’é †ä¼æ’­å®Œäº† result.shape=torch.Size([1, 3072])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­é–‹å§‹ å…¥åŠ›å½¢çŠ¶: torch.Size([1, 3072])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­å®Œäº† å‡ºåŠ›å½¢çŠ¶: torch.Size([1, 1024])
ğŸŸ© Qwen3MLPã®é †ä¼æ’­å®Œäº† x.shape=torch.Size([1, 1024])
ğŸŸ© Qwen3DecoderLayerã®é †ä¼æ’­å®Œäº† hidden_states.shape=torch.Size([1, 1024])
ğŸŸ© Qwen3DecoderLayerã®é †ä¼æ’­ positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 1024]) residual is None=False
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 1024]), residual.shape if residual is not None else None=torch.Size([1, 1024])
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨å®Œäº†
ğŸŸ© Qwen3Attentionã®é †ä¼æ’­ positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­å®Œäº† result.shape=torch.Size([1, 4096])
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 16, 128]), residual.shape if residual is not None else None=None
ğŸŸ© æ­£è¦åŒ–é–‹å§‹ x.shape=torch.Size([1, 16, 128])
ğŸŸ© æ­£è¦åŒ–å®Œäº† x.shape=torch.Size([1, 16, 128])
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 8, 128]), residual.shape if residual is not None else None=None
ğŸŸ© æ­£è¦åŒ–é–‹å§‹ x.shape=torch.Size([1, 8, 128])
ğŸŸ© æ­£è¦åŒ–å®Œäº† x.shape=torch.Size([1, 8, 128])
ğŸŸ© RoPEã®é †ä¼æ’­é–‹å§‹ positions.shape=torch.Size([1]), query.shape=torch.Size([1, 16, 128]), key.shape=torch.Size([1, 8, 128])
ğŸŸ© RoPEã‚’é©ç”¨é–‹å§‹ x.shape=torch.Size([1, 16, 128]), cos.shape=torch.Size([1, 1, 64]), sin.shape=torch.Size([1, 1, 64])
ğŸŸ¦ ãƒãƒ£ãƒ³ã‚¯åŒ– x1.shape=torch.Size([1, 16, 64]), x2.shape=torch.Size([1, 16, 64])
ğŸŸ¦ å›è»¢è¡Œåˆ—é©ç”¨ y1.shape=torch.Size([1, 16, 64]), y2.shape=torch.Size([1, 16, 64])
ğŸŸ© RoPEã‚’é©ç”¨å®Œäº† result.shape=torch.Size([1, 16, 128])
ğŸŸ© RoPEã‚’é©ç”¨é–‹å§‹ x.shape=torch.Size([1, 8, 128]), cos.shape=torch.Size([1, 1, 64]), sin.shape=torch.Size([1, 1, 64])
ğŸŸ¦ ãƒãƒ£ãƒ³ã‚¯åŒ– x1.shape=torch.Size([1, 8, 64]), x2.shape=torch.Size([1, 8, 64])
ğŸŸ¦ å›è»¢è¡Œåˆ—é©ç”¨ y1.shape=torch.Size([1, 8, 64]), y2.shape=torch.Size([1, 8, 64])
ğŸŸ© RoPEã‚’é©ç”¨å®Œäº† result.shape=torch.Size([1, 8, 128])
ğŸŸ© RoPEã®é †ä¼æ’­å®Œäº† query.shape=torch.Size([1, 16, 128]), key.shape=torch.Size([1, 8, 128])
ğŸŸ© Attentionå±¤ã®é †ä¼æ’­é–‹å§‹ q.shape=torch.Size([1, 16, 128]), k.shape=torch.Size([1, 8, 128]), v.shape=torch.Size([1, 8, 128])
ğŸŸ© ç¾åœ¨ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆå–å¾— _CONTEXT=Context(is_prefill=True, cu_seqlens_q=tensor([0, 1], device='cuda:0', dtype=torch.int32), cu_seqlens_k=tensor([0, 1], device='cuda:0', dtype=torch.int32), max_seqlen_q=1, max_seqlen_k=1, slot_mapping=tensor([0], device='cuda:0', dtype=torch.int32), context_lens=None, block_tables=None)
ğŸŸ¦ ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæƒ…å ±å–å¾— context.is_prefill=True, context.slot_mapping=tensor([0], device='cuda:0', dtype=torch.int32), context.max_seqlen_q=1, context.cu_seqlens_q=tensor([0, 1], device='cuda:0', dtype=torch.int32), context.max_seqlen_k=1, context.cu_seqlens_k=tensor([0, 1], device='cuda:0', dtype=torch.int32), context.context_lens=None, context.block_tables=None
ğŸŸ© KVã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ã‚­ãƒ¼ã¨ãƒãƒªãƒ¥ãƒ¼ã‚’ä¿å­˜é–‹å§‹ key.shape=torch.Size([1, 8, 128]), value.shape=torch.Size([1, 8, 128]), k_cache.shape=torch.Size([875, 256, 8, 128]), v_cache.shape=torch.Size([875, 256, 8, 128]), slot_mapping.shape=torch.Size([1])
ğŸŸ© KVã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ã‚­ãƒ¼ã¨ãƒãƒªãƒ¥ãƒ¼ã‚’ä¿å­˜å®Œäº†
ğŸŸ¦ Prefillãƒ¢ãƒ¼ãƒ‰ã§FlashAttentionã‚’å®Ÿè¡Œ context.max_seqlen_q=1, context.cu_seqlens_q=tensor([0, 1], device='cuda:0', dtype=torch.int32), context.max_seqlen_k=1, context.cu_seqlens_k=tensor([0, 1], device='cuda:0', dtype=torch.int32)
ğŸŸ© Attentionå±¤ã®é †ä¼æ’­å®Œäº† o.shape=torch.Size([1, 16, 128])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­é–‹å§‹ å…¥åŠ›å½¢çŠ¶: torch.Size([1, 2048])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­å®Œäº† å‡ºåŠ›å½¢çŠ¶: torch.Size([1, 1024])
ğŸŸ© Qwen3Attentionã®é †ä¼æ’­å®Œäº† output.shape=torch.Size([1, 1024])
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 1024]), residual.shape if residual is not None else None=torch.Size([1, 1024])
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨å®Œäº†
ğŸŸ© Qwen3MLPã®é †ä¼æ’­ x.shape=torch.Size([1, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­å®Œäº† result.shape=torch.Size([1, 6144])
ğŸŸ© SwiGLUã‚’é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 6144])
ğŸŸ¦ ãƒãƒ£ãƒ³ã‚¯åŒ– x.shape=torch.Size([1, 3072]), y.shape=torch.Size([1, 3072])
ğŸŸ© SwiGLUã‚’é †ä¼æ’­å®Œäº† result.shape=torch.Size([1, 3072])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­é–‹å§‹ å…¥åŠ›å½¢çŠ¶: torch.Size([1, 3072])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­å®Œäº† å‡ºåŠ›å½¢çŠ¶: torch.Size([1, 1024])
ğŸŸ© Qwen3MLPã®é †ä¼æ’­å®Œäº† x.shape=torch.Size([1, 1024])
ğŸŸ© Qwen3DecoderLayerã®é †ä¼æ’­å®Œäº† hidden_states.shape=torch.Size([1, 1024])
ğŸŸ© Qwen3DecoderLayerã®é †ä¼æ’­ positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 1024]) residual is None=False
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 1024]), residual.shape if residual is not None else None=torch.Size([1, 1024])
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨å®Œäº†
ğŸŸ© Qwen3Attentionã®é †ä¼æ’­ positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­å®Œäº† result.shape=torch.Size([1, 4096])
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 16, 128]), residual.shape if residual is not None else None=None
ğŸŸ© æ­£è¦åŒ–é–‹å§‹ x.shape=torch.Size([1, 16, 128])
ğŸŸ© æ­£è¦åŒ–å®Œäº† x.shape=torch.Size([1, 16, 128])
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 8, 128]), residual.shape if residual is not None else None=None
ğŸŸ© æ­£è¦åŒ–é–‹å§‹ x.shape=torch.Size([1, 8, 128])
ğŸŸ© æ­£è¦åŒ–å®Œäº† x.shape=torch.Size([1, 8, 128])
ğŸŸ© RoPEã®é †ä¼æ’­é–‹å§‹ positions.shape=torch.Size([1]), query.shape=torch.Size([1, 16, 128]), key.shape=torch.Size([1, 8, 128])
ğŸŸ© RoPEã‚’é©ç”¨é–‹å§‹ x.shape=torch.Size([1, 16, 128]), cos.shape=torch.Size([1, 1, 64]), sin.shape=torch.Size([1, 1, 64])
ğŸŸ¦ ãƒãƒ£ãƒ³ã‚¯åŒ– x1.shape=torch.Size([1, 16, 64]), x2.shape=torch.Size([1, 16, 64])
ğŸŸ¦ å›è»¢è¡Œåˆ—é©ç”¨ y1.shape=torch.Size([1, 16, 64]), y2.shape=torch.Size([1, 16, 64])
ğŸŸ© RoPEã‚’é©ç”¨å®Œäº† result.shape=torch.Size([1, 16, 128])
ğŸŸ© RoPEã‚’é©ç”¨é–‹å§‹ x.shape=torch.Size([1, 8, 128]), cos.shape=torch.Size([1, 1, 64]), sin.shape=torch.Size([1, 1, 64])
ğŸŸ¦ ãƒãƒ£ãƒ³ã‚¯åŒ– x1.shape=torch.Size([1, 8, 64]), x2.shape=torch.Size([1, 8, 64])
ğŸŸ¦ å›è»¢è¡Œåˆ—é©ç”¨ y1.shape=torch.Size([1, 8, 64]), y2.shape=torch.Size([1, 8, 64])
ğŸŸ© RoPEã‚’é©ç”¨å®Œäº† result.shape=torch.Size([1, 8, 128])
ğŸŸ© RoPEã®é †ä¼æ’­å®Œäº† query.shape=torch.Size([1, 16, 128]), key.shape=torch.Size([1, 8, 128])
ğŸŸ© Attentionå±¤ã®é †ä¼æ’­é–‹å§‹ q.shape=torch.Size([1, 16, 128]), k.shape=torch.Size([1, 8, 128]), v.shape=torch.Size([1, 8, 128])
ğŸŸ© ç¾åœ¨ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆå–å¾— _CONTEXT=Context(is_prefill=True, cu_seqlens_q=tensor([0, 1], device='cuda:0', dtype=torch.int32), cu_seqlens_k=tensor([0, 1], device='cuda:0', dtype=torch.int32), max_seqlen_q=1, max_seqlen_k=1, slot_mapping=tensor([0], device='cuda:0', dtype=torch.int32), context_lens=None, block_tables=None)
ğŸŸ¦ ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæƒ…å ±å–å¾— context.is_prefill=True, context.slot_mapping=tensor([0], device='cuda:0', dtype=torch.int32), context.max_seqlen_q=1, context.cu_seqlens_q=tensor([0, 1], device='cuda:0', dtype=torch.int32), context.max_seqlen_k=1, context.cu_seqlens_k=tensor([0, 1], device='cuda:0', dtype=torch.int32), context.context_lens=None, context.block_tables=None
ğŸŸ© KVã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ã‚­ãƒ¼ã¨ãƒãƒªãƒ¥ãƒ¼ã‚’ä¿å­˜é–‹å§‹ key.shape=torch.Size([1, 8, 128]), value.shape=torch.Size([1, 8, 128]), k_cache.shape=torch.Size([875, 256, 8, 128]), v_cache.shape=torch.Size([875, 256, 8, 128]), slot_mapping.shape=torch.Size([1])
ğŸŸ© KVã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ã‚­ãƒ¼ã¨ãƒãƒªãƒ¥ãƒ¼ã‚’ä¿å­˜å®Œäº†
ğŸŸ¦ Prefillãƒ¢ãƒ¼ãƒ‰ã§FlashAttentionã‚’å®Ÿè¡Œ context.max_seqlen_q=1, context.cu_seqlens_q=tensor([0, 1], device='cuda:0', dtype=torch.int32), context.max_seqlen_k=1, context.cu_seqlens_k=tensor([0, 1], device='cuda:0', dtype=torch.int32)
ğŸŸ© Attentionå±¤ã®é †ä¼æ’­å®Œäº† o.shape=torch.Size([1, 16, 128])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­é–‹å§‹ å…¥åŠ›å½¢çŠ¶: torch.Size([1, 2048])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­å®Œäº† å‡ºåŠ›å½¢çŠ¶: torch.Size([1, 1024])
ğŸŸ© Qwen3Attentionã®é †ä¼æ’­å®Œäº† output.shape=torch.Size([1, 1024])
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 1024]), residual.shape if residual is not None else None=torch.Size([1, 1024])
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨å®Œäº†
ğŸŸ© Qwen3MLPã®é †ä¼æ’­ x.shape=torch.Size([1, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­å®Œäº† result.shape=torch.Size([1, 6144])
ğŸŸ© SwiGLUã‚’é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 6144])
ğŸŸ¦ ãƒãƒ£ãƒ³ã‚¯åŒ– x.shape=torch.Size([1, 3072]), y.shape=torch.Size([1, 3072])
ğŸŸ© SwiGLUã‚’é †ä¼æ’­å®Œäº† result.shape=torch.Size([1, 3072])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­é–‹å§‹ å…¥åŠ›å½¢çŠ¶: torch.Size([1, 3072])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­å®Œäº† å‡ºåŠ›å½¢çŠ¶: torch.Size([1, 1024])
ğŸŸ© Qwen3MLPã®é †ä¼æ’­å®Œäº† x.shape=torch.Size([1, 1024])
ğŸŸ© Qwen3DecoderLayerã®é †ä¼æ’­å®Œäº† hidden_states.shape=torch.Size([1, 1024])
ğŸŸ© Qwen3DecoderLayerã®é †ä¼æ’­ positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 1024]) residual is None=False
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 1024]), residual.shape if residual is not None else None=torch.Size([1, 1024])
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨å®Œäº†
ğŸŸ© Qwen3Attentionã®é †ä¼æ’­ positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­å®Œäº† result.shape=torch.Size([1, 4096])
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 16, 128]), residual.shape if residual is not None else None=None
ğŸŸ© æ­£è¦åŒ–é–‹å§‹ x.shape=torch.Size([1, 16, 128])
ğŸŸ© æ­£è¦åŒ–å®Œäº† x.shape=torch.Size([1, 16, 128])
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 8, 128]), residual.shape if residual is not None else None=None
ğŸŸ© æ­£è¦åŒ–é–‹å§‹ x.shape=torch.Size([1, 8, 128])
ğŸŸ© æ­£è¦åŒ–å®Œäº† x.shape=torch.Size([1, 8, 128])
ğŸŸ© RoPEã®é †ä¼æ’­é–‹å§‹ positions.shape=torch.Size([1]), query.shape=torch.Size([1, 16, 128]), key.shape=torch.Size([1, 8, 128])
ğŸŸ© RoPEã‚’é©ç”¨é–‹å§‹ x.shape=torch.Size([1, 16, 128]), cos.shape=torch.Size([1, 1, 64]), sin.shape=torch.Size([1, 1, 64])
ğŸŸ¦ ãƒãƒ£ãƒ³ã‚¯åŒ– x1.shape=torch.Size([1, 16, 64]), x2.shape=torch.Size([1, 16, 64])
ğŸŸ¦ å›è»¢è¡Œåˆ—é©ç”¨ y1.shape=torch.Size([1, 16, 64]), y2.shape=torch.Size([1, 16, 64])
ğŸŸ© RoPEã‚’é©ç”¨å®Œäº† result.shape=torch.Size([1, 16, 128])
ğŸŸ© RoPEã‚’é©ç”¨é–‹å§‹ x.shape=torch.Size([1, 8, 128]), cos.shape=torch.Size([1, 1, 64]), sin.shape=torch.Size([1, 1, 64])
ğŸŸ¦ ãƒãƒ£ãƒ³ã‚¯åŒ– x1.shape=torch.Size([1, 8, 64]), x2.shape=torch.Size([1, 8, 64])
ğŸŸ¦ å›è»¢è¡Œåˆ—é©ç”¨ y1.shape=torch.Size([1, 8, 64]), y2.shape=torch.Size([1, 8, 64])
ğŸŸ© RoPEã‚’é©ç”¨å®Œäº† result.shape=torch.Size([1, 8, 128])
ğŸŸ© RoPEã®é †ä¼æ’­å®Œäº† query.shape=torch.Size([1, 16, 128]), key.shape=torch.Size([1, 8, 128])
ğŸŸ© Attentionå±¤ã®é †ä¼æ’­é–‹å§‹ q.shape=torch.Size([1, 16, 128]), k.shape=torch.Size([1, 8, 128]), v.shape=torch.Size([1, 8, 128])
ğŸŸ© ç¾åœ¨ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆå–å¾— _CONTEXT=Context(is_prefill=True, cu_seqlens_q=tensor([0, 1], device='cuda:0', dtype=torch.int32), cu_seqlens_k=tensor([0, 1], device='cuda:0', dtype=torch.int32), max_seqlen_q=1, max_seqlen_k=1, slot_mapping=tensor([0], device='cuda:0', dtype=torch.int32), context_lens=None, block_tables=None)
ğŸŸ¦ ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæƒ…å ±å–å¾— context.is_prefill=True, context.slot_mapping=tensor([0], device='cuda:0', dtype=torch.int32), context.max_seqlen_q=1, context.cu_seqlens_q=tensor([0, 1], device='cuda:0', dtype=torch.int32), context.max_seqlen_k=1, context.cu_seqlens_k=tensor([0, 1], device='cuda:0', dtype=torch.int32), context.context_lens=None, context.block_tables=None
ğŸŸ© KVã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ã‚­ãƒ¼ã¨ãƒãƒªãƒ¥ãƒ¼ã‚’ä¿å­˜é–‹å§‹ key.shape=torch.Size([1, 8, 128]), value.shape=torch.Size([1, 8, 128]), k_cache.shape=torch.Size([875, 256, 8, 128]), v_cache.shape=torch.Size([875, 256, 8, 128]), slot_mapping.shape=torch.Size([1])
ğŸŸ© KVã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ã‚­ãƒ¼ã¨ãƒãƒªãƒ¥ãƒ¼ã‚’ä¿å­˜å®Œäº†
ğŸŸ¦ Prefillãƒ¢ãƒ¼ãƒ‰ã§FlashAttentionã‚’å®Ÿè¡Œ context.max_seqlen_q=1, context.cu_seqlens_q=tensor([0, 1], device='cuda:0', dtype=torch.int32), context.max_seqlen_k=1, context.cu_seqlens_k=tensor([0, 1], device='cuda:0', dtype=torch.int32)
ğŸŸ© Attentionå±¤ã®é †ä¼æ’­å®Œäº† o.shape=torch.Size([1, 16, 128])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­é–‹å§‹ å…¥åŠ›å½¢çŠ¶: torch.Size([1, 2048])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­å®Œäº† å‡ºåŠ›å½¢çŠ¶: torch.Size([1, 1024])
ğŸŸ© Qwen3Attentionã®é †ä¼æ’­å®Œäº† output.shape=torch.Size([1, 1024])
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 1024]), residual.shape if residual is not None else None=torch.Size([1, 1024])
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨å®Œäº†
ğŸŸ© Qwen3MLPã®é †ä¼æ’­ x.shape=torch.Size([1, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­å®Œäº† result.shape=torch.Size([1, 6144])
ğŸŸ© SwiGLUã‚’é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 6144])
ğŸŸ¦ ãƒãƒ£ãƒ³ã‚¯åŒ– x.shape=torch.Size([1, 3072]), y.shape=torch.Size([1, 3072])
ğŸŸ© SwiGLUã‚’é †ä¼æ’­å®Œäº† result.shape=torch.Size([1, 3072])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­é–‹å§‹ å…¥åŠ›å½¢çŠ¶: torch.Size([1, 3072])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­å®Œäº† å‡ºåŠ›å½¢çŠ¶: torch.Size([1, 1024])
ğŸŸ© Qwen3MLPã®é †ä¼æ’­å®Œäº† x.shape=torch.Size([1, 1024])
ğŸŸ© Qwen3DecoderLayerã®é †ä¼æ’­å®Œäº† hidden_states.shape=torch.Size([1, 1024])
ğŸŸ© Qwen3DecoderLayerã®é †ä¼æ’­ positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 1024]) residual is None=False
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 1024]), residual.shape if residual is not None else None=torch.Size([1, 1024])
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨å®Œäº†
ğŸŸ© Qwen3Attentionã®é †ä¼æ’­ positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­å®Œäº† result.shape=torch.Size([1, 4096])
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 16, 128]), residual.shape if residual is not None else None=None
ğŸŸ© æ­£è¦åŒ–é–‹å§‹ x.shape=torch.Size([1, 16, 128])
ğŸŸ© æ­£è¦åŒ–å®Œäº† x.shape=torch.Size([1, 16, 128])
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 8, 128]), residual.shape if residual is not None else None=None
ğŸŸ© æ­£è¦åŒ–é–‹å§‹ x.shape=torch.Size([1, 8, 128])
ğŸŸ© æ­£è¦åŒ–å®Œäº† x.shape=torch.Size([1, 8, 128])
ğŸŸ© RoPEã®é †ä¼æ’­é–‹å§‹ positions.shape=torch.Size([1]), query.shape=torch.Size([1, 16, 128]), key.shape=torch.Size([1, 8, 128])
ğŸŸ© RoPEã‚’é©ç”¨é–‹å§‹ x.shape=torch.Size([1, 16, 128]), cos.shape=torch.Size([1, 1, 64]), sin.shape=torch.Size([1, 1, 64])
ğŸŸ¦ ãƒãƒ£ãƒ³ã‚¯åŒ– x1.shape=torch.Size([1, 16, 64]), x2.shape=torch.Size([1, 16, 64])
ğŸŸ¦ å›è»¢è¡Œåˆ—é©ç”¨ y1.shape=torch.Size([1, 16, 64]), y2.shape=torch.Size([1, 16, 64])
ğŸŸ© RoPEã‚’é©ç”¨å®Œäº† result.shape=torch.Size([1, 16, 128])
ğŸŸ© RoPEã‚’é©ç”¨é–‹å§‹ x.shape=torch.Size([1, 8, 128]), cos.shape=torch.Size([1, 1, 64]), sin.shape=torch.Size([1, 1, 64])
ğŸŸ¦ ãƒãƒ£ãƒ³ã‚¯åŒ– x1.shape=torch.Size([1, 8, 64]), x2.shape=torch.Size([1, 8, 64])
ğŸŸ¦ å›è»¢è¡Œåˆ—é©ç”¨ y1.shape=torch.Size([1, 8, 64]), y2.shape=torch.Size([1, 8, 64])
ğŸŸ© RoPEã‚’é©ç”¨å®Œäº† result.shape=torch.Size([1, 8, 128])
ğŸŸ© RoPEã®é †ä¼æ’­å®Œäº† query.shape=torch.Size([1, 16, 128]), key.shape=torch.Size([1, 8, 128])
ğŸŸ© Attentionå±¤ã®é †ä¼æ’­é–‹å§‹ q.shape=torch.Size([1, 16, 128]), k.shape=torch.Size([1, 8, 128]), v.shape=torch.Size([1, 8, 128])
ğŸŸ© ç¾åœ¨ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆå–å¾— _CONTEXT=Context(is_prefill=True, cu_seqlens_q=tensor([0, 1], device='cuda:0', dtype=torch.int32), cu_seqlens_k=tensor([0, 1], device='cuda:0', dtype=torch.int32), max_seqlen_q=1, max_seqlen_k=1, slot_mapping=tensor([0], device='cuda:0', dtype=torch.int32), context_lens=None, block_tables=None)
ğŸŸ¦ ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæƒ…å ±å–å¾— context.is_prefill=True, context.slot_mapping=tensor([0], device='cuda:0', dtype=torch.int32), context.max_seqlen_q=1, context.cu_seqlens_q=tensor([0, 1], device='cuda:0', dtype=torch.int32), context.max_seqlen_k=1, context.cu_seqlens_k=tensor([0, 1], device='cuda:0', dtype=torch.int32), context.context_lens=None, context.block_tables=None
ğŸŸ© KVã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ã‚­ãƒ¼ã¨ãƒãƒªãƒ¥ãƒ¼ã‚’ä¿å­˜é–‹å§‹ key.shape=torch.Size([1, 8, 128]), value.shape=torch.Size([1, 8, 128]), k_cache.shape=torch.Size([875, 256, 8, 128]), v_cache.shape=torch.Size([875, 256, 8, 128]), slot_mapping.shape=torch.Size([1])
ğŸŸ© KVã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ã‚­ãƒ¼ã¨ãƒãƒªãƒ¥ãƒ¼ã‚’ä¿å­˜å®Œäº†
ğŸŸ¦ Prefillãƒ¢ãƒ¼ãƒ‰ã§FlashAttentionã‚’å®Ÿè¡Œ context.max_seqlen_q=1, context.cu_seqlens_q=tensor([0, 1], device='cuda:0', dtype=torch.int32), context.max_seqlen_k=1, context.cu_seqlens_k=tensor([0, 1], device='cuda:0', dtype=torch.int32)
ğŸŸ© Attentionå±¤ã®é †ä¼æ’­å®Œäº† o.shape=torch.Size([1, 16, 128])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­é–‹å§‹ å…¥åŠ›å½¢çŠ¶: torch.Size([1, 2048])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­å®Œäº† å‡ºåŠ›å½¢çŠ¶: torch.Size([1, 1024])
ğŸŸ© Qwen3Attentionã®é †ä¼æ’­å®Œäº† output.shape=torch.Size([1, 1024])
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 1024]), residual.shape if residual is not None else None=torch.Size([1, 1024])
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨å®Œäº†
ğŸŸ© Qwen3MLPã®é †ä¼æ’­ x.shape=torch.Size([1, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­å®Œäº† result.shape=torch.Size([1, 6144])
ğŸŸ© SwiGLUã‚’é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 6144])
ğŸŸ¦ ãƒãƒ£ãƒ³ã‚¯åŒ– x.shape=torch.Size([1, 3072]), y.shape=torch.Size([1, 3072])
ğŸŸ© SwiGLUã‚’é †ä¼æ’­å®Œäº† result.shape=torch.Size([1, 3072])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­é–‹å§‹ å…¥åŠ›å½¢çŠ¶: torch.Size([1, 3072])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­å®Œäº† å‡ºåŠ›å½¢çŠ¶: torch.Size([1, 1024])
ğŸŸ© Qwen3MLPã®é †ä¼æ’­å®Œäº† x.shape=torch.Size([1, 1024])
ğŸŸ© Qwen3DecoderLayerã®é †ä¼æ’­å®Œäº† hidden_states.shape=torch.Size([1, 1024])
ğŸŸ© Qwen3DecoderLayerã®é †ä¼æ’­ positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 1024]) residual is None=False
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 1024]), residual.shape if residual is not None else None=torch.Size([1, 1024])
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨å®Œäº†
ğŸŸ© Qwen3Attentionã®é †ä¼æ’­ positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­å®Œäº† result.shape=torch.Size([1, 4096])
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 16, 128]), residual.shape if residual is not None else None=None
ğŸŸ© æ­£è¦åŒ–é–‹å§‹ x.shape=torch.Size([1, 16, 128])
ğŸŸ© æ­£è¦åŒ–å®Œäº† x.shape=torch.Size([1, 16, 128])
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 8, 128]), residual.shape if residual is not None else None=None
ğŸŸ© æ­£è¦åŒ–é–‹å§‹ x.shape=torch.Size([1, 8, 128])
ğŸŸ© æ­£è¦åŒ–å®Œäº† x.shape=torch.Size([1, 8, 128])
ğŸŸ© RoPEã®é †ä¼æ’­é–‹å§‹ positions.shape=torch.Size([1]), query.shape=torch.Size([1, 16, 128]), key.shape=torch.Size([1, 8, 128])
ğŸŸ© RoPEã‚’é©ç”¨é–‹å§‹ x.shape=torch.Size([1, 16, 128]), cos.shape=torch.Size([1, 1, 64]), sin.shape=torch.Size([1, 1, 64])
ğŸŸ¦ ãƒãƒ£ãƒ³ã‚¯åŒ– x1.shape=torch.Size([1, 16, 64]), x2.shape=torch.Size([1, 16, 64])
ğŸŸ¦ å›è»¢è¡Œåˆ—é©ç”¨ y1.shape=torch.Size([1, 16, 64]), y2.shape=torch.Size([1, 16, 64])
ğŸŸ© RoPEã‚’é©ç”¨å®Œäº† result.shape=torch.Size([1, 16, 128])
ğŸŸ© RoPEã‚’é©ç”¨é–‹å§‹ x.shape=torch.Size([1, 8, 128]), cos.shape=torch.Size([1, 1, 64]), sin.shape=torch.Size([1, 1, 64])
ğŸŸ¦ ãƒãƒ£ãƒ³ã‚¯åŒ– x1.shape=torch.Size([1, 8, 64]), x2.shape=torch.Size([1, 8, 64])
ğŸŸ¦ å›è»¢è¡Œåˆ—é©ç”¨ y1.shape=torch.Size([1, 8, 64]), y2.shape=torch.Size([1, 8, 64])
ğŸŸ© RoPEã‚’é©ç”¨å®Œäº† result.shape=torch.Size([1, 8, 128])
ğŸŸ© RoPEã®é †ä¼æ’­å®Œäº† query.shape=torch.Size([1, 16, 128]), key.shape=torch.Size([1, 8, 128])
ğŸŸ© Attentionå±¤ã®é †ä¼æ’­é–‹å§‹ q.shape=torch.Size([1, 16, 128]), k.shape=torch.Size([1, 8, 128]), v.shape=torch.Size([1, 8, 128])
ğŸŸ© ç¾åœ¨ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆå–å¾— _CONTEXT=Context(is_prefill=True, cu_seqlens_q=tensor([0, 1], device='cuda:0', dtype=torch.int32), cu_seqlens_k=tensor([0, 1], device='cuda:0', dtype=torch.int32), max_seqlen_q=1, max_seqlen_k=1, slot_mapping=tensor([0], device='cuda:0', dtype=torch.int32), context_lens=None, block_tables=None)
ğŸŸ¦ ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæƒ…å ±å–å¾— context.is_prefill=True, context.slot_mapping=tensor([0], device='cuda:0', dtype=torch.int32), context.max_seqlen_q=1, context.cu_seqlens_q=tensor([0, 1], device='cuda:0', dtype=torch.int32), context.max_seqlen_k=1, context.cu_seqlens_k=tensor([0, 1], device='cuda:0', dtype=torch.int32), context.context_lens=None, context.block_tables=None
ğŸŸ© KVã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ã‚­ãƒ¼ã¨ãƒãƒªãƒ¥ãƒ¼ã‚’ä¿å­˜é–‹å§‹ key.shape=torch.Size([1, 8, 128]), value.shape=torch.Size([1, 8, 128]), k_cache.shape=torch.Size([875, 256, 8, 128]), v_cache.shape=torch.Size([875, 256, 8, 128]), slot_mapping.shape=torch.Size([1])
ğŸŸ© KVã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ã‚­ãƒ¼ã¨ãƒãƒªãƒ¥ãƒ¼ã‚’ä¿å­˜å®Œäº†
ğŸŸ¦ Prefillãƒ¢ãƒ¼ãƒ‰ã§FlashAttentionã‚’å®Ÿè¡Œ context.max_seqlen_q=1, context.cu_seqlens_q=tensor([0, 1], device='cuda:0', dtype=torch.int32), context.max_seqlen_k=1, context.cu_seqlens_k=tensor([0, 1], device='cuda:0', dtype=torch.int32)
ğŸŸ© Attentionå±¤ã®é †ä¼æ’­å®Œäº† o.shape=torch.Size([1, 16, 128])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­é–‹å§‹ å…¥åŠ›å½¢çŠ¶: torch.Size([1, 2048])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­å®Œäº† å‡ºåŠ›å½¢çŠ¶: torch.Size([1, 1024])
ğŸŸ© Qwen3Attentionã®é †ä¼æ’­å®Œäº† output.shape=torch.Size([1, 1024])
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 1024]), residual.shape if residual is not None else None=torch.Size([1, 1024])
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨å®Œäº†
ğŸŸ© Qwen3MLPã®é †ä¼æ’­ x.shape=torch.Size([1, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­å®Œäº† result.shape=torch.Size([1, 6144])
ğŸŸ© SwiGLUã‚’é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 6144])
ğŸŸ¦ ãƒãƒ£ãƒ³ã‚¯åŒ– x.shape=torch.Size([1, 3072]), y.shape=torch.Size([1, 3072])
ğŸŸ© SwiGLUã‚’é †ä¼æ’­å®Œäº† result.shape=torch.Size([1, 3072])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­é–‹å§‹ å…¥åŠ›å½¢çŠ¶: torch.Size([1, 3072])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­å®Œäº† å‡ºåŠ›å½¢çŠ¶: torch.Size([1, 1024])
ğŸŸ© Qwen3MLPã®é †ä¼æ’­å®Œäº† x.shape=torch.Size([1, 1024])
ğŸŸ© Qwen3DecoderLayerã®é †ä¼æ’­å®Œäº† hidden_states.shape=torch.Size([1, 1024])
ğŸŸ© Qwen3DecoderLayerã®é †ä¼æ’­ positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 1024]) residual is None=False
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 1024]), residual.shape if residual is not None else None=torch.Size([1, 1024])
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨å®Œäº†
ğŸŸ© Qwen3Attentionã®é †ä¼æ’­ positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­å®Œäº† result.shape=torch.Size([1, 4096])
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 16, 128]), residual.shape if residual is not None else None=None
ğŸŸ© æ­£è¦åŒ–é–‹å§‹ x.shape=torch.Size([1, 16, 128])
ğŸŸ© æ­£è¦åŒ–å®Œäº† x.shape=torch.Size([1, 16, 128])
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 8, 128]), residual.shape if residual is not None else None=None
ğŸŸ© æ­£è¦åŒ–é–‹å§‹ x.shape=torch.Size([1, 8, 128])
ğŸŸ© æ­£è¦åŒ–å®Œäº† x.shape=torch.Size([1, 8, 128])
ğŸŸ© RoPEã®é †ä¼æ’­é–‹å§‹ positions.shape=torch.Size([1]), query.shape=torch.Size([1, 16, 128]), key.shape=torch.Size([1, 8, 128])
ğŸŸ© RoPEã‚’é©ç”¨é–‹å§‹ x.shape=torch.Size([1, 16, 128]), cos.shape=torch.Size([1, 1, 64]), sin.shape=torch.Size([1, 1, 64])
ğŸŸ¦ ãƒãƒ£ãƒ³ã‚¯åŒ– x1.shape=torch.Size([1, 16, 64]), x2.shape=torch.Size([1, 16, 64])
ğŸŸ¦ å›è»¢è¡Œåˆ—é©ç”¨ y1.shape=torch.Size([1, 16, 64]), y2.shape=torch.Size([1, 16, 64])
ğŸŸ© RoPEã‚’é©ç”¨å®Œäº† result.shape=torch.Size([1, 16, 128])
ğŸŸ© RoPEã‚’é©ç”¨é–‹å§‹ x.shape=torch.Size([1, 8, 128]), cos.shape=torch.Size([1, 1, 64]), sin.shape=torch.Size([1, 1, 64])
ğŸŸ¦ ãƒãƒ£ãƒ³ã‚¯åŒ– x1.shape=torch.Size([1, 8, 64]), x2.shape=torch.Size([1, 8, 64])
ğŸŸ¦ å›è»¢è¡Œåˆ—é©ç”¨ y1.shape=torch.Size([1, 8, 64]), y2.shape=torch.Size([1, 8, 64])
ğŸŸ© RoPEã‚’é©ç”¨å®Œäº† result.shape=torch.Size([1, 8, 128])
ğŸŸ© RoPEã®é †ä¼æ’­å®Œäº† query.shape=torch.Size([1, 16, 128]), key.shape=torch.Size([1, 8, 128])
ğŸŸ© Attentionå±¤ã®é †ä¼æ’­é–‹å§‹ q.shape=torch.Size([1, 16, 128]), k.shape=torch.Size([1, 8, 128]), v.shape=torch.Size([1, 8, 128])
ğŸŸ© ç¾åœ¨ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆå–å¾— _CONTEXT=Context(is_prefill=True, cu_seqlens_q=tensor([0, 1], device='cuda:0', dtype=torch.int32), cu_seqlens_k=tensor([0, 1], device='cuda:0', dtype=torch.int32), max_seqlen_q=1, max_seqlen_k=1, slot_mapping=tensor([0], device='cuda:0', dtype=torch.int32), context_lens=None, block_tables=None)
ğŸŸ¦ ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæƒ…å ±å–å¾— context.is_prefill=True, context.slot_mapping=tensor([0], device='cuda:0', dtype=torch.int32), context.max_seqlen_q=1, context.cu_seqlens_q=tensor([0, 1], device='cuda:0', dtype=torch.int32), context.max_seqlen_k=1, context.cu_seqlens_k=tensor([0, 1], device='cuda:0', dtype=torch.int32), context.context_lens=None, context.block_tables=None
ğŸŸ© KVã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ã‚­ãƒ¼ã¨ãƒãƒªãƒ¥ãƒ¼ã‚’ä¿å­˜é–‹å§‹ key.shape=torch.Size([1, 8, 128]), value.shape=torch.Size([1, 8, 128]), k_cache.shape=torch.Size([875, 256, 8, 128]), v_cache.shape=torch.Size([875, 256, 8, 128]), slot_mapping.shape=torch.Size([1])
ğŸŸ© KVã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ã‚­ãƒ¼ã¨ãƒãƒªãƒ¥ãƒ¼ã‚’ä¿å­˜å®Œäº†
ğŸŸ¦ Prefillãƒ¢ãƒ¼ãƒ‰ã§FlashAttentionã‚’å®Ÿè¡Œ context.max_seqlen_q=1, context.cu_seqlens_q=tensor([0, 1], device='cuda:0', dtype=torch.int32), context.max_seqlen_k=1, context.cu_seqlens_k=tensor([0, 1], device='cuda:0', dtype=torch.int32)
ğŸŸ© Attentionå±¤ã®é †ä¼æ’­å®Œäº† o.shape=torch.Size([1, 16, 128])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­é–‹å§‹ å…¥åŠ›å½¢çŠ¶: torch.Size([1, 2048])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­å®Œäº† å‡ºåŠ›å½¢çŠ¶: torch.Size([1, 1024])
ğŸŸ© Qwen3Attentionã®é †ä¼æ’­å®Œäº† output.shape=torch.Size([1, 1024])
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 1024]), residual.shape if residual is not None else None=torch.Size([1, 1024])
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨å®Œäº†
ğŸŸ© Qwen3MLPã®é †ä¼æ’­ x.shape=torch.Size([1, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­å®Œäº† result.shape=torch.Size([1, 6144])
ğŸŸ© SwiGLUã‚’é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 6144])
ğŸŸ¦ ãƒãƒ£ãƒ³ã‚¯åŒ– x.shape=torch.Size([1, 3072]), y.shape=torch.Size([1, 3072])
ğŸŸ© SwiGLUã‚’é †ä¼æ’­å®Œäº† result.shape=torch.Size([1, 3072])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­é–‹å§‹ å…¥åŠ›å½¢çŠ¶: torch.Size([1, 3072])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­å®Œäº† å‡ºåŠ›å½¢çŠ¶: torch.Size([1, 1024])
ğŸŸ© Qwen3MLPã®é †ä¼æ’­å®Œäº† x.shape=torch.Size([1, 1024])
ğŸŸ© Qwen3DecoderLayerã®é †ä¼æ’­å®Œäº† hidden_states.shape=torch.Size([1, 1024])
ğŸŸ© Qwen3DecoderLayerã®é †ä¼æ’­ positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 1024]) residual is None=False
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 1024]), residual.shape if residual is not None else None=torch.Size([1, 1024])
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨å®Œäº†
ğŸŸ© Qwen3Attentionã®é †ä¼æ’­ positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­å®Œäº† result.shape=torch.Size([1, 4096])
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 16, 128]), residual.shape if residual is not None else None=None
ğŸŸ© æ­£è¦åŒ–é–‹å§‹ x.shape=torch.Size([1, 16, 128])
ğŸŸ© æ­£è¦åŒ–å®Œäº† x.shape=torch.Size([1, 16, 128])
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 8, 128]), residual.shape if residual is not None else None=None
ğŸŸ© æ­£è¦åŒ–é–‹å§‹ x.shape=torch.Size([1, 8, 128])
ğŸŸ© æ­£è¦åŒ–å®Œäº† x.shape=torch.Size([1, 8, 128])
ğŸŸ© RoPEã®é †ä¼æ’­é–‹å§‹ positions.shape=torch.Size([1]), query.shape=torch.Size([1, 16, 128]), key.shape=torch.Size([1, 8, 128])
ğŸŸ© RoPEã‚’é©ç”¨é–‹å§‹ x.shape=torch.Size([1, 16, 128]), cos.shape=torch.Size([1, 1, 64]), sin.shape=torch.Size([1, 1, 64])
ğŸŸ¦ ãƒãƒ£ãƒ³ã‚¯åŒ– x1.shape=torch.Size([1, 16, 64]), x2.shape=torch.Size([1, 16, 64])
ğŸŸ¦ å›è»¢è¡Œåˆ—é©ç”¨ y1.shape=torch.Size([1, 16, 64]), y2.shape=torch.Size([1, 16, 64])
ğŸŸ© RoPEã‚’é©ç”¨å®Œäº† result.shape=torch.Size([1, 16, 128])
ğŸŸ© RoPEã‚’é©ç”¨é–‹å§‹ x.shape=torch.Size([1, 8, 128]), cos.shape=torch.Size([1, 1, 64]), sin.shape=torch.Size([1, 1, 64])
ğŸŸ¦ ãƒãƒ£ãƒ³ã‚¯åŒ– x1.shape=torch.Size([1, 8, 64]), x2.shape=torch.Size([1, 8, 64])
ğŸŸ¦ å›è»¢è¡Œåˆ—é©ç”¨ y1.shape=torch.Size([1, 8, 64]), y2.shape=torch.Size([1, 8, 64])
ğŸŸ© RoPEã‚’é©ç”¨å®Œäº† result.shape=torch.Size([1, 8, 128])
ğŸŸ© RoPEã®é †ä¼æ’­å®Œäº† query.shape=torch.Size([1, 16, 128]), key.shape=torch.Size([1, 8, 128])
ğŸŸ© Attentionå±¤ã®é †ä¼æ’­é–‹å§‹ q.shape=torch.Size([1, 16, 128]), k.shape=torch.Size([1, 8, 128]), v.shape=torch.Size([1, 8, 128])
ğŸŸ© ç¾åœ¨ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆå–å¾— _CONTEXT=Context(is_prefill=True, cu_seqlens_q=tensor([0, 1], device='cuda:0', dtype=torch.int32), cu_seqlens_k=tensor([0, 1], device='cuda:0', dtype=torch.int32), max_seqlen_q=1, max_seqlen_k=1, slot_mapping=tensor([0], device='cuda:0', dtype=torch.int32), context_lens=None, block_tables=None)
ğŸŸ¦ ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæƒ…å ±å–å¾— context.is_prefill=True, context.slot_mapping=tensor([0], device='cuda:0', dtype=torch.int32), context.max_seqlen_q=1, context.cu_seqlens_q=tensor([0, 1], device='cuda:0', dtype=torch.int32), context.max_seqlen_k=1, context.cu_seqlens_k=tensor([0, 1], device='cuda:0', dtype=torch.int32), context.context_lens=None, context.block_tables=None
ğŸŸ© KVã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ã‚­ãƒ¼ã¨ãƒãƒªãƒ¥ãƒ¼ã‚’ä¿å­˜é–‹å§‹ key.shape=torch.Size([1, 8, 128]), value.shape=torch.Size([1, 8, 128]), k_cache.shape=torch.Size([875, 256, 8, 128]), v_cache.shape=torch.Size([875, 256, 8, 128]), slot_mapping.shape=torch.Size([1])
ğŸŸ© KVã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ã‚­ãƒ¼ã¨ãƒãƒªãƒ¥ãƒ¼ã‚’ä¿å­˜å®Œäº†
ğŸŸ¦ Prefillãƒ¢ãƒ¼ãƒ‰ã§FlashAttentionã‚’å®Ÿè¡Œ context.max_seqlen_q=1, context.cu_seqlens_q=tensor([0, 1], device='cuda:0', dtype=torch.int32), context.max_seqlen_k=1, context.cu_seqlens_k=tensor([0, 1], device='cuda:0', dtype=torch.int32)
ğŸŸ© Attentionå±¤ã®é †ä¼æ’­å®Œäº† o.shape=torch.Size([1, 16, 128])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­é–‹å§‹ å…¥åŠ›å½¢çŠ¶: torch.Size([1, 2048])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­å®Œäº† å‡ºåŠ›å½¢çŠ¶: torch.Size([1, 1024])
ğŸŸ© Qwen3Attentionã®é †ä¼æ’­å®Œäº† output.shape=torch.Size([1, 1024])
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 1024]), residual.shape if residual is not None else None=torch.Size([1, 1024])
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨å®Œäº†
ğŸŸ© Qwen3MLPã®é †ä¼æ’­ x.shape=torch.Size([1, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­å®Œäº† result.shape=torch.Size([1, 6144])
ğŸŸ© SwiGLUã‚’é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 6144])
ğŸŸ¦ ãƒãƒ£ãƒ³ã‚¯åŒ– x.shape=torch.Size([1, 3072]), y.shape=torch.Size([1, 3072])
ğŸŸ© SwiGLUã‚’é †ä¼æ’­å®Œäº† result.shape=torch.Size([1, 3072])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­é–‹å§‹ å…¥åŠ›å½¢çŠ¶: torch.Size([1, 3072])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­å®Œäº† å‡ºåŠ›å½¢çŠ¶: torch.Size([1, 1024])
ğŸŸ© Qwen3MLPã®é †ä¼æ’­å®Œäº† x.shape=torch.Size([1, 1024])
ğŸŸ© Qwen3DecoderLayerã®é †ä¼æ’­å®Œäº† hidden_states.shape=torch.Size([1, 1024])
ğŸŸ© Qwen3DecoderLayerã®é †ä¼æ’­ positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 1024]) residual is None=False
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 1024]), residual.shape if residual is not None else None=torch.Size([1, 1024])
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨å®Œäº†
ğŸŸ© Qwen3Attentionã®é †ä¼æ’­ positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­å®Œäº† result.shape=torch.Size([1, 4096])
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 16, 128]), residual.shape if residual is not None else None=None
ğŸŸ© æ­£è¦åŒ–é–‹å§‹ x.shape=torch.Size([1, 16, 128])
ğŸŸ© æ­£è¦åŒ–å®Œäº† x.shape=torch.Size([1, 16, 128])
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 8, 128]), residual.shape if residual is not None else None=None
ğŸŸ© æ­£è¦åŒ–é–‹å§‹ x.shape=torch.Size([1, 8, 128])
ğŸŸ© æ­£è¦åŒ–å®Œäº† x.shape=torch.Size([1, 8, 128])
ğŸŸ© RoPEã®é †ä¼æ’­é–‹å§‹ positions.shape=torch.Size([1]), query.shape=torch.Size([1, 16, 128]), key.shape=torch.Size([1, 8, 128])
ğŸŸ© RoPEã‚’é©ç”¨é–‹å§‹ x.shape=torch.Size([1, 16, 128]), cos.shape=torch.Size([1, 1, 64]), sin.shape=torch.Size([1, 1, 64])
ğŸŸ¦ ãƒãƒ£ãƒ³ã‚¯åŒ– x1.shape=torch.Size([1, 16, 64]), x2.shape=torch.Size([1, 16, 64])
ğŸŸ¦ å›è»¢è¡Œåˆ—é©ç”¨ y1.shape=torch.Size([1, 16, 64]), y2.shape=torch.Size([1, 16, 64])
ğŸŸ© RoPEã‚’é©ç”¨å®Œäº† result.shape=torch.Size([1, 16, 128])
ğŸŸ© RoPEã‚’é©ç”¨é–‹å§‹ x.shape=torch.Size([1, 8, 128]), cos.shape=torch.Size([1, 1, 64]), sin.shape=torch.Size([1, 1, 64])
ğŸŸ¦ ãƒãƒ£ãƒ³ã‚¯åŒ– x1.shape=torch.Size([1, 8, 64]), x2.shape=torch.Size([1, 8, 64])
ğŸŸ¦ å›è»¢è¡Œåˆ—é©ç”¨ y1.shape=torch.Size([1, 8, 64]), y2.shape=torch.Size([1, 8, 64])
ğŸŸ© RoPEã‚’é©ç”¨å®Œäº† result.shape=torch.Size([1, 8, 128])
ğŸŸ© RoPEã®é †ä¼æ’­å®Œäº† query.shape=torch.Size([1, 16, 128]), key.shape=torch.Size([1, 8, 128])
ğŸŸ© Attentionå±¤ã®é †ä¼æ’­é–‹å§‹ q.shape=torch.Size([1, 16, 128]), k.shape=torch.Size([1, 8, 128]), v.shape=torch.Size([1, 8, 128])
ğŸŸ© ç¾åœ¨ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆå–å¾— _CONTEXT=Context(is_prefill=True, cu_seqlens_q=tensor([0, 1], device='cuda:0', dtype=torch.int32), cu_seqlens_k=tensor([0, 1], device='cuda:0', dtype=torch.int32), max_seqlen_q=1, max_seqlen_k=1, slot_mapping=tensor([0], device='cuda:0', dtype=torch.int32), context_lens=None, block_tables=None)
ğŸŸ¦ ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæƒ…å ±å–å¾— context.is_prefill=True, context.slot_mapping=tensor([0], device='cuda:0', dtype=torch.int32), context.max_seqlen_q=1, context.cu_seqlens_q=tensor([0, 1], device='cuda:0', dtype=torch.int32), context.max_seqlen_k=1, context.cu_seqlens_k=tensor([0, 1], device='cuda:0', dtype=torch.int32), context.context_lens=None, context.block_tables=None
ğŸŸ© KVã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ã‚­ãƒ¼ã¨ãƒãƒªãƒ¥ãƒ¼ã‚’ä¿å­˜é–‹å§‹ key.shape=torch.Size([1, 8, 128]), value.shape=torch.Size([1, 8, 128]), k_cache.shape=torch.Size([875, 256, 8, 128]), v_cache.shape=torch.Size([875, 256, 8, 128]), slot_mapping.shape=torch.Size([1])
ğŸŸ© KVã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ã‚­ãƒ¼ã¨ãƒãƒªãƒ¥ãƒ¼ã‚’ä¿å­˜å®Œäº†
ğŸŸ¦ Prefillãƒ¢ãƒ¼ãƒ‰ã§FlashAttentionã‚’å®Ÿè¡Œ context.max_seqlen_q=1, context.cu_seqlens_q=tensor([0, 1], device='cuda:0', dtype=torch.int32), context.max_seqlen_k=1, context.cu_seqlens_k=tensor([0, 1], device='cuda:0', dtype=torch.int32)
ğŸŸ© Attentionå±¤ã®é †ä¼æ’­å®Œäº† o.shape=torch.Size([1, 16, 128])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­é–‹å§‹ å…¥åŠ›å½¢çŠ¶: torch.Size([1, 2048])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­å®Œäº† å‡ºåŠ›å½¢çŠ¶: torch.Size([1, 1024])
ğŸŸ© Qwen3Attentionã®é †ä¼æ’­å®Œäº† output.shape=torch.Size([1, 1024])
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 1024]), residual.shape if residual is not None else None=torch.Size([1, 1024])
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨å®Œäº†
ğŸŸ© Qwen3MLPã®é †ä¼æ’­ x.shape=torch.Size([1, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­å®Œäº† result.shape=torch.Size([1, 6144])
ğŸŸ© SwiGLUã‚’é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 6144])
ğŸŸ¦ ãƒãƒ£ãƒ³ã‚¯åŒ– x.shape=torch.Size([1, 3072]), y.shape=torch.Size([1, 3072])
ğŸŸ© SwiGLUã‚’é †ä¼æ’­å®Œäº† result.shape=torch.Size([1, 3072])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­é–‹å§‹ å…¥åŠ›å½¢çŠ¶: torch.Size([1, 3072])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­å®Œäº† å‡ºåŠ›å½¢çŠ¶: torch.Size([1, 1024])
ğŸŸ© Qwen3MLPã®é †ä¼æ’­å®Œäº† x.shape=torch.Size([1, 1024])
ğŸŸ© Qwen3DecoderLayerã®é †ä¼æ’­å®Œäº† hidden_states.shape=torch.Size([1, 1024])
ğŸŸ© Qwen3DecoderLayerã®é †ä¼æ’­ positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 1024]) residual is None=False
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 1024]), residual.shape if residual is not None else None=torch.Size([1, 1024])
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨å®Œäº†
ğŸŸ© Qwen3Attentionã®é †ä¼æ’­ positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­å®Œäº† result.shape=torch.Size([1, 4096])
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 16, 128]), residual.shape if residual is not None else None=None
ğŸŸ© æ­£è¦åŒ–é–‹å§‹ x.shape=torch.Size([1, 16, 128])
ğŸŸ© æ­£è¦åŒ–å®Œäº† x.shape=torch.Size([1, 16, 128])
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 8, 128]), residual.shape if residual is not None else None=None
ğŸŸ© æ­£è¦åŒ–é–‹å§‹ x.shape=torch.Size([1, 8, 128])
ğŸŸ© æ­£è¦åŒ–å®Œäº† x.shape=torch.Size([1, 8, 128])
ğŸŸ© RoPEã®é †ä¼æ’­é–‹å§‹ positions.shape=torch.Size([1]), query.shape=torch.Size([1, 16, 128]), key.shape=torch.Size([1, 8, 128])
ğŸŸ© RoPEã‚’é©ç”¨é–‹å§‹ x.shape=torch.Size([1, 16, 128]), cos.shape=torch.Size([1, 1, 64]), sin.shape=torch.Size([1, 1, 64])
ğŸŸ¦ ãƒãƒ£ãƒ³ã‚¯åŒ– x1.shape=torch.Size([1, 16, 64]), x2.shape=torch.Size([1, 16, 64])
ğŸŸ¦ å›è»¢è¡Œåˆ—é©ç”¨ y1.shape=torch.Size([1, 16, 64]), y2.shape=torch.Size([1, 16, 64])
ğŸŸ© RoPEã‚’é©ç”¨å®Œäº† result.shape=torch.Size([1, 16, 128])
ğŸŸ© RoPEã‚’é©ç”¨é–‹å§‹ x.shape=torch.Size([1, 8, 128]), cos.shape=torch.Size([1, 1, 64]), sin.shape=torch.Size([1, 1, 64])
ğŸŸ¦ ãƒãƒ£ãƒ³ã‚¯åŒ– x1.shape=torch.Size([1, 8, 64]), x2.shape=torch.Size([1, 8, 64])
ğŸŸ¦ å›è»¢è¡Œåˆ—é©ç”¨ y1.shape=torch.Size([1, 8, 64]), y2.shape=torch.Size([1, 8, 64])
ğŸŸ© RoPEã‚’é©ç”¨å®Œäº† result.shape=torch.Size([1, 8, 128])
ğŸŸ© RoPEã®é †ä¼æ’­å®Œäº† query.shape=torch.Size([1, 16, 128]), key.shape=torch.Size([1, 8, 128])
ğŸŸ© Attentionå±¤ã®é †ä¼æ’­é–‹å§‹ q.shape=torch.Size([1, 16, 128]), k.shape=torch.Size([1, 8, 128]), v.shape=torch.Size([1, 8, 128])
ğŸŸ© ç¾åœ¨ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆå–å¾— _CONTEXT=Context(is_prefill=True, cu_seqlens_q=tensor([0, 1], device='cuda:0', dtype=torch.int32), cu_seqlens_k=tensor([0, 1], device='cuda:0', dtype=torch.int32), max_seqlen_q=1, max_seqlen_k=1, slot_mapping=tensor([0], device='cuda:0', dtype=torch.int32), context_lens=None, block_tables=None)
ğŸŸ¦ ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæƒ…å ±å–å¾— context.is_prefill=True, context.slot_mapping=tensor([0], device='cuda:0', dtype=torch.int32), context.max_seqlen_q=1, context.cu_seqlens_q=tensor([0, 1], device='cuda:0', dtype=torch.int32), context.max_seqlen_k=1, context.cu_seqlens_k=tensor([0, 1], device='cuda:0', dtype=torch.int32), context.context_lens=None, context.block_tables=None
ğŸŸ© KVã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ã‚­ãƒ¼ã¨ãƒãƒªãƒ¥ãƒ¼ã‚’ä¿å­˜é–‹å§‹ key.shape=torch.Size([1, 8, 128]), value.shape=torch.Size([1, 8, 128]), k_cache.shape=torch.Size([875, 256, 8, 128]), v_cache.shape=torch.Size([875, 256, 8, 128]), slot_mapping.shape=torch.Size([1])
ğŸŸ© KVã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ã‚­ãƒ¼ã¨ãƒãƒªãƒ¥ãƒ¼ã‚’ä¿å­˜å®Œäº†
ğŸŸ¦ Prefillãƒ¢ãƒ¼ãƒ‰ã§FlashAttentionã‚’å®Ÿè¡Œ context.max_seqlen_q=1, context.cu_seqlens_q=tensor([0, 1], device='cuda:0', dtype=torch.int32), context.max_seqlen_k=1, context.cu_seqlens_k=tensor([0, 1], device='cuda:0', dtype=torch.int32)
ğŸŸ© Attentionå±¤ã®é †ä¼æ’­å®Œäº† o.shape=torch.Size([1, 16, 128])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­é–‹å§‹ å…¥åŠ›å½¢çŠ¶: torch.Size([1, 2048])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­å®Œäº† å‡ºåŠ›å½¢çŠ¶: torch.Size([1, 1024])
ğŸŸ© Qwen3Attentionã®é †ä¼æ’­å®Œäº† output.shape=torch.Size([1, 1024])
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 1024]), residual.shape if residual is not None else None=torch.Size([1, 1024])
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨å®Œäº†
ğŸŸ© Qwen3MLPã®é †ä¼æ’­ x.shape=torch.Size([1, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­å®Œäº† result.shape=torch.Size([1, 6144])
ğŸŸ© SwiGLUã‚’é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 6144])
ğŸŸ¦ ãƒãƒ£ãƒ³ã‚¯åŒ– x.shape=torch.Size([1, 3072]), y.shape=torch.Size([1, 3072])
ğŸŸ© SwiGLUã‚’é †ä¼æ’­å®Œäº† result.shape=torch.Size([1, 3072])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­é–‹å§‹ å…¥åŠ›å½¢çŠ¶: torch.Size([1, 3072])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­å®Œäº† å‡ºåŠ›å½¢çŠ¶: torch.Size([1, 1024])
ğŸŸ© Qwen3MLPã®é †ä¼æ’­å®Œäº† x.shape=torch.Size([1, 1024])
ğŸŸ© Qwen3DecoderLayerã®é †ä¼æ’­å®Œäº† hidden_states.shape=torch.Size([1, 1024])
ğŸŸ© Qwen3DecoderLayerã®é †ä¼æ’­ positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 1024]) residual is None=False
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 1024]), residual.shape if residual is not None else None=torch.Size([1, 1024])
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨å®Œäº†
ğŸŸ© Qwen3Attentionã®é †ä¼æ’­ positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­å®Œäº† result.shape=torch.Size([1, 4096])
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 16, 128]), residual.shape if residual is not None else None=None
ğŸŸ© æ­£è¦åŒ–é–‹å§‹ x.shape=torch.Size([1, 16, 128])
ğŸŸ© æ­£è¦åŒ–å®Œäº† x.shape=torch.Size([1, 16, 128])
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 8, 128]), residual.shape if residual is not None else None=None
ğŸŸ© æ­£è¦åŒ–é–‹å§‹ x.shape=torch.Size([1, 8, 128])
ğŸŸ© æ­£è¦åŒ–å®Œäº† x.shape=torch.Size([1, 8, 128])
ğŸŸ© RoPEã®é †ä¼æ’­é–‹å§‹ positions.shape=torch.Size([1]), query.shape=torch.Size([1, 16, 128]), key.shape=torch.Size([1, 8, 128])
ğŸŸ© RoPEã‚’é©ç”¨é–‹å§‹ x.shape=torch.Size([1, 16, 128]), cos.shape=torch.Size([1, 1, 64]), sin.shape=torch.Size([1, 1, 64])
ğŸŸ¦ ãƒãƒ£ãƒ³ã‚¯åŒ– x1.shape=torch.Size([1, 16, 64]), x2.shape=torch.Size([1, 16, 64])
ğŸŸ¦ å›è»¢è¡Œåˆ—é©ç”¨ y1.shape=torch.Size([1, 16, 64]), y2.shape=torch.Size([1, 16, 64])
ğŸŸ© RoPEã‚’é©ç”¨å®Œäº† result.shape=torch.Size([1, 16, 128])
ğŸŸ© RoPEã‚’é©ç”¨é–‹å§‹ x.shape=torch.Size([1, 8, 128]), cos.shape=torch.Size([1, 1, 64]), sin.shape=torch.Size([1, 1, 64])
ğŸŸ¦ ãƒãƒ£ãƒ³ã‚¯åŒ– x1.shape=torch.Size([1, 8, 64]), x2.shape=torch.Size([1, 8, 64])
ğŸŸ¦ å›è»¢è¡Œåˆ—é©ç”¨ y1.shape=torch.Size([1, 8, 64]), y2.shape=torch.Size([1, 8, 64])
ğŸŸ© RoPEã‚’é©ç”¨å®Œäº† result.shape=torch.Size([1, 8, 128])
ğŸŸ© RoPEã®é †ä¼æ’­å®Œäº† query.shape=torch.Size([1, 16, 128]), key.shape=torch.Size([1, 8, 128])
ğŸŸ© Attentionå±¤ã®é †ä¼æ’­é–‹å§‹ q.shape=torch.Size([1, 16, 128]), k.shape=torch.Size([1, 8, 128]), v.shape=torch.Size([1, 8, 128])
ğŸŸ© ç¾åœ¨ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆå–å¾— _CONTEXT=Context(is_prefill=True, cu_seqlens_q=tensor([0, 1], device='cuda:0', dtype=torch.int32), cu_seqlens_k=tensor([0, 1], device='cuda:0', dtype=torch.int32), max_seqlen_q=1, max_seqlen_k=1, slot_mapping=tensor([0], device='cuda:0', dtype=torch.int32), context_lens=None, block_tables=None)
ğŸŸ¦ ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæƒ…å ±å–å¾— context.is_prefill=True, context.slot_mapping=tensor([0], device='cuda:0', dtype=torch.int32), context.max_seqlen_q=1, context.cu_seqlens_q=tensor([0, 1], device='cuda:0', dtype=torch.int32), context.max_seqlen_k=1, context.cu_seqlens_k=tensor([0, 1], device='cuda:0', dtype=torch.int32), context.context_lens=None, context.block_tables=None
ğŸŸ© KVã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ã‚­ãƒ¼ã¨ãƒãƒªãƒ¥ãƒ¼ã‚’ä¿å­˜é–‹å§‹ key.shape=torch.Size([1, 8, 128]), value.shape=torch.Size([1, 8, 128]), k_cache.shape=torch.Size([875, 256, 8, 128]), v_cache.shape=torch.Size([875, 256, 8, 128]), slot_mapping.shape=torch.Size([1])
ğŸŸ© KVã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ã‚­ãƒ¼ã¨ãƒãƒªãƒ¥ãƒ¼ã‚’ä¿å­˜å®Œäº†
ğŸŸ¦ Prefillãƒ¢ãƒ¼ãƒ‰ã§FlashAttentionã‚’å®Ÿè¡Œ context.max_seqlen_q=1, context.cu_seqlens_q=tensor([0, 1], device='cuda:0', dtype=torch.int32), context.max_seqlen_k=1, context.cu_seqlens_k=tensor([0, 1], device='cuda:0', dtype=torch.int32)
ğŸŸ© Attentionå±¤ã®é †ä¼æ’­å®Œäº† o.shape=torch.Size([1, 16, 128])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­é–‹å§‹ å…¥åŠ›å½¢çŠ¶: torch.Size([1, 2048])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­å®Œäº† å‡ºåŠ›å½¢çŠ¶: torch.Size([1, 1024])
ğŸŸ© Qwen3Attentionã®é †ä¼æ’­å®Œäº† output.shape=torch.Size([1, 1024])
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 1024]), residual.shape if residual is not None else None=torch.Size([1, 1024])
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨å®Œäº†
ğŸŸ© Qwen3MLPã®é †ä¼æ’­ x.shape=torch.Size([1, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­å®Œäº† result.shape=torch.Size([1, 6144])
ğŸŸ© SwiGLUã‚’é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 6144])
ğŸŸ¦ ãƒãƒ£ãƒ³ã‚¯åŒ– x.shape=torch.Size([1, 3072]), y.shape=torch.Size([1, 3072])
ğŸŸ© SwiGLUã‚’é †ä¼æ’­å®Œäº† result.shape=torch.Size([1, 3072])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­é–‹å§‹ å…¥åŠ›å½¢çŠ¶: torch.Size([1, 3072])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­å®Œäº† å‡ºåŠ›å½¢çŠ¶: torch.Size([1, 1024])
ğŸŸ© Qwen3MLPã®é †ä¼æ’­å®Œäº† x.shape=torch.Size([1, 1024])
ğŸŸ© Qwen3DecoderLayerã®é †ä¼æ’­å®Œäº† hidden_states.shape=torch.Size([1, 1024])
ğŸŸ© Qwen3DecoderLayerã®é †ä¼æ’­ positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 1024]) residual is None=False
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 1024]), residual.shape if residual is not None else None=torch.Size([1, 1024])
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨å®Œäº†
ğŸŸ© Qwen3Attentionã®é †ä¼æ’­ positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­å®Œäº† result.shape=torch.Size([1, 4096])
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 16, 128]), residual.shape if residual is not None else None=None
ğŸŸ© æ­£è¦åŒ–é–‹å§‹ x.shape=torch.Size([1, 16, 128])
ğŸŸ© æ­£è¦åŒ–å®Œäº† x.shape=torch.Size([1, 16, 128])
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 8, 128]), residual.shape if residual is not None else None=None
ğŸŸ© æ­£è¦åŒ–é–‹å§‹ x.shape=torch.Size([1, 8, 128])
ğŸŸ© æ­£è¦åŒ–å®Œäº† x.shape=torch.Size([1, 8, 128])
ğŸŸ© RoPEã®é †ä¼æ’­é–‹å§‹ positions.shape=torch.Size([1]), query.shape=torch.Size([1, 16, 128]), key.shape=torch.Size([1, 8, 128])
ğŸŸ© RoPEã‚’é©ç”¨é–‹å§‹ x.shape=torch.Size([1, 16, 128]), cos.shape=torch.Size([1, 1, 64]), sin.shape=torch.Size([1, 1, 64])
ğŸŸ¦ ãƒãƒ£ãƒ³ã‚¯åŒ– x1.shape=torch.Size([1, 16, 64]), x2.shape=torch.Size([1, 16, 64])
ğŸŸ¦ å›è»¢è¡Œåˆ—é©ç”¨ y1.shape=torch.Size([1, 16, 64]), y2.shape=torch.Size([1, 16, 64])
ğŸŸ© RoPEã‚’é©ç”¨å®Œäº† result.shape=torch.Size([1, 16, 128])
ğŸŸ© RoPEã‚’é©ç”¨é–‹å§‹ x.shape=torch.Size([1, 8, 128]), cos.shape=torch.Size([1, 1, 64]), sin.shape=torch.Size([1, 1, 64])
ğŸŸ¦ ãƒãƒ£ãƒ³ã‚¯åŒ– x1.shape=torch.Size([1, 8, 64]), x2.shape=torch.Size([1, 8, 64])
ğŸŸ¦ å›è»¢è¡Œåˆ—é©ç”¨ y1.shape=torch.Size([1, 8, 64]), y2.shape=torch.Size([1, 8, 64])
ğŸŸ© RoPEã‚’é©ç”¨å®Œäº† result.shape=torch.Size([1, 8, 128])
ğŸŸ© RoPEã®é †ä¼æ’­å®Œäº† query.shape=torch.Size([1, 16, 128]), key.shape=torch.Size([1, 8, 128])
ğŸŸ© Attentionå±¤ã®é †ä¼æ’­é–‹å§‹ q.shape=torch.Size([1, 16, 128]), k.shape=torch.Size([1, 8, 128]), v.shape=torch.Size([1, 8, 128])
ğŸŸ© ç¾åœ¨ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆå–å¾— _CONTEXT=Context(is_prefill=True, cu_seqlens_q=tensor([0, 1], device='cuda:0', dtype=torch.int32), cu_seqlens_k=tensor([0, 1], device='cuda:0', dtype=torch.int32), max_seqlen_q=1, max_seqlen_k=1, slot_mapping=tensor([0], device='cuda:0', dtype=torch.int32), context_lens=None, block_tables=None)
ğŸŸ¦ ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæƒ…å ±å–å¾— context.is_prefill=True, context.slot_mapping=tensor([0], device='cuda:0', dtype=torch.int32), context.max_seqlen_q=1, context.cu_seqlens_q=tensor([0, 1], device='cuda:0', dtype=torch.int32), context.max_seqlen_k=1, context.cu_seqlens_k=tensor([0, 1], device='cuda:0', dtype=torch.int32), context.context_lens=None, context.block_tables=None
ğŸŸ© KVã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ã‚­ãƒ¼ã¨ãƒãƒªãƒ¥ãƒ¼ã‚’ä¿å­˜é–‹å§‹ key.shape=torch.Size([1, 8, 128]), value.shape=torch.Size([1, 8, 128]), k_cache.shape=torch.Size([875, 256, 8, 128]), v_cache.shape=torch.Size([875, 256, 8, 128]), slot_mapping.shape=torch.Size([1])
ğŸŸ© KVã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ã‚­ãƒ¼ã¨ãƒãƒªãƒ¥ãƒ¼ã‚’ä¿å­˜å®Œäº†
ğŸŸ¦ Prefillãƒ¢ãƒ¼ãƒ‰ã§FlashAttentionã‚’å®Ÿè¡Œ context.max_seqlen_q=1, context.cu_seqlens_q=tensor([0, 1], device='cuda:0', dtype=torch.int32), context.max_seqlen_k=1, context.cu_seqlens_k=tensor([0, 1], device='cuda:0', dtype=torch.int32)
ğŸŸ© Attentionå±¤ã®é †ä¼æ’­å®Œäº† o.shape=torch.Size([1, 16, 128])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­é–‹å§‹ å…¥åŠ›å½¢çŠ¶: torch.Size([1, 2048])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­å®Œäº† å‡ºåŠ›å½¢çŠ¶: torch.Size([1, 1024])
ğŸŸ© Qwen3Attentionã®é †ä¼æ’­å®Œäº† output.shape=torch.Size([1, 1024])
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 1024]), residual.shape if residual is not None else None=torch.Size([1, 1024])
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨å®Œäº†
ğŸŸ© Qwen3MLPã®é †ä¼æ’­ x.shape=torch.Size([1, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­å®Œäº† result.shape=torch.Size([1, 6144])
ğŸŸ© SwiGLUã‚’é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 6144])
ğŸŸ¦ ãƒãƒ£ãƒ³ã‚¯åŒ– x.shape=torch.Size([1, 3072]), y.shape=torch.Size([1, 3072])
ğŸŸ© SwiGLUã‚’é †ä¼æ’­å®Œäº† result.shape=torch.Size([1, 3072])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­é–‹å§‹ å…¥åŠ›å½¢çŠ¶: torch.Size([1, 3072])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­å®Œäº† å‡ºåŠ›å½¢çŠ¶: torch.Size([1, 1024])
ğŸŸ© Qwen3MLPã®é †ä¼æ’­å®Œäº† x.shape=torch.Size([1, 1024])
ğŸŸ© Qwen3DecoderLayerã®é †ä¼æ’­å®Œäº† hidden_states.shape=torch.Size([1, 1024])
ğŸŸ© Qwen3DecoderLayerã®é †ä¼æ’­ positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 1024]) residual is None=False
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 1024]), residual.shape if residual is not None else None=torch.Size([1, 1024])
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨å®Œäº†
ğŸŸ© Qwen3Attentionã®é †ä¼æ’­ positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­å®Œäº† result.shape=torch.Size([1, 4096])
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 16, 128]), residual.shape if residual is not None else None=None
ğŸŸ© æ­£è¦åŒ–é–‹å§‹ x.shape=torch.Size([1, 16, 128])
ğŸŸ© æ­£è¦åŒ–å®Œäº† x.shape=torch.Size([1, 16, 128])
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 8, 128]), residual.shape if residual is not None else None=None
ğŸŸ© æ­£è¦åŒ–é–‹å§‹ x.shape=torch.Size([1, 8, 128])
ğŸŸ© æ­£è¦åŒ–å®Œäº† x.shape=torch.Size([1, 8, 128])
ğŸŸ© RoPEã®é †ä¼æ’­é–‹å§‹ positions.shape=torch.Size([1]), query.shape=torch.Size([1, 16, 128]), key.shape=torch.Size([1, 8, 128])
ğŸŸ© RoPEã‚’é©ç”¨é–‹å§‹ x.shape=torch.Size([1, 16, 128]), cos.shape=torch.Size([1, 1, 64]), sin.shape=torch.Size([1, 1, 64])
ğŸŸ¦ ãƒãƒ£ãƒ³ã‚¯åŒ– x1.shape=torch.Size([1, 16, 64]), x2.shape=torch.Size([1, 16, 64])
ğŸŸ¦ å›è»¢è¡Œåˆ—é©ç”¨ y1.shape=torch.Size([1, 16, 64]), y2.shape=torch.Size([1, 16, 64])
ğŸŸ© RoPEã‚’é©ç”¨å®Œäº† result.shape=torch.Size([1, 16, 128])
ğŸŸ© RoPEã‚’é©ç”¨é–‹å§‹ x.shape=torch.Size([1, 8, 128]), cos.shape=torch.Size([1, 1, 64]), sin.shape=torch.Size([1, 1, 64])
ğŸŸ¦ ãƒãƒ£ãƒ³ã‚¯åŒ– x1.shape=torch.Size([1, 8, 64]), x2.shape=torch.Size([1, 8, 64])
ğŸŸ¦ å›è»¢è¡Œåˆ—é©ç”¨ y1.shape=torch.Size([1, 8, 64]), y2.shape=torch.Size([1, 8, 64])
ğŸŸ© RoPEã‚’é©ç”¨å®Œäº† result.shape=torch.Size([1, 8, 128])
ğŸŸ© RoPEã®é †ä¼æ’­å®Œäº† query.shape=torch.Size([1, 16, 128]), key.shape=torch.Size([1, 8, 128])
ğŸŸ© Attentionå±¤ã®é †ä¼æ’­é–‹å§‹ q.shape=torch.Size([1, 16, 128]), k.shape=torch.Size([1, 8, 128]), v.shape=torch.Size([1, 8, 128])
ğŸŸ© ç¾åœ¨ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆå–å¾— _CONTEXT=Context(is_prefill=True, cu_seqlens_q=tensor([0, 1], device='cuda:0', dtype=torch.int32), cu_seqlens_k=tensor([0, 1], device='cuda:0', dtype=torch.int32), max_seqlen_q=1, max_seqlen_k=1, slot_mapping=tensor([0], device='cuda:0', dtype=torch.int32), context_lens=None, block_tables=None)
ğŸŸ¦ ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæƒ…å ±å–å¾— context.is_prefill=True, context.slot_mapping=tensor([0], device='cuda:0', dtype=torch.int32), context.max_seqlen_q=1, context.cu_seqlens_q=tensor([0, 1], device='cuda:0', dtype=torch.int32), context.max_seqlen_k=1, context.cu_seqlens_k=tensor([0, 1], device='cuda:0', dtype=torch.int32), context.context_lens=None, context.block_tables=None
ğŸŸ© KVã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ã‚­ãƒ¼ã¨ãƒãƒªãƒ¥ãƒ¼ã‚’ä¿å­˜é–‹å§‹ key.shape=torch.Size([1, 8, 128]), value.shape=torch.Size([1, 8, 128]), k_cache.shape=torch.Size([875, 256, 8, 128]), v_cache.shape=torch.Size([875, 256, 8, 128]), slot_mapping.shape=torch.Size([1])
ğŸŸ© KVã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ã‚­ãƒ¼ã¨ãƒãƒªãƒ¥ãƒ¼ã‚’ä¿å­˜å®Œäº†
ğŸŸ¦ Prefillãƒ¢ãƒ¼ãƒ‰ã§FlashAttentionã‚’å®Ÿè¡Œ context.max_seqlen_q=1, context.cu_seqlens_q=tensor([0, 1], device='cuda:0', dtype=torch.int32), context.max_seqlen_k=1, context.cu_seqlens_k=tensor([0, 1], device='cuda:0', dtype=torch.int32)
ğŸŸ© Attentionå±¤ã®é †ä¼æ’­å®Œäº† o.shape=torch.Size([1, 16, 128])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­é–‹å§‹ å…¥åŠ›å½¢çŠ¶: torch.Size([1, 2048])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­å®Œäº† å‡ºåŠ›å½¢çŠ¶: torch.Size([1, 1024])
ğŸŸ© Qwen3Attentionã®é †ä¼æ’­å®Œäº† output.shape=torch.Size([1, 1024])
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 1024]), residual.shape if residual is not None else None=torch.Size([1, 1024])
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨å®Œäº†
ğŸŸ© Qwen3MLPã®é †ä¼æ’­ x.shape=torch.Size([1, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­å®Œäº† result.shape=torch.Size([1, 6144])
ğŸŸ© SwiGLUã‚’é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 6144])
ğŸŸ¦ ãƒãƒ£ãƒ³ã‚¯åŒ– x.shape=torch.Size([1, 3072]), y.shape=torch.Size([1, 3072])
ğŸŸ© SwiGLUã‚’é †ä¼æ’­å®Œäº† result.shape=torch.Size([1, 3072])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­é–‹å§‹ å…¥åŠ›å½¢çŠ¶: torch.Size([1, 3072])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­å®Œäº† å‡ºåŠ›å½¢çŠ¶: torch.Size([1, 1024])
ğŸŸ© Qwen3MLPã®é †ä¼æ’­å®Œäº† x.shape=torch.Size([1, 1024])
ğŸŸ© Qwen3DecoderLayerã®é †ä¼æ’­å®Œäº† hidden_states.shape=torch.Size([1, 1024])
ğŸŸ© Qwen3DecoderLayerã®é †ä¼æ’­ positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 1024]) residual is None=False
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 1024]), residual.shape if residual is not None else None=torch.Size([1, 1024])
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨å®Œäº†
ğŸŸ© Qwen3Attentionã®é †ä¼æ’­ positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­å®Œäº† result.shape=torch.Size([1, 4096])
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 16, 128]), residual.shape if residual is not None else None=None
ğŸŸ© æ­£è¦åŒ–é–‹å§‹ x.shape=torch.Size([1, 16, 128])
ğŸŸ© æ­£è¦åŒ–å®Œäº† x.shape=torch.Size([1, 16, 128])
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 8, 128]), residual.shape if residual is not None else None=None
ğŸŸ© æ­£è¦åŒ–é–‹å§‹ x.shape=torch.Size([1, 8, 128])
ğŸŸ© æ­£è¦åŒ–å®Œäº† x.shape=torch.Size([1, 8, 128])
ğŸŸ© RoPEã®é †ä¼æ’­é–‹å§‹ positions.shape=torch.Size([1]), query.shape=torch.Size([1, 16, 128]), key.shape=torch.Size([1, 8, 128])
ğŸŸ© RoPEã‚’é©ç”¨é–‹å§‹ x.shape=torch.Size([1, 16, 128]), cos.shape=torch.Size([1, 1, 64]), sin.shape=torch.Size([1, 1, 64])
ğŸŸ¦ ãƒãƒ£ãƒ³ã‚¯åŒ– x1.shape=torch.Size([1, 16, 64]), x2.shape=torch.Size([1, 16, 64])
ğŸŸ¦ å›è»¢è¡Œåˆ—é©ç”¨ y1.shape=torch.Size([1, 16, 64]), y2.shape=torch.Size([1, 16, 64])
ğŸŸ© RoPEã‚’é©ç”¨å®Œäº† result.shape=torch.Size([1, 16, 128])
ğŸŸ© RoPEã‚’é©ç”¨é–‹å§‹ x.shape=torch.Size([1, 8, 128]), cos.shape=torch.Size([1, 1, 64]), sin.shape=torch.Size([1, 1, 64])
ğŸŸ¦ ãƒãƒ£ãƒ³ã‚¯åŒ– x1.shape=torch.Size([1, 8, 64]), x2.shape=torch.Size([1, 8, 64])
ğŸŸ¦ å›è»¢è¡Œåˆ—é©ç”¨ y1.shape=torch.Size([1, 8, 64]), y2.shape=torch.Size([1, 8, 64])
ğŸŸ© RoPEã‚’é©ç”¨å®Œäº† result.shape=torch.Size([1, 8, 128])
ğŸŸ© RoPEã®é †ä¼æ’­å®Œäº† query.shape=torch.Size([1, 16, 128]), key.shape=torch.Size([1, 8, 128])
ğŸŸ© Attentionå±¤ã®é †ä¼æ’­é–‹å§‹ q.shape=torch.Size([1, 16, 128]), k.shape=torch.Size([1, 8, 128]), v.shape=torch.Size([1, 8, 128])
ğŸŸ© ç¾åœ¨ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆå–å¾— _CONTEXT=Context(is_prefill=True, cu_seqlens_q=tensor([0, 1], device='cuda:0', dtype=torch.int32), cu_seqlens_k=tensor([0, 1], device='cuda:0', dtype=torch.int32), max_seqlen_q=1, max_seqlen_k=1, slot_mapping=tensor([0], device='cuda:0', dtype=torch.int32), context_lens=None, block_tables=None)
ğŸŸ¦ ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæƒ…å ±å–å¾— context.is_prefill=True, context.slot_mapping=tensor([0], device='cuda:0', dtype=torch.int32), context.max_seqlen_q=1, context.cu_seqlens_q=tensor([0, 1], device='cuda:0', dtype=torch.int32), context.max_seqlen_k=1, context.cu_seqlens_k=tensor([0, 1], device='cuda:0', dtype=torch.int32), context.context_lens=None, context.block_tables=None
ğŸŸ© KVã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ã‚­ãƒ¼ã¨ãƒãƒªãƒ¥ãƒ¼ã‚’ä¿å­˜é–‹å§‹ key.shape=torch.Size([1, 8, 128]), value.shape=torch.Size([1, 8, 128]), k_cache.shape=torch.Size([875, 256, 8, 128]), v_cache.shape=torch.Size([875, 256, 8, 128]), slot_mapping.shape=torch.Size([1])
ğŸŸ© KVã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ã‚­ãƒ¼ã¨ãƒãƒªãƒ¥ãƒ¼ã‚’ä¿å­˜å®Œäº†
ğŸŸ¦ Prefillãƒ¢ãƒ¼ãƒ‰ã§FlashAttentionã‚’å®Ÿè¡Œ context.max_seqlen_q=1, context.cu_seqlens_q=tensor([0, 1], device='cuda:0', dtype=torch.int32), context.max_seqlen_k=1, context.cu_seqlens_k=tensor([0, 1], device='cuda:0', dtype=torch.int32)
ğŸŸ© Attentionå±¤ã®é †ä¼æ’­å®Œäº† o.shape=torch.Size([1, 16, 128])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­é–‹å§‹ å…¥åŠ›å½¢çŠ¶: torch.Size([1, 2048])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­å®Œäº† å‡ºåŠ›å½¢çŠ¶: torch.Size([1, 1024])
ğŸŸ© Qwen3Attentionã®é †ä¼æ’­å®Œäº† output.shape=torch.Size([1, 1024])
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 1024]), residual.shape if residual is not None else None=torch.Size([1, 1024])
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨å®Œäº†
ğŸŸ© Qwen3MLPã®é †ä¼æ’­ x.shape=torch.Size([1, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­å®Œäº† result.shape=torch.Size([1, 6144])
ğŸŸ© SwiGLUã‚’é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 6144])
ğŸŸ¦ ãƒãƒ£ãƒ³ã‚¯åŒ– x.shape=torch.Size([1, 3072]), y.shape=torch.Size([1, 3072])
ğŸŸ© SwiGLUã‚’é †ä¼æ’­å®Œäº† result.shape=torch.Size([1, 3072])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­é–‹å§‹ å…¥åŠ›å½¢çŠ¶: torch.Size([1, 3072])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­å®Œäº† å‡ºåŠ›å½¢çŠ¶: torch.Size([1, 1024])
ğŸŸ© Qwen3MLPã®é †ä¼æ’­å®Œäº† x.shape=torch.Size([1, 1024])
ğŸŸ© Qwen3DecoderLayerã®é †ä¼æ’­å®Œäº† hidden_states.shape=torch.Size([1, 1024])
ğŸŸ© Qwen3DecoderLayerã®é †ä¼æ’­ positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 1024]) residual is None=False
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 1024]), residual.shape if residual is not None else None=torch.Size([1, 1024])
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨å®Œäº†
ğŸŸ© Qwen3Attentionã®é †ä¼æ’­ positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­å®Œäº† result.shape=torch.Size([1, 4096])
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 16, 128]), residual.shape if residual is not None else None=None
ğŸŸ© æ­£è¦åŒ–é–‹å§‹ x.shape=torch.Size([1, 16, 128])
ğŸŸ© æ­£è¦åŒ–å®Œäº† x.shape=torch.Size([1, 16, 128])
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 8, 128]), residual.shape if residual is not None else None=None
ğŸŸ© æ­£è¦åŒ–é–‹å§‹ x.shape=torch.Size([1, 8, 128])
ğŸŸ© æ­£è¦åŒ–å®Œäº† x.shape=torch.Size([1, 8, 128])
ğŸŸ© RoPEã®é †ä¼æ’­é–‹å§‹ positions.shape=torch.Size([1]), query.shape=torch.Size([1, 16, 128]), key.shape=torch.Size([1, 8, 128])
ğŸŸ© RoPEã‚’é©ç”¨é–‹å§‹ x.shape=torch.Size([1, 16, 128]), cos.shape=torch.Size([1, 1, 64]), sin.shape=torch.Size([1, 1, 64])
ğŸŸ¦ ãƒãƒ£ãƒ³ã‚¯åŒ– x1.shape=torch.Size([1, 16, 64]), x2.shape=torch.Size([1, 16, 64])
ğŸŸ¦ å›è»¢è¡Œåˆ—é©ç”¨ y1.shape=torch.Size([1, 16, 64]), y2.shape=torch.Size([1, 16, 64])
ğŸŸ© RoPEã‚’é©ç”¨å®Œäº† result.shape=torch.Size([1, 16, 128])
ğŸŸ© RoPEã‚’é©ç”¨é–‹å§‹ x.shape=torch.Size([1, 8, 128]), cos.shape=torch.Size([1, 1, 64]), sin.shape=torch.Size([1, 1, 64])
ğŸŸ¦ ãƒãƒ£ãƒ³ã‚¯åŒ– x1.shape=torch.Size([1, 8, 64]), x2.shape=torch.Size([1, 8, 64])
ğŸŸ¦ å›è»¢è¡Œåˆ—é©ç”¨ y1.shape=torch.Size([1, 8, 64]), y2.shape=torch.Size([1, 8, 64])
ğŸŸ© RoPEã‚’é©ç”¨å®Œäº† result.shape=torch.Size([1, 8, 128])
ğŸŸ© RoPEã®é †ä¼æ’­å®Œäº† query.shape=torch.Size([1, 16, 128]), key.shape=torch.Size([1, 8, 128])
ğŸŸ© Attentionå±¤ã®é †ä¼æ’­é–‹å§‹ q.shape=torch.Size([1, 16, 128]), k.shape=torch.Size([1, 8, 128]), v.shape=torch.Size([1, 8, 128])
ğŸŸ© ç¾åœ¨ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆå–å¾— _CONTEXT=Context(is_prefill=True, cu_seqlens_q=tensor([0, 1], device='cuda:0', dtype=torch.int32), cu_seqlens_k=tensor([0, 1], device='cuda:0', dtype=torch.int32), max_seqlen_q=1, max_seqlen_k=1, slot_mapping=tensor([0], device='cuda:0', dtype=torch.int32), context_lens=None, block_tables=None)
ğŸŸ¦ ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæƒ…å ±å–å¾— context.is_prefill=True, context.slot_mapping=tensor([0], device='cuda:0', dtype=torch.int32), context.max_seqlen_q=1, context.cu_seqlens_q=tensor([0, 1], device='cuda:0', dtype=torch.int32), context.max_seqlen_k=1, context.cu_seqlens_k=tensor([0, 1], device='cuda:0', dtype=torch.int32), context.context_lens=None, context.block_tables=None
ğŸŸ© KVã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ã‚­ãƒ¼ã¨ãƒãƒªãƒ¥ãƒ¼ã‚’ä¿å­˜é–‹å§‹ key.shape=torch.Size([1, 8, 128]), value.shape=torch.Size([1, 8, 128]), k_cache.shape=torch.Size([875, 256, 8, 128]), v_cache.shape=torch.Size([875, 256, 8, 128]), slot_mapping.shape=torch.Size([1])
ğŸŸ© KVã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ã‚­ãƒ¼ã¨ãƒãƒªãƒ¥ãƒ¼ã‚’ä¿å­˜å®Œäº†
ğŸŸ¦ Prefillãƒ¢ãƒ¼ãƒ‰ã§FlashAttentionã‚’å®Ÿè¡Œ context.max_seqlen_q=1, context.cu_seqlens_q=tensor([0, 1], device='cuda:0', dtype=torch.int32), context.max_seqlen_k=1, context.cu_seqlens_k=tensor([0, 1], device='cuda:0', dtype=torch.int32)
ğŸŸ© Attentionå±¤ã®é †ä¼æ’­å®Œäº† o.shape=torch.Size([1, 16, 128])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­é–‹å§‹ å…¥åŠ›å½¢çŠ¶: torch.Size([1, 2048])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­å®Œäº† å‡ºåŠ›å½¢çŠ¶: torch.Size([1, 1024])
ğŸŸ© Qwen3Attentionã®é †ä¼æ’­å®Œäº† output.shape=torch.Size([1, 1024])
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 1024]), residual.shape if residual is not None else None=torch.Size([1, 1024])
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨å®Œäº†
ğŸŸ© Qwen3MLPã®é †ä¼æ’­ x.shape=torch.Size([1, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­å®Œäº† result.shape=torch.Size([1, 6144])
ğŸŸ© SwiGLUã‚’é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 6144])
ğŸŸ¦ ãƒãƒ£ãƒ³ã‚¯åŒ– x.shape=torch.Size([1, 3072]), y.shape=torch.Size([1, 3072])
ğŸŸ© SwiGLUã‚’é †ä¼æ’­å®Œäº† result.shape=torch.Size([1, 3072])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­é–‹å§‹ å…¥åŠ›å½¢çŠ¶: torch.Size([1, 3072])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­å®Œäº† å‡ºåŠ›å½¢çŠ¶: torch.Size([1, 1024])
ğŸŸ© Qwen3MLPã®é †ä¼æ’­å®Œäº† x.shape=torch.Size([1, 1024])
ğŸŸ© Qwen3DecoderLayerã®é †ä¼æ’­å®Œäº† hidden_states.shape=torch.Size([1, 1024])
ğŸŸ© Qwen3DecoderLayerã®é †ä¼æ’­ positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 1024]) residual is None=False
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 1024]), residual.shape if residual is not None else None=torch.Size([1, 1024])
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨å®Œäº†
ğŸŸ© Qwen3Attentionã®é †ä¼æ’­ positions.shape=torch.Size([1]) hidden_states.shape=torch.Size([1, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­å®Œäº† result.shape=torch.Size([1, 4096])
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 16, 128]), residual.shape if residual is not None else None=None
ğŸŸ© æ­£è¦åŒ–é–‹å§‹ x.shape=torch.Size([1, 16, 128])
ğŸŸ© æ­£è¦åŒ–å®Œäº† x.shape=torch.Size([1, 16, 128])
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 8, 128]), residual.shape if residual is not None else None=None
ğŸŸ© æ­£è¦åŒ–é–‹å§‹ x.shape=torch.Size([1, 8, 128])
ğŸŸ© æ­£è¦åŒ–å®Œäº† x.shape=torch.Size([1, 8, 128])
ğŸŸ© RoPEã®é †ä¼æ’­é–‹å§‹ positions.shape=torch.Size([1]), query.shape=torch.Size([1, 16, 128]), key.shape=torch.Size([1, 8, 128])
ğŸŸ© RoPEã‚’é©ç”¨é–‹å§‹ x.shape=torch.Size([1, 16, 128]), cos.shape=torch.Size([1, 1, 64]), sin.shape=torch.Size([1, 1, 64])
ğŸŸ¦ ãƒãƒ£ãƒ³ã‚¯åŒ– x1.shape=torch.Size([1, 16, 64]), x2.shape=torch.Size([1, 16, 64])
ğŸŸ¦ å›è»¢è¡Œåˆ—é©ç”¨ y1.shape=torch.Size([1, 16, 64]), y2.shape=torch.Size([1, 16, 64])
ğŸŸ© RoPEã‚’é©ç”¨å®Œäº† result.shape=torch.Size([1, 16, 128])
ğŸŸ© RoPEã‚’é©ç”¨é–‹å§‹ x.shape=torch.Size([1, 8, 128]), cos.shape=torch.Size([1, 1, 64]), sin.shape=torch.Size([1, 1, 64])
ğŸŸ¦ ãƒãƒ£ãƒ³ã‚¯åŒ– x1.shape=torch.Size([1, 8, 64]), x2.shape=torch.Size([1, 8, 64])
ğŸŸ¦ å›è»¢è¡Œåˆ—é©ç”¨ y1.shape=torch.Size([1, 8, 64]), y2.shape=torch.Size([1, 8, 64])
ğŸŸ© RoPEã‚’é©ç”¨å®Œäº† result.shape=torch.Size([1, 8, 128])
ğŸŸ© RoPEã®é †ä¼æ’­å®Œäº† query.shape=torch.Size([1, 16, 128]), key.shape=torch.Size([1, 8, 128])
ğŸŸ© Attentionå±¤ã®é †ä¼æ’­é–‹å§‹ q.shape=torch.Size([1, 16, 128]), k.shape=torch.Size([1, 8, 128]), v.shape=torch.Size([1, 8, 128])
ğŸŸ© ç¾åœ¨ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆå–å¾— _CONTEXT=Context(is_prefill=True, cu_seqlens_q=tensor([0, 1], device='cuda:0', dtype=torch.int32), cu_seqlens_k=tensor([0, 1], device='cuda:0', dtype=torch.int32), max_seqlen_q=1, max_seqlen_k=1, slot_mapping=tensor([0], device='cuda:0', dtype=torch.int32), context_lens=None, block_tables=None)
ğŸŸ¦ ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæƒ…å ±å–å¾— context.is_prefill=True, context.slot_mapping=tensor([0], device='cuda:0', dtype=torch.int32), context.max_seqlen_q=1, context.cu_seqlens_q=tensor([0, 1], device='cuda:0', dtype=torch.int32), context.max_seqlen_k=1, context.cu_seqlens_k=tensor([0, 1], device='cuda:0', dtype=torch.int32), context.context_lens=None, context.block_tables=None
ğŸŸ© KVã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ã‚­ãƒ¼ã¨ãƒãƒªãƒ¥ãƒ¼ã‚’ä¿å­˜é–‹å§‹ key.shape=torch.Size([1, 8, 128]), value.shape=torch.Size([1, 8, 128]), k_cache.shape=torch.Size([875, 256, 8, 128]), v_cache.shape=torch.Size([875, 256, 8, 128]), slot_mapping.shape=torch.Size([1])
ğŸŸ© KVã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ã‚­ãƒ¼ã¨ãƒãƒªãƒ¥ãƒ¼ã‚’ä¿å­˜å®Œäº†
ğŸŸ¦ Prefillãƒ¢ãƒ¼ãƒ‰ã§FlashAttentionã‚’å®Ÿè¡Œ context.max_seqlen_q=1, context.cu_seqlens_q=tensor([0, 1], device='cuda:0', dtype=torch.int32), context.max_seqlen_k=1, context.cu_seqlens_k=tensor([0, 1], device='cuda:0', dtype=torch.int32)
ğŸŸ© Attentionå±¤ã®é †ä¼æ’­å®Œäº† o.shape=torch.Size([1, 16, 128])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­é–‹å§‹ å…¥åŠ›å½¢çŠ¶: torch.Size([1, 2048])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­å®Œäº† å‡ºåŠ›å½¢çŠ¶: torch.Size([1, 1024])
ğŸŸ© Qwen3Attentionã®é †ä¼æ’­å®Œäº† output.shape=torch.Size([1, 1024])
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 1024]), residual.shape if residual is not None else None=torch.Size([1, 1024])
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨å®Œäº†
ğŸŸ© Qwen3MLPã®é †ä¼æ’­ x.shape=torch.Size([1, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 1024])
ğŸŸ© å‡ºåŠ›æ¬¡å…ƒã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç·šå½¢å±¤ã®é †ä¼æ’­å®Œäº† result.shape=torch.Size([1, 6144])
ğŸŸ© SwiGLUã‚’é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 6144])
ğŸŸ¦ ãƒãƒ£ãƒ³ã‚¯åŒ– x.shape=torch.Size([1, 3072]), y.shape=torch.Size([1, 3072])
ğŸŸ© SwiGLUã‚’é †ä¼æ’­å®Œäº† result.shape=torch.Size([1, 3072])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­é–‹å§‹ å…¥åŠ›å½¢çŠ¶: torch.Size([1, 3072])
ğŸŸ© RowParallelLinearã®é †ä¼æ’­å®Œäº† å‡ºåŠ›å½¢çŠ¶: torch.Size([1, 1024])
ğŸŸ© Qwen3MLPã®é †ä¼æ’­å®Œäº† x.shape=torch.Size([1, 1024])
ğŸŸ© Qwen3DecoderLayerã®é †ä¼æ’­å®Œäº† hidden_states.shape=torch.Size([1, 1024])
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 1024]), residual.shape if residual is not None else None=torch.Size([1, 1024])
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨
ğŸŸ© æ®‹å·®æ¥ç¶šã‚’é©ç”¨å®Œäº†
ğŸŸ© Qwen3Modelã®é †ä¼æ’­å®Œäº† hidden_states.shape=torch.Size([1, 1024])
ğŸŸ© Qwen3ForCausalLMã®é †ä¼æ’­å®Œäº† output.shape=torch.Size([1, 1024])
ğŸŸ© Qwen3ForCausalLMã®ãƒ­ã‚¸ãƒƒãƒˆè¨ˆç®—é–‹å§‹ hidden_states.shape=torch.Size([1, 1024])
ğŸŸ© å‡ºåŠ›ã®è¨€èªãƒ¢ãƒ‡ãƒ«ãƒ˜ãƒƒãƒ‰ã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 1024])
ğŸŸ© ç¾åœ¨ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆå–å¾— _CONTEXT=Context(is_prefill=True, cu_seqlens_q=tensor([0, 1], device='cuda:0', dtype=torch.int32), cu_seqlens_k=tensor([0, 1], device='cuda:0', dtype=torch.int32), max_seqlen_q=1, max_seqlen_k=1, slot_mapping=tensor([0], device='cuda:0', dtype=torch.int32), context_lens=None, block_tables=None)
ğŸŸ¦ æœ€å¾Œã®ãƒˆãƒ¼ã‚¯ãƒ³ã‚’æŠ½å‡ºï¼ˆPrefillï¼‰ x.shape=torch.Size([1, 1024])
ğŸŸ© å‡ºåŠ›ã®è¨€èªãƒ¢ãƒ‡ãƒ«ãƒ˜ãƒƒãƒ‰ã®é †ä¼æ’­å®Œäº† logits.shape if logits is not None else None=torch.Size([1, 151936])
ğŸŸ© Qwen3ForCausalLMã®ãƒ­ã‚¸ãƒƒãƒˆè¨ˆç®—å®Œäº† logits.shape=torch.Size([1, 151936])
ğŸŸ© ãƒ¢ãƒ‡ãƒ«ã®é †ä¼æ¬ï¼ˆprefillï¼‰ã‚’å®Ÿè¡Œå®Œäº† result.shape=torch.Size([1, 151936])
ğŸŸ© ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°é–‹å§‹ logits.shape=torch.Size([1, 151936]), temperatures.shape=torch.Size([1])
ğŸŸ© ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°å®Œäº† sample_tokens.shape=torch.Size([1])
ğŸŸ© ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒªã‚»ãƒƒãƒˆ
ğŸŸ© æ¨è«–ã®1ã‚µã‚¤ã‚¯ãƒ«ã‚’å®Ÿè¡Œå®Œäº† (len(token_ids) if token_ids else None)=1
ğŸŸ© ç”Ÿæˆãƒˆãƒ¼ã‚¯ãƒ³ã®å¾Œå‡¦ç†ã‚’é–‹å§‹ len(seqs)=1 len(token_ids)=1
ğŸŸ© ãƒˆãƒ¼ã‚¯ãƒ³ã‚’è¿½åŠ  token_id=105962
ğŸŸ© ãƒˆãƒ¼ã‚¯ãƒ³ã‚’è¿½åŠ å®Œäº† token_id=105962 self.num_tokens=2
ğŸŸ© ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã®ãƒ–ãƒ­ãƒƒã‚¯ã‚’è§£æ”¾é–‹å§‹ 4
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’è§£æ”¾é–‹å§‹ block_id=0
ğŸŸ© ãƒ–ãƒ­ãƒƒã‚¯ã‚’è§£æ”¾å®Œäº† block_id=0
ğŸŸ© ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã®ãƒ–ãƒ­ãƒƒã‚¯ã‚’è§£æ”¾å®Œäº† 4
ğŸŸ© ç”Ÿæˆãƒˆãƒ¼ã‚¯ãƒ³ã®å¾Œå‡¦ç†å®Œäº† len(seqs)=1
ğŸŸ© æ¨è«–ã‚¨ãƒ³ã‚¸ãƒ³ã®1ã‚µã‚¤ã‚¯ãƒ«ã‚’å®Ÿè¡Œå®Œäº† outputs=[(4, [105962])] num_tokens=2
ğŸŸ© ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ã®å®Œäº†çŠ¶æ…‹ã‚’ç¢ºèª result=True
ğŸŸ© ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ã«å‡¦ç†ã™ã¹ãã‚¿ã‚¹ã‚¯ãŒæ®‹ã£ã¦ã„ã‚‹ã‹ç¢ºèª res=True
ğŸŸ© ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆã‚’å®Œäº† len(outputs)=1
